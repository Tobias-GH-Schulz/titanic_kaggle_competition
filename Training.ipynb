{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T11:26:51.561995Z",
     "start_time": "2021-01-30T11:26:49.954277Z"
    },
    "cell_id": "00000-f3de8d9a-9007-4fe1-9480-8e83b4b1ae26",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1474,
    "execution_start": 1612003435589,
    "source_hash": "d7ba2abc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('Train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T07:17:20.789044Z",
     "start_time": "2021-01-30T07:17:20.778298Z"
    },
    "cell_id": "00002-b30a25cc-a956-4747-9558-6dff7cf902a1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1611989958749,
    "source_hash": "a5eb888b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T07:22:08.431469Z",
     "start_time": "2021-01-30T07:22:08.426243Z"
    },
    "cell_id": "00003-a394d79d-9ee0-4694-a65f-febac2dc5504",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1611995101277,
    "source_hash": "ccf0ff73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models1 = {\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "    'ExtraTreesClassifier': {'n_estimators': [16, 32] },\n",
    "    'RandomForestClassifier': {'n_estimators': [16, 32] },\n",
    "    'AdaBoostClassifier':  {'n_estimators': [16, 32] },\n",
    "    'GradientBoostingClassifier': {'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-504395c2-f53c-4766-87a0-7a4de61224b1",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models2 = {\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'SVC': SVC(),\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'ExtraTreesClassifier': {'n_estimators': [16, 32] },\n",
    "    'RandomForestClassifier': {'n_estimators': [16, 32] },\n",
    "    'AdaBoostClassifier':  {'n_estimators': [16, 32] },\n",
    "    'GradientBoostingClassifier': {'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
    "    'SVC': [\n",
    "        {'kernel': ['linear'], 'C': [1, 10, 100]},\n",
    "        {'kernel': ['rbf', 'poly', 'sigmoid', 'linear'], 'C':[0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001]},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T11:26:56.337903Z",
     "start_time": "2021-01-30T11:26:56.332072Z"
    },
    "cell_id": "00003-607f4d20-67e1-44f0-81d6-a37a24715feb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1612003468981,
    "source_hash": "9a7fe698",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Survived', \"Unnamed: 0\"], axis=1)\n",
    "y = data.Survived\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T11:27:16.309147Z",
     "start_time": "2021-01-30T11:27:16.300778Z"
    },
    "cell_id": "00004-68e192ee-1a06-408d-a803-83143f5eb94e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1612003788876,
    "source_hash": "580fdd9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T11:27:00.841630Z",
     "start_time": "2021-01-30T11:27:00.636220Z"
    },
    "cell_id": "00008-b5a603c8-2bd5-43b9-9ad8-d31dcc707978",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1612003906645,
    "source_hash": "9d0b3a1d",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ad12b6684352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test_norm)\n",
    "\n",
    "clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-205f02fa-45c8-4e2d-b29c-dcad02dad41f",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "10.0 ** -np.arange(1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T07:22:57.311283Z",
     "start_time": "2021-01-30T07:22:19.745047Z"
    },
    "cell_id": "00001-6006f514-39f9-4243-9f66-02c6ac82ff91",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 97,
    "execution_start": 1611990033981,
    "source_hash": "1cb720cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ExtraTreesClassifier.\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/backend/resource_tracker.py:120: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some folders/sempahores might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "exception calling callback for <Future at 0x7fc790ab3590 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\", line 359, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\", line 792, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\", line 178, in submit\n",
      "    fn, *args, **kwargs)\n",
      "  File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 1102, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "The exit codes of the workers are {EXIT(1)}\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/backend/resource_tracker.py:120: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some folders/sempahores might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {EXIT(1)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-55c3ef2e8eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhelper1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimatorSelectionHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhelper1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5b3de598f71b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cv, n_jobs, verbose, scoring, refit)\u001b[0m\n\u001b[1;32m     18\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                               return_train_score=True)\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_searches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[1;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/_base.py\u001b[0m in \u001b[0;36m_invoke_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exception calling callback for %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \"\"\"\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             return super(_ReusablePoolExecutor, self).submit(\n\u001b[0;32m--> 178\u001b[0;31m                 fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 raise ShutdownExecutorError(\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {EXIT(1)}"
     ]
    }
   ],
   "source": [
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(X_train_norm, y_train, scoring='f1', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T07:23:56.505407Z",
     "start_time": "2021-01-30T07:23:56.452164Z"
    },
    "cell_id": "00002-212c8998-2040-4b46-9f57-35e36a76bd59",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(helper1.score_summary(sort_by='max_score').head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T07:17:58.523966Z",
     "start_time": "2021-01-30T07:17:58.521477Z"
    },
    "cell_id": "00008-05e6b383-ba5a-4286-b054-61003bb52ab2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "440908b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'LogReg': LogisticRegression()\n",
    "'LinReg': LinearRegression()\n",
    "'KNeighborsClassifier': KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T11:19:18.830143Z",
     "start_time": "2021-01-30T11:16:35.550352Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Iteration 1, loss = 0.77481206\n",
      "Iteration 2, loss = 0.71851731\n",
      "Iteration 3, loss = 0.66705555\n",
      "Iteration 4, loss = 0.62726439\n",
      "Iteration 5, loss = 0.59526732\n",
      "Iteration 6, loss = 0.56771339\n",
      "Iteration 7, loss = 0.54434934\n",
      "Iteration 8, loss = 0.52595798\n",
      "Iteration 9, loss = 0.50867696\n",
      "Iteration 10, loss = 0.49471355\n",
      "Iteration 11, loss = 0.48181174\n",
      "Iteration 12, loss = 0.47126835\n",
      "Iteration 13, loss = 0.46111227\n",
      "Iteration 14, loss = 0.45320070\n",
      "Iteration 15, loss = 0.44627017\n",
      "Iteration 16, loss = 0.44053605\n",
      "Iteration 17, loss = 0.43560650\n",
      "Iteration 18, loss = 0.43229979\n",
      "Iteration 19, loss = 0.42867448\n",
      "Iteration 20, loss = 0.42607099\n",
      "Iteration 21, loss = 0.42403569\n",
      "Iteration 22, loss = 0.42198758\n",
      "Iteration 23, loss = 0.42090920\n",
      "Iteration 24, loss = 0.41969361\n",
      "Iteration 25, loss = 0.41905116\n",
      "Iteration 26, loss = 0.41846761\n",
      "Iteration 27, loss = 0.41787796\n",
      "Iteration 28, loss = 0.41749237\n",
      "Iteration 29, loss = 0.41720145\n",
      "Iteration 30, loss = 0.41696517\n",
      "Iteration 31, loss = 0.41671221\n",
      "Iteration 32, loss = 0.41676560\n",
      "Iteration 33, loss = 0.41657523\n",
      "Iteration 34, loss = 0.41639425\n",
      "Iteration 35, loss = 0.41636783\n",
      "Iteration 36, loss = 0.41632436\n",
      "Iteration 37, loss = 0.41623725\n",
      "Iteration 38, loss = 0.41618080\n",
      "Iteration 39, loss = 0.41611020\n",
      "Iteration 40, loss = 0.41615023\n",
      "Iteration 41, loss = 0.41614031\n",
      "Iteration 42, loss = 0.41615545\n",
      "Iteration 43, loss = 0.41620455\n",
      "Iteration 44, loss = 0.41616565\n",
      "Iteration 45, loss = 0.41607521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78431527\n",
      "Iteration 2, loss = 0.72956230\n",
      "Iteration 3, loss = 0.68030858\n",
      "Iteration 4, loss = 0.64281945\n",
      "Iteration 5, loss = 0.61244094\n",
      "Iteration 6, loss = 0.58599775\n",
      "Iteration 7, loss = 0.56310510\n",
      "Iteration 8, loss = 0.54577778\n",
      "Iteration 9, loss = 0.52907132\n",
      "Iteration 10, loss = 0.51565796\n",
      "Iteration 11, loss = 0.50315273\n",
      "Iteration 12, loss = 0.49290568\n",
      "Iteration 13, loss = 0.48348398\n",
      "Iteration 14, loss = 0.47605400\n",
      "Iteration 15, loss = 0.46934860\n",
      "Iteration 16, loss = 0.46407566\n",
      "Iteration 17, loss = 0.45953364\n",
      "Iteration 18, loss = 0.45624455\n",
      "Iteration 19, loss = 0.45287313\n",
      "Iteration 20, loss = 0.45035610\n",
      "Iteration 21, loss = 0.44829266\n",
      "Iteration 22, loss = 0.44652651\n",
      "Iteration 23, loss = 0.44558918\n",
      "Iteration 24, loss = 0.44442637\n",
      "Iteration 25, loss = 0.44382005\n",
      "Iteration 26, loss = 0.44307027\n",
      "Iteration 27, loss = 0.44273500\n",
      "Iteration 28, loss = 0.44233828\n",
      "Iteration 29, loss = 0.44187592\n",
      "Iteration 30, loss = 0.44182862\n",
      "Iteration 31, loss = 0.44144710\n",
      "Iteration 32, loss = 0.44149802\n",
      "Iteration 33, loss = 0.44128938\n",
      "Iteration 34, loss = 0.44107125\n",
      "Iteration 35, loss = 0.44097974\n",
      "Iteration 36, loss = 0.44094991\n",
      "Iteration 37, loss = 0.44083761\n",
      "Iteration 38, loss = 0.44080540\n",
      "Iteration 39, loss = 0.44072113\n",
      "Iteration 40, loss = 0.44080796\n",
      "Iteration 41, loss = 0.44067456\n",
      "Iteration 42, loss = 0.44071265\n",
      "Iteration 43, loss = 0.44069888\n",
      "Iteration 44, loss = 0.44070283\n",
      "Iteration 45, loss = 0.44058475\n",
      "Iteration 46, loss = 0.44060896\n",
      "Iteration 47, loss = 0.44056624\n",
      "Iteration 48, loss = 0.44059169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78408174\n",
      "Iteration 2, loss = 0.72922726\n",
      "Iteration 3, loss = 0.68446995\n",
      "Iteration 4, loss = 0.64709301\n",
      "Iteration 5, loss = 0.61601836\n",
      "Iteration 6, loss = 0.58869884\n",
      "Iteration 7, loss = 0.56661918\n",
      "Iteration 8, loss = 0.54627284\n",
      "Iteration 9, loss = 0.52804351\n",
      "Iteration 10, loss = 0.51229880\n",
      "Iteration 11, loss = 0.49880940\n",
      "Iteration 12, loss = 0.48687837\n",
      "Iteration 13, loss = 0.47679513\n",
      "Iteration 14, loss = 0.46803890\n",
      "Iteration 15, loss = 0.46090476\n",
      "Iteration 16, loss = 0.45510954\n",
      "Iteration 17, loss = 0.45005228\n",
      "Iteration 18, loss = 0.44645934\n",
      "Iteration 19, loss = 0.44291251\n",
      "Iteration 20, loss = 0.43951559\n",
      "Iteration 21, loss = 0.43750709\n",
      "Iteration 22, loss = 0.43557206\n",
      "Iteration 23, loss = 0.43388306\n",
      "Iteration 24, loss = 0.43282565\n",
      "Iteration 25, loss = 0.43189222\n",
      "Iteration 26, loss = 0.43105990\n",
      "Iteration 27, loss = 0.43025289\n",
      "Iteration 28, loss = 0.42992816\n",
      "Iteration 29, loss = 0.42928541\n",
      "Iteration 30, loss = 0.42900946\n",
      "Iteration 31, loss = 0.42880965\n",
      "Iteration 32, loss = 0.42846156\n",
      "Iteration 33, loss = 0.42858821\n",
      "Iteration 34, loss = 0.42807050\n",
      "Iteration 35, loss = 0.42810583\n",
      "Iteration 36, loss = 0.42780084\n",
      "Iteration 37, loss = 0.42782842\n",
      "Iteration 38, loss = 0.42769551\n",
      "Iteration 39, loss = 0.42747628\n",
      "Iteration 40, loss = 0.42756225\n",
      "Iteration 41, loss = 0.42739130\n",
      "Iteration 42, loss = 0.42745138\n",
      "Iteration 43, loss = 0.42719704\n",
      "Iteration 44, loss = 0.42742205\n",
      "Iteration 45, loss = 0.42719478\n",
      "Iteration 46, loss = 0.42708985\n",
      "Iteration 47, loss = 0.42701707\n",
      "Iteration 48, loss = 0.42714497\n",
      "Iteration 49, loss = 0.42700607\n",
      "Iteration 50, loss = 0.42700913\n",
      "Iteration 51, loss = 0.42702209\n",
      "Iteration 52, loss = 0.42708219\n",
      "Iteration 53, loss = 0.42693051\n",
      "Iteration 54, loss = 0.42684972\n",
      "Iteration 55, loss = 0.42685173\n",
      "Iteration 56, loss = 0.42695848\n",
      "Iteration 57, loss = 0.42700757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78717095\n",
      "Iteration 2, loss = 0.73263522\n",
      "Iteration 3, loss = 0.68786644\n",
      "Iteration 4, loss = 0.64886274\n",
      "Iteration 5, loss = 0.61589337\n",
      "Iteration 6, loss = 0.58830387\n",
      "Iteration 7, loss = 0.56537924\n",
      "Iteration 8, loss = 0.54491909\n",
      "Iteration 9, loss = 0.52663969\n",
      "Iteration 10, loss = 0.51072168\n",
      "Iteration 11, loss = 0.49744736\n",
      "Iteration 12, loss = 0.48544885\n",
      "Iteration 13, loss = 0.47517042\n",
      "Iteration 14, loss = 0.46617899\n",
      "Iteration 15, loss = 0.45902898\n",
      "Iteration 16, loss = 0.45267552\n",
      "Iteration 17, loss = 0.44758297\n",
      "Iteration 18, loss = 0.44343302\n",
      "Iteration 19, loss = 0.43935565\n",
      "Iteration 20, loss = 0.43630677\n",
      "Iteration 21, loss = 0.43367116\n",
      "Iteration 22, loss = 0.43152666\n",
      "Iteration 23, loss = 0.42959933\n",
      "Iteration 24, loss = 0.42819875\n",
      "Iteration 25, loss = 0.42723447\n",
      "Iteration 26, loss = 0.42618052\n",
      "Iteration 27, loss = 0.42541404\n",
      "Iteration 28, loss = 0.42494949\n",
      "Iteration 29, loss = 0.42437952\n",
      "Iteration 30, loss = 0.42389073\n",
      "Iteration 31, loss = 0.42374272\n",
      "Iteration 32, loss = 0.42351531\n",
      "Iteration 33, loss = 0.42356705\n",
      "Iteration 34, loss = 0.42307355\n",
      "Iteration 35, loss = 0.42286774\n",
      "Iteration 36, loss = 0.42292877\n",
      "Iteration 37, loss = 0.42279451\n",
      "Iteration 38, loss = 0.42261168\n",
      "Iteration 39, loss = 0.42258069\n",
      "Iteration 40, loss = 0.42268746\n",
      "Iteration 41, loss = 0.42251296\n",
      "Iteration 42, loss = 0.42253822\n",
      "Iteration 43, loss = 0.42242759\n",
      "Iteration 44, loss = 0.42249856\n",
      "Iteration 45, loss = 0.42235105\n",
      "Iteration 46, loss = 0.42233536\n",
      "Iteration 47, loss = 0.42226628\n",
      "Iteration 48, loss = 0.42232567\n",
      "Iteration 49, loss = 0.42231620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77853080\n",
      "Iteration 2, loss = 0.72544398\n",
      "Iteration 3, loss = 0.68030056\n",
      "Iteration 4, loss = 0.64357070\n",
      "Iteration 5, loss = 0.61289769\n",
      "Iteration 6, loss = 0.58735787\n",
      "Iteration 7, loss = 0.56682357\n",
      "Iteration 8, loss = 0.54795223\n",
      "Iteration 9, loss = 0.53208151\n",
      "Iteration 10, loss = 0.51762101\n",
      "Iteration 11, loss = 0.50572400\n",
      "Iteration 12, loss = 0.49572608\n",
      "Iteration 13, loss = 0.48698626\n",
      "Iteration 14, loss = 0.47916755\n",
      "Iteration 15, loss = 0.47364365\n",
      "Iteration 16, loss = 0.46848114\n",
      "Iteration 17, loss = 0.46509217\n",
      "Iteration 18, loss = 0.46155701\n",
      "Iteration 19, loss = 0.45861290\n",
      "Iteration 20, loss = 0.45679619\n",
      "Iteration 21, loss = 0.45528738\n",
      "Iteration 22, loss = 0.45361713\n",
      "Iteration 23, loss = 0.45264005\n",
      "Iteration 24, loss = 0.45183284\n",
      "Iteration 25, loss = 0.45122561\n",
      "Iteration 26, loss = 0.45064282\n",
      "Iteration 27, loss = 0.45028877\n",
      "Iteration 28, loss = 0.45020626\n",
      "Iteration 29, loss = 0.44983649\n",
      "Iteration 30, loss = 0.44958779\n",
      "Iteration 31, loss = 0.44951734\n",
      "Iteration 32, loss = 0.44941108\n",
      "Iteration 33, loss = 0.44943269\n",
      "Iteration 34, loss = 0.44923332\n",
      "Iteration 35, loss = 0.44903853\n",
      "Iteration 36, loss = 0.44912318\n",
      "Iteration 37, loss = 0.44905992\n",
      "Iteration 38, loss = 0.44892896\n",
      "Iteration 39, loss = 0.44886965\n",
      "Iteration 40, loss = 0.44896902\n",
      "Iteration 41, loss = 0.44885448\n",
      "Iteration 42, loss = 0.44885954\n",
      "Iteration 43, loss = 0.44877566\n",
      "Iteration 44, loss = 0.44877975\n",
      "Iteration 45, loss = 0.44874708\n",
      "Iteration 46, loss = 0.44873216\n",
      "Iteration 47, loss = 0.44864621\n",
      "Iteration 48, loss = 0.44868206\n",
      "Iteration 49, loss = 0.44872906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76912207\n",
      "Iteration 2, loss = 0.71291886\n",
      "Iteration 3, loss = 0.66154190\n",
      "Iteration 4, loss = 0.62181042\n",
      "Iteration 5, loss = 0.58985593\n",
      "Iteration 6, loss = 0.56232364\n",
      "Iteration 7, loss = 0.53896777\n",
      "Iteration 8, loss = 0.52057120\n",
      "Iteration 9, loss = 0.50327908\n",
      "Iteration 10, loss = 0.48929961\n",
      "Iteration 11, loss = 0.47637407\n",
      "Iteration 12, loss = 0.46581600\n",
      "Iteration 13, loss = 0.45564525\n",
      "Iteration 14, loss = 0.44772039\n",
      "Iteration 15, loss = 0.44077896\n",
      "Iteration 16, loss = 0.43503680\n",
      "Iteration 17, loss = 0.43009926\n",
      "Iteration 18, loss = 0.42678690\n",
      "Iteration 19, loss = 0.42315510\n",
      "Iteration 20, loss = 0.42054529\n",
      "Iteration 21, loss = 0.41850510\n",
      "Iteration 22, loss = 0.41645358\n",
      "Iteration 23, loss = 0.41537147\n",
      "Iteration 24, loss = 0.41415423\n",
      "Iteration 25, loss = 0.41351051\n",
      "Iteration 26, loss = 0.41292879\n",
      "Iteration 27, loss = 0.41233879\n",
      "Iteration 28, loss = 0.41195561\n",
      "Iteration 29, loss = 0.41166797\n",
      "Iteration 30, loss = 0.41143493\n",
      "Iteration 31, loss = 0.41118506\n",
      "Iteration 32, loss = 0.41124426\n",
      "Iteration 33, loss = 0.41105675\n",
      "Iteration 34, loss = 0.41088124\n",
      "Iteration 35, loss = 0.41086020\n",
      "Iteration 36, loss = 0.41082244\n",
      "Iteration 37, loss = 0.41074046\n",
      "Iteration 38, loss = 0.41068995\n",
      "Iteration 39, loss = 0.41062465\n",
      "Iteration 40, loss = 0.41067112\n",
      "Iteration 41, loss = 0.41066753\n",
      "Iteration 42, loss = 0.41068844\n",
      "Iteration 43, loss = 0.41074454\n",
      "Iteration 44, loss = 0.41071172\n",
      "Iteration 45, loss = 0.41062731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77895945\n",
      "Iteration 2, loss = 0.72419717\n",
      "Iteration 3, loss = 0.67492814\n",
      "Iteration 4, loss = 0.63742997\n",
      "Iteration 5, loss = 0.60704161\n",
      "Iteration 6, loss = 0.58059673\n",
      "Iteration 7, loss = 0.55770053\n",
      "Iteration 8, loss = 0.54037302\n",
      "Iteration 9, loss = 0.52366313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.51024638\n",
      "Iteration 11, loss = 0.49773632\n",
      "Iteration 12, loss = 0.48748271\n",
      "Iteration 13, loss = 0.47805327\n",
      "Iteration 14, loss = 0.47061706\n",
      "Iteration 15, loss = 0.46390349\n",
      "Iteration 16, loss = 0.45862325\n",
      "Iteration 17, loss = 0.45407358\n",
      "Iteration 18, loss = 0.45077836\n",
      "Iteration 19, loss = 0.44740142\n",
      "Iteration 20, loss = 0.44487804\n",
      "Iteration 21, loss = 0.44281006\n",
      "Iteration 22, loss = 0.44103996\n",
      "Iteration 23, loss = 0.44010134\n",
      "Iteration 24, loss = 0.43893616\n",
      "Iteration 25, loss = 0.43832996\n",
      "Iteration 26, loss = 0.43758014\n",
      "Iteration 27, loss = 0.43724675\n",
      "Iteration 28, loss = 0.43685227\n",
      "Iteration 29, loss = 0.43639130\n",
      "Iteration 30, loss = 0.43634936\n",
      "Iteration 31, loss = 0.43596923\n",
      "Iteration 32, loss = 0.43602614\n",
      "Iteration 33, loss = 0.43582167\n",
      "Iteration 34, loss = 0.43560795\n",
      "Iteration 35, loss = 0.43552155\n",
      "Iteration 36, loss = 0.43549766\n",
      "Iteration 37, loss = 0.43539098\n",
      "Iteration 38, loss = 0.43536448\n",
      "Iteration 39, loss = 0.43528596\n",
      "Iteration 40, loss = 0.43538001\n",
      "Iteration 41, loss = 0.43525176\n",
      "Iteration 42, loss = 0.43529673\n",
      "Iteration 43, loss = 0.43529016\n",
      "Iteration 44, loss = 0.43530075\n",
      "Iteration 45, loss = 0.43518846\n",
      "Iteration 46, loss = 0.43522002\n",
      "Iteration 47, loss = 0.43518348\n",
      "Iteration 48, loss = 0.43521595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77871646\n",
      "Iteration 2, loss = 0.72386373\n",
      "Iteration 3, loss = 0.67910199\n",
      "Iteration 4, loss = 0.64172258\n",
      "Iteration 5, loss = 0.61064493\n",
      "Iteration 6, loss = 0.58332174\n",
      "Iteration 7, loss = 0.56124111\n",
      "Iteration 8, loss = 0.54089009\n",
      "Iteration 9, loss = 0.52265478\n",
      "Iteration 10, loss = 0.50690347\n",
      "Iteration 11, loss = 0.49340407\n",
      "Iteration 12, loss = 0.48146126\n",
      "Iteration 13, loss = 0.47136660\n",
      "Iteration 14, loss = 0.46259850\n",
      "Iteration 15, loss = 0.45545247\n",
      "Iteration 16, loss = 0.44964625\n",
      "Iteration 17, loss = 0.44457927\n",
      "Iteration 18, loss = 0.44097660\n",
      "Iteration 19, loss = 0.43742002\n",
      "Iteration 20, loss = 0.43401320\n",
      "Iteration 21, loss = 0.43199786\n",
      "Iteration 22, loss = 0.43005689\n",
      "Iteration 23, loss = 0.42836188\n",
      "Iteration 24, loss = 0.42730069\n",
      "Iteration 25, loss = 0.42636367\n",
      "Iteration 26, loss = 0.42552977\n",
      "Iteration 27, loss = 0.42472049\n",
      "Iteration 28, loss = 0.42439639\n",
      "Iteration 29, loss = 0.42375230\n",
      "Iteration 30, loss = 0.42347837\n",
      "Iteration 31, loss = 0.42328012\n",
      "Iteration 32, loss = 0.42293380\n",
      "Iteration 33, loss = 0.42306500\n",
      "Iteration 34, loss = 0.42254779\n",
      "Iteration 35, loss = 0.42258857\n",
      "Iteration 36, loss = 0.42228638\n",
      "Iteration 37, loss = 0.42231826\n",
      "Iteration 38, loss = 0.42218981\n",
      "Iteration 39, loss = 0.42197413\n",
      "Iteration 40, loss = 0.42206569\n",
      "Iteration 41, loss = 0.42189906\n",
      "Iteration 42, loss = 0.42196548\n",
      "Iteration 43, loss = 0.42171509\n",
      "Iteration 44, loss = 0.42194771\n",
      "Iteration 45, loss = 0.42172500\n",
      "Iteration 46, loss = 0.42162505\n",
      "Iteration 47, loss = 0.42155762\n",
      "Iteration 48, loss = 0.42169330\n",
      "Iteration 49, loss = 0.42155879\n",
      "Iteration 50, loss = 0.42156820\n",
      "Iteration 51, loss = 0.42158853\n",
      "Iteration 52, loss = 0.42165502\n",
      "Iteration 53, loss = 0.42150825\n",
      "Iteration 54, loss = 0.42143350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78177752\n",
      "Iteration 2, loss = 0.72724221\n",
      "Iteration 3, loss = 0.68247318\n",
      "Iteration 4, loss = 0.64346930\n",
      "Iteration 5, loss = 0.61049707\n",
      "Iteration 6, loss = 0.58290550\n",
      "Iteration 7, loss = 0.55997948\n",
      "Iteration 8, loss = 0.53951593\n",
      "Iteration 9, loss = 0.52123286\n",
      "Iteration 10, loss = 0.50531175\n",
      "Iteration 11, loss = 0.49203208\n",
      "Iteration 12, loss = 0.48002647\n",
      "Iteration 13, loss = 0.46974083\n",
      "Iteration 14, loss = 0.46074113\n",
      "Iteration 15, loss = 0.45358246\n",
      "Iteration 16, loss = 0.44722160\n",
      "Iteration 17, loss = 0.44212288\n",
      "Iteration 18, loss = 0.43796583\n",
      "Iteration 19, loss = 0.43388033\n",
      "Iteration 20, loss = 0.43082596\n",
      "Iteration 21, loss = 0.42818384\n",
      "Iteration 22, loss = 0.42603532\n",
      "Iteration 23, loss = 0.42410304\n",
      "Iteration 24, loss = 0.42269933\n",
      "Iteration 25, loss = 0.42173334\n",
      "Iteration 26, loss = 0.42067814\n",
      "Iteration 27, loss = 0.41991117\n",
      "Iteration 28, loss = 0.41944790\n",
      "Iteration 29, loss = 0.41887821\n",
      "Iteration 30, loss = 0.41839123\n",
      "Iteration 31, loss = 0.41824733\n",
      "Iteration 32, loss = 0.41802285\n",
      "Iteration 33, loss = 0.41807980\n",
      "Iteration 34, loss = 0.41758802\n",
      "Iteration 35, loss = 0.41738659\n",
      "Iteration 36, loss = 0.41745414\n",
      "Iteration 37, loss = 0.41732482\n",
      "Iteration 38, loss = 0.41714680\n",
      "Iteration 39, loss = 0.41712160\n",
      "Iteration 40, loss = 0.41723512\n",
      "Iteration 41, loss = 0.41706589\n",
      "Iteration 42, loss = 0.41709764\n",
      "Iteration 43, loss = 0.41699277\n",
      "Iteration 44, loss = 0.41707154\n",
      "Iteration 45, loss = 0.41692978\n",
      "Iteration 46, loss = 0.41692003\n",
      "Iteration 47, loss = 0.41685722\n",
      "Iteration 48, loss = 0.41692553\n",
      "Iteration 49, loss = 0.41692234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77316474\n",
      "Iteration 2, loss = 0.72008036\n",
      "Iteration 3, loss = 0.67493590\n",
      "Iteration 4, loss = 0.63820327\n",
      "Iteration 5, loss = 0.60752610\n",
      "Iteration 6, loss = 0.58198303\n",
      "Iteration 7, loss = 0.56144638\n",
      "Iteration 8, loss = 0.54257132\n",
      "Iteration 9, loss = 0.52669676\n",
      "Iteration 10, loss = 0.51223255\n",
      "Iteration 11, loss = 0.50032897\n",
      "Iteration 12, loss = 0.49032244\n",
      "Iteration 13, loss = 0.48157366\n",
      "Iteration 14, loss = 0.47374463\n",
      "Iteration 15, loss = 0.46821008\n",
      "Iteration 16, loss = 0.46303878\n",
      "Iteration 17, loss = 0.45964359\n",
      "Iteration 18, loss = 0.45610033\n",
      "Iteration 19, loss = 0.45314820\n",
      "Iteration 20, loss = 0.45132719\n",
      "Iteration 21, loss = 0.44981408\n",
      "Iteration 22, loss = 0.44813925\n",
      "Iteration 23, loss = 0.44715925\n",
      "Iteration 24, loss = 0.44635059\n",
      "Iteration 25, loss = 0.44574349\n",
      "Iteration 26, loss = 0.44516091\n",
      "Iteration 27, loss = 0.44480852\n",
      "Iteration 28, loss = 0.44472938\n",
      "Iteration 29, loss = 0.44436141\n",
      "Iteration 30, loss = 0.44411593\n",
      "Iteration 31, loss = 0.44405054\n",
      "Iteration 32, loss = 0.44394818\n",
      "Iteration 33, loss = 0.44397496\n",
      "Iteration 34, loss = 0.44377936\n",
      "Iteration 35, loss = 0.44358909\n",
      "Iteration 36, loss = 0.44367987\n",
      "Iteration 37, loss = 0.44362255\n",
      "Iteration 38, loss = 0.44349649\n",
      "Iteration 39, loss = 0.44344296\n",
      "Iteration 40, loss = 0.44354906\n",
      "Iteration 41, loss = 0.44344001\n",
      "Iteration 42, loss = 0.44345145\n",
      "Iteration 43, loss = 0.44337330\n",
      "Iteration 44, loss = 0.44338396\n",
      "Iteration 45, loss = 0.44335743\n",
      "Iteration 46, loss = 0.44334843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76858129\n",
      "Iteration 2, loss = 0.71237891\n",
      "Iteration 3, loss = 0.66100215\n",
      "Iteration 4, loss = 0.62127072\n",
      "Iteration 5, loss = 0.58931603\n",
      "Iteration 6, loss = 0.56178348\n",
      "Iteration 7, loss = 0.53842701\n",
      "Iteration 8, loss = 0.52002982\n",
      "Iteration 9, loss = 0.50273686\n",
      "Iteration 10, loss = 0.48875633\n",
      "Iteration 11, loss = 0.47582956\n",
      "Iteration 12, loss = 0.46527008\n",
      "Iteration 13, loss = 0.45509777\n",
      "Iteration 14, loss = 0.44717130\n",
      "Iteration 15, loss = 0.44022817\n",
      "Iteration 16, loss = 0.43448442\n",
      "Iteration 17, loss = 0.42954521\n",
      "Iteration 18, loss = 0.42623146\n",
      "Iteration 19, loss = 0.42259829\n",
      "Iteration 20, loss = 0.41998709\n",
      "Iteration 21, loss = 0.41794576\n",
      "Iteration 22, loss = 0.41589309\n",
      "Iteration 23, loss = 0.41481014\n",
      "Iteration 24, loss = 0.41359197\n",
      "Iteration 25, loss = 0.41294757\n",
      "Iteration 26, loss = 0.41236527\n",
      "Iteration 27, loss = 0.41177465\n",
      "Iteration 28, loss = 0.41139104\n",
      "Iteration 29, loss = 0.41110305\n",
      "Iteration 30, loss = 0.41086971\n",
      "Iteration 31, loss = 0.41061956\n",
      "Iteration 32, loss = 0.41067870\n",
      "Iteration 33, loss = 0.41049090\n",
      "Iteration 34, loss = 0.41031534\n",
      "Iteration 35, loss = 0.41029423\n",
      "Iteration 36, loss = 0.41025645\n",
      "Iteration 37, loss = 0.41017441\n",
      "Iteration 38, loss = 0.41012390\n",
      "Iteration 39, loss = 0.41005856\n",
      "Iteration 40, loss = 0.41010507\n",
      "Iteration 41, loss = 0.41010153\n",
      "Iteration 42, loss = 0.41012243\n",
      "Iteration 43, loss = 0.41017863\n",
      "Iteration 44, loss = 0.41014584\n",
      "Iteration 45, loss = 0.41006146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77842207\n",
      "Iteration 2, loss = 0.72365989\n",
      "Iteration 3, loss = 0.67439026\n",
      "Iteration 4, loss = 0.63689134\n",
      "Iteration 5, loss = 0.60650210\n",
      "Iteration 6, loss = 0.58005654\n",
      "Iteration 7, loss = 0.55715953\n",
      "Iteration 8, loss = 0.53983144\n",
      "Iteration 9, loss = 0.52312085\n",
      "Iteration 10, loss = 0.50970330\n",
      "Iteration 11, loss = 0.49719242\n",
      "Iteration 12, loss = 0.48693775\n",
      "Iteration 13, loss = 0.47750703\n",
      "Iteration 14, loss = 0.47006969\n",
      "Iteration 15, loss = 0.46335475\n",
      "Iteration 16, loss = 0.45807318\n",
      "Iteration 17, loss = 0.45352215\n",
      "Iteration 18, loss = 0.45022572\n",
      "Iteration 19, loss = 0.44684763\n",
      "Iteration 20, loss = 0.44432299\n",
      "Iteration 21, loss = 0.44225395\n",
      "Iteration 22, loss = 0.44048284\n",
      "Iteration 23, loss = 0.43954351\n",
      "Iteration 24, loss = 0.43837747\n",
      "Iteration 25, loss = 0.43777067\n",
      "Iteration 26, loss = 0.43702022\n",
      "Iteration 27, loss = 0.43668641\n",
      "Iteration 28, loss = 0.43629153\n",
      "Iteration 29, loss = 0.43583011\n",
      "Iteration 30, loss = 0.43578808\n",
      "Iteration 31, loss = 0.43540750\n",
      "Iteration 32, loss = 0.43546439\n",
      "Iteration 33, loss = 0.43525973\n",
      "Iteration 34, loss = 0.43504586\n",
      "Iteration 35, loss = 0.43495936\n",
      "Iteration 36, loss = 0.43493547\n",
      "Iteration 37, loss = 0.43482875\n",
      "Iteration 38, loss = 0.43480221\n",
      "Iteration 39, loss = 0.43472366\n",
      "Iteration 40, loss = 0.43481783\n",
      "Iteration 41, loss = 0.43468948\n",
      "Iteration 42, loss = 0.43473452\n",
      "Iteration 43, loss = 0.43472806\n",
      "Iteration 44, loss = 0.43473871\n",
      "Iteration 45, loss = 0.43462637\n",
      "Iteration 46, loss = 0.43465806\n",
      "Iteration 47, loss = 0.43462152\n",
      "Iteration 48, loss = 0.43465409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77817994\n",
      "Iteration 2, loss = 0.72332743\n",
      "Iteration 3, loss = 0.67856532\n",
      "Iteration 4, loss = 0.64118556\n",
      "Iteration 5, loss = 0.61010744\n",
      "Iteration 6, loss = 0.58278364\n",
      "Iteration 7, loss = 0.56070256\n",
      "Iteration 8, loss = 0.54035078\n",
      "Iteration 9, loss = 0.52211452\n",
      "Iteration 10, loss = 0.50636216\n",
      "Iteration 11, loss = 0.49286137\n",
      "Iteration 12, loss = 0.48091695\n",
      "Iteration 13, loss = 0.47082068\n",
      "Iteration 14, loss = 0.46205086\n",
      "Iteration 15, loss = 0.45490312\n",
      "Iteration 16, loss = 0.44909525\n",
      "Iteration 17, loss = 0.44402674\n",
      "Iteration 18, loss = 0.44042255\n",
      "Iteration 19, loss = 0.43686442\n",
      "Iteration 20, loss = 0.43345606\n",
      "Iteration 21, loss = 0.43143945\n",
      "Iteration 22, loss = 0.42949730\n",
      "Iteration 23, loss = 0.42780110\n",
      "Iteration 24, loss = 0.42673893\n",
      "Iteration 25, loss = 0.42580094\n",
      "Iteration 26, loss = 0.42496629\n",
      "Iteration 27, loss = 0.42415620\n",
      "Iteration 28, loss = 0.42383155\n",
      "Iteration 29, loss = 0.42318674\n",
      "Iteration 30, loss = 0.42291242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.42271373\n",
      "Iteration 32, loss = 0.42236700\n",
      "Iteration 33, loss = 0.42249805\n",
      "Iteration 34, loss = 0.42198032\n",
      "Iteration 35, loss = 0.42202106\n",
      "Iteration 36, loss = 0.42171855\n",
      "Iteration 37, loss = 0.42175027\n",
      "Iteration 38, loss = 0.42162168\n",
      "Iteration 39, loss = 0.42140576\n",
      "Iteration 40, loss = 0.42149729\n",
      "Iteration 41, loss = 0.42133050\n",
      "Iteration 42, loss = 0.42139696\n",
      "Iteration 43, loss = 0.42114638\n",
      "Iteration 44, loss = 0.42137917\n",
      "Iteration 45, loss = 0.42115632\n",
      "Iteration 46, loss = 0.42105628\n",
      "Iteration 47, loss = 0.42098879\n",
      "Iteration 48, loss = 0.42112466\n",
      "Iteration 49, loss = 0.42098999\n",
      "Iteration 50, loss = 0.42099944\n",
      "Iteration 51, loss = 0.42101994\n",
      "Iteration 52, loss = 0.42108647\n",
      "Iteration 53, loss = 0.42093959\n",
      "Iteration 54, loss = 0.42086487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78124098\n",
      "Iteration 2, loss = 0.72670591\n",
      "Iteration 3, loss = 0.68193662\n",
      "Iteration 4, loss = 0.64293232\n",
      "Iteration 5, loss = 0.60995938\n",
      "Iteration 6, loss = 0.58236707\n",
      "Iteration 7, loss = 0.55944034\n",
      "Iteration 8, loss = 0.53897596\n",
      "Iteration 9, loss = 0.52069209\n",
      "Iteration 10, loss = 0.50477015\n",
      "Iteration 11, loss = 0.49148944\n",
      "Iteration 12, loss = 0.47948262\n",
      "Iteration 13, loss = 0.46919571\n",
      "Iteration 14, loss = 0.46019456\n",
      "Iteration 15, loss = 0.45303440\n",
      "Iteration 16, loss = 0.44667214\n",
      "Iteration 17, loss = 0.44157212\n",
      "Iteration 18, loss = 0.43741369\n",
      "Iteration 19, loss = 0.43332668\n",
      "Iteration 20, loss = 0.43027111\n",
      "Iteration 21, loss = 0.42762766\n",
      "Iteration 22, loss = 0.42547802\n",
      "Iteration 23, loss = 0.42354455\n",
      "Iteration 24, loss = 0.42213984\n",
      "Iteration 25, loss = 0.42117295\n",
      "Iteration 26, loss = 0.42011694\n",
      "Iteration 27, loss = 0.41934921\n",
      "Iteration 28, loss = 0.41888540\n",
      "Iteration 29, loss = 0.41831504\n",
      "Iteration 30, loss = 0.41782755\n",
      "Iteration 31, loss = 0.41768341\n",
      "Iteration 32, loss = 0.41745853\n",
      "Iteration 33, loss = 0.41751533\n",
      "Iteration 34, loss = 0.41702309\n",
      "Iteration 35, loss = 0.41682146\n",
      "Iteration 36, loss = 0.41688903\n",
      "Iteration 37, loss = 0.41675956\n",
      "Iteration 38, loss = 0.41658138\n",
      "Iteration 39, loss = 0.41655614\n",
      "Iteration 40, loss = 0.41666971\n",
      "Iteration 41, loss = 0.41650039\n",
      "Iteration 42, loss = 0.41653217\n",
      "Iteration 43, loss = 0.41642725\n",
      "Iteration 44, loss = 0.41650618\n",
      "Iteration 45, loss = 0.41636437\n",
      "Iteration 46, loss = 0.41635458\n",
      "Iteration 47, loss = 0.41629176\n",
      "Iteration 48, loss = 0.41636034\n",
      "Iteration 49, loss = 0.41635715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77262843\n",
      "Iteration 2, loss = 0.71954447\n",
      "Iteration 3, loss = 0.67440006\n",
      "Iteration 4, loss = 0.63766723\n",
      "Iteration 5, loss = 0.60698957\n",
      "Iteration 6, loss = 0.58144600\n",
      "Iteration 7, loss = 0.56090877\n",
      "Iteration 8, loss = 0.54203302\n",
      "Iteration 9, loss = 0.52615764\n",
      "Iteration 10, loss = 0.51169260\n",
      "Iteration 11, loss = 0.49978784\n",
      "Iteration 12, loss = 0.48977991\n",
      "Iteration 13, loss = 0.48102966\n",
      "Iteration 14, loss = 0.47319894\n",
      "Iteration 15, loss = 0.46766266\n",
      "Iteration 16, loss = 0.46248982\n",
      "Iteration 17, loss = 0.45909335\n",
      "Iteration 18, loss = 0.45554864\n",
      "Iteration 19, loss = 0.45259510\n",
      "Iteration 20, loss = 0.45077306\n",
      "Iteration 21, loss = 0.44925891\n",
      "Iteration 22, loss = 0.44758306\n",
      "Iteration 23, loss = 0.44660216\n",
      "Iteration 24, loss = 0.44579279\n",
      "Iteration 25, loss = 0.44518510\n",
      "Iteration 26, loss = 0.44460193\n",
      "Iteration 27, loss = 0.44424912\n",
      "Iteration 28, loss = 0.44416976\n",
      "Iteration 29, loss = 0.44380135\n",
      "Iteration 30, loss = 0.44355561\n",
      "Iteration 31, loss = 0.44349014\n",
      "Iteration 32, loss = 0.44338756\n",
      "Iteration 33, loss = 0.44341426\n",
      "Iteration 34, loss = 0.44321846\n",
      "Iteration 35, loss = 0.44302807\n",
      "Iteration 36, loss = 0.44311888\n",
      "Iteration 37, loss = 0.44306155\n",
      "Iteration 38, loss = 0.44293541\n",
      "Iteration 39, loss = 0.44288187\n",
      "Iteration 40, loss = 0.44298806\n",
      "Iteration 41, loss = 0.44287897\n",
      "Iteration 42, loss = 0.44289046\n",
      "Iteration 43, loss = 0.44281228\n",
      "Iteration 44, loss = 0.44282301\n",
      "Iteration 45, loss = 0.44279649\n",
      "Iteration 46, loss = 0.44278748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852758\n",
      "Iteration 2, loss = 0.71232522\n",
      "Iteration 3, loss = 0.66094842\n",
      "Iteration 4, loss = 0.62121691\n",
      "Iteration 5, loss = 0.58926212\n",
      "Iteration 6, loss = 0.56172948\n",
      "Iteration 7, loss = 0.53837289\n",
      "Iteration 8, loss = 0.51997561\n",
      "Iteration 9, loss = 0.50268255\n",
      "Iteration 10, loss = 0.48870191\n",
      "Iteration 11, loss = 0.47577503\n",
      "Iteration 12, loss = 0.46521541\n",
      "Iteration 13, loss = 0.45504295\n",
      "Iteration 14, loss = 0.44711631\n",
      "Iteration 15, loss = 0.44017301\n",
      "Iteration 16, loss = 0.43442910\n",
      "Iteration 17, loss = 0.42948971\n",
      "Iteration 18, loss = 0.42617583\n",
      "Iteration 19, loss = 0.42254252\n",
      "Iteration 20, loss = 0.41993118\n",
      "Iteration 21, loss = 0.41788974\n",
      "Iteration 22, loss = 0.41583695\n",
      "Iteration 23, loss = 0.41475391\n",
      "Iteration 24, loss = 0.41353564\n",
      "Iteration 25, loss = 0.41289117\n",
      "Iteration 26, loss = 0.41230881\n",
      "Iteration 27, loss = 0.41171812\n",
      "Iteration 28, loss = 0.41133447\n",
      "Iteration 29, loss = 0.41104644\n",
      "Iteration 30, loss = 0.41081306\n",
      "Iteration 31, loss = 0.41056287\n",
      "Iteration 32, loss = 0.41062200\n",
      "Iteration 33, loss = 0.41043417\n",
      "Iteration 34, loss = 0.41025859\n",
      "Iteration 35, loss = 0.41023748\n",
      "Iteration 36, loss = 0.41019969\n",
      "Iteration 37, loss = 0.41011763\n",
      "Iteration 38, loss = 0.41006712\n",
      "Iteration 39, loss = 0.41000177\n",
      "Iteration 40, loss = 0.41004828\n",
      "Iteration 41, loss = 0.41004474\n",
      "Iteration 42, loss = 0.41006563\n",
      "Iteration 43, loss = 0.41012183\n",
      "Iteration 44, loss = 0.41008904\n",
      "Iteration 45, loss = 0.41000465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836833\n",
      "Iteration 2, loss = 0.72360616\n",
      "Iteration 3, loss = 0.67433647\n",
      "Iteration 4, loss = 0.63683748\n",
      "Iteration 5, loss = 0.60644815\n",
      "Iteration 6, loss = 0.58000251\n",
      "Iteration 7, loss = 0.55710542\n",
      "Iteration 8, loss = 0.53977727\n",
      "Iteration 9, loss = 0.52306660\n",
      "Iteration 10, loss = 0.50964897\n",
      "Iteration 11, loss = 0.49713801\n",
      "Iteration 12, loss = 0.48688322\n",
      "Iteration 13, loss = 0.47745237\n",
      "Iteration 14, loss = 0.47001491\n",
      "Iteration 15, loss = 0.46329982\n",
      "Iteration 16, loss = 0.45801813\n",
      "Iteration 17, loss = 0.45346694\n",
      "Iteration 18, loss = 0.45017040\n",
      "Iteration 19, loss = 0.44679218\n",
      "Iteration 20, loss = 0.44426741\n",
      "Iteration 21, loss = 0.44219826\n",
      "Iteration 22, loss = 0.44042704\n",
      "Iteration 23, loss = 0.43948763\n",
      "Iteration 24, loss = 0.43832151\n",
      "Iteration 25, loss = 0.43771464\n",
      "Iteration 26, loss = 0.43696411\n",
      "Iteration 27, loss = 0.43663026\n",
      "Iteration 28, loss = 0.43623533\n",
      "Iteration 29, loss = 0.43577385\n",
      "Iteration 30, loss = 0.43573181\n",
      "Iteration 31, loss = 0.43535118\n",
      "Iteration 32, loss = 0.43540806\n",
      "Iteration 33, loss = 0.43520338\n",
      "Iteration 34, loss = 0.43498948\n",
      "Iteration 35, loss = 0.43490297\n",
      "Iteration 36, loss = 0.43487907\n",
      "Iteration 37, loss = 0.43477234\n",
      "Iteration 38, loss = 0.43474579\n",
      "Iteration 39, loss = 0.43466724\n",
      "Iteration 40, loss = 0.43476140\n",
      "Iteration 41, loss = 0.43463304\n",
      "Iteration 42, loss = 0.43467808\n",
      "Iteration 43, loss = 0.43467163\n",
      "Iteration 44, loss = 0.43468227\n",
      "Iteration 45, loss = 0.43456992\n",
      "Iteration 46, loss = 0.43460162\n",
      "Iteration 47, loss = 0.43456507\n",
      "Iteration 48, loss = 0.43459764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812629\n",
      "Iteration 2, loss = 0.72327380\n",
      "Iteration 3, loss = 0.67851166\n",
      "Iteration 4, loss = 0.64113186\n",
      "Iteration 5, loss = 0.61005369\n",
      "Iteration 6, loss = 0.58272983\n",
      "Iteration 7, loss = 0.56064870\n",
      "Iteration 8, loss = 0.54029684\n",
      "Iteration 9, loss = 0.52206048\n",
      "Iteration 10, loss = 0.50630801\n",
      "Iteration 11, loss = 0.49280708\n",
      "Iteration 12, loss = 0.48086250\n",
      "Iteration 13, loss = 0.47076606\n",
      "Iteration 14, loss = 0.46199606\n",
      "Iteration 15, loss = 0.45484814\n",
      "Iteration 16, loss = 0.44904010\n",
      "Iteration 17, loss = 0.44397144\n",
      "Iteration 18, loss = 0.44036708\n",
      "Iteration 19, loss = 0.43680880\n",
      "Iteration 20, loss = 0.43340028\n",
      "Iteration 21, loss = 0.43138353\n",
      "Iteration 22, loss = 0.42944126\n",
      "Iteration 23, loss = 0.42774493\n",
      "Iteration 24, loss = 0.42668266\n",
      "Iteration 25, loss = 0.42574457\n",
      "Iteration 26, loss = 0.42490984\n",
      "Iteration 27, loss = 0.42409965\n",
      "Iteration 28, loss = 0.42377495\n",
      "Iteration 29, loss = 0.42313006\n",
      "Iteration 30, loss = 0.42285569\n",
      "Iteration 31, loss = 0.42265695\n",
      "Iteration 32, loss = 0.42231018\n",
      "Iteration 33, loss = 0.42244120\n",
      "Iteration 34, loss = 0.42192341\n",
      "Iteration 35, loss = 0.42196414\n",
      "Iteration 36, loss = 0.42166160\n",
      "Iteration 37, loss = 0.42169329\n",
      "Iteration 38, loss = 0.42156468\n",
      "Iteration 39, loss = 0.42134873\n",
      "Iteration 40, loss = 0.42144026\n",
      "Iteration 41, loss = 0.42127344\n",
      "Iteration 42, loss = 0.42133990\n",
      "Iteration 43, loss = 0.42108929\n",
      "Iteration 44, loss = 0.42132209\n",
      "Iteration 45, loss = 0.42109922\n",
      "Iteration 46, loss = 0.42099917\n",
      "Iteration 47, loss = 0.42093167\n",
      "Iteration 48, loss = 0.42106754\n",
      "Iteration 49, loss = 0.42093285\n",
      "Iteration 50, loss = 0.42094230\n",
      "Iteration 51, loss = 0.42096280\n",
      "Iteration 52, loss = 0.42102933\n",
      "Iteration 53, loss = 0.42088244\n",
      "Iteration 54, loss = 0.42080771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118733\n",
      "Iteration 2, loss = 0.72665229\n",
      "Iteration 3, loss = 0.68188297\n",
      "Iteration 4, loss = 0.64287863\n",
      "Iteration 5, loss = 0.60990561\n",
      "Iteration 6, loss = 0.58231323\n",
      "Iteration 7, loss = 0.55938643\n",
      "Iteration 8, loss = 0.53892197\n",
      "Iteration 9, loss = 0.52063801\n",
      "Iteration 10, loss = 0.50471598\n",
      "Iteration 11, loss = 0.49143517\n",
      "Iteration 12, loss = 0.47942822\n",
      "Iteration 13, loss = 0.46914118\n",
      "Iteration 14, loss = 0.46013988\n",
      "Iteration 15, loss = 0.45297956\n",
      "Iteration 16, loss = 0.44661715\n",
      "Iteration 17, loss = 0.44151699\n",
      "Iteration 18, loss = 0.43735842\n",
      "Iteration 19, loss = 0.43327125\n",
      "Iteration 20, loss = 0.43021556\n",
      "Iteration 21, loss = 0.42757196\n",
      "Iteration 22, loss = 0.42542220\n",
      "Iteration 23, loss = 0.42348861\n",
      "Iteration 24, loss = 0.42208378\n",
      "Iteration 25, loss = 0.42111680\n",
      "Iteration 26, loss = 0.42006070\n",
      "Iteration 27, loss = 0.41929289\n",
      "Iteration 28, loss = 0.41882902\n",
      "Iteration 29, loss = 0.41825859\n",
      "Iteration 30, loss = 0.41777103\n",
      "Iteration 31, loss = 0.41762686\n",
      "Iteration 32, loss = 0.41740193\n",
      "Iteration 33, loss = 0.41745871\n",
      "Iteration 34, loss = 0.41696642\n",
      "Iteration 35, loss = 0.41676476\n",
      "Iteration 36, loss = 0.41683233\n",
      "Iteration 37, loss = 0.41670283\n",
      "Iteration 38, loss = 0.41652463\n",
      "Iteration 39, loss = 0.41649938\n",
      "Iteration 40, loss = 0.41661295\n",
      "Iteration 41, loss = 0.41644362\n",
      "Iteration 42, loss = 0.41647539\n",
      "Iteration 43, loss = 0.41637046\n",
      "Iteration 44, loss = 0.41644939\n",
      "Iteration 45, loss = 0.41630757\n",
      "Iteration 46, loss = 0.41629777\n",
      "Iteration 47, loss = 0.41623495\n",
      "Iteration 48, loss = 0.41630355\n",
      "Iteration 49, loss = 0.41630035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77257477\n",
      "Iteration 2, loss = 0.71949083\n",
      "Iteration 3, loss = 0.67434640\n",
      "Iteration 4, loss = 0.63761353\n",
      "Iteration 5, loss = 0.60693579\n",
      "Iteration 6, loss = 0.58139214\n",
      "Iteration 7, loss = 0.56085485\n",
      "Iteration 8, loss = 0.54197902\n",
      "Iteration 9, loss = 0.52610355\n",
      "Iteration 10, loss = 0.51163843\n",
      "Iteration 11, loss = 0.49973355\n",
      "Iteration 12, loss = 0.48972548\n",
      "Iteration 13, loss = 0.48097510\n",
      "Iteration 14, loss = 0.47314422\n",
      "Iteration 15, loss = 0.46760778\n",
      "Iteration 16, loss = 0.46243478\n",
      "Iteration 17, loss = 0.45903819\n",
      "Iteration 18, loss = 0.45549335\n",
      "Iteration 19, loss = 0.45253967\n",
      "Iteration 20, loss = 0.45071753\n",
      "Iteration 21, loss = 0.44920328\n",
      "Iteration 22, loss = 0.44752732\n",
      "Iteration 23, loss = 0.44654633\n",
      "Iteration 24, loss = 0.44573689\n",
      "Iteration 25, loss = 0.44512914\n",
      "Iteration 26, loss = 0.44454590\n",
      "Iteration 27, loss = 0.44419305\n",
      "Iteration 28, loss = 0.44411366\n",
      "Iteration 29, loss = 0.44374520\n",
      "Iteration 30, loss = 0.44349943\n",
      "Iteration 31, loss = 0.44343395\n",
      "Iteration 32, loss = 0.44333135\n",
      "Iteration 33, loss = 0.44335803\n",
      "Iteration 34, loss = 0.44316220\n",
      "Iteration 35, loss = 0.44297179\n",
      "Iteration 36, loss = 0.44306260\n",
      "Iteration 37, loss = 0.44300527\n",
      "Iteration 38, loss = 0.44287910\n",
      "Iteration 39, loss = 0.44282556\n",
      "Iteration 40, loss = 0.44293176\n",
      "Iteration 41, loss = 0.44282265\n",
      "Iteration 42, loss = 0.44283414\n",
      "Iteration 43, loss = 0.44275595\n",
      "Iteration 44, loss = 0.44276668\n",
      "Iteration 45, loss = 0.44274015\n",
      "Iteration 46, loss = 0.44273114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852221\n",
      "Iteration 2, loss = 0.71231985\n",
      "Iteration 3, loss = 0.66094306\n",
      "Iteration 4, loss = 0.62121154\n",
      "Iteration 5, loss = 0.58925674\n",
      "Iteration 6, loss = 0.56172408\n",
      "Iteration 7, loss = 0.53836748\n",
      "Iteration 8, loss = 0.51997019\n",
      "Iteration 9, loss = 0.50267713\n",
      "Iteration 10, loss = 0.48869648\n",
      "Iteration 11, loss = 0.47576958\n",
      "Iteration 12, loss = 0.46520994\n",
      "Iteration 13, loss = 0.45503747\n",
      "Iteration 14, loss = 0.44711081\n",
      "Iteration 15, loss = 0.44016749\n",
      "Iteration 16, loss = 0.43442356\n",
      "Iteration 17, loss = 0.42948417\n",
      "Iteration 18, loss = 0.42617027\n",
      "Iteration 19, loss = 0.42253695\n",
      "Iteration 20, loss = 0.41992559\n",
      "Iteration 21, loss = 0.41788414\n",
      "Iteration 22, loss = 0.41583133\n",
      "Iteration 23, loss = 0.41474829\n",
      "Iteration 24, loss = 0.41353001\n",
      "Iteration 25, loss = 0.41288553\n",
      "Iteration 26, loss = 0.41230316\n",
      "Iteration 27, loss = 0.41171247\n",
      "Iteration 28, loss = 0.41132881\n",
      "Iteration 29, loss = 0.41104078\n",
      "Iteration 30, loss = 0.41080739\n",
      "Iteration 31, loss = 0.41055720\n",
      "Iteration 32, loss = 0.41061633\n",
      "Iteration 33, loss = 0.41042849\n",
      "Iteration 34, loss = 0.41025292\n",
      "Iteration 35, loss = 0.41023180\n",
      "Iteration 36, loss = 0.41019401\n",
      "Iteration 37, loss = 0.41011195\n",
      "Iteration 38, loss = 0.41006144\n",
      "Iteration 39, loss = 0.40999609\n",
      "Iteration 40, loss = 0.41004260\n",
      "Iteration 41, loss = 0.41003906\n",
      "Iteration 42, loss = 0.41005994\n",
      "Iteration 43, loss = 0.41011615\n",
      "Iteration 44, loss = 0.41008336\n",
      "Iteration 45, loss = 0.40999897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836295\n",
      "Iteration 2, loss = 0.72360079\n",
      "Iteration 3, loss = 0.67433109\n",
      "Iteration 4, loss = 0.63683209\n",
      "Iteration 5, loss = 0.60644275\n",
      "Iteration 6, loss = 0.57999711\n",
      "Iteration 7, loss = 0.55710001\n",
      "Iteration 8, loss = 0.53977185\n",
      "Iteration 9, loss = 0.52306118\n",
      "Iteration 10, loss = 0.50964353\n",
      "Iteration 11, loss = 0.49713257\n",
      "Iteration 12, loss = 0.48687777\n",
      "Iteration 13, loss = 0.47744691\n",
      "Iteration 14, loss = 0.47000943\n",
      "Iteration 15, loss = 0.46329433\n",
      "Iteration 16, loss = 0.45801262\n",
      "Iteration 17, loss = 0.45346142\n",
      "Iteration 18, loss = 0.45016486\n",
      "Iteration 19, loss = 0.44678664\n",
      "Iteration 20, loss = 0.44426185\n",
      "Iteration 21, loss = 0.44219269\n",
      "Iteration 22, loss = 0.44042146\n",
      "Iteration 23, loss = 0.43948204\n",
      "Iteration 24, loss = 0.43831591\n",
      "Iteration 25, loss = 0.43770903\n",
      "Iteration 26, loss = 0.43695850\n",
      "Iteration 27, loss = 0.43662464\n",
      "Iteration 28, loss = 0.43622971\n",
      "Iteration 29, loss = 0.43576823\n",
      "Iteration 30, loss = 0.43572619\n",
      "Iteration 31, loss = 0.43534555\n",
      "Iteration 32, loss = 0.43540243\n",
      "Iteration 33, loss = 0.43519774\n",
      "Iteration 34, loss = 0.43498384\n",
      "Iteration 35, loss = 0.43489733\n",
      "Iteration 36, loss = 0.43487343\n",
      "Iteration 37, loss = 0.43476670\n",
      "Iteration 38, loss = 0.43474015\n",
      "Iteration 39, loss = 0.43466159\n",
      "Iteration 40, loss = 0.43475576\n",
      "Iteration 41, loss = 0.43462739\n",
      "Iteration 42, loss = 0.43467244\n",
      "Iteration 43, loss = 0.43466598\n",
      "Iteration 44, loss = 0.43467662\n",
      "Iteration 45, loss = 0.43456428\n",
      "Iteration 46, loss = 0.43459597\n",
      "Iteration 47, loss = 0.43455942\n",
      "Iteration 48, loss = 0.43459199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812093\n",
      "Iteration 2, loss = 0.72326843\n",
      "Iteration 3, loss = 0.67850629\n",
      "Iteration 4, loss = 0.64112649\n",
      "Iteration 5, loss = 0.61004831\n",
      "Iteration 6, loss = 0.58272445\n",
      "Iteration 7, loss = 0.56064331\n",
      "Iteration 8, loss = 0.54029145\n",
      "Iteration 9, loss = 0.52205508\n",
      "Iteration 10, loss = 0.50630260\n",
      "Iteration 11, loss = 0.49280165\n",
      "Iteration 12, loss = 0.48085705\n",
      "Iteration 13, loss = 0.47076059\n",
      "Iteration 14, loss = 0.46199058\n",
      "Iteration 15, loss = 0.45484264\n",
      "Iteration 16, loss = 0.44903459\n",
      "Iteration 17, loss = 0.44396591\n",
      "Iteration 18, loss = 0.44036154\n",
      "Iteration 19, loss = 0.43680323\n",
      "Iteration 20, loss = 0.43339470\n",
      "Iteration 21, loss = 0.43137794\n",
      "Iteration 22, loss = 0.42943566\n",
      "Iteration 23, loss = 0.42773931\n",
      "Iteration 24, loss = 0.42667704\n",
      "Iteration 25, loss = 0.42573893\n",
      "Iteration 26, loss = 0.42490419\n",
      "Iteration 27, loss = 0.42409400\n",
      "Iteration 28, loss = 0.42376929\n",
      "Iteration 29, loss = 0.42312439\n",
      "Iteration 30, loss = 0.42285002\n",
      "Iteration 31, loss = 0.42265127\n",
      "Iteration 32, loss = 0.42230449\n",
      "Iteration 33, loss = 0.42243552\n",
      "Iteration 34, loss = 0.42191772\n",
      "Iteration 35, loss = 0.42195844\n",
      "Iteration 36, loss = 0.42165590\n",
      "Iteration 37, loss = 0.42168759\n",
      "Iteration 38, loss = 0.42155898\n",
      "Iteration 39, loss = 0.42134302\n",
      "Iteration 40, loss = 0.42143455\n",
      "Iteration 41, loss = 0.42126774\n",
      "Iteration 42, loss = 0.42133419\n",
      "Iteration 43, loss = 0.42108358\n",
      "Iteration 44, loss = 0.42131638\n",
      "Iteration 45, loss = 0.42109351\n",
      "Iteration 46, loss = 0.42099345\n",
      "Iteration 47, loss = 0.42092595\n",
      "Iteration 48, loss = 0.42106183\n",
      "Iteration 49, loss = 0.42092714\n",
      "Iteration 50, loss = 0.42093659\n",
      "Iteration 51, loss = 0.42095709\n",
      "Iteration 52, loss = 0.42102361\n",
      "Iteration 53, loss = 0.42087672\n",
      "Iteration 54, loss = 0.42080199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118196\n",
      "Iteration 2, loss = 0.72664692\n",
      "Iteration 3, loss = 0.68187760\n",
      "Iteration 4, loss = 0.64287326\n",
      "Iteration 5, loss = 0.60990024\n",
      "Iteration 6, loss = 0.58230784\n",
      "Iteration 7, loss = 0.55938104\n",
      "Iteration 8, loss = 0.53891657\n",
      "Iteration 9, loss = 0.52063260\n",
      "Iteration 10, loss = 0.50471056\n",
      "Iteration 11, loss = 0.49142974\n",
      "Iteration 12, loss = 0.47942278\n",
      "Iteration 13, loss = 0.46913573\n",
      "Iteration 14, loss = 0.46013441\n",
      "Iteration 15, loss = 0.45297407\n",
      "Iteration 16, loss = 0.44661165\n",
      "Iteration 17, loss = 0.44151148\n",
      "Iteration 18, loss = 0.43735289\n",
      "Iteration 19, loss = 0.43326571\n",
      "Iteration 20, loss = 0.43021000\n",
      "Iteration 21, loss = 0.42756638\n",
      "Iteration 22, loss = 0.42541662\n",
      "Iteration 23, loss = 0.42348301\n",
      "Iteration 24, loss = 0.42207818\n",
      "Iteration 25, loss = 0.42111118\n",
      "Iteration 26, loss = 0.42005507\n",
      "Iteration 27, loss = 0.41928726\n",
      "Iteration 28, loss = 0.41882338\n",
      "Iteration 29, loss = 0.41825294\n",
      "Iteration 30, loss = 0.41776538\n",
      "Iteration 31, loss = 0.41762121\n",
      "Iteration 32, loss = 0.41739627\n",
      "Iteration 33, loss = 0.41745305\n",
      "Iteration 34, loss = 0.41696075\n",
      "Iteration 35, loss = 0.41675909\n",
      "Iteration 36, loss = 0.41682666\n",
      "Iteration 37, loss = 0.41669716\n",
      "Iteration 38, loss = 0.41651896\n",
      "Iteration 39, loss = 0.41649370\n",
      "Iteration 40, loss = 0.41660727\n",
      "Iteration 41, loss = 0.41643794\n",
      "Iteration 42, loss = 0.41646971\n",
      "Iteration 43, loss = 0.41636478\n",
      "Iteration 44, loss = 0.41644371\n",
      "Iteration 45, loss = 0.41630189\n",
      "Iteration 46, loss = 0.41629209\n",
      "Iteration 47, loss = 0.41622926\n",
      "Iteration 48, loss = 0.41629786\n",
      "Iteration 49, loss = 0.41629466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77256941\n",
      "Iteration 2, loss = 0.71948547\n",
      "Iteration 3, loss = 0.67434103\n",
      "Iteration 4, loss = 0.63760816\n",
      "Iteration 5, loss = 0.60693042\n",
      "Iteration 6, loss = 0.58138676\n",
      "Iteration 7, loss = 0.56084946\n",
      "Iteration 8, loss = 0.54197361\n",
      "Iteration 9, loss = 0.52609814\n",
      "Iteration 10, loss = 0.51163301\n",
      "Iteration 11, loss = 0.49972812\n",
      "Iteration 12, loss = 0.48972004\n",
      "Iteration 13, loss = 0.48096964\n",
      "Iteration 14, loss = 0.47313874\n",
      "Iteration 15, loss = 0.46760229\n",
      "Iteration 16, loss = 0.46242928\n",
      "Iteration 17, loss = 0.45903268\n",
      "Iteration 18, loss = 0.45548782\n",
      "Iteration 19, loss = 0.45253413\n",
      "Iteration 20, loss = 0.45071198\n",
      "Iteration 21, loss = 0.44919772\n",
      "Iteration 22, loss = 0.44752174\n",
      "Iteration 23, loss = 0.44654074\n",
      "Iteration 24, loss = 0.44573129\n",
      "Iteration 25, loss = 0.44512354\n",
      "Iteration 26, loss = 0.44454030\n",
      "Iteration 27, loss = 0.44418744\n",
      "Iteration 28, loss = 0.44410805\n",
      "Iteration 29, loss = 0.44373958\n",
      "Iteration 30, loss = 0.44349381\n",
      "Iteration 31, loss = 0.44342833\n",
      "Iteration 32, loss = 0.44332572\n",
      "Iteration 33, loss = 0.44335240\n",
      "Iteration 34, loss = 0.44315657\n",
      "Iteration 35, loss = 0.44296616\n",
      "Iteration 36, loss = 0.44305697\n",
      "Iteration 37, loss = 0.44299964\n",
      "Iteration 38, loss = 0.44287347\n",
      "Iteration 39, loss = 0.44281993\n",
      "Iteration 40, loss = 0.44292612\n",
      "Iteration 41, loss = 0.44281702\n",
      "Iteration 42, loss = 0.44282851\n",
      "Iteration 43, loss = 0.44275032\n",
      "Iteration 44, loss = 0.44276104\n",
      "Iteration 45, loss = 0.44273452\n",
      "Iteration 46, loss = 0.44272550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852168\n",
      "Iteration 2, loss = 0.71231932\n",
      "Iteration 3, loss = 0.66094252\n",
      "Iteration 4, loss = 0.62121101\n",
      "Iteration 5, loss = 0.58925620\n",
      "Iteration 6, loss = 0.56172354\n",
      "Iteration 7, loss = 0.53836694\n",
      "Iteration 8, loss = 0.51996965\n",
      "Iteration 9, loss = 0.50267658\n",
      "Iteration 10, loss = 0.48869593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.47576904\n",
      "Iteration 12, loss = 0.46520940\n",
      "Iteration 13, loss = 0.45503692\n",
      "Iteration 14, loss = 0.44711026\n",
      "Iteration 15, loss = 0.44016694\n",
      "Iteration 16, loss = 0.43442301\n",
      "Iteration 17, loss = 0.42948361\n",
      "Iteration 18, loss = 0.42616971\n",
      "Iteration 19, loss = 0.42253639\n",
      "Iteration 20, loss = 0.41992503\n",
      "Iteration 21, loss = 0.41788358\n",
      "Iteration 22, loss = 0.41583077\n",
      "Iteration 23, loss = 0.41474773\n",
      "Iteration 24, loss = 0.41352945\n",
      "Iteration 25, loss = 0.41288496\n",
      "Iteration 26, loss = 0.41230260\n",
      "Iteration 27, loss = 0.41171190\n",
      "Iteration 28, loss = 0.41132824\n",
      "Iteration 29, loss = 0.41104021\n",
      "Iteration 30, loss = 0.41080683\n",
      "Iteration 31, loss = 0.41055664\n",
      "Iteration 32, loss = 0.41061577\n",
      "Iteration 33, loss = 0.41042792\n",
      "Iteration 34, loss = 0.41025235\n",
      "Iteration 35, loss = 0.41023123\n",
      "Iteration 36, loss = 0.41019344\n",
      "Iteration 37, loss = 0.41011139\n",
      "Iteration 38, loss = 0.41006087\n",
      "Iteration 39, loss = 0.40999552\n",
      "Iteration 40, loss = 0.41004203\n",
      "Iteration 41, loss = 0.41003849\n",
      "Iteration 42, loss = 0.41005938\n",
      "Iteration 43, loss = 0.41011558\n",
      "Iteration 44, loss = 0.41008279\n",
      "Iteration 45, loss = 0.40999840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836242\n",
      "Iteration 2, loss = 0.72360025\n",
      "Iteration 3, loss = 0.67433055\n",
      "Iteration 4, loss = 0.63683156\n",
      "Iteration 5, loss = 0.60644221\n",
      "Iteration 6, loss = 0.57999657\n",
      "Iteration 7, loss = 0.55709946\n",
      "Iteration 8, loss = 0.53977131\n",
      "Iteration 9, loss = 0.52306064\n",
      "Iteration 10, loss = 0.50964299\n",
      "Iteration 11, loss = 0.49713202\n",
      "Iteration 12, loss = 0.48687722\n",
      "Iteration 13, loss = 0.47744636\n",
      "Iteration 14, loss = 0.47000888\n",
      "Iteration 15, loss = 0.46329378\n",
      "Iteration 16, loss = 0.45801207\n",
      "Iteration 17, loss = 0.45346087\n",
      "Iteration 18, loss = 0.45016431\n",
      "Iteration 19, loss = 0.44678608\n",
      "Iteration 20, loss = 0.44426130\n",
      "Iteration 21, loss = 0.44219213\n",
      "Iteration 22, loss = 0.44042090\n",
      "Iteration 23, loss = 0.43948148\n",
      "Iteration 24, loss = 0.43831535\n",
      "Iteration 25, loss = 0.43770847\n",
      "Iteration 26, loss = 0.43695794\n",
      "Iteration 27, loss = 0.43662408\n",
      "Iteration 28, loss = 0.43622915\n",
      "Iteration 29, loss = 0.43576767\n",
      "Iteration 30, loss = 0.43572562\n",
      "Iteration 31, loss = 0.43534498\n",
      "Iteration 32, loss = 0.43540187\n",
      "Iteration 33, loss = 0.43519718\n",
      "Iteration 34, loss = 0.43498328\n",
      "Iteration 35, loss = 0.43489677\n",
      "Iteration 36, loss = 0.43487287\n",
      "Iteration 37, loss = 0.43476613\n",
      "Iteration 38, loss = 0.43473958\n",
      "Iteration 39, loss = 0.43466103\n",
      "Iteration 40, loss = 0.43475520\n",
      "Iteration 41, loss = 0.43462682\n",
      "Iteration 42, loss = 0.43467187\n",
      "Iteration 43, loss = 0.43466542\n",
      "Iteration 44, loss = 0.43467606\n",
      "Iteration 45, loss = 0.43456371\n",
      "Iteration 46, loss = 0.43459541\n",
      "Iteration 47, loss = 0.43455886\n",
      "Iteration 48, loss = 0.43459143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812039\n",
      "Iteration 2, loss = 0.72326790\n",
      "Iteration 3, loss = 0.67850575\n",
      "Iteration 4, loss = 0.64112596\n",
      "Iteration 5, loss = 0.61004777\n",
      "Iteration 6, loss = 0.58272391\n",
      "Iteration 7, loss = 0.56064277\n",
      "Iteration 8, loss = 0.54029091\n",
      "Iteration 9, loss = 0.52205454\n",
      "Iteration 10, loss = 0.50630205\n",
      "Iteration 11, loss = 0.49280111\n",
      "Iteration 12, loss = 0.48085651\n",
      "Iteration 13, loss = 0.47076005\n",
      "Iteration 14, loss = 0.46199003\n",
      "Iteration 15, loss = 0.45484209\n",
      "Iteration 16, loss = 0.44903404\n",
      "Iteration 17, loss = 0.44396536\n",
      "Iteration 18, loss = 0.44036098\n",
      "Iteration 19, loss = 0.43680268\n",
      "Iteration 20, loss = 0.43339414\n",
      "Iteration 21, loss = 0.43137738\n",
      "Iteration 22, loss = 0.42943510\n",
      "Iteration 23, loss = 0.42773875\n",
      "Iteration 24, loss = 0.42667647\n",
      "Iteration 25, loss = 0.42573837\n",
      "Iteration 26, loss = 0.42490362\n",
      "Iteration 27, loss = 0.42409343\n",
      "Iteration 28, loss = 0.42376872\n",
      "Iteration 29, loss = 0.42312382\n",
      "Iteration 30, loss = 0.42284945\n",
      "Iteration 31, loss = 0.42265070\n",
      "Iteration 32, loss = 0.42230392\n",
      "Iteration 33, loss = 0.42243495\n",
      "Iteration 34, loss = 0.42191715\n",
      "Iteration 35, loss = 0.42195787\n",
      "Iteration 36, loss = 0.42165533\n",
      "Iteration 37, loss = 0.42168702\n",
      "Iteration 38, loss = 0.42155841\n",
      "Iteration 39, loss = 0.42134245\n",
      "Iteration 40, loss = 0.42143398\n",
      "Iteration 41, loss = 0.42126717\n",
      "Iteration 42, loss = 0.42133362\n",
      "Iteration 43, loss = 0.42108301\n",
      "Iteration 44, loss = 0.42131581\n",
      "Iteration 45, loss = 0.42109294\n",
      "Iteration 46, loss = 0.42099288\n",
      "Iteration 47, loss = 0.42092538\n",
      "Iteration 48, loss = 0.42106126\n",
      "Iteration 49, loss = 0.42092657\n",
      "Iteration 50, loss = 0.42093602\n",
      "Iteration 51, loss = 0.42095652\n",
      "Iteration 52, loss = 0.42102304\n",
      "Iteration 53, loss = 0.42087615\n",
      "Iteration 54, loss = 0.42080142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118143\n",
      "Iteration 2, loss = 0.72664639\n",
      "Iteration 3, loss = 0.68187706\n",
      "Iteration 4, loss = 0.64287272\n",
      "Iteration 5, loss = 0.60989970\n",
      "Iteration 6, loss = 0.58230731\n",
      "Iteration 7, loss = 0.55938050\n",
      "Iteration 8, loss = 0.53891603\n",
      "Iteration 9, loss = 0.52063206\n",
      "Iteration 10, loss = 0.50471002\n",
      "Iteration 11, loss = 0.49142920\n",
      "Iteration 12, loss = 0.47942224\n",
      "Iteration 13, loss = 0.46913518\n",
      "Iteration 14, loss = 0.46013386\n",
      "Iteration 15, loss = 0.45297352\n",
      "Iteration 16, loss = 0.44661110\n",
      "Iteration 17, loss = 0.44151092\n",
      "Iteration 18, loss = 0.43735234\n",
      "Iteration 19, loss = 0.43326515\n",
      "Iteration 20, loss = 0.43020944\n",
      "Iteration 21, loss = 0.42756583\n",
      "Iteration 22, loss = 0.42541606\n",
      "Iteration 23, loss = 0.42348245\n",
      "Iteration 24, loss = 0.42207762\n",
      "Iteration 25, loss = 0.42111062\n",
      "Iteration 26, loss = 0.42005451\n",
      "Iteration 27, loss = 0.41928669\n",
      "Iteration 28, loss = 0.41882282\n",
      "Iteration 29, loss = 0.41825237\n",
      "Iteration 30, loss = 0.41776481\n",
      "Iteration 31, loss = 0.41762064\n",
      "Iteration 32, loss = 0.41739570\n",
      "Iteration 33, loss = 0.41745248\n",
      "Iteration 34, loss = 0.41696018\n",
      "Iteration 35, loss = 0.41675852\n",
      "Iteration 36, loss = 0.41682609\n",
      "Iteration 37, loss = 0.41669659\n",
      "Iteration 38, loss = 0.41651839\n",
      "Iteration 39, loss = 0.41649313\n",
      "Iteration 40, loss = 0.41660670\n",
      "Iteration 41, loss = 0.41643737\n",
      "Iteration 42, loss = 0.41646915\n",
      "Iteration 43, loss = 0.41636421\n",
      "Iteration 44, loss = 0.41644314\n",
      "Iteration 45, loss = 0.41630132\n",
      "Iteration 46, loss = 0.41629152\n",
      "Iteration 47, loss = 0.41622869\n",
      "Iteration 48, loss = 0.41629730\n",
      "Iteration 49, loss = 0.41629409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77256887\n",
      "Iteration 2, loss = 0.71948493\n",
      "Iteration 3, loss = 0.67434049\n",
      "Iteration 4, loss = 0.63760762\n",
      "Iteration 5, loss = 0.60692988\n",
      "Iteration 6, loss = 0.58138622\n",
      "Iteration 7, loss = 0.56084892\n",
      "Iteration 8, loss = 0.54197307\n",
      "Iteration 9, loss = 0.52609759\n",
      "Iteration 10, loss = 0.51163247\n",
      "Iteration 11, loss = 0.49972758\n",
      "Iteration 12, loss = 0.48971950\n",
      "Iteration 13, loss = 0.48096909\n",
      "Iteration 14, loss = 0.47313820\n",
      "Iteration 15, loss = 0.46760174\n",
      "Iteration 16, loss = 0.46242873\n",
      "Iteration 17, loss = 0.45903213\n",
      "Iteration 18, loss = 0.45548727\n",
      "Iteration 19, loss = 0.45253358\n",
      "Iteration 20, loss = 0.45071142\n",
      "Iteration 21, loss = 0.44919716\n",
      "Iteration 22, loss = 0.44752119\n",
      "Iteration 23, loss = 0.44654019\n",
      "Iteration 24, loss = 0.44573074\n",
      "Iteration 25, loss = 0.44512298\n",
      "Iteration 26, loss = 0.44453974\n",
      "Iteration 27, loss = 0.44418688\n",
      "Iteration 28, loss = 0.44410749\n",
      "Iteration 29, loss = 0.44373902\n",
      "Iteration 30, loss = 0.44349325\n",
      "Iteration 31, loss = 0.44342777\n",
      "Iteration 32, loss = 0.44332516\n",
      "Iteration 33, loss = 0.44335184\n",
      "Iteration 34, loss = 0.44315601\n",
      "Iteration 35, loss = 0.44296560\n",
      "Iteration 36, loss = 0.44305640\n",
      "Iteration 37, loss = 0.44299907\n",
      "Iteration 38, loss = 0.44287291\n",
      "Iteration 39, loss = 0.44281937\n",
      "Iteration 40, loss = 0.44292556\n",
      "Iteration 41, loss = 0.44281645\n",
      "Iteration 42, loss = 0.44282795\n",
      "Iteration 43, loss = 0.44274975\n",
      "Iteration 44, loss = 0.44276048\n",
      "Iteration 45, loss = 0.44273396\n",
      "Iteration 46, loss = 0.44272494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75172330\n",
      "Iteration 2, loss = 0.71730302\n",
      "Iteration 3, loss = 0.68850250\n",
      "Iteration 4, loss = 0.66532281\n",
      "Iteration 5, loss = 0.64976537\n",
      "Iteration 6, loss = 0.63849846\n",
      "Iteration 7, loss = 0.63046567\n",
      "Iteration 8, loss = 0.62548138\n",
      "Iteration 9, loss = 0.62045553\n",
      "Iteration 10, loss = 0.61575445\n",
      "Iteration 11, loss = 0.61104546\n",
      "Iteration 12, loss = 0.60552488\n",
      "Iteration 13, loss = 0.59948762\n",
      "Iteration 14, loss = 0.59417297\n",
      "Iteration 15, loss = 0.58882313\n",
      "Iteration 16, loss = 0.58377637\n",
      "Iteration 17, loss = 0.57894930\n",
      "Iteration 18, loss = 0.57438554\n",
      "Iteration 19, loss = 0.57006792\n",
      "Iteration 20, loss = 0.56579470\n",
      "Iteration 21, loss = 0.56153468\n",
      "Iteration 22, loss = 0.55736253\n",
      "Iteration 23, loss = 0.55317038\n",
      "Iteration 24, loss = 0.54903845\n",
      "Iteration 25, loss = 0.54504278\n",
      "Iteration 26, loss = 0.54130154\n",
      "Iteration 27, loss = 0.53739037\n",
      "Iteration 28, loss = 0.53363942\n",
      "Iteration 29, loss = 0.53008810\n",
      "Iteration 30, loss = 0.52641073\n",
      "Iteration 31, loss = 0.52300871\n",
      "Iteration 32, loss = 0.51972559\n",
      "Iteration 33, loss = 0.51624293\n",
      "Iteration 34, loss = 0.51293562\n",
      "Iteration 35, loss = 0.50973895\n",
      "Iteration 36, loss = 0.50685714\n",
      "Iteration 37, loss = 0.50371075\n",
      "Iteration 38, loss = 0.50080842\n",
      "Iteration 39, loss = 0.49797175\n",
      "Iteration 40, loss = 0.49512392\n",
      "Iteration 41, loss = 0.49233520\n",
      "Iteration 42, loss = 0.48971014\n",
      "Iteration 43, loss = 0.48713829\n",
      "Iteration 44, loss = 0.48460892\n",
      "Iteration 45, loss = 0.48214614\n",
      "Iteration 46, loss = 0.47974340\n",
      "Iteration 47, loss = 0.47748346\n",
      "Iteration 48, loss = 0.47506600\n",
      "Iteration 49, loss = 0.47281226\n",
      "Iteration 50, loss = 0.47081977\n",
      "Iteration 51, loss = 0.46866236\n",
      "Iteration 52, loss = 0.46667278\n",
      "Iteration 53, loss = 0.46456898\n",
      "Iteration 54, loss = 0.46275398\n",
      "Iteration 55, loss = 0.46092176\n",
      "Iteration 56, loss = 0.45905868\n",
      "Iteration 57, loss = 0.45730393\n",
      "Iteration 58, loss = 0.45560347\n",
      "Iteration 59, loss = 0.45391989\n",
      "Iteration 60, loss = 0.45233327\n",
      "Iteration 61, loss = 0.45077804\n",
      "Iteration 62, loss = 0.44940399\n",
      "Iteration 63, loss = 0.44787200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.44648110\n",
      "Iteration 65, loss = 0.44508367\n",
      "Iteration 66, loss = 0.44372645\n",
      "Iteration 67, loss = 0.44249018\n",
      "Iteration 68, loss = 0.44130092\n",
      "Iteration 69, loss = 0.44016545\n",
      "Iteration 70, loss = 0.43907240\n",
      "Iteration 71, loss = 0.43798036\n",
      "Iteration 72, loss = 0.43691928\n",
      "Iteration 73, loss = 0.43588553\n",
      "Iteration 74, loss = 0.43503492\n",
      "Iteration 75, loss = 0.43415715\n",
      "Iteration 76, loss = 0.43325043\n",
      "Iteration 77, loss = 0.43237963\n",
      "Iteration 78, loss = 0.43161453\n",
      "Iteration 79, loss = 0.43090887\n",
      "Iteration 80, loss = 0.43032781\n",
      "Iteration 81, loss = 0.42960982\n",
      "Iteration 82, loss = 0.42887629\n",
      "Iteration 83, loss = 0.42837581\n",
      "Iteration 84, loss = 0.42772950\n",
      "Iteration 85, loss = 0.42719685\n",
      "Iteration 86, loss = 0.42659590\n",
      "Iteration 87, loss = 0.42614888\n",
      "Iteration 88, loss = 0.42578914\n",
      "Iteration 89, loss = 0.42521831\n",
      "Iteration 90, loss = 0.42475191\n",
      "Iteration 91, loss = 0.42449390\n",
      "Iteration 92, loss = 0.42412998\n",
      "Iteration 93, loss = 0.42373526\n",
      "Iteration 94, loss = 0.42352070\n",
      "Iteration 95, loss = 0.42309295\n",
      "Iteration 96, loss = 0.42287068\n",
      "Iteration 97, loss = 0.42253365\n",
      "Iteration 98, loss = 0.42233259\n",
      "Iteration 99, loss = 0.42201018\n",
      "Iteration 100, loss = 0.42174803\n",
      "Iteration 101, loss = 0.42161239\n",
      "Iteration 102, loss = 0.42130843\n",
      "Iteration 103, loss = 0.42116351\n",
      "Iteration 104, loss = 0.42097994\n",
      "Iteration 105, loss = 0.42076198\n",
      "Iteration 106, loss = 0.42057860\n",
      "Iteration 107, loss = 0.42049844\n",
      "Iteration 108, loss = 0.42037498\n",
      "Iteration 109, loss = 0.42017154\n",
      "Iteration 110, loss = 0.42002497\n",
      "Iteration 111, loss = 0.41988851\n",
      "Iteration 112, loss = 0.41978472\n",
      "Iteration 113, loss = 0.41967922\n",
      "Iteration 114, loss = 0.41955694\n",
      "Iteration 115, loss = 0.41945841\n",
      "Iteration 116, loss = 0.41934694\n",
      "Iteration 117, loss = 0.41928473\n",
      "Iteration 118, loss = 0.41919667\n",
      "Iteration 119, loss = 0.41908128\n",
      "Iteration 120, loss = 0.41898595\n",
      "Iteration 121, loss = 0.41892122\n",
      "Iteration 122, loss = 0.41895436\n",
      "Iteration 123, loss = 0.41874332\n",
      "Iteration 124, loss = 0.41881933\n",
      "Iteration 125, loss = 0.41860211\n",
      "Iteration 126, loss = 0.41857773\n",
      "Iteration 127, loss = 0.41849502\n",
      "Iteration 128, loss = 0.41845283\n",
      "Iteration 129, loss = 0.41843056\n",
      "Iteration 130, loss = 0.41829482\n",
      "Iteration 131, loss = 0.41833045\n",
      "Iteration 132, loss = 0.41828709\n",
      "Iteration 133, loss = 0.41826024\n",
      "Iteration 134, loss = 0.41811749\n",
      "Iteration 135, loss = 0.41812091\n",
      "Iteration 136, loss = 0.41804064\n",
      "Iteration 137, loss = 0.41806104\n",
      "Iteration 138, loss = 0.41793315\n",
      "Iteration 139, loss = 0.41797471\n",
      "Iteration 140, loss = 0.41788613\n",
      "Iteration 141, loss = 0.41783309\n",
      "Iteration 142, loss = 0.41779391\n",
      "Iteration 143, loss = 0.41774654\n",
      "Iteration 144, loss = 0.41769053\n",
      "Iteration 145, loss = 0.41781311\n",
      "Iteration 146, loss = 0.41770831\n",
      "Iteration 147, loss = 0.41776122\n",
      "Iteration 148, loss = 0.41763572\n",
      "Iteration 149, loss = 0.41754199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75271921\n",
      "Iteration 2, loss = 0.71821707\n",
      "Iteration 3, loss = 0.68906612\n",
      "Iteration 4, loss = 0.66707242\n",
      "Iteration 5, loss = 0.65231432\n",
      "Iteration 6, loss = 0.64131112\n",
      "Iteration 7, loss = 0.63338020\n",
      "Iteration 8, loss = 0.62940206\n",
      "Iteration 9, loss = 0.62459600\n",
      "Iteration 10, loss = 0.62022429\n",
      "Iteration 11, loss = 0.61595107\n",
      "Iteration 12, loss = 0.61119525\n",
      "Iteration 13, loss = 0.60573763\n",
      "Iteration 14, loss = 0.60109478\n",
      "Iteration 15, loss = 0.59636614\n",
      "Iteration 16, loss = 0.59192906\n",
      "Iteration 17, loss = 0.58779230\n",
      "Iteration 18, loss = 0.58371259\n",
      "Iteration 19, loss = 0.58000732\n",
      "Iteration 20, loss = 0.57628212\n",
      "Iteration 21, loss = 0.57262536\n",
      "Iteration 22, loss = 0.56907829\n",
      "Iteration 23, loss = 0.56550361\n",
      "Iteration 24, loss = 0.56200561\n",
      "Iteration 25, loss = 0.55854844\n",
      "Iteration 26, loss = 0.55531147\n",
      "Iteration 27, loss = 0.55208567\n",
      "Iteration 28, loss = 0.54886129\n",
      "Iteration 29, loss = 0.54575138\n",
      "Iteration 30, loss = 0.54258576\n",
      "Iteration 31, loss = 0.53956885\n",
      "Iteration 32, loss = 0.53692262\n",
      "Iteration 33, loss = 0.53388525\n",
      "Iteration 34, loss = 0.53098956\n",
      "Iteration 35, loss = 0.52821678\n",
      "Iteration 36, loss = 0.52574999\n",
      "Iteration 37, loss = 0.52293509\n",
      "Iteration 38, loss = 0.52045392\n",
      "Iteration 39, loss = 0.51796875\n",
      "Iteration 40, loss = 0.51548986\n",
      "Iteration 41, loss = 0.51297591\n",
      "Iteration 42, loss = 0.51061848\n",
      "Iteration 43, loss = 0.50831726\n",
      "Iteration 44, loss = 0.50601434\n",
      "Iteration 45, loss = 0.50383022\n",
      "Iteration 46, loss = 0.50169593\n",
      "Iteration 47, loss = 0.49951165\n",
      "Iteration 48, loss = 0.49745224\n",
      "Iteration 49, loss = 0.49542329\n",
      "Iteration 50, loss = 0.49352404\n",
      "Iteration 51, loss = 0.49161022\n",
      "Iteration 52, loss = 0.48962691\n",
      "Iteration 53, loss = 0.48768165\n",
      "Iteration 54, loss = 0.48604623\n",
      "Iteration 55, loss = 0.48427080\n",
      "Iteration 56, loss = 0.48249684\n",
      "Iteration 57, loss = 0.48087422\n",
      "Iteration 58, loss = 0.47921664\n",
      "Iteration 59, loss = 0.47773709\n",
      "Iteration 60, loss = 0.47601949\n",
      "Iteration 61, loss = 0.47469841\n",
      "Iteration 62, loss = 0.47326943\n",
      "Iteration 63, loss = 0.47188034\n",
      "Iteration 64, loss = 0.47032352\n",
      "Iteration 65, loss = 0.46908469\n",
      "Iteration 66, loss = 0.46791272\n",
      "Iteration 67, loss = 0.46653158\n",
      "Iteration 68, loss = 0.46537357\n",
      "Iteration 69, loss = 0.46426879\n",
      "Iteration 70, loss = 0.46320669\n",
      "Iteration 71, loss = 0.46210964\n",
      "Iteration 72, loss = 0.46098074\n",
      "Iteration 73, loss = 0.45997013\n",
      "Iteration 74, loss = 0.45922505\n",
      "Iteration 75, loss = 0.45817893\n",
      "Iteration 76, loss = 0.45735841\n",
      "Iteration 77, loss = 0.45648275\n",
      "Iteration 78, loss = 0.45568195\n",
      "Iteration 79, loss = 0.45490239\n",
      "Iteration 80, loss = 0.45425118\n",
      "Iteration 81, loss = 0.45359398\n",
      "Iteration 82, loss = 0.45282267\n",
      "Iteration 83, loss = 0.45242161\n",
      "Iteration 84, loss = 0.45160187\n",
      "Iteration 85, loss = 0.45107666\n",
      "Iteration 86, loss = 0.45049118\n",
      "Iteration 87, loss = 0.44999685\n",
      "Iteration 88, loss = 0.44957309\n",
      "Iteration 89, loss = 0.44905628\n",
      "Iteration 90, loss = 0.44858045\n",
      "Iteration 91, loss = 0.44823742\n",
      "Iteration 92, loss = 0.44782011\n",
      "Iteration 93, loss = 0.44743271\n",
      "Iteration 94, loss = 0.44714606\n",
      "Iteration 95, loss = 0.44676941\n",
      "Iteration 96, loss = 0.44644527\n",
      "Iteration 97, loss = 0.44627703\n",
      "Iteration 98, loss = 0.44606452\n",
      "Iteration 99, loss = 0.44560423\n",
      "Iteration 100, loss = 0.44534227\n",
      "Iteration 101, loss = 0.44518509\n",
      "Iteration 102, loss = 0.44487673\n",
      "Iteration 103, loss = 0.44468494\n",
      "Iteration 104, loss = 0.44456776\n",
      "Iteration 105, loss = 0.44435352\n",
      "Iteration 106, loss = 0.44408991\n",
      "Iteration 107, loss = 0.44396082\n",
      "Iteration 108, loss = 0.44380584\n",
      "Iteration 109, loss = 0.44367538\n",
      "Iteration 110, loss = 0.44357390\n",
      "Iteration 111, loss = 0.44341511\n",
      "Iteration 112, loss = 0.44326447\n",
      "Iteration 113, loss = 0.44318574\n",
      "Iteration 114, loss = 0.44310812\n",
      "Iteration 115, loss = 0.44293504\n",
      "Iteration 116, loss = 0.44285602\n",
      "Iteration 117, loss = 0.44278495\n",
      "Iteration 118, loss = 0.44275318\n",
      "Iteration 119, loss = 0.44259253\n",
      "Iteration 120, loss = 0.44256380\n",
      "Iteration 121, loss = 0.44245025\n",
      "Iteration 122, loss = 0.44244556\n",
      "Iteration 123, loss = 0.44225928\n",
      "Iteration 124, loss = 0.44223913\n",
      "Iteration 125, loss = 0.44221523\n",
      "Iteration 126, loss = 0.44212902\n",
      "Iteration 127, loss = 0.44205374\n",
      "Iteration 128, loss = 0.44198175\n",
      "Iteration 129, loss = 0.44196878\n",
      "Iteration 130, loss = 0.44186242\n",
      "Iteration 131, loss = 0.44185734\n",
      "Iteration 132, loss = 0.44177011\n",
      "Iteration 133, loss = 0.44180303\n",
      "Iteration 134, loss = 0.44171155\n",
      "Iteration 135, loss = 0.44172393\n",
      "Iteration 136, loss = 0.44162707\n",
      "Iteration 137, loss = 0.44161362\n",
      "Iteration 138, loss = 0.44160114\n",
      "Iteration 139, loss = 0.44160080\n",
      "Iteration 140, loss = 0.44148412\n",
      "Iteration 141, loss = 0.44152522\n",
      "Iteration 142, loss = 0.44145325\n",
      "Iteration 143, loss = 0.44137026\n",
      "Iteration 144, loss = 0.44143605\n",
      "Iteration 145, loss = 0.44135129\n",
      "Iteration 146, loss = 0.44139084\n",
      "Iteration 147, loss = 0.44157533\n",
      "Iteration 148, loss = 0.44140701\n",
      "Iteration 149, loss = 0.44138117\n",
      "Iteration 150, loss = 0.44122908\n",
      "Iteration 151, loss = 0.44119782\n",
      "Iteration 152, loss = 0.44121820\n",
      "Iteration 153, loss = 0.44111427\n",
      "Iteration 154, loss = 0.44114959\n",
      "Iteration 155, loss = 0.44123078\n",
      "Iteration 156, loss = 0.44112461\n",
      "Iteration 157, loss = 0.44103035\n",
      "Iteration 158, loss = 0.44108548\n",
      "Iteration 159, loss = 0.44105817\n",
      "Iteration 160, loss = 0.44095167\n",
      "Iteration 161, loss = 0.44096099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75444020\n",
      "Iteration 2, loss = 0.72079191\n",
      "Iteration 3, loss = 0.69132973\n",
      "Iteration 4, loss = 0.67066670\n",
      "Iteration 5, loss = 0.65464324\n",
      "Iteration 6, loss = 0.64375024\n",
      "Iteration 7, loss = 0.63604443\n",
      "Iteration 8, loss = 0.63108585\n",
      "Iteration 9, loss = 0.62670886\n",
      "Iteration 10, loss = 0.62240109\n",
      "Iteration 11, loss = 0.61829974\n",
      "Iteration 12, loss = 0.61355972\n",
      "Iteration 13, loss = 0.60888356\n",
      "Iteration 14, loss = 0.60484404\n",
      "Iteration 15, loss = 0.59943836\n",
      "Iteration 16, loss = 0.59505806\n",
      "Iteration 17, loss = 0.59111465\n",
      "Iteration 18, loss = 0.58704019\n",
      "Iteration 19, loss = 0.58342996\n",
      "Iteration 20, loss = 0.57936906\n",
      "Iteration 21, loss = 0.57587180\n",
      "Iteration 22, loss = 0.57233166\n",
      "Iteration 23, loss = 0.56856976\n",
      "Iteration 24, loss = 0.56511147\n",
      "Iteration 25, loss = 0.56178168\n",
      "Iteration 26, loss = 0.55826623\n",
      "Iteration 27, loss = 0.55489229\n",
      "Iteration 28, loss = 0.55176065\n",
      "Iteration 29, loss = 0.54843885\n",
      "Iteration 30, loss = 0.54537277\n",
      "Iteration 31, loss = 0.54219596\n",
      "Iteration 32, loss = 0.53918768\n",
      "Iteration 33, loss = 0.53621091\n",
      "Iteration 34, loss = 0.53320861\n",
      "Iteration 35, loss = 0.53040821\n",
      "Iteration 36, loss = 0.52753033\n",
      "Iteration 37, loss = 0.52470563\n",
      "Iteration 38, loss = 0.52186491\n",
      "Iteration 39, loss = 0.51915870\n",
      "Iteration 40, loss = 0.51654378\n",
      "Iteration 41, loss = 0.51383848\n",
      "Iteration 42, loss = 0.51135500\n",
      "Iteration 43, loss = 0.50872395\n",
      "Iteration 44, loss = 0.50639549\n",
      "Iteration 45, loss = 0.50380059\n",
      "Iteration 46, loss = 0.50144683\n",
      "Iteration 47, loss = 0.49898552\n",
      "Iteration 48, loss = 0.49663717\n",
      "Iteration 49, loss = 0.49435318\n",
      "Iteration 50, loss = 0.49226919\n",
      "Iteration 51, loss = 0.48992562\n",
      "Iteration 52, loss = 0.48769030\n",
      "Iteration 53, loss = 0.48571982\n",
      "Iteration 54, loss = 0.48367945\n",
      "Iteration 55, loss = 0.48155506\n",
      "Iteration 56, loss = 0.47956085\n",
      "Iteration 57, loss = 0.47769226\n",
      "Iteration 58, loss = 0.47586896\n",
      "Iteration 59, loss = 0.47391801\n",
      "Iteration 60, loss = 0.47209491\n",
      "Iteration 61, loss = 0.47034251\n",
      "Iteration 62, loss = 0.46878014\n",
      "Iteration 63, loss = 0.46702267\n",
      "Iteration 64, loss = 0.46542882\n",
      "Iteration 65, loss = 0.46386134\n",
      "Iteration 66, loss = 0.46239639\n",
      "Iteration 67, loss = 0.46105115\n",
      "Iteration 68, loss = 0.45953369\n",
      "Iteration 69, loss = 0.45817591\n",
      "Iteration 70, loss = 0.45707131\n",
      "Iteration 71, loss = 0.45563920\n",
      "Iteration 72, loss = 0.45446722\n",
      "Iteration 73, loss = 0.45330172\n",
      "Iteration 74, loss = 0.45214421\n",
      "Iteration 75, loss = 0.45107446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.45013237\n",
      "Iteration 77, loss = 0.44907129\n",
      "Iteration 78, loss = 0.44825778\n",
      "Iteration 79, loss = 0.44727599\n",
      "Iteration 80, loss = 0.44633586\n",
      "Iteration 81, loss = 0.44557830\n",
      "Iteration 82, loss = 0.44489782\n",
      "Iteration 83, loss = 0.44411638\n",
      "Iteration 84, loss = 0.44332306\n",
      "Iteration 85, loss = 0.44276850\n",
      "Iteration 86, loss = 0.44201414\n",
      "Iteration 87, loss = 0.44142106\n",
      "Iteration 88, loss = 0.44100529\n",
      "Iteration 89, loss = 0.44029697\n",
      "Iteration 90, loss = 0.43976138\n",
      "Iteration 91, loss = 0.43928973\n",
      "Iteration 92, loss = 0.43892008\n",
      "Iteration 93, loss = 0.43831875\n",
      "Iteration 94, loss = 0.43799770\n",
      "Iteration 95, loss = 0.43743843\n",
      "Iteration 96, loss = 0.43708865\n",
      "Iteration 97, loss = 0.43672649\n",
      "Iteration 98, loss = 0.43636396\n",
      "Iteration 99, loss = 0.43599833\n",
      "Iteration 100, loss = 0.43568348\n",
      "Iteration 101, loss = 0.43550428\n",
      "Iteration 102, loss = 0.43512419\n",
      "Iteration 103, loss = 0.43485720\n",
      "Iteration 104, loss = 0.43460773\n",
      "Iteration 105, loss = 0.43432927\n",
      "Iteration 106, loss = 0.43409796\n",
      "Iteration 107, loss = 0.43386657\n",
      "Iteration 108, loss = 0.43367218\n",
      "Iteration 109, loss = 0.43342045\n",
      "Iteration 110, loss = 0.43322045\n",
      "Iteration 111, loss = 0.43310622\n",
      "Iteration 112, loss = 0.43286731\n",
      "Iteration 113, loss = 0.43268803\n",
      "Iteration 114, loss = 0.43253608\n",
      "Iteration 115, loss = 0.43243650\n",
      "Iteration 116, loss = 0.43220956\n",
      "Iteration 117, loss = 0.43206634\n",
      "Iteration 118, loss = 0.43191956\n",
      "Iteration 119, loss = 0.43178763\n",
      "Iteration 120, loss = 0.43164412\n",
      "Iteration 121, loss = 0.43151645\n",
      "Iteration 122, loss = 0.43142150\n",
      "Iteration 123, loss = 0.43127759\n",
      "Iteration 124, loss = 0.43115773\n",
      "Iteration 125, loss = 0.43114534\n",
      "Iteration 126, loss = 0.43093599\n",
      "Iteration 127, loss = 0.43082946\n",
      "Iteration 128, loss = 0.43078141\n",
      "Iteration 129, loss = 0.43066651\n",
      "Iteration 130, loss = 0.43061956\n",
      "Iteration 131, loss = 0.43046119\n",
      "Iteration 132, loss = 0.43046639\n",
      "Iteration 133, loss = 0.43030707\n",
      "Iteration 134, loss = 0.43021948\n",
      "Iteration 135, loss = 0.43014286\n",
      "Iteration 136, loss = 0.43010670\n",
      "Iteration 137, loss = 0.42997695\n",
      "Iteration 138, loss = 0.42991585\n",
      "Iteration 139, loss = 0.42986964\n",
      "Iteration 140, loss = 0.42983249\n",
      "Iteration 141, loss = 0.42970377\n",
      "Iteration 142, loss = 0.42980624\n",
      "Iteration 143, loss = 0.42973259\n",
      "Iteration 144, loss = 0.42955845\n",
      "Iteration 145, loss = 0.42948483\n",
      "Iteration 146, loss = 0.42946687\n",
      "Iteration 147, loss = 0.42941913\n",
      "Iteration 148, loss = 0.42932900\n",
      "Iteration 149, loss = 0.42926418\n",
      "Iteration 150, loss = 0.42915228\n",
      "Iteration 151, loss = 0.42909930\n",
      "Iteration 152, loss = 0.42907815\n",
      "Iteration 153, loss = 0.42918192\n",
      "Iteration 154, loss = 0.42896225\n",
      "Iteration 155, loss = 0.42888404\n",
      "Iteration 156, loss = 0.42887269\n",
      "Iteration 157, loss = 0.42882675\n",
      "Iteration 158, loss = 0.42882008\n",
      "Iteration 159, loss = 0.42886538\n",
      "Iteration 160, loss = 0.42873885\n",
      "Iteration 161, loss = 0.42866034\n",
      "Iteration 162, loss = 0.42863731\n",
      "Iteration 163, loss = 0.42858281\n",
      "Iteration 164, loss = 0.42865439\n",
      "Iteration 165, loss = 0.42855688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75362886\n",
      "Iteration 2, loss = 0.72026850\n",
      "Iteration 3, loss = 0.69057632\n",
      "Iteration 4, loss = 0.67002073\n",
      "Iteration 5, loss = 0.65396109\n",
      "Iteration 6, loss = 0.64326605\n",
      "Iteration 7, loss = 0.63562367\n",
      "Iteration 8, loss = 0.63044223\n",
      "Iteration 9, loss = 0.62606230\n",
      "Iteration 10, loss = 0.62163862\n",
      "Iteration 11, loss = 0.61732580\n",
      "Iteration 12, loss = 0.61260433\n",
      "Iteration 13, loss = 0.60763690\n",
      "Iteration 14, loss = 0.60359660\n",
      "Iteration 15, loss = 0.59812059\n",
      "Iteration 16, loss = 0.59360690\n",
      "Iteration 17, loss = 0.58940745\n",
      "Iteration 18, loss = 0.58541073\n",
      "Iteration 19, loss = 0.58165104\n",
      "Iteration 20, loss = 0.57751667\n",
      "Iteration 21, loss = 0.57362950\n",
      "Iteration 22, loss = 0.56994100\n",
      "Iteration 23, loss = 0.56614372\n",
      "Iteration 24, loss = 0.56254598\n",
      "Iteration 25, loss = 0.55895038\n",
      "Iteration 26, loss = 0.55537872\n",
      "Iteration 27, loss = 0.55174342\n",
      "Iteration 28, loss = 0.54847432\n",
      "Iteration 29, loss = 0.54500571\n",
      "Iteration 30, loss = 0.54173366\n",
      "Iteration 31, loss = 0.53849302\n",
      "Iteration 32, loss = 0.53521941\n",
      "Iteration 33, loss = 0.53204391\n",
      "Iteration 34, loss = 0.52882213\n",
      "Iteration 35, loss = 0.52583879\n",
      "Iteration 36, loss = 0.52282838\n",
      "Iteration 37, loss = 0.51983086\n",
      "Iteration 38, loss = 0.51687956\n",
      "Iteration 39, loss = 0.51397134\n",
      "Iteration 40, loss = 0.51113642\n",
      "Iteration 41, loss = 0.50839865\n",
      "Iteration 42, loss = 0.50567122\n",
      "Iteration 43, loss = 0.50297938\n",
      "Iteration 44, loss = 0.50030305\n",
      "Iteration 45, loss = 0.49782306\n",
      "Iteration 46, loss = 0.49516841\n",
      "Iteration 47, loss = 0.49269494\n",
      "Iteration 48, loss = 0.49021719\n",
      "Iteration 49, loss = 0.48788132\n",
      "Iteration 50, loss = 0.48546544\n",
      "Iteration 51, loss = 0.48313934\n",
      "Iteration 52, loss = 0.48089180\n",
      "Iteration 53, loss = 0.47891173\n",
      "Iteration 54, loss = 0.47657461\n",
      "Iteration 55, loss = 0.47442328\n",
      "Iteration 56, loss = 0.47229133\n",
      "Iteration 57, loss = 0.47040311\n",
      "Iteration 58, loss = 0.46858256\n",
      "Iteration 59, loss = 0.46652004\n",
      "Iteration 60, loss = 0.46459007\n",
      "Iteration 61, loss = 0.46291742\n",
      "Iteration 62, loss = 0.46126887\n",
      "Iteration 63, loss = 0.45956634\n",
      "Iteration 64, loss = 0.45787727\n",
      "Iteration 65, loss = 0.45621947\n",
      "Iteration 66, loss = 0.45474052\n",
      "Iteration 67, loss = 0.45329894\n",
      "Iteration 68, loss = 0.45188296\n",
      "Iteration 69, loss = 0.45041311\n",
      "Iteration 70, loss = 0.44927046\n",
      "Iteration 71, loss = 0.44786594\n",
      "Iteration 72, loss = 0.44667908\n",
      "Iteration 73, loss = 0.44550026\n",
      "Iteration 74, loss = 0.44428840\n",
      "Iteration 75, loss = 0.44322088\n",
      "Iteration 76, loss = 0.44224375\n",
      "Iteration 77, loss = 0.44126621\n",
      "Iteration 78, loss = 0.44037198\n",
      "Iteration 79, loss = 0.43938013\n",
      "Iteration 80, loss = 0.43853742\n",
      "Iteration 81, loss = 0.43770411\n",
      "Iteration 82, loss = 0.43699362\n",
      "Iteration 83, loss = 0.43632604\n",
      "Iteration 84, loss = 0.43554872\n",
      "Iteration 85, loss = 0.43492868\n",
      "Iteration 86, loss = 0.43417518\n",
      "Iteration 87, loss = 0.43368377\n",
      "Iteration 88, loss = 0.43315179\n",
      "Iteration 89, loss = 0.43256075\n",
      "Iteration 90, loss = 0.43203849\n",
      "Iteration 91, loss = 0.43152416\n",
      "Iteration 92, loss = 0.43110186\n",
      "Iteration 93, loss = 0.43062331\n",
      "Iteration 94, loss = 0.43029511\n",
      "Iteration 95, loss = 0.42984887\n",
      "Iteration 96, loss = 0.42953369\n",
      "Iteration 97, loss = 0.42916627\n",
      "Iteration 98, loss = 0.42881081\n",
      "Iteration 99, loss = 0.42853085\n",
      "Iteration 100, loss = 0.42821861\n",
      "Iteration 101, loss = 0.42809270\n",
      "Iteration 102, loss = 0.42772642\n",
      "Iteration 103, loss = 0.42751061\n",
      "Iteration 104, loss = 0.42727213\n",
      "Iteration 105, loss = 0.42703912\n",
      "Iteration 106, loss = 0.42688077\n",
      "Iteration 107, loss = 0.42662429\n",
      "Iteration 108, loss = 0.42649499\n",
      "Iteration 109, loss = 0.42635068\n",
      "Iteration 110, loss = 0.42618304\n",
      "Iteration 111, loss = 0.42599325\n",
      "Iteration 112, loss = 0.42588711\n",
      "Iteration 113, loss = 0.42573917\n",
      "Iteration 114, loss = 0.42561599\n",
      "Iteration 115, loss = 0.42549609\n",
      "Iteration 116, loss = 0.42534833\n",
      "Iteration 117, loss = 0.42524966\n",
      "Iteration 118, loss = 0.42521313\n",
      "Iteration 119, loss = 0.42515206\n",
      "Iteration 120, loss = 0.42501976\n",
      "Iteration 121, loss = 0.42496601\n",
      "Iteration 122, loss = 0.42504411\n",
      "Iteration 123, loss = 0.42478144\n",
      "Iteration 124, loss = 0.42473776\n",
      "Iteration 125, loss = 0.42460430\n",
      "Iteration 126, loss = 0.42453726\n",
      "Iteration 127, loss = 0.42450580\n",
      "Iteration 128, loss = 0.42447580\n",
      "Iteration 129, loss = 0.42440982\n",
      "Iteration 130, loss = 0.42439379\n",
      "Iteration 131, loss = 0.42431873\n",
      "Iteration 132, loss = 0.42425364\n",
      "Iteration 133, loss = 0.42422432\n",
      "Iteration 134, loss = 0.42413699\n",
      "Iteration 135, loss = 0.42409407\n",
      "Iteration 136, loss = 0.42406124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75435554\n",
      "Iteration 2, loss = 0.71980844\n",
      "Iteration 3, loss = 0.69073607\n",
      "Iteration 4, loss = 0.66889749\n",
      "Iteration 5, loss = 0.65261334\n",
      "Iteration 6, loss = 0.64319081\n",
      "Iteration 7, loss = 0.63500209\n",
      "Iteration 8, loss = 0.63018267\n",
      "Iteration 9, loss = 0.62585370\n",
      "Iteration 10, loss = 0.62160591\n",
      "Iteration 11, loss = 0.61734006\n",
      "Iteration 12, loss = 0.61283159\n",
      "Iteration 13, loss = 0.60790511\n",
      "Iteration 14, loss = 0.60385519\n",
      "Iteration 15, loss = 0.59862172\n",
      "Iteration 16, loss = 0.59431808\n",
      "Iteration 17, loss = 0.59001876\n",
      "Iteration 18, loss = 0.58613508\n",
      "Iteration 19, loss = 0.58265626\n",
      "Iteration 20, loss = 0.57875558\n",
      "Iteration 21, loss = 0.57522188\n",
      "Iteration 22, loss = 0.57157280\n",
      "Iteration 23, loss = 0.56810250\n",
      "Iteration 24, loss = 0.56457174\n",
      "Iteration 25, loss = 0.56116713\n",
      "Iteration 26, loss = 0.55782356\n",
      "Iteration 27, loss = 0.55458169\n",
      "Iteration 28, loss = 0.55150923\n",
      "Iteration 29, loss = 0.54839592\n",
      "Iteration 30, loss = 0.54532762\n",
      "Iteration 31, loss = 0.54240125\n",
      "Iteration 32, loss = 0.53941125\n",
      "Iteration 33, loss = 0.53655181\n",
      "Iteration 34, loss = 0.53374337\n",
      "Iteration 35, loss = 0.53100679\n",
      "Iteration 36, loss = 0.52845906\n",
      "Iteration 37, loss = 0.52570156\n",
      "Iteration 38, loss = 0.52325583\n",
      "Iteration 39, loss = 0.52068731\n",
      "Iteration 40, loss = 0.51825183\n",
      "Iteration 41, loss = 0.51594010\n",
      "Iteration 42, loss = 0.51356627\n",
      "Iteration 43, loss = 0.51124326\n",
      "Iteration 44, loss = 0.50911821\n",
      "Iteration 45, loss = 0.50700852\n",
      "Iteration 46, loss = 0.50475720\n",
      "Iteration 47, loss = 0.50278563\n",
      "Iteration 48, loss = 0.50077988\n",
      "Iteration 49, loss = 0.49880306\n",
      "Iteration 50, loss = 0.49694648\n",
      "Iteration 51, loss = 0.49508936\n",
      "Iteration 52, loss = 0.49325617\n",
      "Iteration 53, loss = 0.49169169\n",
      "Iteration 54, loss = 0.48983417\n",
      "Iteration 55, loss = 0.48815946\n",
      "Iteration 56, loss = 0.48646298\n",
      "Iteration 57, loss = 0.48508048\n",
      "Iteration 58, loss = 0.48372644\n",
      "Iteration 59, loss = 0.48204651\n",
      "Iteration 60, loss = 0.48068479\n",
      "Iteration 61, loss = 0.47918528\n",
      "Iteration 62, loss = 0.47797860\n",
      "Iteration 63, loss = 0.47673054\n",
      "Iteration 64, loss = 0.47546050\n",
      "Iteration 65, loss = 0.47438448\n",
      "Iteration 66, loss = 0.47318661\n",
      "Iteration 67, loss = 0.47204700\n",
      "Iteration 68, loss = 0.47107030\n",
      "Iteration 69, loss = 0.46996996\n",
      "Iteration 70, loss = 0.46900151\n",
      "Iteration 71, loss = 0.46812955\n",
      "Iteration 72, loss = 0.46720457\n",
      "Iteration 73, loss = 0.46629510\n",
      "Iteration 74, loss = 0.46536800\n",
      "Iteration 75, loss = 0.46467115\n",
      "Iteration 76, loss = 0.46391913\n",
      "Iteration 77, loss = 0.46332549\n",
      "Iteration 78, loss = 0.46264309\n",
      "Iteration 79, loss = 0.46185566\n",
      "Iteration 80, loss = 0.46128658\n",
      "Iteration 81, loss = 0.46064479\n",
      "Iteration 82, loss = 0.46013041\n",
      "Iteration 83, loss = 0.45983866\n",
      "Iteration 84, loss = 0.45918918\n",
      "Iteration 85, loss = 0.45872000\n",
      "Iteration 86, loss = 0.45816153\n",
      "Iteration 87, loss = 0.45792890\n",
      "Iteration 88, loss = 0.45753980\n",
      "Iteration 89, loss = 0.45702871\n",
      "Iteration 90, loss = 0.45665949\n",
      "Iteration 91, loss = 0.45632853\n",
      "Iteration 92, loss = 0.45601331\n",
      "Iteration 93, loss = 0.45566340\n",
      "Iteration 94, loss = 0.45543855\n",
      "Iteration 95, loss = 0.45514239\n",
      "Iteration 96, loss = 0.45490944\n",
      "Iteration 97, loss = 0.45467700\n",
      "Iteration 98, loss = 0.45438451\n",
      "Iteration 99, loss = 0.45416088\n",
      "Iteration 100, loss = 0.45395368\n",
      "Iteration 101, loss = 0.45387355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 102, loss = 0.45364006\n",
      "Iteration 103, loss = 0.45350753\n",
      "Iteration 104, loss = 0.45322527\n",
      "Iteration 105, loss = 0.45309944\n",
      "Iteration 106, loss = 0.45299565\n",
      "Iteration 107, loss = 0.45279014\n",
      "Iteration 108, loss = 0.45273746\n",
      "Iteration 109, loss = 0.45261824\n",
      "Iteration 110, loss = 0.45245495\n",
      "Iteration 111, loss = 0.45227056\n",
      "Iteration 112, loss = 0.45216901\n",
      "Iteration 113, loss = 0.45202041\n",
      "Iteration 114, loss = 0.45192837\n",
      "Iteration 115, loss = 0.45187223\n",
      "Iteration 116, loss = 0.45175469\n",
      "Iteration 117, loss = 0.45165710\n",
      "Iteration 118, loss = 0.45160038\n",
      "Iteration 119, loss = 0.45156001\n",
      "Iteration 120, loss = 0.45147986\n",
      "Iteration 121, loss = 0.45135781\n",
      "Iteration 122, loss = 0.45142805\n",
      "Iteration 123, loss = 0.45129306\n",
      "Iteration 124, loss = 0.45116098\n",
      "Iteration 125, loss = 0.45105255\n",
      "Iteration 126, loss = 0.45106230\n",
      "Iteration 127, loss = 0.45096272\n",
      "Iteration 128, loss = 0.45092239\n",
      "Iteration 129, loss = 0.45086107\n",
      "Iteration 130, loss = 0.45087784\n",
      "Iteration 131, loss = 0.45074691\n",
      "Iteration 132, loss = 0.45068362\n",
      "Iteration 133, loss = 0.45068702\n",
      "Iteration 134, loss = 0.45059044\n",
      "Iteration 135, loss = 0.45057672\n",
      "Iteration 136, loss = 0.45056235\n",
      "Iteration 137, loss = 0.45056909\n",
      "Iteration 138, loss = 0.45039819\n",
      "Iteration 139, loss = 0.45041531\n",
      "Iteration 140, loss = 0.45043752\n",
      "Iteration 141, loss = 0.45033969\n",
      "Iteration 142, loss = 0.45042231\n",
      "Iteration 143, loss = 0.45024831\n",
      "Iteration 144, loss = 0.45021885\n",
      "Iteration 145, loss = 0.45019585\n",
      "Iteration 146, loss = 0.45024287\n",
      "Iteration 147, loss = 0.45023227\n",
      "Iteration 148, loss = 0.45007078\n",
      "Iteration 149, loss = 0.45001976\n",
      "Iteration 150, loss = 0.45003126\n",
      "Iteration 151, loss = 0.45000600\n",
      "Iteration 152, loss = 0.45002232\n",
      "Iteration 153, loss = 0.45010018\n",
      "Iteration 154, loss = 0.44992841\n",
      "Iteration 155, loss = 0.44989144\n",
      "Iteration 156, loss = 0.44979414\n",
      "Iteration 157, loss = 0.44979617\n",
      "Iteration 158, loss = 0.44976495\n",
      "Iteration 159, loss = 0.44971680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74993101\n",
      "Iteration 2, loss = 0.71550939\n",
      "Iteration 3, loss = 0.68670283\n",
      "Iteration 4, loss = 0.66351435\n",
      "Iteration 5, loss = 0.64794291\n",
      "Iteration 6, loss = 0.63665932\n",
      "Iteration 7, loss = 0.62860531\n",
      "Iteration 8, loss = 0.62359627\n",
      "Iteration 9, loss = 0.61854122\n",
      "Iteration 10, loss = 0.61380692\n",
      "Iteration 11, loss = 0.60906157\n",
      "Iteration 12, loss = 0.60349963\n",
      "Iteration 13, loss = 0.59741594\n",
      "Iteration 14, loss = 0.59205438\n",
      "Iteration 15, loss = 0.58665197\n",
      "Iteration 16, loss = 0.58154973\n",
      "Iteration 17, loss = 0.57666187\n",
      "Iteration 18, loss = 0.57203756\n",
      "Iteration 19, loss = 0.56765495\n",
      "Iteration 20, loss = 0.56331483\n",
      "Iteration 21, loss = 0.55898559\n",
      "Iteration 22, loss = 0.55474342\n",
      "Iteration 23, loss = 0.55047653\n",
      "Iteration 24, loss = 0.54626871\n",
      "Iteration 25, loss = 0.54219546\n",
      "Iteration 26, loss = 0.53837935\n",
      "Iteration 27, loss = 0.53438676\n",
      "Iteration 28, loss = 0.53055504\n",
      "Iteration 29, loss = 0.52692651\n",
      "Iteration 30, loss = 0.52316827\n",
      "Iteration 31, loss = 0.51968682\n",
      "Iteration 32, loss = 0.51632622\n",
      "Iteration 33, loss = 0.51276269\n",
      "Iteration 34, loss = 0.50937721\n",
      "Iteration 35, loss = 0.50610568\n",
      "Iteration 36, loss = 0.50315064\n",
      "Iteration 37, loss = 0.49992921\n",
      "Iteration 38, loss = 0.49695672\n",
      "Iteration 39, loss = 0.49404883\n",
      "Iteration 40, loss = 0.49113210\n",
      "Iteration 41, loss = 0.48827787\n",
      "Iteration 42, loss = 0.48558860\n",
      "Iteration 43, loss = 0.48295642\n",
      "Iteration 44, loss = 0.48036372\n",
      "Iteration 45, loss = 0.47784815\n",
      "Iteration 46, loss = 0.47538638\n",
      "Iteration 47, loss = 0.47307573\n",
      "Iteration 48, loss = 0.47060542\n",
      "Iteration 49, loss = 0.46829767\n",
      "Iteration 50, loss = 0.46626253\n",
      "Iteration 51, loss = 0.46405635\n",
      "Iteration 52, loss = 0.46202013\n",
      "Iteration 53, loss = 0.45987386\n",
      "Iteration 54, loss = 0.45801768\n",
      "Iteration 55, loss = 0.45614290\n",
      "Iteration 56, loss = 0.45424208\n",
      "Iteration 57, loss = 0.45245082\n",
      "Iteration 58, loss = 0.45071445\n",
      "Iteration 59, loss = 0.44899655\n",
      "Iteration 60, loss = 0.44737103\n",
      "Iteration 61, loss = 0.44578353\n",
      "Iteration 62, loss = 0.44437441\n",
      "Iteration 63, loss = 0.44280887\n",
      "Iteration 64, loss = 0.44139341\n",
      "Iteration 65, loss = 0.43996388\n",
      "Iteration 66, loss = 0.43857719\n",
      "Iteration 67, loss = 0.43730924\n",
      "Iteration 68, loss = 0.43609323\n",
      "Iteration 69, loss = 0.43493084\n",
      "Iteration 70, loss = 0.43380933\n",
      "Iteration 71, loss = 0.43269282\n",
      "Iteration 72, loss = 0.43160364\n",
      "Iteration 73, loss = 0.43054220\n",
      "Iteration 74, loss = 0.42967028\n",
      "Iteration 75, loss = 0.42876600\n",
      "Iteration 76, loss = 0.42783435\n",
      "Iteration 77, loss = 0.42694520\n",
      "Iteration 78, loss = 0.42615582\n",
      "Iteration 79, loss = 0.42542904\n",
      "Iteration 80, loss = 0.42482963\n",
      "Iteration 81, loss = 0.42409217\n",
      "Iteration 82, loss = 0.42333649\n",
      "Iteration 83, loss = 0.42281841\n",
      "Iteration 84, loss = 0.42215229\n",
      "Iteration 85, loss = 0.42160505\n",
      "Iteration 86, loss = 0.42098623\n",
      "Iteration 87, loss = 0.42052684\n",
      "Iteration 88, loss = 0.42015720\n",
      "Iteration 89, loss = 0.41956998\n",
      "Iteration 90, loss = 0.41908822\n",
      "Iteration 91, loss = 0.41882429\n",
      "Iteration 92, loss = 0.41845307\n",
      "Iteration 93, loss = 0.41804680\n",
      "Iteration 94, loss = 0.41782106\n",
      "Iteration 95, loss = 0.41738883\n",
      "Iteration 96, loss = 0.41715819\n",
      "Iteration 97, loss = 0.41681941\n",
      "Iteration 98, loss = 0.41660758\n",
      "Iteration 99, loss = 0.41628060\n",
      "Iteration 100, loss = 0.41601642\n",
      "Iteration 101, loss = 0.41588169\n",
      "Iteration 102, loss = 0.41557166\n",
      "Iteration 103, loss = 0.41543078\n",
      "Iteration 104, loss = 0.41524081\n",
      "Iteration 105, loss = 0.41502447\n",
      "Iteration 106, loss = 0.41483971\n",
      "Iteration 107, loss = 0.41476407\n",
      "Iteration 108, loss = 0.41464082\n",
      "Iteration 109, loss = 0.41443725\n",
      "Iteration 110, loss = 0.41429109\n",
      "Iteration 111, loss = 0.41415746\n",
      "Iteration 112, loss = 0.41405575\n",
      "Iteration 113, loss = 0.41395125\n",
      "Iteration 114, loss = 0.41383992\n",
      "Iteration 115, loss = 0.41374234\n",
      "Iteration 116, loss = 0.41363481\n",
      "Iteration 117, loss = 0.41358074\n",
      "Iteration 118, loss = 0.41349248\n",
      "Iteration 119, loss = 0.41338499\n",
      "Iteration 120, loss = 0.41329340\n",
      "Iteration 121, loss = 0.41323332\n",
      "Iteration 122, loss = 0.41327388\n",
      "Iteration 123, loss = 0.41306605\n",
      "Iteration 124, loss = 0.41314856\n",
      "Iteration 125, loss = 0.41293829\n",
      "Iteration 126, loss = 0.41291957\n",
      "Iteration 127, loss = 0.41284407\n",
      "Iteration 128, loss = 0.41280873\n",
      "Iteration 129, loss = 0.41279043\n",
      "Iteration 130, loss = 0.41266251\n",
      "Iteration 131, loss = 0.41270484\n",
      "Iteration 132, loss = 0.41266914\n",
      "Iteration 133, loss = 0.41264940\n",
      "Iteration 134, loss = 0.41251214\n",
      "Iteration 135, loss = 0.41252415\n",
      "Iteration 136, loss = 0.41244850\n",
      "Iteration 137, loss = 0.41247360\n",
      "Iteration 138, loss = 0.41235400\n",
      "Iteration 139, loss = 0.41239995\n",
      "Iteration 140, loss = 0.41231733\n",
      "Iteration 141, loss = 0.41227166\n",
      "Iteration 142, loss = 0.41223831\n",
      "Iteration 143, loss = 0.41219911\n",
      "Iteration 144, loss = 0.41214970\n",
      "Iteration 145, loss = 0.41227467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75092189\n",
      "Iteration 2, loss = 0.71641932\n",
      "Iteration 3, loss = 0.68726260\n",
      "Iteration 4, loss = 0.66525902\n",
      "Iteration 5, loss = 0.65048560\n",
      "Iteration 6, loss = 0.63946446\n",
      "Iteration 7, loss = 0.63151104\n",
      "Iteration 8, loss = 0.62750761\n",
      "Iteration 9, loss = 0.62267156\n",
      "Iteration 10, loss = 0.61826583\n",
      "Iteration 11, loss = 0.61395613\n",
      "Iteration 12, loss = 0.60915936\n",
      "Iteration 13, loss = 0.60365656\n",
      "Iteration 14, loss = 0.59896714\n",
      "Iteration 15, loss = 0.59418770\n",
      "Iteration 16, loss = 0.58969690\n",
      "Iteration 17, loss = 0.58550321\n",
      "Iteration 18, loss = 0.58136654\n",
      "Iteration 19, loss = 0.57760081\n",
      "Iteration 20, loss = 0.57381393\n",
      "Iteration 21, loss = 0.57009469\n",
      "Iteration 22, loss = 0.56648420\n",
      "Iteration 23, loss = 0.56284254\n",
      "Iteration 24, loss = 0.55927832\n",
      "Iteration 25, loss = 0.55575058\n",
      "Iteration 26, loss = 0.55244898\n",
      "Iteration 27, loss = 0.54915406\n",
      "Iteration 28, loss = 0.54585866\n",
      "Iteration 29, loss = 0.54268227\n",
      "Iteration 30, loss = 0.53944638\n",
      "Iteration 31, loss = 0.53636218\n",
      "Iteration 32, loss = 0.53365107\n",
      "Iteration 33, loss = 0.53054526\n",
      "Iteration 34, loss = 0.52758342\n",
      "Iteration 35, loss = 0.52474897\n",
      "Iteration 36, loss = 0.52222344\n",
      "Iteration 37, loss = 0.51934380\n",
      "Iteration 38, loss = 0.51680809\n",
      "Iteration 39, loss = 0.51426468\n",
      "Iteration 40, loss = 0.51172860\n",
      "Iteration 41, loss = 0.50915993\n",
      "Iteration 42, loss = 0.50675182\n",
      "Iteration 43, loss = 0.50440073\n",
      "Iteration 44, loss = 0.50204834\n",
      "Iteration 45, loss = 0.49981833\n",
      "Iteration 46, loss = 0.49763779\n",
      "Iteration 47, loss = 0.49540980\n",
      "Iteration 48, loss = 0.49331039\n",
      "Iteration 49, loss = 0.49123797\n",
      "Iteration 50, loss = 0.48930518\n",
      "Iteration 51, loss = 0.48734844\n",
      "Iteration 52, loss = 0.48532969\n",
      "Iteration 53, loss = 0.48334681\n",
      "Iteration 54, loss = 0.48168089\n",
      "Iteration 55, loss = 0.47986909\n",
      "Iteration 56, loss = 0.47806195\n",
      "Iteration 57, loss = 0.47641305\n",
      "Iteration 58, loss = 0.47472507\n",
      "Iteration 59, loss = 0.47321504\n",
      "Iteration 60, loss = 0.47146240\n",
      "Iteration 61, loss = 0.47011668\n",
      "Iteration 62, loss = 0.46866186\n",
      "Iteration 63, loss = 0.46724427\n",
      "Iteration 64, loss = 0.46566092\n",
      "Iteration 65, loss = 0.46439599\n",
      "Iteration 66, loss = 0.46319958\n",
      "Iteration 67, loss = 0.46179100\n",
      "Iteration 68, loss = 0.46060848\n",
      "Iteration 69, loss = 0.45948197\n",
      "Iteration 70, loss = 0.45839625\n",
      "Iteration 71, loss = 0.45727503\n",
      "Iteration 72, loss = 0.45612338\n",
      "Iteration 73, loss = 0.45508677\n",
      "Iteration 74, loss = 0.45432025\n",
      "Iteration 75, loss = 0.45325483\n",
      "Iteration 76, loss = 0.45241022\n",
      "Iteration 77, loss = 0.45151667\n",
      "Iteration 78, loss = 0.45069424\n",
      "Iteration 79, loss = 0.44989593\n",
      "Iteration 80, loss = 0.44922460\n",
      "Iteration 81, loss = 0.44854724\n",
      "Iteration 82, loss = 0.44775775\n",
      "Iteration 83, loss = 0.44733975\n",
      "Iteration 84, loss = 0.44649892\n",
      "Iteration 85, loss = 0.44595709\n",
      "Iteration 86, loss = 0.44535597\n",
      "Iteration 87, loss = 0.44484679\n",
      "Iteration 88, loss = 0.44440890\n",
      "Iteration 89, loss = 0.44387420\n",
      "Iteration 90, loss = 0.44338309\n",
      "Iteration 91, loss = 0.44302764\n",
      "Iteration 92, loss = 0.44259815\n",
      "Iteration 93, loss = 0.44219960\n",
      "Iteration 94, loss = 0.44189916\n",
      "Iteration 95, loss = 0.44150854\n",
      "Iteration 96, loss = 0.44117362\n",
      "Iteration 97, loss = 0.44099979\n",
      "Iteration 98, loss = 0.44077396\n",
      "Iteration 99, loss = 0.44030427\n",
      "Iteration 100, loss = 0.44003512\n",
      "Iteration 101, loss = 0.43987172\n",
      "Iteration 102, loss = 0.43955256\n",
      "Iteration 103, loss = 0.43935445\n",
      "Iteration 104, loss = 0.43922958\n",
      "Iteration 105, loss = 0.43900895\n",
      "Iteration 106, loss = 0.43874202\n",
      "Iteration 107, loss = 0.43860917\n",
      "Iteration 108, loss = 0.43844866\n",
      "Iteration 109, loss = 0.43831142\n",
      "Iteration 110, loss = 0.43820765\n",
      "Iteration 111, loss = 0.43804502\n",
      "Iteration 112, loss = 0.43789268\n",
      "Iteration 113, loss = 0.43780686\n",
      "Iteration 114, loss = 0.43773573\n",
      "Iteration 115, loss = 0.43755824\n",
      "Iteration 116, loss = 0.43747382\n",
      "Iteration 117, loss = 0.43740589\n",
      "Iteration 118, loss = 0.43736984\n",
      "Iteration 119, loss = 0.43720926\n",
      "Iteration 120, loss = 0.43718003\n",
      "Iteration 121, loss = 0.43706854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 122, loss = 0.43706423\n",
      "Iteration 123, loss = 0.43687758\n",
      "Iteration 124, loss = 0.43685740\n",
      "Iteration 125, loss = 0.43683564\n",
      "Iteration 126, loss = 0.43675074\n",
      "Iteration 127, loss = 0.43667877\n",
      "Iteration 128, loss = 0.43660879\n",
      "Iteration 129, loss = 0.43659492\n",
      "Iteration 130, loss = 0.43649161\n",
      "Iteration 131, loss = 0.43649104\n",
      "Iteration 132, loss = 0.43640560\n",
      "Iteration 133, loss = 0.43644081\n",
      "Iteration 134, loss = 0.43635229\n",
      "Iteration 135, loss = 0.43636896\n",
      "Iteration 136, loss = 0.43627426\n",
      "Iteration 137, loss = 0.43626330\n",
      "Iteration 138, loss = 0.43625829\n",
      "Iteration 139, loss = 0.43625771\n",
      "Iteration 140, loss = 0.43614465\n",
      "Iteration 141, loss = 0.43618544\n",
      "Iteration 142, loss = 0.43612035\n",
      "Iteration 143, loss = 0.43604139\n",
      "Iteration 144, loss = 0.43610975\n",
      "Iteration 145, loss = 0.43602720\n",
      "Iteration 146, loss = 0.43607242\n",
      "Iteration 147, loss = 0.43626466\n",
      "Iteration 148, loss = 0.43609673\n",
      "Iteration 149, loss = 0.43607719\n",
      "Iteration 150, loss = 0.43592771\n",
      "Iteration 151, loss = 0.43590065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75265081\n",
      "Iteration 2, loss = 0.71900162\n",
      "Iteration 3, loss = 0.68953380\n",
      "Iteration 4, loss = 0.66886107\n",
      "Iteration 5, loss = 0.65282323\n",
      "Iteration 6, loss = 0.64191148\n",
      "Iteration 7, loss = 0.63418329\n",
      "Iteration 8, loss = 0.62919875\n",
      "Iteration 9, loss = 0.62479202\n",
      "Iteration 10, loss = 0.62045073\n",
      "Iteration 11, loss = 0.61631211\n",
      "Iteration 12, loss = 0.61153053\n",
      "Iteration 13, loss = 0.60681107\n",
      "Iteration 14, loss = 0.60272453\n",
      "Iteration 15, loss = 0.59726733\n",
      "Iteration 16, loss = 0.59283363\n",
      "Iteration 17, loss = 0.58883361\n",
      "Iteration 18, loss = 0.58469976\n",
      "Iteration 19, loss = 0.58102951\n",
      "Iteration 20, loss = 0.57690476\n",
      "Iteration 21, loss = 0.57334314\n",
      "Iteration 22, loss = 0.56973818\n",
      "Iteration 23, loss = 0.56590748\n",
      "Iteration 24, loss = 0.56238074\n",
      "Iteration 25, loss = 0.55898052\n",
      "Iteration 26, loss = 0.55539455\n",
      "Iteration 27, loss = 0.55195097\n",
      "Iteration 28, loss = 0.54875141\n",
      "Iteration 29, loss = 0.54535678\n",
      "Iteration 30, loss = 0.54222082\n",
      "Iteration 31, loss = 0.53897379\n",
      "Iteration 32, loss = 0.53589949\n",
      "Iteration 33, loss = 0.53285597\n",
      "Iteration 34, loss = 0.52978651\n",
      "Iteration 35, loss = 0.52692700\n",
      "Iteration 36, loss = 0.52398244\n",
      "Iteration 37, loss = 0.52109388\n",
      "Iteration 38, loss = 0.51819507\n",
      "Iteration 39, loss = 0.51543303\n",
      "Iteration 40, loss = 0.51276105\n",
      "Iteration 41, loss = 0.51000085\n",
      "Iteration 42, loss = 0.50746661\n",
      "Iteration 43, loss = 0.50478391\n",
      "Iteration 44, loss = 0.50240909\n",
      "Iteration 45, loss = 0.49976346\n",
      "Iteration 46, loss = 0.49736283\n",
      "Iteration 47, loss = 0.49485917\n",
      "Iteration 48, loss = 0.49246141\n",
      "Iteration 49, loss = 0.49013831\n",
      "Iteration 50, loss = 0.48801366\n",
      "Iteration 51, loss = 0.48563155\n",
      "Iteration 52, loss = 0.48335358\n",
      "Iteration 53, loss = 0.48134793\n",
      "Iteration 54, loss = 0.47926628\n",
      "Iteration 55, loss = 0.47710430\n",
      "Iteration 56, loss = 0.47507383\n",
      "Iteration 57, loss = 0.47317135\n",
      "Iteration 58, loss = 0.47131082\n",
      "Iteration 59, loss = 0.46932769\n",
      "Iteration 60, loss = 0.46746850\n",
      "Iteration 61, loss = 0.46568196\n",
      "Iteration 62, loss = 0.46408918\n",
      "Iteration 63, loss = 0.46229773\n",
      "Iteration 64, loss = 0.46067106\n",
      "Iteration 65, loss = 0.45907436\n",
      "Iteration 66, loss = 0.45757924\n",
      "Iteration 67, loss = 0.45619843\n",
      "Iteration 68, loss = 0.45465705\n",
      "Iteration 69, loss = 0.45326815\n",
      "Iteration 70, loss = 0.45213438\n",
      "Iteration 71, loss = 0.45067791\n",
      "Iteration 72, loss = 0.44947668\n",
      "Iteration 73, loss = 0.44828315\n",
      "Iteration 74, loss = 0.44710112\n",
      "Iteration 75, loss = 0.44600719\n",
      "Iteration 76, loss = 0.44503754\n",
      "Iteration 77, loss = 0.44395837\n",
      "Iteration 78, loss = 0.44311880\n",
      "Iteration 79, loss = 0.44211679\n",
      "Iteration 80, loss = 0.44115536\n",
      "Iteration 81, loss = 0.44037551\n",
      "Iteration 82, loss = 0.43967627\n",
      "Iteration 83, loss = 0.43887541\n",
      "Iteration 84, loss = 0.43806068\n",
      "Iteration 85, loss = 0.43749531\n",
      "Iteration 86, loss = 0.43671965\n",
      "Iteration 87, loss = 0.43611109\n",
      "Iteration 88, loss = 0.43568314\n",
      "Iteration 89, loss = 0.43495711\n",
      "Iteration 90, loss = 0.43440464\n",
      "Iteration 91, loss = 0.43392625\n",
      "Iteration 92, loss = 0.43353261\n",
      "Iteration 93, loss = 0.43292822\n",
      "Iteration 94, loss = 0.43259682\n",
      "Iteration 95, loss = 0.43202721\n",
      "Iteration 96, loss = 0.43166735\n",
      "Iteration 97, loss = 0.43129525\n",
      "Iteration 98, loss = 0.43092399\n",
      "Iteration 99, loss = 0.43055008\n",
      "Iteration 100, loss = 0.43023021\n",
      "Iteration 101, loss = 0.43004350\n",
      "Iteration 102, loss = 0.42965439\n",
      "Iteration 103, loss = 0.42938527\n",
      "Iteration 104, loss = 0.42912704\n",
      "Iteration 105, loss = 0.42884365\n",
      "Iteration 106, loss = 0.42860325\n",
      "Iteration 107, loss = 0.42837105\n",
      "Iteration 108, loss = 0.42817372\n",
      "Iteration 109, loss = 0.42791714\n",
      "Iteration 110, loss = 0.42771390\n",
      "Iteration 111, loss = 0.42759508\n",
      "Iteration 112, loss = 0.42735510\n",
      "Iteration 113, loss = 0.42717692\n",
      "Iteration 114, loss = 0.42702185\n",
      "Iteration 115, loss = 0.42691631\n",
      "Iteration 116, loss = 0.42669167\n",
      "Iteration 117, loss = 0.42654711\n",
      "Iteration 118, loss = 0.42639846\n",
      "Iteration 119, loss = 0.42626641\n",
      "Iteration 120, loss = 0.42611844\n",
      "Iteration 121, loss = 0.42599204\n",
      "Iteration 122, loss = 0.42590106\n",
      "Iteration 123, loss = 0.42575259\n",
      "Iteration 124, loss = 0.42563081\n",
      "Iteration 125, loss = 0.42561935\n",
      "Iteration 126, loss = 0.42540991\n",
      "Iteration 127, loss = 0.42530597\n",
      "Iteration 128, loss = 0.42525287\n",
      "Iteration 129, loss = 0.42513815\n",
      "Iteration 130, loss = 0.42509477\n",
      "Iteration 131, loss = 0.42493604\n",
      "Iteration 132, loss = 0.42494320\n",
      "Iteration 133, loss = 0.42478063\n",
      "Iteration 134, loss = 0.42469343\n",
      "Iteration 135, loss = 0.42461857\n",
      "Iteration 136, loss = 0.42458001\n",
      "Iteration 137, loss = 0.42445235\n",
      "Iteration 138, loss = 0.42439149\n",
      "Iteration 139, loss = 0.42434397\n",
      "Iteration 140, loss = 0.42431029\n",
      "Iteration 141, loss = 0.42418087\n",
      "Iteration 142, loss = 0.42428380\n",
      "Iteration 143, loss = 0.42420921\n",
      "Iteration 144, loss = 0.42403566\n",
      "Iteration 145, loss = 0.42396076\n",
      "Iteration 146, loss = 0.42394518\n",
      "Iteration 147, loss = 0.42389747\n",
      "Iteration 148, loss = 0.42380743\n",
      "Iteration 149, loss = 0.42374315\n",
      "Iteration 150, loss = 0.42363490\n",
      "Iteration 151, loss = 0.42358227\n",
      "Iteration 152, loss = 0.42355925\n",
      "Iteration 153, loss = 0.42365907\n",
      "Iteration 154, loss = 0.42344331\n",
      "Iteration 155, loss = 0.42336873\n",
      "Iteration 156, loss = 0.42335730\n",
      "Iteration 157, loss = 0.42331283\n",
      "Iteration 158, loss = 0.42330618\n",
      "Iteration 159, loss = 0.42335110\n",
      "Iteration 160, loss = 0.42322439\n",
      "Iteration 161, loss = 0.42314946\n",
      "Iteration 162, loss = 0.42312637\n",
      "Iteration 163, loss = 0.42307406\n",
      "Iteration 164, loss = 0.42314486\n",
      "Iteration 165, loss = 0.42304859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75183985\n",
      "Iteration 2, loss = 0.71847798\n",
      "Iteration 3, loss = 0.68878111\n",
      "Iteration 4, loss = 0.66821631\n",
      "Iteration 5, loss = 0.65214328\n",
      "Iteration 6, loss = 0.64143199\n",
      "Iteration 7, loss = 0.63376959\n",
      "Iteration 8, loss = 0.62856575\n",
      "Iteration 9, loss = 0.62416079\n",
      "Iteration 10, loss = 0.61970702\n",
      "Iteration 11, loss = 0.61536278\n",
      "Iteration 12, loss = 0.61060582\n",
      "Iteration 13, loss = 0.60560015\n",
      "Iteration 14, loss = 0.60151998\n",
      "Iteration 15, loss = 0.59599842\n",
      "Iteration 16, loss = 0.59143795\n",
      "Iteration 17, loss = 0.58718816\n",
      "Iteration 18, loss = 0.58313961\n",
      "Iteration 19, loss = 0.57932588\n",
      "Iteration 20, loss = 0.57513540\n",
      "Iteration 21, loss = 0.57118944\n",
      "Iteration 22, loss = 0.56744184\n",
      "Iteration 23, loss = 0.56358456\n",
      "Iteration 24, loss = 0.55992355\n",
      "Iteration 25, loss = 0.55626273\n",
      "Iteration 26, loss = 0.55262883\n",
      "Iteration 27, loss = 0.54892453\n",
      "Iteration 28, loss = 0.54559427\n",
      "Iteration 29, loss = 0.54205761\n",
      "Iteration 30, loss = 0.53871929\n",
      "Iteration 31, loss = 0.53541395\n",
      "Iteration 32, loss = 0.53207380\n",
      "Iteration 33, loss = 0.52883576\n",
      "Iteration 34, loss = 0.52554792\n",
      "Iteration 35, loss = 0.52250505\n",
      "Iteration 36, loss = 0.51943095\n",
      "Iteration 37, loss = 0.51637035\n",
      "Iteration 38, loss = 0.51335945\n",
      "Iteration 39, loss = 0.51039323\n",
      "Iteration 40, loss = 0.50750023\n",
      "Iteration 41, loss = 0.50470879\n",
      "Iteration 42, loss = 0.50192634\n",
      "Iteration 43, loss = 0.49917913\n",
      "Iteration 44, loss = 0.49644973\n",
      "Iteration 45, loss = 0.49391888\n",
      "Iteration 46, loss = 0.49121460\n",
      "Iteration 47, loss = 0.48869123\n",
      "Iteration 48, loss = 0.48616565\n",
      "Iteration 49, loss = 0.48378332\n",
      "Iteration 50, loss = 0.48132227\n",
      "Iteration 51, loss = 0.47895389\n",
      "Iteration 52, loss = 0.47666123\n",
      "Iteration 53, loss = 0.47464064\n",
      "Iteration 54, loss = 0.47225891\n",
      "Iteration 55, loss = 0.47006548\n",
      "Iteration 56, loss = 0.46789435\n",
      "Iteration 57, loss = 0.46597021\n",
      "Iteration 58, loss = 0.46410915\n",
      "Iteration 59, loss = 0.46201334\n",
      "Iteration 60, loss = 0.46004146\n",
      "Iteration 61, loss = 0.45833441\n",
      "Iteration 62, loss = 0.45665349\n",
      "Iteration 63, loss = 0.45491870\n",
      "Iteration 64, loss = 0.45319230\n",
      "Iteration 65, loss = 0.45150504\n",
      "Iteration 66, loss = 0.44999447\n",
      "Iteration 67, loss = 0.44851390\n",
      "Iteration 68, loss = 0.44707434\n",
      "Iteration 69, loss = 0.44557485\n",
      "Iteration 70, loss = 0.44440473\n",
      "Iteration 71, loss = 0.44296848\n",
      "Iteration 72, loss = 0.44175505\n",
      "Iteration 73, loss = 0.44054915\n",
      "Iteration 74, loss = 0.43930894\n",
      "Iteration 75, loss = 0.43821825\n",
      "Iteration 76, loss = 0.43721526\n",
      "Iteration 77, loss = 0.43621614\n",
      "Iteration 78, loss = 0.43529446\n",
      "Iteration 79, loss = 0.43428090\n",
      "Iteration 80, loss = 0.43341259\n",
      "Iteration 81, loss = 0.43255874\n",
      "Iteration 82, loss = 0.43182730\n",
      "Iteration 83, loss = 0.43113877\n",
      "Iteration 84, loss = 0.43034076\n",
      "Iteration 85, loss = 0.42970318\n",
      "Iteration 86, loss = 0.42893076\n",
      "Iteration 87, loss = 0.42841791\n",
      "Iteration 88, loss = 0.42787142\n",
      "Iteration 89, loss = 0.42726165\n",
      "Iteration 90, loss = 0.42672126\n",
      "Iteration 91, loss = 0.42619658\n",
      "Iteration 92, loss = 0.42574901\n",
      "Iteration 93, loss = 0.42526422\n",
      "Iteration 94, loss = 0.42492133\n",
      "Iteration 95, loss = 0.42446351\n",
      "Iteration 96, loss = 0.42413532\n",
      "Iteration 97, loss = 0.42375317\n",
      "Iteration 98, loss = 0.42338776\n",
      "Iteration 99, loss = 0.42309650\n",
      "Iteration 100, loss = 0.42277630\n",
      "Iteration 101, loss = 0.42263989\n",
      "Iteration 102, loss = 0.42226083\n",
      "Iteration 103, loss = 0.42204112\n",
      "Iteration 104, loss = 0.42179156\n",
      "Iteration 105, loss = 0.42155496\n",
      "Iteration 106, loss = 0.42138577\n",
      "Iteration 107, loss = 0.42112459\n",
      "Iteration 108, loss = 0.42099101\n",
      "Iteration 109, loss = 0.42083671\n",
      "Iteration 110, loss = 0.42066997\n",
      "Iteration 111, loss = 0.42047363\n",
      "Iteration 112, loss = 0.42036394\n",
      "Iteration 113, loss = 0.42021509\n",
      "Iteration 114, loss = 0.42008737\n",
      "Iteration 115, loss = 0.41996146\n",
      "Iteration 116, loss = 0.41981574\n",
      "Iteration 117, loss = 0.41971418\n",
      "Iteration 118, loss = 0.41967677\n",
      "Iteration 119, loss = 0.41961509\n",
      "Iteration 120, loss = 0.41948139\n",
      "Iteration 121, loss = 0.41942926\n",
      "Iteration 122, loss = 0.41950872\n",
      "Iteration 123, loss = 0.41924530\n",
      "Iteration 124, loss = 0.41920268\n",
      "Iteration 125, loss = 0.41906932\n",
      "Iteration 126, loss = 0.41900238\n",
      "Iteration 127, loss = 0.41897401\n",
      "Iteration 128, loss = 0.41894395\n",
      "Iteration 129, loss = 0.41887768\n",
      "Iteration 130, loss = 0.41886734\n",
      "Iteration 131, loss = 0.41879328\n",
      "Iteration 132, loss = 0.41873211\n",
      "Iteration 133, loss = 0.41870254\n",
      "Iteration 134, loss = 0.41861995\n",
      "Iteration 135, loss = 0.41858002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 136, loss = 0.41854900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75256678\n",
      "Iteration 2, loss = 0.71801819\n",
      "Iteration 3, loss = 0.68894080\n",
      "Iteration 4, loss = 0.66709225\n",
      "Iteration 5, loss = 0.65079413\n",
      "Iteration 6, loss = 0.64135371\n",
      "Iteration 7, loss = 0.63314362\n",
      "Iteration 8, loss = 0.62829873\n",
      "Iteration 9, loss = 0.62394173\n",
      "Iteration 10, loss = 0.61966015\n",
      "Iteration 11, loss = 0.61535917\n",
      "Iteration 12, loss = 0.61081066\n",
      "Iteration 13, loss = 0.60584062\n",
      "Iteration 14, loss = 0.60174401\n",
      "Iteration 15, loss = 0.59646031\n",
      "Iteration 16, loss = 0.59210350\n",
      "Iteration 17, loss = 0.58774770\n",
      "Iteration 18, loss = 0.58380461\n",
      "Iteration 19, loss = 0.58026592\n",
      "Iteration 20, loss = 0.57630186\n",
      "Iteration 21, loss = 0.57270455\n",
      "Iteration 22, loss = 0.56898778\n",
      "Iteration 23, loss = 0.56544934\n",
      "Iteration 24, loss = 0.56184779\n",
      "Iteration 25, loss = 0.55837092\n",
      "Iteration 26, loss = 0.55495528\n",
      "Iteration 27, loss = 0.55164160\n",
      "Iteration 28, loss = 0.54849753\n",
      "Iteration 29, loss = 0.54531068\n",
      "Iteration 30, loss = 0.54216915\n",
      "Iteration 31, loss = 0.53917209\n",
      "Iteration 32, loss = 0.53610714\n",
      "Iteration 33, loss = 0.53317774\n",
      "Iteration 34, loss = 0.53029929\n",
      "Iteration 35, loss = 0.52749339\n",
      "Iteration 36, loss = 0.52487935\n",
      "Iteration 37, loss = 0.52205346\n",
      "Iteration 38, loss = 0.51954269\n",
      "Iteration 39, loss = 0.51691113\n",
      "Iteration 40, loss = 0.51441393\n",
      "Iteration 41, loss = 0.51204489\n",
      "Iteration 42, loss = 0.50961347\n",
      "Iteration 43, loss = 0.50723193\n",
      "Iteration 44, loss = 0.50505494\n",
      "Iteration 45, loss = 0.50289127\n",
      "Iteration 46, loss = 0.50059009\n",
      "Iteration 47, loss = 0.49856975\n",
      "Iteration 48, loss = 0.49651597\n",
      "Iteration 49, loss = 0.49449456\n",
      "Iteration 50, loss = 0.49259415\n",
      "Iteration 51, loss = 0.49069761\n",
      "Iteration 52, loss = 0.48882433\n",
      "Iteration 53, loss = 0.48722181\n",
      "Iteration 54, loss = 0.48532211\n",
      "Iteration 55, loss = 0.48360855\n",
      "Iteration 56, loss = 0.48187742\n",
      "Iteration 57, loss = 0.48046591\n",
      "Iteration 58, loss = 0.47907390\n",
      "Iteration 59, loss = 0.47736614\n",
      "Iteration 60, loss = 0.47596982\n",
      "Iteration 61, loss = 0.47443793\n",
      "Iteration 62, loss = 0.47320356\n",
      "Iteration 63, loss = 0.47193187\n",
      "Iteration 64, loss = 0.47062711\n",
      "Iteration 65, loss = 0.46952937\n",
      "Iteration 66, loss = 0.46830515\n",
      "Iteration 67, loss = 0.46713399\n",
      "Iteration 68, loss = 0.46614276\n",
      "Iteration 69, loss = 0.46501569\n",
      "Iteration 70, loss = 0.46402355\n",
      "Iteration 71, loss = 0.46313124\n",
      "Iteration 72, loss = 0.46218698\n",
      "Iteration 73, loss = 0.46125599\n",
      "Iteration 74, loss = 0.46030546\n",
      "Iteration 75, loss = 0.45959472\n",
      "Iteration 76, loss = 0.45882400\n",
      "Iteration 77, loss = 0.45821571\n",
      "Iteration 78, loss = 0.45751240\n",
      "Iteration 79, loss = 0.45671217\n",
      "Iteration 80, loss = 0.45612556\n",
      "Iteration 81, loss = 0.45547064\n",
      "Iteration 82, loss = 0.45494389\n",
      "Iteration 83, loss = 0.45464056\n",
      "Iteration 84, loss = 0.45397899\n",
      "Iteration 85, loss = 0.45350084\n",
      "Iteration 86, loss = 0.45292993\n",
      "Iteration 87, loss = 0.45268650\n",
      "Iteration 88, loss = 0.45229199\n",
      "Iteration 89, loss = 0.45177355\n",
      "Iteration 90, loss = 0.45139634\n",
      "Iteration 91, loss = 0.45106153\n",
      "Iteration 92, loss = 0.45073271\n",
      "Iteration 93, loss = 0.45038178\n",
      "Iteration 94, loss = 0.45015286\n",
      "Iteration 95, loss = 0.44985332\n",
      "Iteration 96, loss = 0.44961838\n",
      "Iteration 97, loss = 0.44938001\n",
      "Iteration 98, loss = 0.44908650\n",
      "Iteration 99, loss = 0.44885823\n",
      "Iteration 100, loss = 0.44865192\n",
      "Iteration 101, loss = 0.44856945\n",
      "Iteration 102, loss = 0.44833462\n",
      "Iteration 103, loss = 0.44820333\n",
      "Iteration 104, loss = 0.44791739\n",
      "Iteration 105, loss = 0.44779364\n",
      "Iteration 106, loss = 0.44768981\n",
      "Iteration 107, loss = 0.44748553\n",
      "Iteration 108, loss = 0.44743307\n",
      "Iteration 109, loss = 0.44731084\n",
      "Iteration 110, loss = 0.44715348\n",
      "Iteration 111, loss = 0.44697175\n",
      "Iteration 112, loss = 0.44686901\n",
      "Iteration 113, loss = 0.44672620\n",
      "Iteration 114, loss = 0.44663888\n",
      "Iteration 115, loss = 0.44658231\n",
      "Iteration 116, loss = 0.44646866\n",
      "Iteration 117, loss = 0.44637434\n",
      "Iteration 118, loss = 0.44631948\n",
      "Iteration 119, loss = 0.44628444\n",
      "Iteration 120, loss = 0.44620496\n",
      "Iteration 121, loss = 0.44608911\n",
      "Iteration 122, loss = 0.44616763\n",
      "Iteration 123, loss = 0.44603097\n",
      "Iteration 124, loss = 0.44590442\n",
      "Iteration 125, loss = 0.44579968\n",
      "Iteration 126, loss = 0.44581168\n",
      "Iteration 127, loss = 0.44571994\n",
      "Iteration 128, loss = 0.44568170\n",
      "Iteration 129, loss = 0.44562495\n",
      "Iteration 130, loss = 0.44564636\n",
      "Iteration 131, loss = 0.44552048\n",
      "Iteration 132, loss = 0.44546124\n",
      "Iteration 133, loss = 0.44546682\n",
      "Iteration 134, loss = 0.44537718\n",
      "Iteration 135, loss = 0.44536650\n",
      "Iteration 136, loss = 0.44535667\n",
      "Iteration 137, loss = 0.44536716\n",
      "Iteration 138, loss = 0.44520460\n",
      "Iteration 139, loss = 0.44522296\n",
      "Iteration 140, loss = 0.44524949\n",
      "Iteration 141, loss = 0.44515753\n",
      "Iteration 142, loss = 0.44524532\n",
      "Iteration 143, loss = 0.44507528\n",
      "Iteration 144, loss = 0.44504946\n",
      "Iteration 145, loss = 0.44503123\n",
      "Iteration 146, loss = 0.44508005\n",
      "Iteration 147, loss = 0.44507702\n",
      "Iteration 148, loss = 0.44491894\n",
      "Iteration 149, loss = 0.44487249\n",
      "Iteration 150, loss = 0.44489042\n",
      "Iteration 151, loss = 0.44487254\n",
      "Iteration 152, loss = 0.44488865\n",
      "Iteration 153, loss = 0.44496846\n",
      "Iteration 154, loss = 0.44480295\n",
      "Iteration 155, loss = 0.44477177\n",
      "Iteration 156, loss = 0.44467947\n",
      "Iteration 157, loss = 0.44468853\n",
      "Iteration 158, loss = 0.44465859\n",
      "Iteration 159, loss = 0.44461437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74975171\n",
      "Iteration 2, loss = 0.71532951\n",
      "Iteration 3, loss = 0.68652202\n",
      "Iteration 4, loss = 0.66333237\n",
      "Iteration 5, loss = 0.64775934\n",
      "Iteration 6, loss = 0.63647382\n",
      "Iteration 7, loss = 0.62841742\n",
      "Iteration 8, loss = 0.62340559\n",
      "Iteration 9, loss = 0.61834730\n",
      "Iteration 10, loss = 0.61360934\n",
      "Iteration 11, loss = 0.60886003\n",
      "Iteration 12, loss = 0.60329364\n",
      "Iteration 13, loss = 0.59720499\n",
      "Iteration 14, loss = 0.59183842\n",
      "Iteration 15, loss = 0.58643043\n",
      "Iteration 16, loss = 0.58132232\n",
      "Iteration 17, loss = 0.57642806\n",
      "Iteration 18, loss = 0.57179739\n",
      "Iteration 19, loss = 0.56740795\n",
      "Iteration 20, loss = 0.56306085\n",
      "Iteration 21, loss = 0.55872438\n",
      "Iteration 22, loss = 0.55447487\n",
      "Iteration 23, loss = 0.55020021\n",
      "Iteration 24, loss = 0.54598447\n",
      "Iteration 25, loss = 0.54190314\n",
      "Iteration 26, loss = 0.53807919\n",
      "Iteration 27, loss = 0.53407811\n",
      "Iteration 28, loss = 0.53023795\n",
      "Iteration 29, loss = 0.52660131\n",
      "Iteration 30, loss = 0.52283458\n",
      "Iteration 31, loss = 0.51934476\n",
      "Iteration 32, loss = 0.51597595\n",
      "Iteration 33, loss = 0.51240387\n",
      "Iteration 34, loss = 0.50901008\n",
      "Iteration 35, loss = 0.50573052\n",
      "Iteration 36, loss = 0.50276760\n",
      "Iteration 37, loss = 0.49953807\n",
      "Iteration 38, loss = 0.49655795\n",
      "Iteration 39, loss = 0.49364228\n",
      "Iteration 40, loss = 0.49071800\n",
      "Iteration 41, loss = 0.48785653\n",
      "Iteration 42, loss = 0.48516011\n",
      "Iteration 43, loss = 0.48252119\n",
      "Iteration 44, loss = 0.47992139\n",
      "Iteration 45, loss = 0.47739978\n",
      "Iteration 46, loss = 0.47493132\n",
      "Iteration 47, loss = 0.47261481\n",
      "Iteration 48, loss = 0.47013841\n",
      "Iteration 49, loss = 0.46782446\n",
      "Iteration 50, loss = 0.46578424\n",
      "Iteration 51, loss = 0.46357233\n",
      "Iteration 52, loss = 0.46153061\n",
      "Iteration 53, loss = 0.45937931\n",
      "Iteration 54, loss = 0.45751815\n",
      "Iteration 55, loss = 0.45563828\n",
      "Iteration 56, loss = 0.45373285\n",
      "Iteration 57, loss = 0.45193713\n",
      "Iteration 58, loss = 0.45019637\n",
      "Iteration 59, loss = 0.44847423\n",
      "Iteration 60, loss = 0.44684394\n",
      "Iteration 61, loss = 0.44525236\n",
      "Iteration 62, loss = 0.44383893\n",
      "Iteration 63, loss = 0.44226921\n",
      "Iteration 64, loss = 0.44085052\n",
      "Iteration 65, loss = 0.43941702\n",
      "Iteration 66, loss = 0.43802657\n",
      "Iteration 67, loss = 0.43675461\n",
      "Iteration 68, loss = 0.43553517\n",
      "Iteration 69, loss = 0.43436927\n",
      "Iteration 70, loss = 0.43324403\n",
      "Iteration 71, loss = 0.43212430\n",
      "Iteration 72, loss = 0.43103146\n",
      "Iteration 73, loss = 0.42996643\n",
      "Iteration 74, loss = 0.42909155\n",
      "Iteration 75, loss = 0.42818370\n",
      "Iteration 76, loss = 0.42724873\n",
      "Iteration 77, loss = 0.42635694\n",
      "Iteration 78, loss = 0.42556430\n",
      "Iteration 79, loss = 0.42483451\n",
      "Iteration 80, loss = 0.42423237\n",
      "Iteration 81, loss = 0.42349207\n",
      "Iteration 82, loss = 0.42273328\n",
      "Iteration 83, loss = 0.42221257\n",
      "Iteration 84, loss = 0.42154353\n",
      "Iteration 85, loss = 0.42099392\n",
      "Iteration 86, loss = 0.42037247\n",
      "Iteration 87, loss = 0.41991092\n",
      "Iteration 88, loss = 0.41953942\n",
      "Iteration 89, loss = 0.41894965\n",
      "Iteration 90, loss = 0.41846539\n",
      "Iteration 91, loss = 0.41820009\n",
      "Iteration 92, loss = 0.41782720\n",
      "Iteration 93, loss = 0.41741887\n",
      "Iteration 94, loss = 0.41719116\n",
      "Iteration 95, loss = 0.41675759\n",
      "Iteration 96, loss = 0.41652522\n",
      "Iteration 97, loss = 0.41618545\n",
      "Iteration 98, loss = 0.41597167\n",
      "Iteration 99, loss = 0.41564332\n",
      "Iteration 100, loss = 0.41537813\n",
      "Iteration 101, loss = 0.41524267\n",
      "Iteration 102, loss = 0.41493115\n",
      "Iteration 103, loss = 0.41478995\n",
      "Iteration 104, loss = 0.41459840\n",
      "Iteration 105, loss = 0.41438148\n",
      "Iteration 106, loss = 0.41419574\n",
      "Iteration 107, loss = 0.41411979\n",
      "Iteration 108, loss = 0.41399581\n",
      "Iteration 109, loss = 0.41379140\n",
      "Iteration 110, loss = 0.41364449\n",
      "Iteration 111, loss = 0.41351035\n",
      "Iteration 112, loss = 0.41340813\n",
      "Iteration 113, loss = 0.41330297\n",
      "Iteration 114, loss = 0.41319203\n",
      "Iteration 115, loss = 0.41309377\n",
      "Iteration 116, loss = 0.41298589\n",
      "Iteration 117, loss = 0.41293201\n",
      "Iteration 118, loss = 0.41284290\n",
      "Iteration 119, loss = 0.41273554\n",
      "Iteration 120, loss = 0.41264361\n",
      "Iteration 121, loss = 0.41258327\n",
      "Iteration 122, loss = 0.41262397\n",
      "Iteration 123, loss = 0.41241572\n",
      "Iteration 124, loss = 0.41249814\n",
      "Iteration 125, loss = 0.41228798\n",
      "Iteration 126, loss = 0.41226921\n",
      "Iteration 127, loss = 0.41219374\n",
      "Iteration 128, loss = 0.41215840\n",
      "Iteration 129, loss = 0.41213984\n",
      "Iteration 130, loss = 0.41201212\n",
      "Iteration 131, loss = 0.41205449\n",
      "Iteration 132, loss = 0.41201893\n",
      "Iteration 133, loss = 0.41199927\n",
      "Iteration 134, loss = 0.41186194\n",
      "Iteration 135, loss = 0.41187429\n",
      "Iteration 136, loss = 0.41179839\n",
      "Iteration 137, loss = 0.41182331\n",
      "Iteration 138, loss = 0.41170396\n",
      "Iteration 139, loss = 0.41174967\n",
      "Iteration 140, loss = 0.41166713\n",
      "Iteration 141, loss = 0.41162163\n",
      "Iteration 142, loss = 0.41158820\n",
      "Iteration 143, loss = 0.41154925\n",
      "Iteration 144, loss = 0.41149992\n",
      "Iteration 145, loss = 0.41162448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75074257\n",
      "Iteration 2, loss = 0.71623946\n",
      "Iteration 3, loss = 0.68708186\n",
      "Iteration 4, loss = 0.66507706\n",
      "Iteration 5, loss = 0.65030209\n",
      "Iteration 6, loss = 0.63927895\n",
      "Iteration 7, loss = 0.63132306\n",
      "Iteration 8, loss = 0.62731676\n",
      "Iteration 9, loss = 0.62247735\n",
      "Iteration 10, loss = 0.61806781\n",
      "Iteration 11, loss = 0.61375406\n",
      "Iteration 12, loss = 0.60895280\n",
      "Iteration 13, loss = 0.60344511\n",
      "Iteration 14, loss = 0.59875066\n",
      "Iteration 15, loss = 0.59396578\n",
      "Iteration 16, loss = 0.58946925\n",
      "Iteration 17, loss = 0.58526949\n",
      "Iteration 18, loss = 0.58112679\n",
      "Iteration 19, loss = 0.57735460\n",
      "Iteration 20, loss = 0.57356119\n",
      "Iteration 21, loss = 0.56983530\n",
      "Iteration 22, loss = 0.56621805\n",
      "Iteration 23, loss = 0.56256928\n",
      "Iteration 24, loss = 0.55899802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.55546283\n",
      "Iteration 26, loss = 0.55215430\n",
      "Iteration 27, loss = 0.54885201\n",
      "Iteration 28, loss = 0.54554906\n",
      "Iteration 29, loss = 0.54236551\n",
      "Iteration 30, loss = 0.53912212\n",
      "Iteration 31, loss = 0.53603066\n",
      "Iteration 32, loss = 0.53331251\n",
      "Iteration 33, loss = 0.53019928\n",
      "Iteration 34, loss = 0.52723026\n",
      "Iteration 35, loss = 0.52438904\n",
      "Iteration 36, loss = 0.52185698\n",
      "Iteration 37, loss = 0.51897023\n",
      "Iteration 38, loss = 0.51642837\n",
      "Iteration 39, loss = 0.51387846\n",
      "Iteration 40, loss = 0.51133597\n",
      "Iteration 41, loss = 0.50876112\n",
      "Iteration 42, loss = 0.50634720\n",
      "Iteration 43, loss = 0.50399039\n",
      "Iteration 44, loss = 0.50163231\n",
      "Iteration 45, loss = 0.49939695\n",
      "Iteration 46, loss = 0.49721101\n",
      "Iteration 47, loss = 0.49497790\n",
      "Iteration 48, loss = 0.49287374\n",
      "Iteration 49, loss = 0.49079624\n",
      "Iteration 50, loss = 0.48885935\n",
      "Iteration 51, loss = 0.48689749\n",
      "Iteration 52, loss = 0.48487450\n",
      "Iteration 53, loss = 0.48288711\n",
      "Iteration 54, loss = 0.48121743\n",
      "Iteration 55, loss = 0.47940125\n",
      "Iteration 56, loss = 0.47759003\n",
      "Iteration 57, loss = 0.47593784\n",
      "Iteration 58, loss = 0.47424610\n",
      "Iteration 59, loss = 0.47273234\n",
      "Iteration 60, loss = 0.47097542\n",
      "Iteration 61, loss = 0.46962652\n",
      "Iteration 62, loss = 0.46816851\n",
      "Iteration 63, loss = 0.46674735\n",
      "Iteration 64, loss = 0.46516068\n",
      "Iteration 65, loss = 0.46389251\n",
      "Iteration 66, loss = 0.46269301\n",
      "Iteration 67, loss = 0.46128099\n",
      "Iteration 68, loss = 0.46009539\n",
      "Iteration 69, loss = 0.45896608\n",
      "Iteration 70, loss = 0.45787732\n",
      "Iteration 71, loss = 0.45675303\n",
      "Iteration 72, loss = 0.45559851\n",
      "Iteration 73, loss = 0.45455864\n",
      "Iteration 74, loss = 0.45378935\n",
      "Iteration 75, loss = 0.45272140\n",
      "Iteration 76, loss = 0.45187367\n",
      "Iteration 77, loss = 0.45097778\n",
      "Iteration 78, loss = 0.45015257\n",
      "Iteration 79, loss = 0.44935173\n",
      "Iteration 80, loss = 0.44867772\n",
      "Iteration 81, loss = 0.44799757\n",
      "Iteration 82, loss = 0.44720575\n",
      "Iteration 83, loss = 0.44678540\n",
      "Iteration 84, loss = 0.44594169\n",
      "Iteration 85, loss = 0.44539755\n",
      "Iteration 86, loss = 0.44479431\n",
      "Iteration 87, loss = 0.44428291\n",
      "Iteration 88, loss = 0.44384293\n",
      "Iteration 89, loss = 0.44330573\n",
      "Iteration 90, loss = 0.44281237\n",
      "Iteration 91, loss = 0.44245504\n",
      "Iteration 92, loss = 0.44202357\n",
      "Iteration 93, loss = 0.44162327\n",
      "Iteration 94, loss = 0.44132072\n",
      "Iteration 95, loss = 0.44092791\n",
      "Iteration 96, loss = 0.44059125\n",
      "Iteration 97, loss = 0.44041616\n",
      "Iteration 98, loss = 0.44018830\n",
      "Iteration 99, loss = 0.43971684\n",
      "Iteration 100, loss = 0.43944628\n",
      "Iteration 101, loss = 0.43928157\n",
      "Iteration 102, loss = 0.43896057\n",
      "Iteration 103, loss = 0.43876102\n",
      "Iteration 104, loss = 0.43863464\n",
      "Iteration 105, loss = 0.43841262\n",
      "Iteration 106, loss = 0.43814467\n",
      "Iteration 107, loss = 0.43801068\n",
      "Iteration 108, loss = 0.43784888\n",
      "Iteration 109, loss = 0.43771018\n",
      "Iteration 110, loss = 0.43760542\n",
      "Iteration 111, loss = 0.43744159\n",
      "Iteration 112, loss = 0.43728844\n",
      "Iteration 113, loss = 0.43720111\n",
      "Iteration 114, loss = 0.43712992\n",
      "Iteration 115, loss = 0.43695118\n",
      "Iteration 116, loss = 0.43686540\n",
      "Iteration 117, loss = 0.43679714\n",
      "Iteration 118, loss = 0.43675980\n",
      "Iteration 119, loss = 0.43659845\n",
      "Iteration 120, loss = 0.43656844\n",
      "Iteration 121, loss = 0.43645643\n",
      "Iteration 122, loss = 0.43645141\n",
      "Iteration 123, loss = 0.43626394\n",
      "Iteration 124, loss = 0.43624296\n",
      "Iteration 125, loss = 0.43622071\n",
      "Iteration 126, loss = 0.43613519\n",
      "Iteration 127, loss = 0.43606276\n",
      "Iteration 128, loss = 0.43599222\n",
      "Iteration 129, loss = 0.43597741\n",
      "Iteration 130, loss = 0.43587373\n",
      "Iteration 131, loss = 0.43587286\n",
      "Iteration 132, loss = 0.43578687\n",
      "Iteration 133, loss = 0.43582152\n",
      "Iteration 134, loss = 0.43573253\n",
      "Iteration 135, loss = 0.43574896\n",
      "Iteration 136, loss = 0.43565366\n",
      "Iteration 137, loss = 0.43564216\n",
      "Iteration 138, loss = 0.43563717\n",
      "Iteration 139, loss = 0.43563578\n",
      "Iteration 140, loss = 0.43552241\n",
      "Iteration 141, loss = 0.43556242\n",
      "Iteration 142, loss = 0.43549724\n",
      "Iteration 143, loss = 0.43541796\n",
      "Iteration 144, loss = 0.43548575\n",
      "Iteration 145, loss = 0.43540274\n",
      "Iteration 146, loss = 0.43544772\n",
      "Iteration 147, loss = 0.43564014\n",
      "Iteration 148, loss = 0.43547142\n",
      "Iteration 149, loss = 0.43545182\n",
      "Iteration 150, loss = 0.43530183\n",
      "Iteration 151, loss = 0.43527446\n",
      "Iteration 152, loss = 0.43530037\n",
      "Iteration 153, loss = 0.43519901\n",
      "Iteration 154, loss = 0.43523553\n",
      "Iteration 155, loss = 0.43531521\n",
      "Iteration 156, loss = 0.43521791\n",
      "Iteration 157, loss = 0.43513172\n",
      "Iteration 158, loss = 0.43519005\n",
      "Iteration 159, loss = 0.43516299\n",
      "Iteration 160, loss = 0.43506530\n",
      "Iteration 161, loss = 0.43507731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75247095\n",
      "Iteration 2, loss = 0.71882122\n",
      "Iteration 3, loss = 0.68935235\n",
      "Iteration 4, loss = 0.66867844\n",
      "Iteration 5, loss = 0.65263897\n",
      "Iteration 6, loss = 0.64172520\n",
      "Iteration 7, loss = 0.63399452\n",
      "Iteration 8, loss = 0.62900715\n",
      "Iteration 9, loss = 0.62459716\n",
      "Iteration 10, loss = 0.62025221\n",
      "Iteration 11, loss = 0.61610958\n",
      "Iteration 12, loss = 0.61132355\n",
      "Iteration 13, loss = 0.60659944\n",
      "Iteration 14, loss = 0.60250786\n",
      "Iteration 15, loss = 0.59704522\n",
      "Iteration 16, loss = 0.59260583\n",
      "Iteration 17, loss = 0.58859981\n",
      "Iteration 18, loss = 0.58445969\n",
      "Iteration 19, loss = 0.58078310\n",
      "Iteration 20, loss = 0.57665163\n",
      "Iteration 21, loss = 0.57308322\n",
      "Iteration 22, loss = 0.56947140\n",
      "Iteration 23, loss = 0.56563345\n",
      "Iteration 24, loss = 0.56209948\n",
      "Iteration 25, loss = 0.55869180\n",
      "Iteration 26, loss = 0.55509835\n",
      "Iteration 27, loss = 0.55164736\n",
      "Iteration 28, loss = 0.54844052\n",
      "Iteration 29, loss = 0.54503813\n",
      "Iteration 30, loss = 0.54189469\n",
      "Iteration 31, loss = 0.53864010\n",
      "Iteration 32, loss = 0.53555862\n",
      "Iteration 33, loss = 0.53250784\n",
      "Iteration 34, loss = 0.52943108\n",
      "Iteration 35, loss = 0.52656502\n",
      "Iteration 36, loss = 0.52361315\n",
      "Iteration 37, loss = 0.52071758\n",
      "Iteration 38, loss = 0.51781229\n",
      "Iteration 39, loss = 0.51504398\n",
      "Iteration 40, loss = 0.51236558\n",
      "Iteration 41, loss = 0.50959920\n",
      "Iteration 42, loss = 0.50705919\n",
      "Iteration 43, loss = 0.50437064\n",
      "Iteration 44, loss = 0.50199048\n",
      "Iteration 45, loss = 0.49933908\n",
      "Iteration 46, loss = 0.49693307\n",
      "Iteration 47, loss = 0.49442453\n",
      "Iteration 48, loss = 0.49202111\n",
      "Iteration 49, loss = 0.48969344\n",
      "Iteration 50, loss = 0.48756403\n",
      "Iteration 51, loss = 0.48517746\n",
      "Iteration 52, loss = 0.48289454\n",
      "Iteration 53, loss = 0.48088470\n",
      "Iteration 54, loss = 0.47879827\n",
      "Iteration 55, loss = 0.47663187\n",
      "Iteration 56, loss = 0.47459711\n",
      "Iteration 57, loss = 0.47269062\n",
      "Iteration 58, loss = 0.47082570\n",
      "Iteration 59, loss = 0.46883873\n",
      "Iteration 60, loss = 0.46697526\n",
      "Iteration 61, loss = 0.46518469\n",
      "Iteration 62, loss = 0.46358818\n",
      "Iteration 63, loss = 0.46179276\n",
      "Iteration 64, loss = 0.46016213\n",
      "Iteration 65, loss = 0.45856190\n",
      "Iteration 66, loss = 0.45706310\n",
      "Iteration 67, loss = 0.45567799\n",
      "Iteration 68, loss = 0.45413374\n",
      "Iteration 69, loss = 0.45274094\n",
      "Iteration 70, loss = 0.45160355\n",
      "Iteration 71, loss = 0.45014410\n",
      "Iteration 72, loss = 0.44893916\n",
      "Iteration 73, loss = 0.44774224\n",
      "Iteration 74, loss = 0.44655705\n",
      "Iteration 75, loss = 0.44546011\n",
      "Iteration 76, loss = 0.44448693\n",
      "Iteration 77, loss = 0.44340536\n",
      "Iteration 78, loss = 0.44256246\n",
      "Iteration 79, loss = 0.44155773\n",
      "Iteration 80, loss = 0.44059349\n",
      "Iteration 81, loss = 0.43981067\n",
      "Iteration 82, loss = 0.43910887\n",
      "Iteration 83, loss = 0.43830542\n",
      "Iteration 84, loss = 0.43748782\n",
      "Iteration 85, loss = 0.43692068\n",
      "Iteration 86, loss = 0.43614222\n",
      "Iteration 87, loss = 0.43553137\n",
      "Iteration 88, loss = 0.43510163\n",
      "Iteration 89, loss = 0.43437300\n",
      "Iteration 90, loss = 0.43381812\n",
      "Iteration 91, loss = 0.43333841\n",
      "Iteration 92, loss = 0.43294149\n",
      "Iteration 93, loss = 0.43233630\n",
      "Iteration 94, loss = 0.43200307\n",
      "Iteration 95, loss = 0.43143187\n",
      "Iteration 96, loss = 0.43107028\n",
      "Iteration 97, loss = 0.43069647\n",
      "Iteration 98, loss = 0.43032365\n",
      "Iteration 99, loss = 0.42994819\n",
      "Iteration 100, loss = 0.42962719\n",
      "Iteration 101, loss = 0.42943897\n",
      "Iteration 102, loss = 0.42904825\n",
      "Iteration 103, loss = 0.42877834\n",
      "Iteration 104, loss = 0.42851835\n",
      "Iteration 105, loss = 0.42823387\n",
      "Iteration 106, loss = 0.42799181\n",
      "Iteration 107, loss = 0.42775891\n",
      "Iteration 108, loss = 0.42756068\n",
      "Iteration 109, loss = 0.42730283\n",
      "Iteration 110, loss = 0.42709862\n",
      "Iteration 111, loss = 0.42697860\n",
      "Iteration 112, loss = 0.42673788\n",
      "Iteration 113, loss = 0.42655911\n",
      "Iteration 114, loss = 0.42640308\n",
      "Iteration 115, loss = 0.42629626\n",
      "Iteration 116, loss = 0.42607122\n",
      "Iteration 117, loss = 0.42592586\n",
      "Iteration 118, loss = 0.42577631\n",
      "Iteration 119, loss = 0.42564360\n",
      "Iteration 120, loss = 0.42549449\n",
      "Iteration 121, loss = 0.42536753\n",
      "Iteration 122, loss = 0.42527637\n",
      "Iteration 123, loss = 0.42512674\n",
      "Iteration 124, loss = 0.42500402\n",
      "Iteration 125, loss = 0.42499203\n",
      "Iteration 126, loss = 0.42478192\n",
      "Iteration 127, loss = 0.42467764\n",
      "Iteration 128, loss = 0.42462326\n",
      "Iteration 129, loss = 0.42450788\n",
      "Iteration 130, loss = 0.42446427\n",
      "Iteration 131, loss = 0.42430489\n",
      "Iteration 132, loss = 0.42431154\n",
      "Iteration 133, loss = 0.42414800\n",
      "Iteration 134, loss = 0.42406015\n",
      "Iteration 135, loss = 0.42398479\n",
      "Iteration 136, loss = 0.42394528\n",
      "Iteration 137, loss = 0.42381718\n",
      "Iteration 138, loss = 0.42375574\n",
      "Iteration 139, loss = 0.42370743\n",
      "Iteration 140, loss = 0.42367338\n",
      "Iteration 141, loss = 0.42354324\n",
      "Iteration 142, loss = 0.42364559\n",
      "Iteration 143, loss = 0.42357011\n",
      "Iteration 144, loss = 0.42339603\n",
      "Iteration 145, loss = 0.42332031\n",
      "Iteration 146, loss = 0.42330420\n",
      "Iteration 147, loss = 0.42325588\n",
      "Iteration 148, loss = 0.42316517\n",
      "Iteration 149, loss = 0.42310028\n",
      "Iteration 150, loss = 0.42299176\n",
      "Iteration 151, loss = 0.42293848\n",
      "Iteration 152, loss = 0.42291453\n",
      "Iteration 153, loss = 0.42301320\n",
      "Iteration 154, loss = 0.42279724\n",
      "Iteration 155, loss = 0.42272239\n",
      "Iteration 156, loss = 0.42271028\n",
      "Iteration 157, loss = 0.42266523\n",
      "Iteration 158, loss = 0.42265792\n",
      "Iteration 159, loss = 0.42270206\n",
      "Iteration 160, loss = 0.42257461\n",
      "Iteration 161, loss = 0.42249948\n",
      "Iteration 162, loss = 0.42247564\n",
      "Iteration 163, loss = 0.42242287\n",
      "Iteration 164, loss = 0.42249278\n",
      "Iteration 165, loss = 0.42239603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75166090\n",
      "Iteration 2, loss = 0.71829844\n",
      "Iteration 3, loss = 0.68860063\n",
      "Iteration 4, loss = 0.66803455\n",
      "Iteration 5, loss = 0.65195989\n",
      "Iteration 6, loss = 0.64124667\n",
      "Iteration 7, loss = 0.63358190\n",
      "Iteration 8, loss = 0.62837544\n",
      "Iteration 9, loss = 0.62396756\n",
      "Iteration 10, loss = 0.61951036\n",
      "Iteration 11, loss = 0.61516253\n",
      "Iteration 12, loss = 0.61040162\n",
      "Iteration 13, loss = 0.60539166\n",
      "Iteration 14, loss = 0.60130709\n",
      "Iteration 15, loss = 0.59578056\n",
      "Iteration 16, loss = 0.59121500\n",
      "Iteration 17, loss = 0.58695978\n",
      "Iteration 18, loss = 0.58290565\n",
      "Iteration 19, loss = 0.57908611\n",
      "Iteration 20, loss = 0.57488967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.57093749\n",
      "Iteration 22, loss = 0.56718359\n",
      "Iteration 23, loss = 0.56331994\n",
      "Iteration 24, loss = 0.55965225\n",
      "Iteration 25, loss = 0.55598456\n",
      "Iteration 26, loss = 0.55234403\n",
      "Iteration 27, loss = 0.54863250\n",
      "Iteration 28, loss = 0.54529570\n",
      "Iteration 29, loss = 0.54175185\n",
      "Iteration 30, loss = 0.53840651\n",
      "Iteration 31, loss = 0.53509427\n",
      "Iteration 32, loss = 0.53174701\n",
      "Iteration 33, loss = 0.52850225\n",
      "Iteration 34, loss = 0.52520736\n",
      "Iteration 35, loss = 0.52215803\n",
      "Iteration 36, loss = 0.51907706\n",
      "Iteration 37, loss = 0.51600965\n",
      "Iteration 38, loss = 0.51299228\n",
      "Iteration 39, loss = 0.51001968\n",
      "Iteration 40, loss = 0.50712029\n",
      "Iteration 41, loss = 0.50432290\n",
      "Iteration 42, loss = 0.50153436\n",
      "Iteration 43, loss = 0.49878100\n",
      "Iteration 44, loss = 0.49604565\n",
      "Iteration 45, loss = 0.49350906\n",
      "Iteration 46, loss = 0.49079919\n",
      "Iteration 47, loss = 0.48827016\n",
      "Iteration 48, loss = 0.48573911\n",
      "Iteration 49, loss = 0.48335145\n",
      "Iteration 50, loss = 0.48088521\n",
      "Iteration 51, loss = 0.47851193\n",
      "Iteration 52, loss = 0.47621403\n",
      "Iteration 53, loss = 0.47418869\n",
      "Iteration 54, loss = 0.47180182\n",
      "Iteration 55, loss = 0.46960346\n",
      "Iteration 56, loss = 0.46742771\n",
      "Iteration 57, loss = 0.46549930\n",
      "Iteration 58, loss = 0.46363346\n",
      "Iteration 59, loss = 0.46153365\n",
      "Iteration 60, loss = 0.45955686\n",
      "Iteration 61, loss = 0.45784567\n",
      "Iteration 62, loss = 0.45616084\n",
      "Iteration 63, loss = 0.45442214\n",
      "Iteration 64, loss = 0.45269132\n",
      "Iteration 65, loss = 0.45100051\n",
      "Iteration 66, loss = 0.44948603\n",
      "Iteration 67, loss = 0.44800086\n",
      "Iteration 68, loss = 0.44655839\n",
      "Iteration 69, loss = 0.44505525\n",
      "Iteration 70, loss = 0.44388170\n",
      "Iteration 71, loss = 0.44244163\n",
      "Iteration 72, loss = 0.44122486\n",
      "Iteration 73, loss = 0.44001571\n",
      "Iteration 74, loss = 0.43877197\n",
      "Iteration 75, loss = 0.43767835\n",
      "Iteration 76, loss = 0.43667213\n",
      "Iteration 77, loss = 0.43567028\n",
      "Iteration 78, loss = 0.43474515\n",
      "Iteration 79, loss = 0.43372886\n",
      "Iteration 80, loss = 0.43285721\n",
      "Iteration 81, loss = 0.43200075\n",
      "Iteration 82, loss = 0.43126664\n",
      "Iteration 83, loss = 0.43057541\n",
      "Iteration 84, loss = 0.42977464\n",
      "Iteration 85, loss = 0.42913467\n",
      "Iteration 86, loss = 0.42835980\n",
      "Iteration 87, loss = 0.42784402\n",
      "Iteration 88, loss = 0.42729560\n",
      "Iteration 89, loss = 0.42668324\n",
      "Iteration 90, loss = 0.42614034\n",
      "Iteration 91, loss = 0.42561410\n",
      "Iteration 92, loss = 0.42516312\n",
      "Iteration 93, loss = 0.42467729\n",
      "Iteration 94, loss = 0.42433215\n",
      "Iteration 95, loss = 0.42387262\n",
      "Iteration 96, loss = 0.42354244\n",
      "Iteration 97, loss = 0.42315816\n",
      "Iteration 98, loss = 0.42279103\n",
      "Iteration 99, loss = 0.42249795\n",
      "Iteration 100, loss = 0.42217631\n",
      "Iteration 101, loss = 0.42203807\n",
      "Iteration 102, loss = 0.42165707\n",
      "Iteration 103, loss = 0.42143631\n",
      "Iteration 104, loss = 0.42118479\n",
      "Iteration 105, loss = 0.42094723\n",
      "Iteration 106, loss = 0.42077622\n",
      "Iteration 107, loss = 0.42051390\n",
      "Iteration 108, loss = 0.42037916\n",
      "Iteration 109, loss = 0.42022303\n",
      "Iteration 110, loss = 0.42005577\n",
      "Iteration 111, loss = 0.41985800\n",
      "Iteration 112, loss = 0.41974724\n",
      "Iteration 113, loss = 0.41959756\n",
      "Iteration 114, loss = 0.41946866\n",
      "Iteration 115, loss = 0.41934139\n",
      "Iteration 116, loss = 0.41919519\n",
      "Iteration 117, loss = 0.41909260\n",
      "Iteration 118, loss = 0.41905434\n",
      "Iteration 119, loss = 0.41899186\n",
      "Iteration 120, loss = 0.41885729\n",
      "Iteration 121, loss = 0.41880463\n",
      "Iteration 122, loss = 0.41888350\n",
      "Iteration 123, loss = 0.41861925\n",
      "Iteration 124, loss = 0.41857595\n",
      "Iteration 125, loss = 0.41844190\n",
      "Iteration 126, loss = 0.41837425\n",
      "Iteration 127, loss = 0.41834546\n",
      "Iteration 128, loss = 0.41831462\n",
      "Iteration 129, loss = 0.41824754\n",
      "Iteration 130, loss = 0.41823707\n",
      "Iteration 131, loss = 0.41816242\n",
      "Iteration 132, loss = 0.41810089\n",
      "Iteration 133, loss = 0.41807060\n",
      "Iteration 134, loss = 0.41798774\n",
      "Iteration 135, loss = 0.41794734\n",
      "Iteration 136, loss = 0.41791577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75238777\n",
      "Iteration 2, loss = 0.71783857\n",
      "Iteration 3, loss = 0.68876032\n",
      "Iteration 4, loss = 0.66691054\n",
      "Iteration 5, loss = 0.65061089\n",
      "Iteration 6, loss = 0.64116853\n",
      "Iteration 7, loss = 0.63295609\n",
      "Iteration 8, loss = 0.62810841\n",
      "Iteration 9, loss = 0.62374831\n",
      "Iteration 10, loss = 0.61946306\n",
      "Iteration 11, loss = 0.61515822\n",
      "Iteration 12, loss = 0.61060539\n",
      "Iteration 13, loss = 0.60563065\n",
      "Iteration 14, loss = 0.60152902\n",
      "Iteration 15, loss = 0.59623998\n",
      "Iteration 16, loss = 0.59187749\n",
      "Iteration 17, loss = 0.58751569\n",
      "Iteration 18, loss = 0.58356633\n",
      "Iteration 19, loss = 0.58002127\n",
      "Iteration 20, loss = 0.57605051\n",
      "Iteration 21, loss = 0.57244648\n",
      "Iteration 22, loss = 0.56872258\n",
      "Iteration 23, loss = 0.56517693\n",
      "Iteration 24, loss = 0.56156791\n",
      "Iteration 25, loss = 0.55808344\n",
      "Iteration 26, loss = 0.55466017\n",
      "Iteration 27, loss = 0.55133888\n",
      "Iteration 28, loss = 0.54818720\n",
      "Iteration 29, loss = 0.54499252\n",
      "Iteration 30, loss = 0.54184320\n",
      "Iteration 31, loss = 0.53883855\n",
      "Iteration 32, loss = 0.53576559\n",
      "Iteration 33, loss = 0.53282863\n",
      "Iteration 34, loss = 0.52994260\n",
      "Iteration 35, loss = 0.52712918\n",
      "Iteration 36, loss = 0.52450786\n",
      "Iteration 37, loss = 0.52167451\n",
      "Iteration 38, loss = 0.51915657\n",
      "Iteration 39, loss = 0.51651801\n",
      "Iteration 40, loss = 0.51401390\n",
      "Iteration 41, loss = 0.51163839\n",
      "Iteration 42, loss = 0.50920047\n",
      "Iteration 43, loss = 0.50681232\n",
      "Iteration 44, loss = 0.50462937\n",
      "Iteration 45, loss = 0.50245949\n",
      "Iteration 46, loss = 0.50015253\n",
      "Iteration 47, loss = 0.49812650\n",
      "Iteration 48, loss = 0.49606712\n",
      "Iteration 49, loss = 0.49404044\n",
      "Iteration 50, loss = 0.49213480\n",
      "Iteration 51, loss = 0.49023354\n",
      "Iteration 52, loss = 0.48835543\n",
      "Iteration 53, loss = 0.48674827\n",
      "Iteration 54, loss = 0.48484354\n",
      "Iteration 55, loss = 0.48312528\n",
      "Iteration 56, loss = 0.48138984\n",
      "Iteration 57, loss = 0.47997462\n",
      "Iteration 58, loss = 0.47857792\n",
      "Iteration 59, loss = 0.47686662\n",
      "Iteration 60, loss = 0.47546599\n",
      "Iteration 61, loss = 0.47393004\n",
      "Iteration 62, loss = 0.47269207\n",
      "Iteration 63, loss = 0.47141719\n",
      "Iteration 64, loss = 0.47010807\n",
      "Iteration 65, loss = 0.46900735\n",
      "Iteration 66, loss = 0.46777966\n",
      "Iteration 67, loss = 0.46660445\n",
      "Iteration 68, loss = 0.46561105\n",
      "Iteration 69, loss = 0.46448036\n",
      "Iteration 70, loss = 0.46348500\n",
      "Iteration 71, loss = 0.46258984\n",
      "Iteration 72, loss = 0.46164276\n",
      "Iteration 73, loss = 0.46070885\n",
      "Iteration 74, loss = 0.45975511\n",
      "Iteration 75, loss = 0.45904215\n",
      "Iteration 76, loss = 0.45826874\n",
      "Iteration 77, loss = 0.45765807\n",
      "Iteration 78, loss = 0.45695186\n",
      "Iteration 79, loss = 0.45614956\n",
      "Iteration 80, loss = 0.45556026\n",
      "Iteration 81, loss = 0.45490327\n",
      "Iteration 82, loss = 0.45437451\n",
      "Iteration 83, loss = 0.45406919\n",
      "Iteration 84, loss = 0.45340556\n",
      "Iteration 85, loss = 0.45292569\n",
      "Iteration 86, loss = 0.45235282\n",
      "Iteration 87, loss = 0.45210734\n",
      "Iteration 88, loss = 0.45171165\n",
      "Iteration 89, loss = 0.45119165\n",
      "Iteration 90, loss = 0.45081286\n",
      "Iteration 91, loss = 0.45047694\n",
      "Iteration 92, loss = 0.45014584\n",
      "Iteration 93, loss = 0.44979416\n",
      "Iteration 94, loss = 0.44956407\n",
      "Iteration 95, loss = 0.44926350\n",
      "Iteration 96, loss = 0.44902762\n",
      "Iteration 97, loss = 0.44878790\n",
      "Iteration 98, loss = 0.44849348\n",
      "Iteration 99, loss = 0.44826408\n",
      "Iteration 100, loss = 0.44805709\n",
      "Iteration 101, loss = 0.44797363\n",
      "Iteration 102, loss = 0.44773794\n",
      "Iteration 103, loss = 0.44760606\n",
      "Iteration 104, loss = 0.44731898\n",
      "Iteration 105, loss = 0.44719480\n",
      "Iteration 106, loss = 0.44709024\n",
      "Iteration 107, loss = 0.44688535\n",
      "Iteration 108, loss = 0.44683216\n",
      "Iteration 109, loss = 0.44670878\n",
      "Iteration 110, loss = 0.44655148\n",
      "Iteration 111, loss = 0.44636929\n",
      "Iteration 112, loss = 0.44626571\n",
      "Iteration 113, loss = 0.44612274\n",
      "Iteration 114, loss = 0.44603529\n",
      "Iteration 115, loss = 0.44597783\n",
      "Iteration 116, loss = 0.44586394\n",
      "Iteration 117, loss = 0.44576929\n",
      "Iteration 118, loss = 0.44571384\n",
      "Iteration 119, loss = 0.44567870\n",
      "Iteration 120, loss = 0.44559851\n",
      "Iteration 121, loss = 0.44548264\n",
      "Iteration 122, loss = 0.44556133\n",
      "Iteration 123, loss = 0.44542374\n",
      "Iteration 124, loss = 0.44529714\n",
      "Iteration 125, loss = 0.44519204\n",
      "Iteration 126, loss = 0.44520361\n",
      "Iteration 127, loss = 0.44511200\n",
      "Iteration 128, loss = 0.44507326\n",
      "Iteration 129, loss = 0.44501627\n",
      "Iteration 130, loss = 0.44503739\n",
      "Iteration 131, loss = 0.44491141\n",
      "Iteration 132, loss = 0.44485190\n",
      "Iteration 133, loss = 0.44485706\n",
      "Iteration 134, loss = 0.44476738\n",
      "Iteration 135, loss = 0.44475630\n",
      "Iteration 136, loss = 0.44474630\n",
      "Iteration 137, loss = 0.44475641\n",
      "Iteration 138, loss = 0.44459408\n",
      "Iteration 139, loss = 0.44461189\n",
      "Iteration 140, loss = 0.44463814\n",
      "Iteration 141, loss = 0.44454612\n",
      "Iteration 142, loss = 0.44463381\n",
      "Iteration 143, loss = 0.44446345\n",
      "Iteration 144, loss = 0.44443731\n",
      "Iteration 145, loss = 0.44441895\n",
      "Iteration 146, loss = 0.44446716\n",
      "Iteration 147, loss = 0.44446432\n",
      "Iteration 148, loss = 0.44430587\n",
      "Iteration 149, loss = 0.44425918\n",
      "Iteration 150, loss = 0.44427713\n",
      "Iteration 151, loss = 0.44425941\n",
      "Iteration 152, loss = 0.44427467\n",
      "Iteration 153, loss = 0.44435401\n",
      "Iteration 154, loss = 0.44418867\n",
      "Iteration 155, loss = 0.44415731\n",
      "Iteration 156, loss = 0.44406490\n",
      "Iteration 157, loss = 0.44407393\n",
      "Iteration 158, loss = 0.44404343\n",
      "Iteration 159, loss = 0.44399899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973378\n",
      "Iteration 2, loss = 0.71531152\n",
      "Iteration 3, loss = 0.68650393\n",
      "Iteration 4, loss = 0.66331417\n",
      "Iteration 5, loss = 0.64774098\n",
      "Iteration 6, loss = 0.63645526\n",
      "Iteration 7, loss = 0.62839862\n",
      "Iteration 8, loss = 0.62338652\n",
      "Iteration 9, loss = 0.61832790\n",
      "Iteration 10, loss = 0.61358958\n",
      "Iteration 11, loss = 0.60883986\n",
      "Iteration 12, loss = 0.60327302\n",
      "Iteration 13, loss = 0.59718387\n",
      "Iteration 14, loss = 0.59181679\n",
      "Iteration 15, loss = 0.58640824\n",
      "Iteration 16, loss = 0.58129955\n",
      "Iteration 17, loss = 0.57640464\n",
      "Iteration 18, loss = 0.57177334\n",
      "Iteration 19, loss = 0.56738321\n",
      "Iteration 20, loss = 0.56303541\n",
      "Iteration 21, loss = 0.55869821\n",
      "Iteration 22, loss = 0.55444797\n",
      "Iteration 23, loss = 0.55017252\n",
      "Iteration 24, loss = 0.54595599\n",
      "Iteration 25, loss = 0.54187386\n",
      "Iteration 26, loss = 0.53804911\n",
      "Iteration 27, loss = 0.53404718\n",
      "Iteration 28, loss = 0.53020618\n",
      "Iteration 29, loss = 0.52656872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.52280114\n",
      "Iteration 31, loss = 0.51931049\n",
      "Iteration 32, loss = 0.51594085\n",
      "Iteration 33, loss = 0.51236792\n",
      "Iteration 34, loss = 0.50897328\n",
      "Iteration 35, loss = 0.50569292\n",
      "Iteration 36, loss = 0.50272920\n",
      "Iteration 37, loss = 0.49949887\n",
      "Iteration 38, loss = 0.49651797\n",
      "Iteration 39, loss = 0.49360152\n",
      "Iteration 40, loss = 0.49067648\n",
      "Iteration 41, loss = 0.48781428\n",
      "Iteration 42, loss = 0.48511713\n",
      "Iteration 43, loss = 0.48247753\n",
      "Iteration 44, loss = 0.47987701\n",
      "Iteration 45, loss = 0.47735479\n",
      "Iteration 46, loss = 0.47488566\n",
      "Iteration 47, loss = 0.47256855\n",
      "Iteration 48, loss = 0.47009154\n",
      "Iteration 49, loss = 0.46777695\n",
      "Iteration 50, loss = 0.46573622\n",
      "Iteration 51, loss = 0.46352372\n",
      "Iteration 52, loss = 0.46148144\n",
      "Iteration 53, loss = 0.45932963\n",
      "Iteration 54, loss = 0.45746796\n",
      "Iteration 55, loss = 0.45558758\n",
      "Iteration 56, loss = 0.45368167\n",
      "Iteration 57, loss = 0.45188549\n",
      "Iteration 58, loss = 0.45014428\n",
      "Iteration 59, loss = 0.44842171\n",
      "Iteration 60, loss = 0.44679092\n",
      "Iteration 61, loss = 0.44519893\n",
      "Iteration 62, loss = 0.44378505\n",
      "Iteration 63, loss = 0.44221490\n",
      "Iteration 64, loss = 0.44079588\n",
      "Iteration 65, loss = 0.43936197\n",
      "Iteration 66, loss = 0.43797113\n",
      "Iteration 67, loss = 0.43669876\n",
      "Iteration 68, loss = 0.43547896\n",
      "Iteration 69, loss = 0.43431270\n",
      "Iteration 70, loss = 0.43318707\n",
      "Iteration 71, loss = 0.43206701\n",
      "Iteration 72, loss = 0.43097379\n",
      "Iteration 73, loss = 0.42990838\n",
      "Iteration 74, loss = 0.42903320\n",
      "Iteration 75, loss = 0.42812498\n",
      "Iteration 76, loss = 0.42718966\n",
      "Iteration 77, loss = 0.42629759\n",
      "Iteration 78, loss = 0.42550461\n",
      "Iteration 79, loss = 0.42477451\n",
      "Iteration 80, loss = 0.42417208\n",
      "Iteration 81, loss = 0.42343148\n",
      "Iteration 82, loss = 0.42267237\n",
      "Iteration 83, loss = 0.42215138\n",
      "Iteration 84, loss = 0.42148203\n",
      "Iteration 85, loss = 0.42093217\n",
      "Iteration 86, loss = 0.42031044\n",
      "Iteration 87, loss = 0.41984866\n",
      "Iteration 88, loss = 0.41947695\n",
      "Iteration 89, loss = 0.41888691\n",
      "Iteration 90, loss = 0.41840239\n",
      "Iteration 91, loss = 0.41813694\n",
      "Iteration 92, loss = 0.41776387\n",
      "Iteration 93, loss = 0.41735532\n",
      "Iteration 94, loss = 0.41712740\n",
      "Iteration 95, loss = 0.41669368\n",
      "Iteration 96, loss = 0.41646112\n",
      "Iteration 97, loss = 0.41612124\n",
      "Iteration 98, loss = 0.41590725\n",
      "Iteration 99, loss = 0.41557875\n",
      "Iteration 100, loss = 0.41531344\n",
      "Iteration 101, loss = 0.41517789\n",
      "Iteration 102, loss = 0.41486621\n",
      "Iteration 103, loss = 0.41472497\n",
      "Iteration 104, loss = 0.41453324\n",
      "Iteration 105, loss = 0.41431625\n",
      "Iteration 106, loss = 0.41413040\n",
      "Iteration 107, loss = 0.41405440\n",
      "Iteration 108, loss = 0.41393034\n",
      "Iteration 109, loss = 0.41372583\n",
      "Iteration 110, loss = 0.41357883\n",
      "Iteration 111, loss = 0.41344462\n",
      "Iteration 112, loss = 0.41334234\n",
      "Iteration 113, loss = 0.41323710\n",
      "Iteration 114, loss = 0.41312619\n",
      "Iteration 115, loss = 0.41302784\n",
      "Iteration 116, loss = 0.41291992\n",
      "Iteration 117, loss = 0.41286605\n",
      "Iteration 118, loss = 0.41277684\n",
      "Iteration 119, loss = 0.41266948\n",
      "Iteration 120, loss = 0.41257750\n",
      "Iteration 121, loss = 0.41251713\n",
      "Iteration 122, loss = 0.41255783\n",
      "Iteration 123, loss = 0.41234952\n",
      "Iteration 124, loss = 0.41243192\n",
      "Iteration 125, loss = 0.41222176\n",
      "Iteration 126, loss = 0.41220297\n",
      "Iteration 127, loss = 0.41212749\n",
      "Iteration 128, loss = 0.41209214\n",
      "Iteration 129, loss = 0.41207354\n",
      "Iteration 130, loss = 0.41194583\n",
      "Iteration 131, loss = 0.41198819\n",
      "Iteration 132, loss = 0.41195264\n",
      "Iteration 133, loss = 0.41193297\n",
      "Iteration 134, loss = 0.41179562\n",
      "Iteration 135, loss = 0.41180800\n",
      "Iteration 136, loss = 0.41173206\n",
      "Iteration 137, loss = 0.41175696\n",
      "Iteration 138, loss = 0.41163762\n",
      "Iteration 139, loss = 0.41168329\n",
      "Iteration 140, loss = 0.41160074\n",
      "Iteration 141, loss = 0.41155525\n",
      "Iteration 142, loss = 0.41152180\n",
      "Iteration 143, loss = 0.41148287\n",
      "Iteration 144, loss = 0.41143353\n",
      "Iteration 145, loss = 0.41155804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072464\n",
      "Iteration 2, loss = 0.71622147\n",
      "Iteration 3, loss = 0.68706378\n",
      "Iteration 4, loss = 0.66505886\n",
      "Iteration 5, loss = 0.65028373\n",
      "Iteration 6, loss = 0.63926039\n",
      "Iteration 7, loss = 0.63130425\n",
      "Iteration 8, loss = 0.62729766\n",
      "Iteration 9, loss = 0.62245792\n",
      "Iteration 10, loss = 0.61804800\n",
      "Iteration 11, loss = 0.61373384\n",
      "Iteration 12, loss = 0.60893213\n",
      "Iteration 13, loss = 0.60342394\n",
      "Iteration 14, loss = 0.59872899\n",
      "Iteration 15, loss = 0.59394356\n",
      "Iteration 16, loss = 0.58944645\n",
      "Iteration 17, loss = 0.58524608\n",
      "Iteration 18, loss = 0.58110277\n",
      "Iteration 19, loss = 0.57732994\n",
      "Iteration 20, loss = 0.57353587\n",
      "Iteration 21, loss = 0.56980930\n",
      "Iteration 22, loss = 0.56619137\n",
      "Iteration 23, loss = 0.56254190\n",
      "Iteration 24, loss = 0.55896992\n",
      "Iteration 25, loss = 0.55543399\n",
      "Iteration 26, loss = 0.55212476\n",
      "Iteration 27, loss = 0.54882173\n",
      "Iteration 28, loss = 0.54551802\n",
      "Iteration 29, loss = 0.54233375\n",
      "Iteration 30, loss = 0.53908961\n",
      "Iteration 31, loss = 0.53599742\n",
      "Iteration 32, loss = 0.53327856\n",
      "Iteration 33, loss = 0.53016458\n",
      "Iteration 34, loss = 0.52719484\n",
      "Iteration 35, loss = 0.52435293\n",
      "Iteration 36, loss = 0.52182022\n",
      "Iteration 37, loss = 0.51893275\n",
      "Iteration 38, loss = 0.51639027\n",
      "Iteration 39, loss = 0.51383970\n",
      "Iteration 40, loss = 0.51129656\n",
      "Iteration 41, loss = 0.50872109\n",
      "Iteration 42, loss = 0.50630658\n",
      "Iteration 43, loss = 0.50394919\n",
      "Iteration 44, loss = 0.50159053\n",
      "Iteration 45, loss = 0.49935462\n",
      "Iteration 46, loss = 0.49716814\n",
      "Iteration 47, loss = 0.49493450\n",
      "Iteration 48, loss = 0.49282986\n",
      "Iteration 49, loss = 0.49075185\n",
      "Iteration 50, loss = 0.48881453\n",
      "Iteration 51, loss = 0.48685215\n",
      "Iteration 52, loss = 0.48482872\n",
      "Iteration 53, loss = 0.48284087\n",
      "Iteration 54, loss = 0.48117080\n",
      "Iteration 55, loss = 0.47935418\n",
      "Iteration 56, loss = 0.47754254\n",
      "Iteration 57, loss = 0.47589000\n",
      "Iteration 58, loss = 0.47419789\n",
      "Iteration 59, loss = 0.47268374\n",
      "Iteration 60, loss = 0.47092638\n",
      "Iteration 61, loss = 0.46957715\n",
      "Iteration 62, loss = 0.46811881\n",
      "Iteration 63, loss = 0.46669728\n",
      "Iteration 64, loss = 0.46511027\n",
      "Iteration 65, loss = 0.46384177\n",
      "Iteration 66, loss = 0.46264194\n",
      "Iteration 67, loss = 0.46122956\n",
      "Iteration 68, loss = 0.46004365\n",
      "Iteration 69, loss = 0.45891405\n",
      "Iteration 70, loss = 0.45782497\n",
      "Iteration 71, loss = 0.45670036\n",
      "Iteration 72, loss = 0.45554554\n",
      "Iteration 73, loss = 0.45450533\n",
      "Iteration 74, loss = 0.45373575\n",
      "Iteration 75, loss = 0.45266754\n",
      "Iteration 76, loss = 0.45181949\n",
      "Iteration 77, loss = 0.45092335\n",
      "Iteration 78, loss = 0.45009785\n",
      "Iteration 79, loss = 0.44929675\n",
      "Iteration 80, loss = 0.44862246\n",
      "Iteration 81, loss = 0.44794201\n",
      "Iteration 82, loss = 0.44714995\n",
      "Iteration 83, loss = 0.44672935\n",
      "Iteration 84, loss = 0.44588534\n",
      "Iteration 85, loss = 0.44534096\n",
      "Iteration 86, loss = 0.44473749\n",
      "Iteration 87, loss = 0.44422586\n",
      "Iteration 88, loss = 0.44378566\n",
      "Iteration 89, loss = 0.44324820\n",
      "Iteration 90, loss = 0.44275460\n",
      "Iteration 91, loss = 0.44239707\n",
      "Iteration 92, loss = 0.44196539\n",
      "Iteration 93, loss = 0.44156490\n",
      "Iteration 94, loss = 0.44126214\n",
      "Iteration 95, loss = 0.44086909\n",
      "Iteration 96, loss = 0.44053224\n",
      "Iteration 97, loss = 0.44035701\n",
      "Iteration 98, loss = 0.44012894\n",
      "Iteration 99, loss = 0.43965729\n",
      "Iteration 100, loss = 0.43938658\n",
      "Iteration 101, loss = 0.43922173\n",
      "Iteration 102, loss = 0.43890053\n",
      "Iteration 103, loss = 0.43870082\n",
      "Iteration 104, loss = 0.43857428\n",
      "Iteration 105, loss = 0.43835211\n",
      "Iteration 106, loss = 0.43808405\n",
      "Iteration 107, loss = 0.43794993\n",
      "Iteration 108, loss = 0.43778799\n",
      "Iteration 109, loss = 0.43764913\n",
      "Iteration 110, loss = 0.43754426\n",
      "Iteration 111, loss = 0.43738030\n",
      "Iteration 112, loss = 0.43722706\n",
      "Iteration 113, loss = 0.43713956\n",
      "Iteration 114, loss = 0.43706836\n",
      "Iteration 115, loss = 0.43688948\n",
      "Iteration 116, loss = 0.43680355\n",
      "Iteration 117, loss = 0.43673524\n",
      "Iteration 118, loss = 0.43669776\n",
      "Iteration 119, loss = 0.43653632\n",
      "Iteration 120, loss = 0.43650622\n",
      "Iteration 121, loss = 0.43639414\n",
      "Iteration 122, loss = 0.43638905\n",
      "Iteration 123, loss = 0.43620149\n",
      "Iteration 124, loss = 0.43618041\n",
      "Iteration 125, loss = 0.43615809\n",
      "Iteration 126, loss = 0.43607250\n",
      "Iteration 127, loss = 0.43600001\n",
      "Iteration 128, loss = 0.43592941\n",
      "Iteration 129, loss = 0.43591449\n",
      "Iteration 130, loss = 0.43581076\n",
      "Iteration 131, loss = 0.43580986\n",
      "Iteration 132, loss = 0.43572380\n",
      "Iteration 133, loss = 0.43575837\n",
      "Iteration 134, loss = 0.43566933\n",
      "Iteration 135, loss = 0.43568572\n",
      "Iteration 136, loss = 0.43559035\n",
      "Iteration 137, loss = 0.43557878\n",
      "Iteration 138, loss = 0.43557378\n",
      "Iteration 139, loss = 0.43557230\n",
      "Iteration 140, loss = 0.43545888\n",
      "Iteration 141, loss = 0.43549881\n",
      "Iteration 142, loss = 0.43543360\n",
      "Iteration 143, loss = 0.43535428\n",
      "Iteration 144, loss = 0.43542199\n",
      "Iteration 145, loss = 0.43533893\n",
      "Iteration 146, loss = 0.43538387\n",
      "Iteration 147, loss = 0.43557630\n",
      "Iteration 148, loss = 0.43540749\n",
      "Iteration 149, loss = 0.43538787\n",
      "Iteration 150, loss = 0.43523781\n",
      "Iteration 151, loss = 0.43521040\n",
      "Iteration 152, loss = 0.43523629\n",
      "Iteration 153, loss = 0.43513487\n",
      "Iteration 154, loss = 0.43517132\n",
      "Iteration 155, loss = 0.43525089\n",
      "Iteration 156, loss = 0.43515360\n",
      "Iteration 157, loss = 0.43506742\n",
      "Iteration 158, loss = 0.43512570\n",
      "Iteration 159, loss = 0.43509856\n",
      "Iteration 160, loss = 0.43500087\n",
      "Iteration 161, loss = 0.43501283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245305\n",
      "Iteration 2, loss = 0.71880328\n",
      "Iteration 3, loss = 0.68933433\n",
      "Iteration 4, loss = 0.66866033\n",
      "Iteration 5, loss = 0.65262072\n",
      "Iteration 6, loss = 0.64170675\n",
      "Iteration 7, loss = 0.63397583\n",
      "Iteration 8, loss = 0.62898817\n",
      "Iteration 9, loss = 0.62457784\n",
      "Iteration 10, loss = 0.62023252\n",
      "Iteration 11, loss = 0.61608946\n",
      "Iteration 12, loss = 0.61130297\n",
      "Iteration 13, loss = 0.60657838\n",
      "Iteration 14, loss = 0.60248630\n",
      "Iteration 15, loss = 0.59702309\n",
      "Iteration 16, loss = 0.59258314\n",
      "Iteration 17, loss = 0.58857651\n",
      "Iteration 18, loss = 0.58443576\n",
      "Iteration 19, loss = 0.58075852\n",
      "Iteration 20, loss = 0.57662637\n",
      "Iteration 21, loss = 0.57305727\n",
      "Iteration 22, loss = 0.56944476\n",
      "Iteration 23, loss = 0.56560607\n",
      "Iteration 24, loss = 0.56207137\n",
      "Iteration 25, loss = 0.55866294\n",
      "Iteration 26, loss = 0.55506874\n",
      "Iteration 27, loss = 0.55161699\n",
      "Iteration 28, loss = 0.54840941\n",
      "Iteration 29, loss = 0.54500624\n",
      "Iteration 30, loss = 0.54186204\n",
      "Iteration 31, loss = 0.53860668\n",
      "Iteration 32, loss = 0.53552448\n",
      "Iteration 33, loss = 0.53247296\n",
      "Iteration 34, loss = 0.52939545\n",
      "Iteration 35, loss = 0.52652872\n",
      "Iteration 36, loss = 0.52357612\n",
      "Iteration 37, loss = 0.52067983\n",
      "Iteration 38, loss = 0.51777389\n",
      "Iteration 39, loss = 0.51500494\n",
      "Iteration 40, loss = 0.51232588\n",
      "Iteration 41, loss = 0.50955887\n",
      "Iteration 42, loss = 0.50701827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.50432913\n",
      "Iteration 44, loss = 0.50194842\n",
      "Iteration 45, loss = 0.49929643\n",
      "Iteration 46, loss = 0.49688987\n",
      "Iteration 47, loss = 0.49438083\n",
      "Iteration 48, loss = 0.49197684\n",
      "Iteration 49, loss = 0.48964870\n",
      "Iteration 50, loss = 0.48751879\n",
      "Iteration 51, loss = 0.48513177\n",
      "Iteration 52, loss = 0.48284835\n",
      "Iteration 53, loss = 0.48083807\n",
      "Iteration 54, loss = 0.47875116\n",
      "Iteration 55, loss = 0.47658430\n",
      "Iteration 56, loss = 0.47454910\n",
      "Iteration 57, loss = 0.47264221\n",
      "Iteration 58, loss = 0.47077684\n",
      "Iteration 59, loss = 0.46878947\n",
      "Iteration 60, loss = 0.46692557\n",
      "Iteration 61, loss = 0.46513459\n",
      "Iteration 62, loss = 0.46353769\n",
      "Iteration 63, loss = 0.46174185\n",
      "Iteration 64, loss = 0.46011082\n",
      "Iteration 65, loss = 0.45851023\n",
      "Iteration 66, loss = 0.45701104\n",
      "Iteration 67, loss = 0.45562550\n",
      "Iteration 68, loss = 0.45408095\n",
      "Iteration 69, loss = 0.45268775\n",
      "Iteration 70, loss = 0.45154998\n",
      "Iteration 71, loss = 0.45009022\n",
      "Iteration 72, loss = 0.44888490\n",
      "Iteration 73, loss = 0.44768763\n",
      "Iteration 74, loss = 0.44650211\n",
      "Iteration 75, loss = 0.44540486\n",
      "Iteration 76, loss = 0.44443132\n",
      "Iteration 77, loss = 0.44334950\n",
      "Iteration 78, loss = 0.44250625\n",
      "Iteration 79, loss = 0.44150125\n",
      "Iteration 80, loss = 0.44053670\n",
      "Iteration 81, loss = 0.43975358\n",
      "Iteration 82, loss = 0.43905151\n",
      "Iteration 83, loss = 0.43824780\n",
      "Iteration 84, loss = 0.43742990\n",
      "Iteration 85, loss = 0.43686257\n",
      "Iteration 86, loss = 0.43608382\n",
      "Iteration 87, loss = 0.43547272\n",
      "Iteration 88, loss = 0.43504279\n",
      "Iteration 89, loss = 0.43431389\n",
      "Iteration 90, loss = 0.43375876\n",
      "Iteration 91, loss = 0.43327890\n",
      "Iteration 92, loss = 0.43288165\n",
      "Iteration 93, loss = 0.43227636\n",
      "Iteration 94, loss = 0.43194294\n",
      "Iteration 95, loss = 0.43137157\n",
      "Iteration 96, loss = 0.43100980\n",
      "Iteration 97, loss = 0.43063581\n",
      "Iteration 98, loss = 0.43026282\n",
      "Iteration 99, loss = 0.42988719\n",
      "Iteration 100, loss = 0.42956606\n",
      "Iteration 101, loss = 0.42937768\n",
      "Iteration 102, loss = 0.42898680\n",
      "Iteration 103, loss = 0.42871680\n",
      "Iteration 104, loss = 0.42845662\n",
      "Iteration 105, loss = 0.42817201\n",
      "Iteration 106, loss = 0.42792978\n",
      "Iteration 107, loss = 0.42769680\n",
      "Iteration 108, loss = 0.42749847\n",
      "Iteration 109, loss = 0.42724048\n",
      "Iteration 110, loss = 0.42703617\n",
      "Iteration 111, loss = 0.42691601\n",
      "Iteration 112, loss = 0.42667521\n",
      "Iteration 113, loss = 0.42649637\n",
      "Iteration 114, loss = 0.42634023\n",
      "Iteration 115, loss = 0.42623328\n",
      "Iteration 116, loss = 0.42600818\n",
      "Iteration 117, loss = 0.42586273\n",
      "Iteration 118, loss = 0.42571308\n",
      "Iteration 119, loss = 0.42558030\n",
      "Iteration 120, loss = 0.42543106\n",
      "Iteration 121, loss = 0.42530404\n",
      "Iteration 122, loss = 0.42521285\n",
      "Iteration 123, loss = 0.42506309\n",
      "Iteration 124, loss = 0.42494026\n",
      "Iteration 125, loss = 0.42492821\n",
      "Iteration 126, loss = 0.42471802\n",
      "Iteration 127, loss = 0.42461370\n",
      "Iteration 128, loss = 0.42455917\n",
      "Iteration 129, loss = 0.42444372\n",
      "Iteration 130, loss = 0.42440008\n",
      "Iteration 131, loss = 0.42424062\n",
      "Iteration 132, loss = 0.42424722\n",
      "Iteration 133, loss = 0.42408357\n",
      "Iteration 134, loss = 0.42399564\n",
      "Iteration 135, loss = 0.42392022\n",
      "Iteration 136, loss = 0.42388060\n",
      "Iteration 137, loss = 0.42375245\n",
      "Iteration 138, loss = 0.42369093\n",
      "Iteration 139, loss = 0.42364254\n",
      "Iteration 140, loss = 0.42360844\n",
      "Iteration 141, loss = 0.42347822\n",
      "Iteration 142, loss = 0.42358050\n",
      "Iteration 143, loss = 0.42350492\n",
      "Iteration 144, loss = 0.42333077\n",
      "Iteration 145, loss = 0.42325496\n",
      "Iteration 146, loss = 0.42323878\n",
      "Iteration 147, loss = 0.42319039\n",
      "Iteration 148, loss = 0.42309960\n",
      "Iteration 149, loss = 0.42303464\n",
      "Iteration 150, loss = 0.42292609\n",
      "Iteration 151, loss = 0.42287273\n",
      "Iteration 152, loss = 0.42284867\n",
      "Iteration 153, loss = 0.42294721\n",
      "Iteration 154, loss = 0.42273122\n",
      "Iteration 155, loss = 0.42265633\n",
      "Iteration 156, loss = 0.42264415\n",
      "Iteration 157, loss = 0.42259903\n",
      "Iteration 158, loss = 0.42259164\n",
      "Iteration 159, loss = 0.42263569\n",
      "Iteration 160, loss = 0.42250815\n",
      "Iteration 161, loss = 0.42243299\n",
      "Iteration 162, loss = 0.42240906\n",
      "Iteration 163, loss = 0.42235624\n",
      "Iteration 164, loss = 0.42242605\n",
      "Iteration 165, loss = 0.42232924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164301\n",
      "Iteration 2, loss = 0.71828047\n",
      "Iteration 3, loss = 0.68858255\n",
      "Iteration 4, loss = 0.66801634\n",
      "Iteration 5, loss = 0.65194152\n",
      "Iteration 6, loss = 0.64122810\n",
      "Iteration 7, loss = 0.63356309\n",
      "Iteration 8, loss = 0.62835637\n",
      "Iteration 9, loss = 0.62394819\n",
      "Iteration 10, loss = 0.61949065\n",
      "Iteration 11, loss = 0.61514245\n",
      "Iteration 12, loss = 0.61038115\n",
      "Iteration 13, loss = 0.60537075\n",
      "Iteration 14, loss = 0.60128573\n",
      "Iteration 15, loss = 0.59575871\n",
      "Iteration 16, loss = 0.59119262\n",
      "Iteration 17, loss = 0.58693686\n",
      "Iteration 18, loss = 0.58288217\n",
      "Iteration 19, loss = 0.57906204\n",
      "Iteration 20, loss = 0.57486500\n",
      "Iteration 21, loss = 0.57091219\n",
      "Iteration 22, loss = 0.56715766\n",
      "Iteration 23, loss = 0.56329337\n",
      "Iteration 24, loss = 0.55962501\n",
      "Iteration 25, loss = 0.55595662\n",
      "Iteration 26, loss = 0.55231543\n",
      "Iteration 27, loss = 0.54860317\n",
      "Iteration 28, loss = 0.54526571\n",
      "Iteration 29, loss = 0.54172114\n",
      "Iteration 30, loss = 0.53837509\n",
      "Iteration 31, loss = 0.53506217\n",
      "Iteration 32, loss = 0.53171418\n",
      "Iteration 33, loss = 0.52846875\n",
      "Iteration 34, loss = 0.52517316\n",
      "Iteration 35, loss = 0.52212317\n",
      "Iteration 36, loss = 0.51904151\n",
      "Iteration 37, loss = 0.51597341\n",
      "Iteration 38, loss = 0.51295539\n",
      "Iteration 39, loss = 0.50998215\n",
      "Iteration 40, loss = 0.50708211\n",
      "Iteration 41, loss = 0.50428412\n",
      "Iteration 42, loss = 0.50149496\n",
      "Iteration 43, loss = 0.49874098\n",
      "Iteration 44, loss = 0.49600503\n",
      "Iteration 45, loss = 0.49346786\n",
      "Iteration 46, loss = 0.49075742\n",
      "Iteration 47, loss = 0.48822782\n",
      "Iteration 48, loss = 0.48569622\n",
      "Iteration 49, loss = 0.48330801\n",
      "Iteration 50, loss = 0.48084124\n",
      "Iteration 51, loss = 0.47846747\n",
      "Iteration 52, loss = 0.47616903\n",
      "Iteration 53, loss = 0.47414321\n",
      "Iteration 54, loss = 0.47175582\n",
      "Iteration 55, loss = 0.46955696\n",
      "Iteration 56, loss = 0.46738073\n",
      "Iteration 57, loss = 0.46545188\n",
      "Iteration 58, loss = 0.46358555\n",
      "Iteration 59, loss = 0.46148534\n",
      "Iteration 60, loss = 0.45950805\n",
      "Iteration 61, loss = 0.45779643\n",
      "Iteration 62, loss = 0.45611120\n",
      "Iteration 63, loss = 0.45437211\n",
      "Iteration 64, loss = 0.45264083\n",
      "Iteration 65, loss = 0.45094965\n",
      "Iteration 66, loss = 0.44943477\n",
      "Iteration 67, loss = 0.44794913\n",
      "Iteration 68, loss = 0.44650636\n",
      "Iteration 69, loss = 0.44500284\n",
      "Iteration 70, loss = 0.44382894\n",
      "Iteration 71, loss = 0.44238848\n",
      "Iteration 72, loss = 0.44117136\n",
      "Iteration 73, loss = 0.43996187\n",
      "Iteration 74, loss = 0.43871777\n",
      "Iteration 75, loss = 0.43762385\n",
      "Iteration 76, loss = 0.43661730\n",
      "Iteration 77, loss = 0.43561516\n",
      "Iteration 78, loss = 0.43468968\n",
      "Iteration 79, loss = 0.43367310\n",
      "Iteration 80, loss = 0.43280111\n",
      "Iteration 81, loss = 0.43194437\n",
      "Iteration 82, loss = 0.43120998\n",
      "Iteration 83, loss = 0.43051848\n",
      "Iteration 84, loss = 0.42971742\n",
      "Iteration 85, loss = 0.42907720\n",
      "Iteration 86, loss = 0.42830208\n",
      "Iteration 87, loss = 0.42778600\n",
      "Iteration 88, loss = 0.42723737\n",
      "Iteration 89, loss = 0.42662474\n",
      "Iteration 90, loss = 0.42608158\n",
      "Iteration 91, loss = 0.42555517\n",
      "Iteration 92, loss = 0.42510383\n",
      "Iteration 93, loss = 0.42461789\n",
      "Iteration 94, loss = 0.42427252\n",
      "Iteration 95, loss = 0.42381280\n",
      "Iteration 96, loss = 0.42348242\n",
      "Iteration 97, loss = 0.42309791\n",
      "Iteration 98, loss = 0.42273060\n",
      "Iteration 99, loss = 0.42243733\n",
      "Iteration 100, loss = 0.42211553\n",
      "Iteration 101, loss = 0.42197710\n",
      "Iteration 102, loss = 0.42159589\n",
      "Iteration 103, loss = 0.42137502\n",
      "Iteration 104, loss = 0.42112329\n",
      "Iteration 105, loss = 0.42088562\n",
      "Iteration 106, loss = 0.42071442\n",
      "Iteration 107, loss = 0.42045198\n",
      "Iteration 108, loss = 0.42031710\n",
      "Iteration 109, loss = 0.42016078\n",
      "Iteration 110, loss = 0.41999347\n",
      "Iteration 111, loss = 0.41979554\n",
      "Iteration 112, loss = 0.41968466\n",
      "Iteration 113, loss = 0.41953489\n",
      "Iteration 114, loss = 0.41940585\n",
      "Iteration 115, loss = 0.41927844\n",
      "Iteration 116, loss = 0.41913218\n",
      "Iteration 117, loss = 0.41902948\n",
      "Iteration 118, loss = 0.41899112\n",
      "Iteration 119, loss = 0.41892855\n",
      "Iteration 120, loss = 0.41879388\n",
      "Iteration 121, loss = 0.41874115\n",
      "Iteration 122, loss = 0.41881995\n",
      "Iteration 123, loss = 0.41855561\n",
      "Iteration 124, loss = 0.41851223\n",
      "Iteration 125, loss = 0.41837810\n",
      "Iteration 126, loss = 0.41831037\n",
      "Iteration 127, loss = 0.41828153\n",
      "Iteration 128, loss = 0.41825059\n",
      "Iteration 129, loss = 0.41818343\n",
      "Iteration 130, loss = 0.41817293\n",
      "Iteration 131, loss = 0.41809821\n",
      "Iteration 132, loss = 0.41803663\n",
      "Iteration 133, loss = 0.41800626\n",
      "Iteration 134, loss = 0.41792336\n",
      "Iteration 135, loss = 0.41788290\n",
      "Iteration 136, loss = 0.41785127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236987\n",
      "Iteration 2, loss = 0.71782060\n",
      "Iteration 3, loss = 0.68874227\n",
      "Iteration 4, loss = 0.66689237\n",
      "Iteration 5, loss = 0.65059256\n",
      "Iteration 6, loss = 0.64115001\n",
      "Iteration 7, loss = 0.63293734\n",
      "Iteration 8, loss = 0.62808938\n",
      "Iteration 9, loss = 0.62372897\n",
      "Iteration 10, loss = 0.61944335\n",
      "Iteration 11, loss = 0.61513811\n",
      "Iteration 12, loss = 0.61058485\n",
      "Iteration 13, loss = 0.60560964\n",
      "Iteration 14, loss = 0.60150750\n",
      "Iteration 15, loss = 0.59621793\n",
      "Iteration 16, loss = 0.59185487\n",
      "Iteration 17, loss = 0.58749247\n",
      "Iteration 18, loss = 0.58354247\n",
      "Iteration 19, loss = 0.57999677\n",
      "Iteration 20, loss = 0.57602534\n",
      "Iteration 21, loss = 0.57242063\n",
      "Iteration 22, loss = 0.56869602\n",
      "Iteration 23, loss = 0.56514964\n",
      "Iteration 24, loss = 0.56153987\n",
      "Iteration 25, loss = 0.55805464\n",
      "Iteration 26, loss = 0.55463061\n",
      "Iteration 27, loss = 0.55130855\n",
      "Iteration 28, loss = 0.54815610\n",
      "Iteration 29, loss = 0.54496064\n",
      "Iteration 30, loss = 0.54181054\n",
      "Iteration 31, loss = 0.53880512\n",
      "Iteration 32, loss = 0.53573135\n",
      "Iteration 33, loss = 0.53279364\n",
      "Iteration 34, loss = 0.52990684\n",
      "Iteration 35, loss = 0.52709267\n",
      "Iteration 36, loss = 0.52447061\n",
      "Iteration 37, loss = 0.52163651\n",
      "Iteration 38, loss = 0.51911784\n",
      "Iteration 39, loss = 0.51647858\n",
      "Iteration 40, loss = 0.51397377\n",
      "Iteration 41, loss = 0.51159760\n",
      "Iteration 42, loss = 0.50915903\n",
      "Iteration 43, loss = 0.50677020\n",
      "Iteration 44, loss = 0.50458665\n",
      "Iteration 45, loss = 0.50241613\n",
      "Iteration 46, loss = 0.50010859\n",
      "Iteration 47, loss = 0.49808198\n",
      "Iteration 48, loss = 0.49602202\n",
      "Iteration 49, loss = 0.49399480\n",
      "Iteration 50, loss = 0.49208864\n",
      "Iteration 51, loss = 0.49018689\n",
      "Iteration 52, loss = 0.48830828\n",
      "Iteration 53, loss = 0.48670065\n",
      "Iteration 54, loss = 0.48479540\n",
      "Iteration 55, loss = 0.48307666\n",
      "Iteration 56, loss = 0.48134078\n",
      "Iteration 57, loss = 0.47992518\n",
      "Iteration 58, loss = 0.47852799\n",
      "Iteration 59, loss = 0.47681633\n",
      "Iteration 60, loss = 0.47541526\n",
      "Iteration 61, loss = 0.47387889\n",
      "Iteration 62, loss = 0.47264054\n",
      "Iteration 63, loss = 0.47136533\n",
      "Iteration 64, loss = 0.47005576\n",
      "Iteration 65, loss = 0.46895473\n",
      "Iteration 66, loss = 0.46772668\n",
      "Iteration 67, loss = 0.46655104\n",
      "Iteration 68, loss = 0.46555741\n",
      "Iteration 69, loss = 0.46442635\n",
      "Iteration 70, loss = 0.46343066\n",
      "Iteration 71, loss = 0.46253520\n",
      "Iteration 72, loss = 0.46158781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.46065360\n",
      "Iteration 74, loss = 0.45969953\n",
      "Iteration 75, loss = 0.45898632\n",
      "Iteration 76, loss = 0.45821264\n",
      "Iteration 77, loss = 0.45760171\n",
      "Iteration 78, loss = 0.45689520\n",
      "Iteration 79, loss = 0.45609268\n",
      "Iteration 80, loss = 0.45550310\n",
      "Iteration 81, loss = 0.45484588\n",
      "Iteration 82, loss = 0.45431691\n",
      "Iteration 83, loss = 0.45401138\n",
      "Iteration 84, loss = 0.45334752\n",
      "Iteration 85, loss = 0.45286746\n",
      "Iteration 86, loss = 0.45229438\n",
      "Iteration 87, loss = 0.45204869\n",
      "Iteration 88, loss = 0.45165287\n",
      "Iteration 89, loss = 0.45113269\n",
      "Iteration 90, loss = 0.45075374\n",
      "Iteration 91, loss = 0.45041769\n",
      "Iteration 92, loss = 0.45008635\n",
      "Iteration 93, loss = 0.44973458\n",
      "Iteration 94, loss = 0.44950436\n",
      "Iteration 95, loss = 0.44920368\n",
      "Iteration 96, loss = 0.44896769\n",
      "Iteration 97, loss = 0.44872782\n",
      "Iteration 98, loss = 0.44843330\n",
      "Iteration 99, loss = 0.44820377\n",
      "Iteration 100, loss = 0.44799669\n",
      "Iteration 101, loss = 0.44791313\n",
      "Iteration 102, loss = 0.44767734\n",
      "Iteration 103, loss = 0.44754539\n",
      "Iteration 104, loss = 0.44725818\n",
      "Iteration 105, loss = 0.44713394\n",
      "Iteration 106, loss = 0.44702930\n",
      "Iteration 107, loss = 0.44682434\n",
      "Iteration 108, loss = 0.44677106\n",
      "Iteration 109, loss = 0.44664755\n",
      "Iteration 110, loss = 0.44649025\n",
      "Iteration 111, loss = 0.44630800\n",
      "Iteration 112, loss = 0.44620432\n",
      "Iteration 113, loss = 0.44606133\n",
      "Iteration 114, loss = 0.44597385\n",
      "Iteration 115, loss = 0.44591629\n",
      "Iteration 116, loss = 0.44580236\n",
      "Iteration 117, loss = 0.44570766\n",
      "Iteration 118, loss = 0.44565214\n",
      "Iteration 119, loss = 0.44561698\n",
      "Iteration 120, loss = 0.44553671\n",
      "Iteration 121, loss = 0.44542083\n",
      "Iteration 122, loss = 0.44549953\n",
      "Iteration 123, loss = 0.44536183\n",
      "Iteration 124, loss = 0.44523522\n",
      "Iteration 125, loss = 0.44513007\n",
      "Iteration 126, loss = 0.44514158\n",
      "Iteration 127, loss = 0.44504997\n",
      "Iteration 128, loss = 0.44501116\n",
      "Iteration 129, loss = 0.44495415\n",
      "Iteration 130, loss = 0.44497523\n",
      "Iteration 131, loss = 0.44484922\n",
      "Iteration 132, loss = 0.44478967\n",
      "Iteration 133, loss = 0.44479478\n",
      "Iteration 134, loss = 0.44470508\n",
      "Iteration 135, loss = 0.44469395\n",
      "Iteration 136, loss = 0.44468391\n",
      "Iteration 137, loss = 0.44469398\n",
      "Iteration 138, loss = 0.44453166\n",
      "Iteration 139, loss = 0.44454940\n",
      "Iteration 140, loss = 0.44457561\n",
      "Iteration 141, loss = 0.44448358\n",
      "Iteration 142, loss = 0.44457125\n",
      "Iteration 143, loss = 0.44440084\n",
      "Iteration 144, loss = 0.44437465\n",
      "Iteration 145, loss = 0.44435627\n",
      "Iteration 146, loss = 0.44440441\n",
      "Iteration 147, loss = 0.44440157\n",
      "Iteration 148, loss = 0.44424307\n",
      "Iteration 149, loss = 0.44419634\n",
      "Iteration 150, loss = 0.44421429\n",
      "Iteration 151, loss = 0.44419657\n",
      "Iteration 152, loss = 0.44421173\n",
      "Iteration 153, loss = 0.44429101\n",
      "Iteration 154, loss = 0.44412568\n",
      "Iteration 155, loss = 0.44409428\n",
      "Iteration 156, loss = 0.44400186\n",
      "Iteration 157, loss = 0.44401086\n",
      "Iteration 158, loss = 0.44398030\n",
      "Iteration 159, loss = 0.44393583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973198\n",
      "Iteration 2, loss = 0.71530972\n",
      "Iteration 3, loss = 0.68650212\n",
      "Iteration 4, loss = 0.66331235\n",
      "Iteration 5, loss = 0.64773914\n",
      "Iteration 6, loss = 0.63645341\n",
      "Iteration 7, loss = 0.62839674\n",
      "Iteration 8, loss = 0.62338461\n",
      "Iteration 9, loss = 0.61832596\n",
      "Iteration 10, loss = 0.61358760\n",
      "Iteration 11, loss = 0.60883785\n",
      "Iteration 12, loss = 0.60327096\n",
      "Iteration 13, loss = 0.59718176\n",
      "Iteration 14, loss = 0.59181463\n",
      "Iteration 15, loss = 0.58640602\n",
      "Iteration 16, loss = 0.58129727\n",
      "Iteration 17, loss = 0.57640230\n",
      "Iteration 18, loss = 0.57177093\n",
      "Iteration 19, loss = 0.56738073\n",
      "Iteration 20, loss = 0.56303287\n",
      "Iteration 21, loss = 0.55869559\n",
      "Iteration 22, loss = 0.55444528\n",
      "Iteration 23, loss = 0.55016975\n",
      "Iteration 24, loss = 0.54595314\n",
      "Iteration 25, loss = 0.54187093\n",
      "Iteration 26, loss = 0.53804610\n",
      "Iteration 27, loss = 0.53404409\n",
      "Iteration 28, loss = 0.53020300\n",
      "Iteration 29, loss = 0.52656546\n",
      "Iteration 30, loss = 0.52279780\n",
      "Iteration 31, loss = 0.51930706\n",
      "Iteration 32, loss = 0.51593733\n",
      "Iteration 33, loss = 0.51236432\n",
      "Iteration 34, loss = 0.50896960\n",
      "Iteration 35, loss = 0.50568916\n",
      "Iteration 36, loss = 0.50272536\n",
      "Iteration 37, loss = 0.49949495\n",
      "Iteration 38, loss = 0.49651397\n",
      "Iteration 39, loss = 0.49359745\n",
      "Iteration 40, loss = 0.49067233\n",
      "Iteration 41, loss = 0.48781005\n",
      "Iteration 42, loss = 0.48511284\n",
      "Iteration 43, loss = 0.48247317\n",
      "Iteration 44, loss = 0.47987258\n",
      "Iteration 45, loss = 0.47735029\n",
      "Iteration 46, loss = 0.47488109\n",
      "Iteration 47, loss = 0.47256392\n",
      "Iteration 48, loss = 0.47008685\n",
      "Iteration 49, loss = 0.46777220\n",
      "Iteration 50, loss = 0.46573142\n",
      "Iteration 51, loss = 0.46351886\n",
      "Iteration 52, loss = 0.46147652\n",
      "Iteration 53, loss = 0.45932466\n",
      "Iteration 54, loss = 0.45746294\n",
      "Iteration 55, loss = 0.45558250\n",
      "Iteration 56, loss = 0.45367655\n",
      "Iteration 57, loss = 0.45188032\n",
      "Iteration 58, loss = 0.45013907\n",
      "Iteration 59, loss = 0.44841645\n",
      "Iteration 60, loss = 0.44678562\n",
      "Iteration 61, loss = 0.44519358\n",
      "Iteration 62, loss = 0.44377966\n",
      "Iteration 63, loss = 0.44220947\n",
      "Iteration 64, loss = 0.44079041\n",
      "Iteration 65, loss = 0.43935646\n",
      "Iteration 66, loss = 0.43796558\n",
      "Iteration 67, loss = 0.43669317\n",
      "Iteration 68, loss = 0.43547334\n",
      "Iteration 69, loss = 0.43430704\n",
      "Iteration 70, loss = 0.43318137\n",
      "Iteration 71, loss = 0.43206127\n",
      "Iteration 72, loss = 0.43096802\n",
      "Iteration 73, loss = 0.42990257\n",
      "Iteration 74, loss = 0.42902736\n",
      "Iteration 75, loss = 0.42811910\n",
      "Iteration 76, loss = 0.42718375\n",
      "Iteration 77, loss = 0.42629165\n",
      "Iteration 78, loss = 0.42549863\n",
      "Iteration 79, loss = 0.42476850\n",
      "Iteration 80, loss = 0.42416604\n",
      "Iteration 81, loss = 0.42342541\n",
      "Iteration 82, loss = 0.42266627\n",
      "Iteration 83, loss = 0.42214526\n",
      "Iteration 84, loss = 0.42147587\n",
      "Iteration 85, loss = 0.42092599\n",
      "Iteration 86, loss = 0.42030423\n",
      "Iteration 87, loss = 0.41984243\n",
      "Iteration 88, loss = 0.41947070\n",
      "Iteration 89, loss = 0.41888063\n",
      "Iteration 90, loss = 0.41839608\n",
      "Iteration 91, loss = 0.41813062\n",
      "Iteration 92, loss = 0.41775753\n",
      "Iteration 93, loss = 0.41734896\n",
      "Iteration 94, loss = 0.41712101\n",
      "Iteration 95, loss = 0.41668728\n",
      "Iteration 96, loss = 0.41645470\n",
      "Iteration 97, loss = 0.41611481\n",
      "Iteration 98, loss = 0.41590080\n",
      "Iteration 99, loss = 0.41557228\n",
      "Iteration 100, loss = 0.41530696\n",
      "Iteration 101, loss = 0.41517140\n",
      "Iteration 102, loss = 0.41485970\n",
      "Iteration 103, loss = 0.41471846\n",
      "Iteration 104, loss = 0.41452672\n",
      "Iteration 105, loss = 0.41430972\n",
      "Iteration 106, loss = 0.41412385\n",
      "Iteration 107, loss = 0.41404786\n",
      "Iteration 108, loss = 0.41392378\n",
      "Iteration 109, loss = 0.41371926\n",
      "Iteration 110, loss = 0.41357226\n",
      "Iteration 111, loss = 0.41343804\n",
      "Iteration 112, loss = 0.41333576\n",
      "Iteration 113, loss = 0.41323051\n",
      "Iteration 114, loss = 0.41311959\n",
      "Iteration 115, loss = 0.41302124\n",
      "Iteration 116, loss = 0.41291331\n",
      "Iteration 117, loss = 0.41285944\n",
      "Iteration 118, loss = 0.41277022\n",
      "Iteration 119, loss = 0.41266286\n",
      "Iteration 120, loss = 0.41257088\n",
      "Iteration 121, loss = 0.41251050\n",
      "Iteration 122, loss = 0.41255120\n",
      "Iteration 123, loss = 0.41234289\n",
      "Iteration 124, loss = 0.41242528\n",
      "Iteration 125, loss = 0.41221512\n",
      "Iteration 126, loss = 0.41219634\n",
      "Iteration 127, loss = 0.41212086\n",
      "Iteration 128, loss = 0.41208551\n",
      "Iteration 129, loss = 0.41206690\n",
      "Iteration 130, loss = 0.41193919\n",
      "Iteration 131, loss = 0.41198155\n",
      "Iteration 132, loss = 0.41194600\n",
      "Iteration 133, loss = 0.41192632\n",
      "Iteration 134, loss = 0.41178898\n",
      "Iteration 135, loss = 0.41180135\n",
      "Iteration 136, loss = 0.41172541\n",
      "Iteration 137, loss = 0.41175031\n",
      "Iteration 138, loss = 0.41163097\n",
      "Iteration 139, loss = 0.41167664\n",
      "Iteration 140, loss = 0.41159409\n",
      "Iteration 141, loss = 0.41154859\n",
      "Iteration 142, loss = 0.41151515\n",
      "Iteration 143, loss = 0.41147621\n",
      "Iteration 144, loss = 0.41142688\n",
      "Iteration 145, loss = 0.41155138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072285\n",
      "Iteration 2, loss = 0.71621967\n",
      "Iteration 3, loss = 0.68706197\n",
      "Iteration 4, loss = 0.66505704\n",
      "Iteration 5, loss = 0.65028189\n",
      "Iteration 6, loss = 0.63925854\n",
      "Iteration 7, loss = 0.63130237\n",
      "Iteration 8, loss = 0.62729575\n",
      "Iteration 9, loss = 0.62245598\n",
      "Iteration 10, loss = 0.61804602\n",
      "Iteration 11, loss = 0.61373182\n",
      "Iteration 12, loss = 0.60893006\n",
      "Iteration 13, loss = 0.60342183\n",
      "Iteration 14, loss = 0.59872682\n",
      "Iteration 15, loss = 0.59394134\n",
      "Iteration 16, loss = 0.58944417\n",
      "Iteration 17, loss = 0.58524374\n",
      "Iteration 18, loss = 0.58110037\n",
      "Iteration 19, loss = 0.57732747\n",
      "Iteration 20, loss = 0.57353333\n",
      "Iteration 21, loss = 0.56980670\n",
      "Iteration 22, loss = 0.56618871\n",
      "Iteration 23, loss = 0.56253916\n",
      "Iteration 24, loss = 0.55896711\n",
      "Iteration 25, loss = 0.55543110\n",
      "Iteration 26, loss = 0.55212181\n",
      "Iteration 27, loss = 0.54881870\n",
      "Iteration 28, loss = 0.54551491\n",
      "Iteration 29, loss = 0.54233057\n",
      "Iteration 30, loss = 0.53908636\n",
      "Iteration 31, loss = 0.53599410\n",
      "Iteration 32, loss = 0.53327516\n",
      "Iteration 33, loss = 0.53016111\n",
      "Iteration 34, loss = 0.52719130\n",
      "Iteration 35, loss = 0.52434932\n",
      "Iteration 36, loss = 0.52181654\n",
      "Iteration 37, loss = 0.51892900\n",
      "Iteration 38, loss = 0.51638646\n",
      "Iteration 39, loss = 0.51383582\n",
      "Iteration 40, loss = 0.51129262\n",
      "Iteration 41, loss = 0.50871709\n",
      "Iteration 42, loss = 0.50630251\n",
      "Iteration 43, loss = 0.50394506\n",
      "Iteration 44, loss = 0.50158635\n",
      "Iteration 45, loss = 0.49935039\n",
      "Iteration 46, loss = 0.49716385\n",
      "Iteration 47, loss = 0.49493016\n",
      "Iteration 48, loss = 0.49282547\n",
      "Iteration 49, loss = 0.49074741\n",
      "Iteration 50, loss = 0.48881004\n",
      "Iteration 51, loss = 0.48684761\n",
      "Iteration 52, loss = 0.48482414\n",
      "Iteration 53, loss = 0.48283625\n",
      "Iteration 54, loss = 0.48116614\n",
      "Iteration 55, loss = 0.47934947\n",
      "Iteration 56, loss = 0.47753779\n",
      "Iteration 57, loss = 0.47588522\n",
      "Iteration 58, loss = 0.47419306\n",
      "Iteration 59, loss = 0.47267887\n",
      "Iteration 60, loss = 0.47092147\n",
      "Iteration 61, loss = 0.46957220\n",
      "Iteration 62, loss = 0.46811384\n",
      "Iteration 63, loss = 0.46669226\n",
      "Iteration 64, loss = 0.46510522\n",
      "Iteration 65, loss = 0.46383669\n",
      "Iteration 66, loss = 0.46263683\n",
      "Iteration 67, loss = 0.46122442\n",
      "Iteration 68, loss = 0.46003847\n",
      "Iteration 69, loss = 0.45890884\n",
      "Iteration 70, loss = 0.45781973\n",
      "Iteration 71, loss = 0.45669509\n",
      "Iteration 72, loss = 0.45554024\n",
      "Iteration 73, loss = 0.45449999\n",
      "Iteration 74, loss = 0.45373039\n",
      "Iteration 75, loss = 0.45266215\n",
      "Iteration 76, loss = 0.45181407\n",
      "Iteration 77, loss = 0.45091790\n",
      "Iteration 78, loss = 0.45009237\n",
      "Iteration 79, loss = 0.44929124\n",
      "Iteration 80, loss = 0.44861693\n",
      "Iteration 81, loss = 0.44793645\n",
      "Iteration 82, loss = 0.44714437\n",
      "Iteration 83, loss = 0.44672374\n",
      "Iteration 84, loss = 0.44587970\n",
      "Iteration 85, loss = 0.44533529\n",
      "Iteration 86, loss = 0.44473180\n",
      "Iteration 87, loss = 0.44422015\n",
      "Iteration 88, loss = 0.44377993\n",
      "Iteration 89, loss = 0.44324244\n",
      "Iteration 90, loss = 0.44274882\n",
      "Iteration 91, loss = 0.44239126\n",
      "Iteration 92, loss = 0.44195957\n",
      "Iteration 93, loss = 0.44155906\n",
      "Iteration 94, loss = 0.44125627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, loss = 0.44086320\n",
      "Iteration 96, loss = 0.44052634\n",
      "Iteration 97, loss = 0.44035109\n",
      "Iteration 98, loss = 0.44012300\n",
      "Iteration 99, loss = 0.43965133\n",
      "Iteration 100, loss = 0.43938060\n",
      "Iteration 101, loss = 0.43921573\n",
      "Iteration 102, loss = 0.43889451\n",
      "Iteration 103, loss = 0.43869479\n",
      "Iteration 104, loss = 0.43856823\n",
      "Iteration 105, loss = 0.43834605\n",
      "Iteration 106, loss = 0.43807798\n",
      "Iteration 107, loss = 0.43794385\n",
      "Iteration 108, loss = 0.43778189\n",
      "Iteration 109, loss = 0.43764301\n",
      "Iteration 110, loss = 0.43753813\n",
      "Iteration 111, loss = 0.43737416\n",
      "Iteration 112, loss = 0.43722091\n",
      "Iteration 113, loss = 0.43713340\n",
      "Iteration 114, loss = 0.43706219\n",
      "Iteration 115, loss = 0.43688330\n",
      "Iteration 116, loss = 0.43679735\n",
      "Iteration 117, loss = 0.43672904\n",
      "Iteration 118, loss = 0.43669154\n",
      "Iteration 119, loss = 0.43653010\n",
      "Iteration 120, loss = 0.43649998\n",
      "Iteration 121, loss = 0.43638790\n",
      "Iteration 122, loss = 0.43638280\n",
      "Iteration 123, loss = 0.43619523\n",
      "Iteration 124, loss = 0.43617414\n",
      "Iteration 125, loss = 0.43615182\n",
      "Iteration 126, loss = 0.43606622\n",
      "Iteration 127, loss = 0.43599373\n",
      "Iteration 128, loss = 0.43592311\n",
      "Iteration 129, loss = 0.43590818\n",
      "Iteration 130, loss = 0.43580445\n",
      "Iteration 131, loss = 0.43580354\n",
      "Iteration 132, loss = 0.43571748\n",
      "Iteration 133, loss = 0.43575205\n",
      "Iteration 134, loss = 0.43566300\n",
      "Iteration 135, loss = 0.43567938\n",
      "Iteration 136, loss = 0.43558401\n",
      "Iteration 137, loss = 0.43557242\n",
      "Iteration 138, loss = 0.43556743\n",
      "Iteration 139, loss = 0.43556593\n",
      "Iteration 140, loss = 0.43545252\n",
      "Iteration 141, loss = 0.43549243\n",
      "Iteration 142, loss = 0.43542722\n",
      "Iteration 143, loss = 0.43534789\n",
      "Iteration 144, loss = 0.43541560\n",
      "Iteration 145, loss = 0.43533253\n",
      "Iteration 146, loss = 0.43537747\n",
      "Iteration 147, loss = 0.43556990\n",
      "Iteration 148, loss = 0.43540108\n",
      "Iteration 149, loss = 0.43538146\n",
      "Iteration 150, loss = 0.43523140\n",
      "Iteration 151, loss = 0.43520398\n",
      "Iteration 152, loss = 0.43522986\n",
      "Iteration 153, loss = 0.43512844\n",
      "Iteration 154, loss = 0.43516488\n",
      "Iteration 155, loss = 0.43524445\n",
      "Iteration 156, loss = 0.43514715\n",
      "Iteration 157, loss = 0.43506097\n",
      "Iteration 158, loss = 0.43511925\n",
      "Iteration 159, loss = 0.43509210\n",
      "Iteration 160, loss = 0.43499441\n",
      "Iteration 161, loss = 0.43500636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245126\n",
      "Iteration 2, loss = 0.71880148\n",
      "Iteration 3, loss = 0.68933253\n",
      "Iteration 4, loss = 0.66865852\n",
      "Iteration 5, loss = 0.65261890\n",
      "Iteration 6, loss = 0.64170491\n",
      "Iteration 7, loss = 0.63397397\n",
      "Iteration 8, loss = 0.62898627\n",
      "Iteration 9, loss = 0.62457592\n",
      "Iteration 10, loss = 0.62023055\n",
      "Iteration 11, loss = 0.61608745\n",
      "Iteration 12, loss = 0.61130092\n",
      "Iteration 13, loss = 0.60657628\n",
      "Iteration 14, loss = 0.60248415\n",
      "Iteration 15, loss = 0.59702089\n",
      "Iteration 16, loss = 0.59258087\n",
      "Iteration 17, loss = 0.58857418\n",
      "Iteration 18, loss = 0.58443337\n",
      "Iteration 19, loss = 0.58075607\n",
      "Iteration 20, loss = 0.57662385\n",
      "Iteration 21, loss = 0.57305468\n",
      "Iteration 22, loss = 0.56944210\n",
      "Iteration 23, loss = 0.56560334\n",
      "Iteration 24, loss = 0.56206857\n",
      "Iteration 25, loss = 0.55866006\n",
      "Iteration 26, loss = 0.55506578\n",
      "Iteration 27, loss = 0.55161395\n",
      "Iteration 28, loss = 0.54840630\n",
      "Iteration 29, loss = 0.54500306\n",
      "Iteration 30, loss = 0.54185878\n",
      "Iteration 31, loss = 0.53860334\n",
      "Iteration 32, loss = 0.53552106\n",
      "Iteration 33, loss = 0.53246947\n",
      "Iteration 34, loss = 0.52939189\n",
      "Iteration 35, loss = 0.52652510\n",
      "Iteration 36, loss = 0.52357242\n",
      "Iteration 37, loss = 0.52067606\n",
      "Iteration 38, loss = 0.51777005\n",
      "Iteration 39, loss = 0.51500104\n",
      "Iteration 40, loss = 0.51232191\n",
      "Iteration 41, loss = 0.50955484\n",
      "Iteration 42, loss = 0.50701418\n",
      "Iteration 43, loss = 0.50432497\n",
      "Iteration 44, loss = 0.50194421\n",
      "Iteration 45, loss = 0.49929216\n",
      "Iteration 46, loss = 0.49688555\n",
      "Iteration 47, loss = 0.49437646\n",
      "Iteration 48, loss = 0.49197241\n",
      "Iteration 49, loss = 0.48964422\n",
      "Iteration 50, loss = 0.48751427\n",
      "Iteration 51, loss = 0.48512720\n",
      "Iteration 52, loss = 0.48284373\n",
      "Iteration 53, loss = 0.48083341\n",
      "Iteration 54, loss = 0.47874644\n",
      "Iteration 55, loss = 0.47657954\n",
      "Iteration 56, loss = 0.47454430\n",
      "Iteration 57, loss = 0.47263736\n",
      "Iteration 58, loss = 0.47077194\n",
      "Iteration 59, loss = 0.46878454\n",
      "Iteration 60, loss = 0.46692059\n",
      "Iteration 61, loss = 0.46512957\n",
      "Iteration 62, loss = 0.46353264\n",
      "Iteration 63, loss = 0.46173676\n",
      "Iteration 64, loss = 0.46010568\n",
      "Iteration 65, loss = 0.45850506\n",
      "Iteration 66, loss = 0.45700583\n",
      "Iteration 67, loss = 0.45562025\n",
      "Iteration 68, loss = 0.45407567\n",
      "Iteration 69, loss = 0.45268243\n",
      "Iteration 70, loss = 0.45154462\n",
      "Iteration 71, loss = 0.45008483\n",
      "Iteration 72, loss = 0.44887947\n",
      "Iteration 73, loss = 0.44768217\n",
      "Iteration 74, loss = 0.44649661\n",
      "Iteration 75, loss = 0.44539933\n",
      "Iteration 76, loss = 0.44442576\n",
      "Iteration 77, loss = 0.44334391\n",
      "Iteration 78, loss = 0.44250063\n",
      "Iteration 79, loss = 0.44149559\n",
      "Iteration 80, loss = 0.44053102\n",
      "Iteration 81, loss = 0.43974787\n",
      "Iteration 82, loss = 0.43904577\n",
      "Iteration 83, loss = 0.43824203\n",
      "Iteration 84, loss = 0.43742410\n",
      "Iteration 85, loss = 0.43685675\n",
      "Iteration 86, loss = 0.43607797\n",
      "Iteration 87, loss = 0.43546685\n",
      "Iteration 88, loss = 0.43503690\n",
      "Iteration 89, loss = 0.43430798\n",
      "Iteration 90, loss = 0.43375281\n",
      "Iteration 91, loss = 0.43327295\n",
      "Iteration 92, loss = 0.43287566\n",
      "Iteration 93, loss = 0.43227036\n",
      "Iteration 94, loss = 0.43193692\n",
      "Iteration 95, loss = 0.43136554\n",
      "Iteration 96, loss = 0.43100374\n",
      "Iteration 97, loss = 0.43062974\n",
      "Iteration 98, loss = 0.43025673\n",
      "Iteration 99, loss = 0.42988108\n",
      "Iteration 100, loss = 0.42955994\n",
      "Iteration 101, loss = 0.42937155\n",
      "Iteration 102, loss = 0.42898064\n",
      "Iteration 103, loss = 0.42871063\n",
      "Iteration 104, loss = 0.42845044\n",
      "Iteration 105, loss = 0.42816582\n",
      "Iteration 106, loss = 0.42792357\n",
      "Iteration 107, loss = 0.42769058\n",
      "Iteration 108, loss = 0.42749224\n",
      "Iteration 109, loss = 0.42723424\n",
      "Iteration 110, loss = 0.42702991\n",
      "Iteration 111, loss = 0.42690974\n",
      "Iteration 112, loss = 0.42666893\n",
      "Iteration 113, loss = 0.42649009\n",
      "Iteration 114, loss = 0.42633393\n",
      "Iteration 115, loss = 0.42622697\n",
      "Iteration 116, loss = 0.42600187\n",
      "Iteration 117, loss = 0.42585641\n",
      "Iteration 118, loss = 0.42570675\n",
      "Iteration 119, loss = 0.42557396\n",
      "Iteration 120, loss = 0.42542471\n",
      "Iteration 121, loss = 0.42529768\n",
      "Iteration 122, loss = 0.42520649\n",
      "Iteration 123, loss = 0.42505672\n",
      "Iteration 124, loss = 0.42493387\n",
      "Iteration 125, loss = 0.42492181\n",
      "Iteration 126, loss = 0.42471162\n",
      "Iteration 127, loss = 0.42460730\n",
      "Iteration 128, loss = 0.42455275\n",
      "Iteration 129, loss = 0.42443729\n",
      "Iteration 130, loss = 0.42439365\n",
      "Iteration 131, loss = 0.42423419\n",
      "Iteration 132, loss = 0.42424077\n",
      "Iteration 133, loss = 0.42407711\n",
      "Iteration 134, loss = 0.42398918\n",
      "Iteration 135, loss = 0.42391375\n",
      "Iteration 136, loss = 0.42387412\n",
      "Iteration 137, loss = 0.42374596\n",
      "Iteration 138, loss = 0.42368444\n",
      "Iteration 139, loss = 0.42363603\n",
      "Iteration 140, loss = 0.42360193\n",
      "Iteration 141, loss = 0.42347171\n",
      "Iteration 142, loss = 0.42357398\n",
      "Iteration 143, loss = 0.42349839\n",
      "Iteration 144, loss = 0.42332423\n",
      "Iteration 145, loss = 0.42324841\n",
      "Iteration 146, loss = 0.42323223\n",
      "Iteration 147, loss = 0.42318383\n",
      "Iteration 148, loss = 0.42309303\n",
      "Iteration 149, loss = 0.42302806\n",
      "Iteration 150, loss = 0.42291951\n",
      "Iteration 151, loss = 0.42286614\n",
      "Iteration 152, loss = 0.42284207\n",
      "Iteration 153, loss = 0.42294060\n",
      "Iteration 154, loss = 0.42272461\n",
      "Iteration 155, loss = 0.42264971\n",
      "Iteration 156, loss = 0.42263752\n",
      "Iteration 157, loss = 0.42259239\n",
      "Iteration 158, loss = 0.42258499\n",
      "Iteration 159, loss = 0.42262903\n",
      "Iteration 160, loss = 0.42250149\n",
      "Iteration 161, loss = 0.42242633\n",
      "Iteration 162, loss = 0.42240239\n",
      "Iteration 163, loss = 0.42234956\n",
      "Iteration 164, loss = 0.42241936\n",
      "Iteration 165, loss = 0.42232254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164122\n",
      "Iteration 2, loss = 0.71827868\n",
      "Iteration 3, loss = 0.68858075\n",
      "Iteration 4, loss = 0.66801452\n",
      "Iteration 5, loss = 0.65193968\n",
      "Iteration 6, loss = 0.64122624\n",
      "Iteration 7, loss = 0.63356121\n",
      "Iteration 8, loss = 0.62835446\n",
      "Iteration 9, loss = 0.62394626\n",
      "Iteration 10, loss = 0.61948867\n",
      "Iteration 11, loss = 0.61514044\n",
      "Iteration 12, loss = 0.61037910\n",
      "Iteration 13, loss = 0.60536866\n",
      "Iteration 14, loss = 0.60128359\n",
      "Iteration 15, loss = 0.59575652\n",
      "Iteration 16, loss = 0.59119039\n",
      "Iteration 17, loss = 0.58693456\n",
      "Iteration 18, loss = 0.58287982\n",
      "Iteration 19, loss = 0.57905963\n",
      "Iteration 20, loss = 0.57486253\n",
      "Iteration 21, loss = 0.57090966\n",
      "Iteration 22, loss = 0.56715506\n",
      "Iteration 23, loss = 0.56329071\n",
      "Iteration 24, loss = 0.55962228\n",
      "Iteration 25, loss = 0.55595383\n",
      "Iteration 26, loss = 0.55231257\n",
      "Iteration 27, loss = 0.54860023\n",
      "Iteration 28, loss = 0.54526271\n",
      "Iteration 29, loss = 0.54171806\n",
      "Iteration 30, loss = 0.53837195\n",
      "Iteration 31, loss = 0.53505895\n",
      "Iteration 32, loss = 0.53171090\n",
      "Iteration 33, loss = 0.52846540\n",
      "Iteration 34, loss = 0.52516973\n",
      "Iteration 35, loss = 0.52211969\n",
      "Iteration 36, loss = 0.51903796\n",
      "Iteration 37, loss = 0.51596979\n",
      "Iteration 38, loss = 0.51295170\n",
      "Iteration 39, loss = 0.50997839\n",
      "Iteration 40, loss = 0.50707829\n",
      "Iteration 41, loss = 0.50428024\n",
      "Iteration 42, loss = 0.50149102\n",
      "Iteration 43, loss = 0.49873698\n",
      "Iteration 44, loss = 0.49600097\n",
      "Iteration 45, loss = 0.49346373\n",
      "Iteration 46, loss = 0.49075325\n",
      "Iteration 47, loss = 0.48822359\n",
      "Iteration 48, loss = 0.48569193\n",
      "Iteration 49, loss = 0.48330367\n",
      "Iteration 50, loss = 0.48083684\n",
      "Iteration 51, loss = 0.47846302\n",
      "Iteration 52, loss = 0.47616453\n",
      "Iteration 53, loss = 0.47413866\n",
      "Iteration 54, loss = 0.47175122\n",
      "Iteration 55, loss = 0.46955230\n",
      "Iteration 56, loss = 0.46737603\n",
      "Iteration 57, loss = 0.46544714\n",
      "Iteration 58, loss = 0.46358076\n",
      "Iteration 59, loss = 0.46148051\n",
      "Iteration 60, loss = 0.45950316\n",
      "Iteration 61, loss = 0.45779150\n",
      "Iteration 62, loss = 0.45610624\n",
      "Iteration 63, loss = 0.45436710\n",
      "Iteration 64, loss = 0.45263578\n",
      "Iteration 65, loss = 0.45094456\n",
      "Iteration 66, loss = 0.44942964\n",
      "Iteration 67, loss = 0.44794395\n",
      "Iteration 68, loss = 0.44650115\n",
      "Iteration 69, loss = 0.44499759\n",
      "Iteration 70, loss = 0.44382366\n",
      "Iteration 71, loss = 0.44238316\n",
      "Iteration 72, loss = 0.44116600\n",
      "Iteration 73, loss = 0.43995648\n",
      "Iteration 74, loss = 0.43871234\n",
      "Iteration 75, loss = 0.43761840\n",
      "Iteration 76, loss = 0.43661181\n",
      "Iteration 77, loss = 0.43560964\n",
      "Iteration 78, loss = 0.43468412\n",
      "Iteration 79, loss = 0.43366752\n",
      "Iteration 80, loss = 0.43279549\n",
      "Iteration 81, loss = 0.43193873\n",
      "Iteration 82, loss = 0.43120431\n",
      "Iteration 83, loss = 0.43051278\n",
      "Iteration 84, loss = 0.42971169\n",
      "Iteration 85, loss = 0.42907145\n",
      "Iteration 86, loss = 0.42829630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87, loss = 0.42778019\n",
      "Iteration 88, loss = 0.42723154\n",
      "Iteration 89, loss = 0.42661888\n",
      "Iteration 90, loss = 0.42607569\n",
      "Iteration 91, loss = 0.42554927\n",
      "Iteration 92, loss = 0.42509790\n",
      "Iteration 93, loss = 0.42461194\n",
      "Iteration 94, loss = 0.42426655\n",
      "Iteration 95, loss = 0.42380681\n",
      "Iteration 96, loss = 0.42347641\n",
      "Iteration 97, loss = 0.42309188\n",
      "Iteration 98, loss = 0.42272455\n",
      "Iteration 99, loss = 0.42243126\n",
      "Iteration 100, loss = 0.42210944\n",
      "Iteration 101, loss = 0.42197099\n",
      "Iteration 102, loss = 0.42158976\n",
      "Iteration 103, loss = 0.42136888\n",
      "Iteration 104, loss = 0.42111713\n",
      "Iteration 105, loss = 0.42087945\n",
      "Iteration 106, loss = 0.42070823\n",
      "Iteration 107, loss = 0.42044578\n",
      "Iteration 108, loss = 0.42031089\n",
      "Iteration 109, loss = 0.42015455\n",
      "Iteration 110, loss = 0.41998723\n",
      "Iteration 111, loss = 0.41978929\n",
      "Iteration 112, loss = 0.41967839\n",
      "Iteration 113, loss = 0.41952861\n",
      "Iteration 114, loss = 0.41939956\n",
      "Iteration 115, loss = 0.41927214\n",
      "Iteration 116, loss = 0.41912587\n",
      "Iteration 117, loss = 0.41902315\n",
      "Iteration 118, loss = 0.41898478\n",
      "Iteration 119, loss = 0.41892221\n",
      "Iteration 120, loss = 0.41878753\n",
      "Iteration 121, loss = 0.41873479\n",
      "Iteration 122, loss = 0.41881358\n",
      "Iteration 123, loss = 0.41854924\n",
      "Iteration 124, loss = 0.41850585\n",
      "Iteration 125, loss = 0.41837171\n",
      "Iteration 126, loss = 0.41830397\n",
      "Iteration 127, loss = 0.41827513\n",
      "Iteration 128, loss = 0.41824418\n",
      "Iteration 129, loss = 0.41817700\n",
      "Iteration 130, loss = 0.41816650\n",
      "Iteration 131, loss = 0.41809178\n",
      "Iteration 132, loss = 0.41803020\n",
      "Iteration 133, loss = 0.41799981\n",
      "Iteration 134, loss = 0.41791691\n",
      "Iteration 135, loss = 0.41787644\n",
      "Iteration 136, loss = 0.41784480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236808\n",
      "Iteration 2, loss = 0.71781880\n",
      "Iteration 3, loss = 0.68874046\n",
      "Iteration 4, loss = 0.66689055\n",
      "Iteration 5, loss = 0.65059073\n",
      "Iteration 6, loss = 0.64114816\n",
      "Iteration 7, loss = 0.63293547\n",
      "Iteration 8, loss = 0.62808748\n",
      "Iteration 9, loss = 0.62372704\n",
      "Iteration 10, loss = 0.61944137\n",
      "Iteration 11, loss = 0.61513610\n",
      "Iteration 12, loss = 0.61058280\n",
      "Iteration 13, loss = 0.60560754\n",
      "Iteration 14, loss = 0.60150535\n",
      "Iteration 15, loss = 0.59621572\n",
      "Iteration 16, loss = 0.59185260\n",
      "Iteration 17, loss = 0.58749014\n",
      "Iteration 18, loss = 0.58354009\n",
      "Iteration 19, loss = 0.57999432\n",
      "Iteration 20, loss = 0.57602282\n",
      "Iteration 21, loss = 0.57241805\n",
      "Iteration 22, loss = 0.56869336\n",
      "Iteration 23, loss = 0.56514691\n",
      "Iteration 24, loss = 0.56153707\n",
      "Iteration 25, loss = 0.55805176\n",
      "Iteration 26, loss = 0.55462765\n",
      "Iteration 27, loss = 0.55130552\n",
      "Iteration 28, loss = 0.54815299\n",
      "Iteration 29, loss = 0.54495745\n",
      "Iteration 30, loss = 0.54180727\n",
      "Iteration 31, loss = 0.53880178\n",
      "Iteration 32, loss = 0.53572793\n",
      "Iteration 33, loss = 0.53279014\n",
      "Iteration 34, loss = 0.52990326\n",
      "Iteration 35, loss = 0.52708901\n",
      "Iteration 36, loss = 0.52446689\n",
      "Iteration 37, loss = 0.52163271\n",
      "Iteration 38, loss = 0.51911396\n",
      "Iteration 39, loss = 0.51647463\n",
      "Iteration 40, loss = 0.51396975\n",
      "Iteration 41, loss = 0.51159352\n",
      "Iteration 42, loss = 0.50915488\n",
      "Iteration 43, loss = 0.50676599\n",
      "Iteration 44, loss = 0.50458237\n",
      "Iteration 45, loss = 0.50241180\n",
      "Iteration 46, loss = 0.50010419\n",
      "Iteration 47, loss = 0.49807753\n",
      "Iteration 48, loss = 0.49601751\n",
      "Iteration 49, loss = 0.49399024\n",
      "Iteration 50, loss = 0.49208402\n",
      "Iteration 51, loss = 0.49018222\n",
      "Iteration 52, loss = 0.48830357\n",
      "Iteration 53, loss = 0.48669588\n",
      "Iteration 54, loss = 0.48479059\n",
      "Iteration 55, loss = 0.48307179\n",
      "Iteration 56, loss = 0.48133587\n",
      "Iteration 57, loss = 0.47992023\n",
      "Iteration 58, loss = 0.47852299\n",
      "Iteration 59, loss = 0.47681130\n",
      "Iteration 60, loss = 0.47541018\n",
      "Iteration 61, loss = 0.47387377\n",
      "Iteration 62, loss = 0.47263538\n",
      "Iteration 63, loss = 0.47136014\n",
      "Iteration 64, loss = 0.47005052\n",
      "Iteration 65, loss = 0.46894946\n",
      "Iteration 66, loss = 0.46772137\n",
      "Iteration 67, loss = 0.46654570\n",
      "Iteration 68, loss = 0.46555204\n",
      "Iteration 69, loss = 0.46442095\n",
      "Iteration 70, loss = 0.46342522\n",
      "Iteration 71, loss = 0.46252973\n",
      "Iteration 72, loss = 0.46158231\n",
      "Iteration 73, loss = 0.46064807\n",
      "Iteration 74, loss = 0.45969396\n",
      "Iteration 75, loss = 0.45898074\n",
      "Iteration 76, loss = 0.45820702\n",
      "Iteration 77, loss = 0.45759607\n",
      "Iteration 78, loss = 0.45688952\n",
      "Iteration 79, loss = 0.45608698\n",
      "Iteration 80, loss = 0.45549737\n",
      "Iteration 81, loss = 0.45484013\n",
      "Iteration 82, loss = 0.45431114\n",
      "Iteration 83, loss = 0.45400559\n",
      "Iteration 84, loss = 0.45334171\n",
      "Iteration 85, loss = 0.45286163\n",
      "Iteration 86, loss = 0.45228853\n",
      "Iteration 87, loss = 0.45204281\n",
      "Iteration 88, loss = 0.45164698\n",
      "Iteration 89, loss = 0.45112679\n",
      "Iteration 90, loss = 0.45074782\n",
      "Iteration 91, loss = 0.45041176\n",
      "Iteration 92, loss = 0.45008039\n",
      "Iteration 93, loss = 0.44972861\n",
      "Iteration 94, loss = 0.44949838\n",
      "Iteration 95, loss = 0.44919769\n",
      "Iteration 96, loss = 0.44896168\n",
      "Iteration 97, loss = 0.44872180\n",
      "Iteration 98, loss = 0.44842727\n",
      "Iteration 99, loss = 0.44819773\n",
      "Iteration 100, loss = 0.44799064\n",
      "Iteration 101, loss = 0.44790707\n",
      "Iteration 102, loss = 0.44767127\n",
      "Iteration 103, loss = 0.44753931\n",
      "Iteration 104, loss = 0.44725209\n",
      "Iteration 105, loss = 0.44712785\n",
      "Iteration 106, loss = 0.44702319\n",
      "Iteration 107, loss = 0.44681822\n",
      "Iteration 108, loss = 0.44676494\n",
      "Iteration 109, loss = 0.44664142\n",
      "Iteration 110, loss = 0.44648411\n",
      "Iteration 111, loss = 0.44630186\n",
      "Iteration 112, loss = 0.44619817\n",
      "Iteration 113, loss = 0.44605517\n",
      "Iteration 114, loss = 0.44596769\n",
      "Iteration 115, loss = 0.44591012\n",
      "Iteration 116, loss = 0.44579619\n",
      "Iteration 117, loss = 0.44570149\n",
      "Iteration 118, loss = 0.44564596\n",
      "Iteration 119, loss = 0.44561080\n",
      "Iteration 120, loss = 0.44553052\n",
      "Iteration 121, loss = 0.44541463\n",
      "Iteration 122, loss = 0.44549334\n",
      "Iteration 123, loss = 0.44535563\n",
      "Iteration 124, loss = 0.44522901\n",
      "Iteration 125, loss = 0.44512386\n",
      "Iteration 126, loss = 0.44513537\n",
      "Iteration 127, loss = 0.44504376\n",
      "Iteration 128, loss = 0.44500494\n",
      "Iteration 129, loss = 0.44494792\n",
      "Iteration 130, loss = 0.44496899\n",
      "Iteration 131, loss = 0.44484299\n",
      "Iteration 132, loss = 0.44478343\n",
      "Iteration 133, loss = 0.44478854\n",
      "Iteration 134, loss = 0.44469884\n",
      "Iteration 135, loss = 0.44468770\n",
      "Iteration 136, loss = 0.44467766\n",
      "Iteration 137, loss = 0.44468772\n",
      "Iteration 138, loss = 0.44452541\n",
      "Iteration 139, loss = 0.44454314\n",
      "Iteration 140, loss = 0.44456935\n",
      "Iteration 141, loss = 0.44447731\n",
      "Iteration 142, loss = 0.44456497\n",
      "Iteration 143, loss = 0.44439456\n",
      "Iteration 144, loss = 0.44436837\n",
      "Iteration 145, loss = 0.44434998\n",
      "Iteration 146, loss = 0.44439812\n",
      "Iteration 147, loss = 0.44439528\n",
      "Iteration 148, loss = 0.44423677\n",
      "Iteration 149, loss = 0.44419004\n",
      "Iteration 150, loss = 0.44420799\n",
      "Iteration 151, loss = 0.44419027\n",
      "Iteration 152, loss = 0.44420542\n",
      "Iteration 153, loss = 0.44428470\n",
      "Iteration 154, loss = 0.44411936\n",
      "Iteration 155, loss = 0.44408796\n",
      "Iteration 156, loss = 0.44399554\n",
      "Iteration 157, loss = 0.44400454\n",
      "Iteration 158, loss = 0.44397397\n",
      "Iteration 159, loss = 0.44392950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973181\n",
      "Iteration 2, loss = 0.71530954\n",
      "Iteration 3, loss = 0.68650194\n",
      "Iteration 4, loss = 0.66331217\n",
      "Iteration 5, loss = 0.64773896\n",
      "Iteration 6, loss = 0.63645322\n",
      "Iteration 7, loss = 0.62839655\n",
      "Iteration 8, loss = 0.62338442\n",
      "Iteration 9, loss = 0.61832576\n",
      "Iteration 10, loss = 0.61358740\n",
      "Iteration 11, loss = 0.60883765\n",
      "Iteration 12, loss = 0.60327075\n",
      "Iteration 13, loss = 0.59718155\n",
      "Iteration 14, loss = 0.59181441\n",
      "Iteration 15, loss = 0.58640580\n",
      "Iteration 16, loss = 0.58129704\n",
      "Iteration 17, loss = 0.57640207\n",
      "Iteration 18, loss = 0.57177069\n",
      "Iteration 19, loss = 0.56738049\n",
      "Iteration 20, loss = 0.56303261\n",
      "Iteration 21, loss = 0.55869533\n",
      "Iteration 22, loss = 0.55444501\n",
      "Iteration 23, loss = 0.55016948\n",
      "Iteration 24, loss = 0.54595286\n",
      "Iteration 25, loss = 0.54187063\n",
      "Iteration 26, loss = 0.53804580\n",
      "Iteration 27, loss = 0.53404378\n",
      "Iteration 28, loss = 0.53020269\n",
      "Iteration 29, loss = 0.52656514\n",
      "Iteration 30, loss = 0.52279747\n",
      "Iteration 31, loss = 0.51930671\n",
      "Iteration 32, loss = 0.51593698\n",
      "Iteration 33, loss = 0.51236396\n",
      "Iteration 34, loss = 0.50896924\n",
      "Iteration 35, loss = 0.50568878\n",
      "Iteration 36, loss = 0.50272498\n",
      "Iteration 37, loss = 0.49949455\n",
      "Iteration 38, loss = 0.49651357\n",
      "Iteration 39, loss = 0.49359704\n",
      "Iteration 40, loss = 0.49067191\n",
      "Iteration 41, loss = 0.48780963\n",
      "Iteration 42, loss = 0.48511241\n",
      "Iteration 43, loss = 0.48247273\n",
      "Iteration 44, loss = 0.47987213\n",
      "Iteration 45, loss = 0.47734984\n",
      "Iteration 46, loss = 0.47488063\n",
      "Iteration 47, loss = 0.47256346\n",
      "Iteration 48, loss = 0.47008638\n",
      "Iteration 49, loss = 0.46777172\n",
      "Iteration 50, loss = 0.46573094\n",
      "Iteration 51, loss = 0.46351837\n",
      "Iteration 52, loss = 0.46147603\n",
      "Iteration 53, loss = 0.45932416\n",
      "Iteration 54, loss = 0.45746244\n",
      "Iteration 55, loss = 0.45558199\n",
      "Iteration 56, loss = 0.45367604\n",
      "Iteration 57, loss = 0.45187981\n",
      "Iteration 58, loss = 0.45013855\n",
      "Iteration 59, loss = 0.44841592\n",
      "Iteration 60, loss = 0.44678509\n",
      "Iteration 61, loss = 0.44519305\n",
      "Iteration 62, loss = 0.44377912\n",
      "Iteration 63, loss = 0.44220893\n",
      "Iteration 64, loss = 0.44078986\n",
      "Iteration 65, loss = 0.43935591\n",
      "Iteration 66, loss = 0.43796503\n",
      "Iteration 67, loss = 0.43669261\n",
      "Iteration 68, loss = 0.43547277\n",
      "Iteration 69, loss = 0.43430647\n",
      "Iteration 70, loss = 0.43318080\n",
      "Iteration 71, loss = 0.43206070\n",
      "Iteration 72, loss = 0.43096744\n",
      "Iteration 73, loss = 0.42990199\n",
      "Iteration 74, loss = 0.42902677\n",
      "Iteration 75, loss = 0.42811851\n",
      "Iteration 76, loss = 0.42718316\n",
      "Iteration 77, loss = 0.42629106\n",
      "Iteration 78, loss = 0.42549804\n",
      "Iteration 79, loss = 0.42476790\n",
      "Iteration 80, loss = 0.42416544\n",
      "Iteration 81, loss = 0.42342481\n",
      "Iteration 82, loss = 0.42266566\n",
      "Iteration 83, loss = 0.42214464\n",
      "Iteration 84, loss = 0.42147526\n",
      "Iteration 85, loss = 0.42092537\n",
      "Iteration 86, loss = 0.42030361\n",
      "Iteration 87, loss = 0.41984181\n",
      "Iteration 88, loss = 0.41947008\n",
      "Iteration 89, loss = 0.41888000\n",
      "Iteration 90, loss = 0.41839545\n",
      "Iteration 91, loss = 0.41812999\n",
      "Iteration 92, loss = 0.41775689\n",
      "Iteration 93, loss = 0.41734832\n",
      "Iteration 94, loss = 0.41712038\n",
      "Iteration 95, loss = 0.41668664\n",
      "Iteration 96, loss = 0.41645406\n",
      "Iteration 97, loss = 0.41611417\n",
      "Iteration 98, loss = 0.41590015\n",
      "Iteration 99, loss = 0.41557164\n",
      "Iteration 100, loss = 0.41530631\n",
      "Iteration 101, loss = 0.41517075\n",
      "Iteration 102, loss = 0.41485905\n",
      "Iteration 103, loss = 0.41471781\n",
      "Iteration 104, loss = 0.41452607\n",
      "Iteration 105, loss = 0.41430907\n",
      "Iteration 106, loss = 0.41412320\n",
      "Iteration 107, loss = 0.41404720\n",
      "Iteration 108, loss = 0.41392312\n",
      "Iteration 109, loss = 0.41371860\n",
      "Iteration 110, loss = 0.41357160\n",
      "Iteration 111, loss = 0.41343738\n",
      "Iteration 112, loss = 0.41333510\n",
      "Iteration 113, loss = 0.41322985\n",
      "Iteration 114, loss = 0.41311893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 115, loss = 0.41302058\n",
      "Iteration 116, loss = 0.41291265\n",
      "Iteration 117, loss = 0.41285878\n",
      "Iteration 118, loss = 0.41276956\n",
      "Iteration 119, loss = 0.41266220\n",
      "Iteration 120, loss = 0.41257022\n",
      "Iteration 121, loss = 0.41250984\n",
      "Iteration 122, loss = 0.41255054\n",
      "Iteration 123, loss = 0.41234222\n",
      "Iteration 124, loss = 0.41242462\n",
      "Iteration 125, loss = 0.41221446\n",
      "Iteration 126, loss = 0.41219567\n",
      "Iteration 127, loss = 0.41212019\n",
      "Iteration 128, loss = 0.41208484\n",
      "Iteration 129, loss = 0.41206624\n",
      "Iteration 130, loss = 0.41193852\n",
      "Iteration 131, loss = 0.41198089\n",
      "Iteration 132, loss = 0.41194533\n",
      "Iteration 133, loss = 0.41192566\n",
      "Iteration 134, loss = 0.41178831\n",
      "Iteration 135, loss = 0.41180069\n",
      "Iteration 136, loss = 0.41172475\n",
      "Iteration 137, loss = 0.41174964\n",
      "Iteration 138, loss = 0.41163030\n",
      "Iteration 139, loss = 0.41167597\n",
      "Iteration 140, loss = 0.41159343\n",
      "Iteration 141, loss = 0.41154793\n",
      "Iteration 142, loss = 0.41151448\n",
      "Iteration 143, loss = 0.41147555\n",
      "Iteration 144, loss = 0.41142621\n",
      "Iteration 145, loss = 0.41155071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072267\n",
      "Iteration 2, loss = 0.71621949\n",
      "Iteration 3, loss = 0.68706179\n",
      "Iteration 4, loss = 0.66505685\n",
      "Iteration 5, loss = 0.65028171\n",
      "Iteration 6, loss = 0.63925835\n",
      "Iteration 7, loss = 0.63130218\n",
      "Iteration 8, loss = 0.62729556\n",
      "Iteration 9, loss = 0.62245579\n",
      "Iteration 10, loss = 0.61804582\n",
      "Iteration 11, loss = 0.61373162\n",
      "Iteration 12, loss = 0.60892985\n",
      "Iteration 13, loss = 0.60342161\n",
      "Iteration 14, loss = 0.59872660\n",
      "Iteration 15, loss = 0.59394112\n",
      "Iteration 16, loss = 0.58944394\n",
      "Iteration 17, loss = 0.58524351\n",
      "Iteration 18, loss = 0.58110013\n",
      "Iteration 19, loss = 0.57732722\n",
      "Iteration 20, loss = 0.57353308\n",
      "Iteration 21, loss = 0.56980644\n",
      "Iteration 22, loss = 0.56618844\n",
      "Iteration 23, loss = 0.56253888\n",
      "Iteration 24, loss = 0.55896683\n",
      "Iteration 25, loss = 0.55543081\n",
      "Iteration 26, loss = 0.55212151\n",
      "Iteration 27, loss = 0.54881840\n",
      "Iteration 28, loss = 0.54551460\n",
      "Iteration 29, loss = 0.54233026\n",
      "Iteration 30, loss = 0.53908603\n",
      "Iteration 31, loss = 0.53599376\n",
      "Iteration 32, loss = 0.53327482\n",
      "Iteration 33, loss = 0.53016076\n",
      "Iteration 34, loss = 0.52719094\n",
      "Iteration 35, loss = 0.52434896\n",
      "Iteration 36, loss = 0.52181617\n",
      "Iteration 37, loss = 0.51892862\n",
      "Iteration 38, loss = 0.51638608\n",
      "Iteration 39, loss = 0.51383543\n",
      "Iteration 40, loss = 0.51129223\n",
      "Iteration 41, loss = 0.50871669\n",
      "Iteration 42, loss = 0.50630211\n",
      "Iteration 43, loss = 0.50394465\n",
      "Iteration 44, loss = 0.50158593\n",
      "Iteration 45, loss = 0.49934996\n",
      "Iteration 46, loss = 0.49716342\n",
      "Iteration 47, loss = 0.49492972\n",
      "Iteration 48, loss = 0.49282503\n",
      "Iteration 49, loss = 0.49074696\n",
      "Iteration 50, loss = 0.48880960\n",
      "Iteration 51, loss = 0.48684716\n",
      "Iteration 52, loss = 0.48482368\n",
      "Iteration 53, loss = 0.48283578\n",
      "Iteration 54, loss = 0.48116567\n",
      "Iteration 55, loss = 0.47934900\n",
      "Iteration 56, loss = 0.47753731\n",
      "Iteration 57, loss = 0.47588474\n",
      "Iteration 58, loss = 0.47419258\n",
      "Iteration 59, loss = 0.47267839\n",
      "Iteration 60, loss = 0.47092098\n",
      "Iteration 61, loss = 0.46957171\n",
      "Iteration 62, loss = 0.46811334\n",
      "Iteration 63, loss = 0.46669176\n",
      "Iteration 64, loss = 0.46510472\n",
      "Iteration 65, loss = 0.46383618\n",
      "Iteration 66, loss = 0.46263632\n",
      "Iteration 67, loss = 0.46122390\n",
      "Iteration 68, loss = 0.46003795\n",
      "Iteration 69, loss = 0.45890832\n",
      "Iteration 70, loss = 0.45781921\n",
      "Iteration 71, loss = 0.45669456\n",
      "Iteration 72, loss = 0.45553970\n",
      "Iteration 73, loss = 0.45449946\n",
      "Iteration 74, loss = 0.45372985\n",
      "Iteration 75, loss = 0.45266161\n",
      "Iteration 76, loss = 0.45181352\n",
      "Iteration 77, loss = 0.45091736\n",
      "Iteration 78, loss = 0.45009182\n",
      "Iteration 79, loss = 0.44929069\n",
      "Iteration 80, loss = 0.44861638\n",
      "Iteration 81, loss = 0.44793589\n",
      "Iteration 82, loss = 0.44714381\n",
      "Iteration 83, loss = 0.44672318\n",
      "Iteration 84, loss = 0.44587914\n",
      "Iteration 85, loss = 0.44533472\n",
      "Iteration 86, loss = 0.44473123\n",
      "Iteration 87, loss = 0.44421958\n",
      "Iteration 88, loss = 0.44377935\n",
      "Iteration 89, loss = 0.44324187\n",
      "Iteration 90, loss = 0.44274824\n",
      "Iteration 91, loss = 0.44239068\n",
      "Iteration 92, loss = 0.44195899\n",
      "Iteration 93, loss = 0.44155848\n",
      "Iteration 94, loss = 0.44125568\n",
      "Iteration 95, loss = 0.44086261\n",
      "Iteration 96, loss = 0.44052574\n",
      "Iteration 97, loss = 0.44035050\n",
      "Iteration 98, loss = 0.44012240\n",
      "Iteration 99, loss = 0.43965073\n",
      "Iteration 100, loss = 0.43938001\n",
      "Iteration 101, loss = 0.43921513\n",
      "Iteration 102, loss = 0.43889391\n",
      "Iteration 103, loss = 0.43869419\n",
      "Iteration 104, loss = 0.43856763\n",
      "Iteration 105, loss = 0.43834544\n",
      "Iteration 106, loss = 0.43807737\n",
      "Iteration 107, loss = 0.43794324\n",
      "Iteration 108, loss = 0.43778128\n",
      "Iteration 109, loss = 0.43764240\n",
      "Iteration 110, loss = 0.43753752\n",
      "Iteration 111, loss = 0.43737355\n",
      "Iteration 112, loss = 0.43722029\n",
      "Iteration 113, loss = 0.43713278\n",
      "Iteration 114, loss = 0.43706157\n",
      "Iteration 115, loss = 0.43688268\n",
      "Iteration 116, loss = 0.43679673\n",
      "Iteration 117, loss = 0.43672842\n",
      "Iteration 118, loss = 0.43669092\n",
      "Iteration 119, loss = 0.43652947\n",
      "Iteration 120, loss = 0.43649936\n",
      "Iteration 121, loss = 0.43638728\n",
      "Iteration 122, loss = 0.43638218\n",
      "Iteration 123, loss = 0.43619460\n",
      "Iteration 124, loss = 0.43617351\n",
      "Iteration 125, loss = 0.43615119\n",
      "Iteration 126, loss = 0.43606559\n",
      "Iteration 127, loss = 0.43599310\n",
      "Iteration 128, loss = 0.43592249\n",
      "Iteration 129, loss = 0.43590755\n",
      "Iteration 130, loss = 0.43580382\n",
      "Iteration 131, loss = 0.43580291\n",
      "Iteration 132, loss = 0.43571684\n",
      "Iteration 133, loss = 0.43575141\n",
      "Iteration 134, loss = 0.43566236\n",
      "Iteration 135, loss = 0.43567875\n",
      "Iteration 136, loss = 0.43558337\n",
      "Iteration 137, loss = 0.43557179\n",
      "Iteration 138, loss = 0.43556680\n",
      "Iteration 139, loss = 0.43556530\n",
      "Iteration 140, loss = 0.43545188\n",
      "Iteration 141, loss = 0.43549179\n",
      "Iteration 142, loss = 0.43542658\n",
      "Iteration 143, loss = 0.43534725\n",
      "Iteration 144, loss = 0.43541497\n",
      "Iteration 145, loss = 0.43533189\n",
      "Iteration 146, loss = 0.43537683\n",
      "Iteration 147, loss = 0.43556926\n",
      "Iteration 148, loss = 0.43540044\n",
      "Iteration 149, loss = 0.43538082\n",
      "Iteration 150, loss = 0.43523076\n",
      "Iteration 151, loss = 0.43520333\n",
      "Iteration 152, loss = 0.43522922\n",
      "Iteration 153, loss = 0.43512779\n",
      "Iteration 154, loss = 0.43516424\n",
      "Iteration 155, loss = 0.43524380\n",
      "Iteration 156, loss = 0.43514650\n",
      "Iteration 157, loss = 0.43506033\n",
      "Iteration 158, loss = 0.43511860\n",
      "Iteration 159, loss = 0.43509145\n",
      "Iteration 160, loss = 0.43499376\n",
      "Iteration 161, loss = 0.43500572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245108\n",
      "Iteration 2, loss = 0.71880130\n",
      "Iteration 3, loss = 0.68933235\n",
      "Iteration 4, loss = 0.66865834\n",
      "Iteration 5, loss = 0.65261871\n",
      "Iteration 6, loss = 0.64170473\n",
      "Iteration 7, loss = 0.63397378\n",
      "Iteration 8, loss = 0.62898609\n",
      "Iteration 9, loss = 0.62457573\n",
      "Iteration 10, loss = 0.62023036\n",
      "Iteration 11, loss = 0.61608725\n",
      "Iteration 12, loss = 0.61130071\n",
      "Iteration 13, loss = 0.60657607\n",
      "Iteration 14, loss = 0.60248393\n",
      "Iteration 15, loss = 0.59702067\n",
      "Iteration 16, loss = 0.59258065\n",
      "Iteration 17, loss = 0.58857395\n",
      "Iteration 18, loss = 0.58443313\n",
      "Iteration 19, loss = 0.58075582\n",
      "Iteration 20, loss = 0.57662359\n",
      "Iteration 21, loss = 0.57305442\n",
      "Iteration 22, loss = 0.56944183\n",
      "Iteration 23, loss = 0.56560306\n",
      "Iteration 24, loss = 0.56206829\n",
      "Iteration 25, loss = 0.55865977\n",
      "Iteration 26, loss = 0.55506548\n",
      "Iteration 27, loss = 0.55161365\n",
      "Iteration 28, loss = 0.54840599\n",
      "Iteration 29, loss = 0.54500274\n",
      "Iteration 30, loss = 0.54185845\n",
      "Iteration 31, loss = 0.53860301\n",
      "Iteration 32, loss = 0.53552072\n",
      "Iteration 33, loss = 0.53246912\n",
      "Iteration 34, loss = 0.52939153\n",
      "Iteration 35, loss = 0.52652473\n",
      "Iteration 36, loss = 0.52357205\n",
      "Iteration 37, loss = 0.52067568\n",
      "Iteration 38, loss = 0.51776966\n",
      "Iteration 39, loss = 0.51500064\n",
      "Iteration 40, loss = 0.51232151\n",
      "Iteration 41, loss = 0.50955444\n",
      "Iteration 42, loss = 0.50701377\n",
      "Iteration 43, loss = 0.50432456\n",
      "Iteration 44, loss = 0.50194379\n",
      "Iteration 45, loss = 0.49929173\n",
      "Iteration 46, loss = 0.49688512\n",
      "Iteration 47, loss = 0.49437602\n",
      "Iteration 48, loss = 0.49197196\n",
      "Iteration 49, loss = 0.48964377\n",
      "Iteration 50, loss = 0.48751382\n",
      "Iteration 51, loss = 0.48512674\n",
      "Iteration 52, loss = 0.48284327\n",
      "Iteration 53, loss = 0.48083294\n",
      "Iteration 54, loss = 0.47874597\n",
      "Iteration 55, loss = 0.47657907\n",
      "Iteration 56, loss = 0.47454382\n",
      "Iteration 57, loss = 0.47263688\n",
      "Iteration 58, loss = 0.47077146\n",
      "Iteration 59, loss = 0.46878405\n",
      "Iteration 60, loss = 0.46692010\n",
      "Iteration 61, loss = 0.46512907\n",
      "Iteration 62, loss = 0.46353213\n",
      "Iteration 63, loss = 0.46173625\n",
      "Iteration 64, loss = 0.46010517\n",
      "Iteration 65, loss = 0.45850454\n",
      "Iteration 66, loss = 0.45700531\n",
      "Iteration 67, loss = 0.45561972\n",
      "Iteration 68, loss = 0.45407514\n",
      "Iteration 69, loss = 0.45268190\n",
      "Iteration 70, loss = 0.45154409\n",
      "Iteration 71, loss = 0.45008429\n",
      "Iteration 72, loss = 0.44887893\n",
      "Iteration 73, loss = 0.44768162\n",
      "Iteration 74, loss = 0.44649606\n",
      "Iteration 75, loss = 0.44539878\n",
      "Iteration 76, loss = 0.44442520\n",
      "Iteration 77, loss = 0.44334335\n",
      "Iteration 78, loss = 0.44250006\n",
      "Iteration 79, loss = 0.44149503\n",
      "Iteration 80, loss = 0.44053045\n",
      "Iteration 81, loss = 0.43974730\n",
      "Iteration 82, loss = 0.43904520\n",
      "Iteration 83, loss = 0.43824145\n",
      "Iteration 84, loss = 0.43742352\n",
      "Iteration 85, loss = 0.43685617\n",
      "Iteration 86, loss = 0.43607738\n",
      "Iteration 87, loss = 0.43546626\n",
      "Iteration 88, loss = 0.43503631\n",
      "Iteration 89, loss = 0.43430738\n",
      "Iteration 90, loss = 0.43375222\n",
      "Iteration 91, loss = 0.43327235\n",
      "Iteration 92, loss = 0.43287506\n",
      "Iteration 93, loss = 0.43226976\n",
      "Iteration 94, loss = 0.43193632\n",
      "Iteration 95, loss = 0.43136493\n",
      "Iteration 96, loss = 0.43100314\n",
      "Iteration 97, loss = 0.43062913\n",
      "Iteration 98, loss = 0.43025612\n",
      "Iteration 99, loss = 0.42988047\n",
      "Iteration 100, loss = 0.42955933\n",
      "Iteration 101, loss = 0.42937093\n",
      "Iteration 102, loss = 0.42898003\n",
      "Iteration 103, loss = 0.42871002\n",
      "Iteration 104, loss = 0.42844982\n",
      "Iteration 105, loss = 0.42816520\n",
      "Iteration 106, loss = 0.42792295\n",
      "Iteration 107, loss = 0.42768996\n",
      "Iteration 108, loss = 0.42749162\n",
      "Iteration 109, loss = 0.42723361\n",
      "Iteration 110, loss = 0.42702929\n",
      "Iteration 111, loss = 0.42690911\n",
      "Iteration 112, loss = 0.42666830\n",
      "Iteration 113, loss = 0.42648946\n",
      "Iteration 114, loss = 0.42633330\n",
      "Iteration 115, loss = 0.42622634\n",
      "Iteration 116, loss = 0.42600124\n",
      "Iteration 117, loss = 0.42585577\n",
      "Iteration 118, loss = 0.42570612\n",
      "Iteration 119, loss = 0.42557332\n",
      "Iteration 120, loss = 0.42542407\n",
      "Iteration 121, loss = 0.42529704\n",
      "Iteration 122, loss = 0.42520585\n",
      "Iteration 123, loss = 0.42505608\n",
      "Iteration 124, loss = 0.42493324\n",
      "Iteration 125, loss = 0.42492117\n",
      "Iteration 126, loss = 0.42471098\n",
      "Iteration 127, loss = 0.42460666\n",
      "Iteration 128, loss = 0.42455211\n",
      "Iteration 129, loss = 0.42443665\n",
      "Iteration 130, loss = 0.42439300\n",
      "Iteration 131, loss = 0.42423354\n",
      "Iteration 132, loss = 0.42424013\n",
      "Iteration 133, loss = 0.42407647\n",
      "Iteration 134, loss = 0.42398853\n",
      "Iteration 135, loss = 0.42391310\n",
      "Iteration 136, loss = 0.42387348\n",
      "Iteration 137, loss = 0.42374531\n",
      "Iteration 138, loss = 0.42368379\n",
      "Iteration 139, loss = 0.42363538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140, loss = 0.42360128\n",
      "Iteration 141, loss = 0.42347105\n",
      "Iteration 142, loss = 0.42357333\n",
      "Iteration 143, loss = 0.42349773\n",
      "Iteration 144, loss = 0.42332358\n",
      "Iteration 145, loss = 0.42324776\n",
      "Iteration 146, loss = 0.42323157\n",
      "Iteration 147, loss = 0.42318317\n",
      "Iteration 148, loss = 0.42309237\n",
      "Iteration 149, loss = 0.42302741\n",
      "Iteration 150, loss = 0.42291885\n",
      "Iteration 151, loss = 0.42286548\n",
      "Iteration 152, loss = 0.42284141\n",
      "Iteration 153, loss = 0.42293994\n",
      "Iteration 154, loss = 0.42272394\n",
      "Iteration 155, loss = 0.42264905\n",
      "Iteration 156, loss = 0.42263686\n",
      "Iteration 157, loss = 0.42259173\n",
      "Iteration 158, loss = 0.42258433\n",
      "Iteration 159, loss = 0.42262837\n",
      "Iteration 160, loss = 0.42250082\n",
      "Iteration 161, loss = 0.42242566\n",
      "Iteration 162, loss = 0.42240172\n",
      "Iteration 163, loss = 0.42234889\n",
      "Iteration 164, loss = 0.42241869\n",
      "Iteration 165, loss = 0.42232187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164104\n",
      "Iteration 2, loss = 0.71827850\n",
      "Iteration 3, loss = 0.68858057\n",
      "Iteration 4, loss = 0.66801434\n",
      "Iteration 5, loss = 0.65193950\n",
      "Iteration 6, loss = 0.64122605\n",
      "Iteration 7, loss = 0.63356102\n",
      "Iteration 8, loss = 0.62835427\n",
      "Iteration 9, loss = 0.62394606\n",
      "Iteration 10, loss = 0.61948848\n",
      "Iteration 11, loss = 0.61514024\n",
      "Iteration 12, loss = 0.61037890\n",
      "Iteration 13, loss = 0.60536845\n",
      "Iteration 14, loss = 0.60128338\n",
      "Iteration 15, loss = 0.59575630\n",
      "Iteration 16, loss = 0.59119016\n",
      "Iteration 17, loss = 0.58693434\n",
      "Iteration 18, loss = 0.58287958\n",
      "Iteration 19, loss = 0.57905939\n",
      "Iteration 20, loss = 0.57486229\n",
      "Iteration 21, loss = 0.57090941\n",
      "Iteration 22, loss = 0.56715480\n",
      "Iteration 23, loss = 0.56329045\n",
      "Iteration 24, loss = 0.55962201\n",
      "Iteration 25, loss = 0.55595355\n",
      "Iteration 26, loss = 0.55231228\n",
      "Iteration 27, loss = 0.54859994\n",
      "Iteration 28, loss = 0.54526241\n",
      "Iteration 29, loss = 0.54171776\n",
      "Iteration 30, loss = 0.53837163\n",
      "Iteration 31, loss = 0.53505863\n",
      "Iteration 32, loss = 0.53171057\n",
      "Iteration 33, loss = 0.52846507\n",
      "Iteration 34, loss = 0.52516939\n",
      "Iteration 35, loss = 0.52211934\n",
      "Iteration 36, loss = 0.51903760\n",
      "Iteration 37, loss = 0.51596943\n",
      "Iteration 38, loss = 0.51295133\n",
      "Iteration 39, loss = 0.50997802\n",
      "Iteration 40, loss = 0.50707791\n",
      "Iteration 41, loss = 0.50427985\n",
      "Iteration 42, loss = 0.50149063\n",
      "Iteration 43, loss = 0.49873658\n",
      "Iteration 44, loss = 0.49600056\n",
      "Iteration 45, loss = 0.49346332\n",
      "Iteration 46, loss = 0.49075283\n",
      "Iteration 47, loss = 0.48822316\n",
      "Iteration 48, loss = 0.48569150\n",
      "Iteration 49, loss = 0.48330323\n",
      "Iteration 50, loss = 0.48083640\n",
      "Iteration 51, loss = 0.47846257\n",
      "Iteration 52, loss = 0.47616408\n",
      "Iteration 53, loss = 0.47413820\n",
      "Iteration 54, loss = 0.47175076\n",
      "Iteration 55, loss = 0.46955184\n",
      "Iteration 56, loss = 0.46737556\n",
      "Iteration 57, loss = 0.46544666\n",
      "Iteration 58, loss = 0.46358028\n",
      "Iteration 59, loss = 0.46148003\n",
      "Iteration 60, loss = 0.45950267\n",
      "Iteration 61, loss = 0.45779101\n",
      "Iteration 62, loss = 0.45610574\n",
      "Iteration 63, loss = 0.45436660\n",
      "Iteration 64, loss = 0.45263528\n",
      "Iteration 65, loss = 0.45094405\n",
      "Iteration 66, loss = 0.44942913\n",
      "Iteration 67, loss = 0.44794344\n",
      "Iteration 68, loss = 0.44650063\n",
      "Iteration 69, loss = 0.44499707\n",
      "Iteration 70, loss = 0.44382313\n",
      "Iteration 71, loss = 0.44238263\n",
      "Iteration 72, loss = 0.44116547\n",
      "Iteration 73, loss = 0.43995594\n",
      "Iteration 74, loss = 0.43871180\n",
      "Iteration 75, loss = 0.43761785\n",
      "Iteration 76, loss = 0.43661126\n",
      "Iteration 77, loss = 0.43560909\n",
      "Iteration 78, loss = 0.43468357\n",
      "Iteration 79, loss = 0.43366696\n",
      "Iteration 80, loss = 0.43279493\n",
      "Iteration 81, loss = 0.43193817\n",
      "Iteration 82, loss = 0.43120375\n",
      "Iteration 83, loss = 0.43051221\n",
      "Iteration 84, loss = 0.42971112\n",
      "Iteration 85, loss = 0.42907087\n",
      "Iteration 86, loss = 0.42829572\n",
      "Iteration 87, loss = 0.42777960\n",
      "Iteration 88, loss = 0.42723096\n",
      "Iteration 89, loss = 0.42661829\n",
      "Iteration 90, loss = 0.42607511\n",
      "Iteration 91, loss = 0.42554868\n",
      "Iteration 92, loss = 0.42509730\n",
      "Iteration 93, loss = 0.42461135\n",
      "Iteration 94, loss = 0.42426595\n",
      "Iteration 95, loss = 0.42380621\n",
      "Iteration 96, loss = 0.42347581\n",
      "Iteration 97, loss = 0.42309128\n",
      "Iteration 98, loss = 0.42272394\n",
      "Iteration 99, loss = 0.42243065\n",
      "Iteration 100, loss = 0.42210883\n",
      "Iteration 101, loss = 0.42197038\n",
      "Iteration 102, loss = 0.42158915\n",
      "Iteration 103, loss = 0.42136827\n",
      "Iteration 104, loss = 0.42111651\n",
      "Iteration 105, loss = 0.42087883\n",
      "Iteration 106, loss = 0.42070761\n",
      "Iteration 107, loss = 0.42044516\n",
      "Iteration 108, loss = 0.42031027\n",
      "Iteration 109, loss = 0.42015392\n",
      "Iteration 110, loss = 0.41998660\n",
      "Iteration 111, loss = 0.41978866\n",
      "Iteration 112, loss = 0.41967777\n",
      "Iteration 113, loss = 0.41952798\n",
      "Iteration 114, loss = 0.41939893\n",
      "Iteration 115, loss = 0.41927151\n",
      "Iteration 116, loss = 0.41912524\n",
      "Iteration 117, loss = 0.41902252\n",
      "Iteration 118, loss = 0.41898415\n",
      "Iteration 119, loss = 0.41892157\n",
      "Iteration 120, loss = 0.41878689\n",
      "Iteration 121, loss = 0.41873415\n",
      "Iteration 122, loss = 0.41881295\n",
      "Iteration 123, loss = 0.41854860\n",
      "Iteration 124, loss = 0.41850521\n",
      "Iteration 125, loss = 0.41837107\n",
      "Iteration 126, loss = 0.41830333\n",
      "Iteration 127, loss = 0.41827448\n",
      "Iteration 128, loss = 0.41824354\n",
      "Iteration 129, loss = 0.41817636\n",
      "Iteration 130, loss = 0.41816586\n",
      "Iteration 131, loss = 0.41809114\n",
      "Iteration 132, loss = 0.41802955\n",
      "Iteration 133, loss = 0.41799917\n",
      "Iteration 134, loss = 0.41791627\n",
      "Iteration 135, loss = 0.41787579\n",
      "Iteration 136, loss = 0.41784416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236790\n",
      "Iteration 2, loss = 0.71781862\n",
      "Iteration 3, loss = 0.68874028\n",
      "Iteration 4, loss = 0.66689037\n",
      "Iteration 5, loss = 0.65059055\n",
      "Iteration 6, loss = 0.64114797\n",
      "Iteration 7, loss = 0.63293528\n",
      "Iteration 8, loss = 0.62808729\n",
      "Iteration 9, loss = 0.62372684\n",
      "Iteration 10, loss = 0.61944118\n",
      "Iteration 11, loss = 0.61513590\n",
      "Iteration 12, loss = 0.61058260\n",
      "Iteration 13, loss = 0.60560733\n",
      "Iteration 14, loss = 0.60150513\n",
      "Iteration 15, loss = 0.59621550\n",
      "Iteration 16, loss = 0.59185238\n",
      "Iteration 17, loss = 0.58748991\n",
      "Iteration 18, loss = 0.58353985\n",
      "Iteration 19, loss = 0.57999408\n",
      "Iteration 20, loss = 0.57602257\n",
      "Iteration 21, loss = 0.57241779\n",
      "Iteration 22, loss = 0.56869310\n",
      "Iteration 23, loss = 0.56514664\n",
      "Iteration 24, loss = 0.56153679\n",
      "Iteration 25, loss = 0.55805147\n",
      "Iteration 26, loss = 0.55462736\n",
      "Iteration 27, loss = 0.55130521\n",
      "Iteration 28, loss = 0.54815268\n",
      "Iteration 29, loss = 0.54495713\n",
      "Iteration 30, loss = 0.54180694\n",
      "Iteration 31, loss = 0.53880144\n",
      "Iteration 32, loss = 0.53572758\n",
      "Iteration 33, loss = 0.53278979\n",
      "Iteration 34, loss = 0.52990290\n",
      "Iteration 35, loss = 0.52708865\n",
      "Iteration 36, loss = 0.52446652\n",
      "Iteration 37, loss = 0.52163233\n",
      "Iteration 38, loss = 0.51911358\n",
      "Iteration 39, loss = 0.51647424\n",
      "Iteration 40, loss = 0.51396935\n",
      "Iteration 41, loss = 0.51159312\n",
      "Iteration 42, loss = 0.50915447\n",
      "Iteration 43, loss = 0.50676557\n",
      "Iteration 44, loss = 0.50458194\n",
      "Iteration 45, loss = 0.50241136\n",
      "Iteration 46, loss = 0.50010375\n",
      "Iteration 47, loss = 0.49807708\n",
      "Iteration 48, loss = 0.49601706\n",
      "Iteration 49, loss = 0.49398978\n",
      "Iteration 50, loss = 0.49208356\n",
      "Iteration 51, loss = 0.49018175\n",
      "Iteration 52, loss = 0.48830309\n",
      "Iteration 53, loss = 0.48669541\n",
      "Iteration 54, loss = 0.48479011\n",
      "Iteration 55, loss = 0.48307131\n",
      "Iteration 56, loss = 0.48133538\n",
      "Iteration 57, loss = 0.47991974\n",
      "Iteration 58, loss = 0.47852249\n",
      "Iteration 59, loss = 0.47681080\n",
      "Iteration 60, loss = 0.47540967\n",
      "Iteration 61, loss = 0.47387326\n",
      "Iteration 62, loss = 0.47263487\n",
      "Iteration 63, loss = 0.47135962\n",
      "Iteration 64, loss = 0.47005000\n",
      "Iteration 65, loss = 0.46894893\n",
      "Iteration 66, loss = 0.46772084\n",
      "Iteration 67, loss = 0.46654516\n",
      "Iteration 68, loss = 0.46555151\n",
      "Iteration 69, loss = 0.46442041\n",
      "Iteration 70, loss = 0.46342467\n",
      "Iteration 71, loss = 0.46252918\n",
      "Iteration 72, loss = 0.46158176\n",
      "Iteration 73, loss = 0.46064752\n",
      "Iteration 74, loss = 0.45969341\n",
      "Iteration 75, loss = 0.45898018\n",
      "Iteration 76, loss = 0.45820646\n",
      "Iteration 77, loss = 0.45759551\n",
      "Iteration 78, loss = 0.45688896\n",
      "Iteration 79, loss = 0.45608641\n",
      "Iteration 80, loss = 0.45549680\n",
      "Iteration 81, loss = 0.45483956\n",
      "Iteration 82, loss = 0.45431056\n",
      "Iteration 83, loss = 0.45400501\n",
      "Iteration 84, loss = 0.45334113\n",
      "Iteration 85, loss = 0.45286105\n",
      "Iteration 86, loss = 0.45228795\n",
      "Iteration 87, loss = 0.45204223\n",
      "Iteration 88, loss = 0.45164639\n",
      "Iteration 89, loss = 0.45112620\n",
      "Iteration 90, loss = 0.45074722\n",
      "Iteration 91, loss = 0.45041116\n",
      "Iteration 92, loss = 0.45007979\n",
      "Iteration 93, loss = 0.44972802\n",
      "Iteration 94, loss = 0.44949778\n",
      "Iteration 95, loss = 0.44919709\n",
      "Iteration 96, loss = 0.44896108\n",
      "Iteration 97, loss = 0.44872120\n",
      "Iteration 98, loss = 0.44842666\n",
      "Iteration 99, loss = 0.44819713\n",
      "Iteration 100, loss = 0.44799004\n",
      "Iteration 101, loss = 0.44790647\n",
      "Iteration 102, loss = 0.44767066\n",
      "Iteration 103, loss = 0.44753870\n",
      "Iteration 104, loss = 0.44725148\n",
      "Iteration 105, loss = 0.44712724\n",
      "Iteration 106, loss = 0.44702258\n",
      "Iteration 107, loss = 0.44681761\n",
      "Iteration 108, loss = 0.44676433\n",
      "Iteration 109, loss = 0.44664081\n",
      "Iteration 110, loss = 0.44648350\n",
      "Iteration 111, loss = 0.44630124\n",
      "Iteration 112, loss = 0.44619756\n",
      "Iteration 113, loss = 0.44605456\n",
      "Iteration 114, loss = 0.44596708\n",
      "Iteration 115, loss = 0.44590950\n",
      "Iteration 116, loss = 0.44579558\n",
      "Iteration 117, loss = 0.44570087\n",
      "Iteration 118, loss = 0.44564534\n",
      "Iteration 119, loss = 0.44561018\n",
      "Iteration 120, loss = 0.44552990\n",
      "Iteration 121, loss = 0.44541401\n",
      "Iteration 122, loss = 0.44549272\n",
      "Iteration 123, loss = 0.44535501\n",
      "Iteration 124, loss = 0.44522839\n",
      "Iteration 125, loss = 0.44512324\n",
      "Iteration 126, loss = 0.44513474\n",
      "Iteration 127, loss = 0.44504313\n",
      "Iteration 128, loss = 0.44500432\n",
      "Iteration 129, loss = 0.44494730\n",
      "Iteration 130, loss = 0.44496837\n",
      "Iteration 131, loss = 0.44484237\n",
      "Iteration 132, loss = 0.44478281\n",
      "Iteration 133, loss = 0.44478792\n",
      "Iteration 134, loss = 0.44469822\n",
      "Iteration 135, loss = 0.44468708\n",
      "Iteration 136, loss = 0.44467704\n",
      "Iteration 137, loss = 0.44468709\n",
      "Iteration 138, loss = 0.44452478\n",
      "Iteration 139, loss = 0.44454251\n",
      "Iteration 140, loss = 0.44456872\n",
      "Iteration 141, loss = 0.44447668\n",
      "Iteration 142, loss = 0.44456435\n",
      "Iteration 143, loss = 0.44439393\n",
      "Iteration 144, loss = 0.44436774\n",
      "Iteration 145, loss = 0.44434935\n",
      "Iteration 146, loss = 0.44439749\n",
      "Iteration 147, loss = 0.44439465\n",
      "Iteration 148, loss = 0.44423614\n",
      "Iteration 149, loss = 0.44418941\n",
      "Iteration 150, loss = 0.44420736\n",
      "Iteration 151, loss = 0.44418964\n",
      "Iteration 152, loss = 0.44420478\n",
      "Iteration 153, loss = 0.44428406\n",
      "Iteration 154, loss = 0.44411873\n",
      "Iteration 155, loss = 0.44408733\n",
      "Iteration 156, loss = 0.44399490\n",
      "Iteration 157, loss = 0.44400391\n",
      "Iteration 158, loss = 0.44397333\n",
      "Iteration 159, loss = 0.44392886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75655102\n",
      "Iteration 2, loss = 0.70781657\n",
      "Iteration 3, loss = 0.66314489\n",
      "Iteration 4, loss = 0.62732861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.59753880\n",
      "Iteration 6, loss = 0.57181014\n",
      "Iteration 7, loss = 0.54953529\n",
      "Iteration 8, loss = 0.53175424\n",
      "Iteration 9, loss = 0.51508273\n",
      "Iteration 10, loss = 0.50150213\n",
      "Iteration 11, loss = 0.48937772\n",
      "Iteration 12, loss = 0.47916104\n",
      "Iteration 13, loss = 0.46905657\n",
      "Iteration 14, loss = 0.46119326\n",
      "Iteration 15, loss = 0.45410954\n",
      "Iteration 16, loss = 0.44785418\n",
      "Iteration 17, loss = 0.44234217\n",
      "Iteration 18, loss = 0.43850239\n",
      "Iteration 19, loss = 0.43444529\n",
      "Iteration 20, loss = 0.43116446\n",
      "Iteration 21, loss = 0.42859814\n",
      "Iteration 22, loss = 0.42605802\n",
      "Iteration 23, loss = 0.42449832\n",
      "Iteration 24, loss = 0.42284434\n",
      "Iteration 25, loss = 0.42174250\n",
      "Iteration 26, loss = 0.42068692\n",
      "Iteration 27, loss = 0.41972525\n",
      "Iteration 28, loss = 0.41904601\n",
      "Iteration 29, loss = 0.41842408\n",
      "Iteration 30, loss = 0.41788128\n",
      "Iteration 31, loss = 0.41743592\n",
      "Iteration 32, loss = 0.41721547\n",
      "Iteration 33, loss = 0.41685562\n",
      "Iteration 34, loss = 0.41651371\n",
      "Iteration 35, loss = 0.41630088\n",
      "Iteration 36, loss = 0.41610896\n",
      "Iteration 37, loss = 0.41586169\n",
      "Iteration 38, loss = 0.41568035\n",
      "Iteration 39, loss = 0.41550236\n",
      "Iteration 40, loss = 0.41538168\n",
      "Iteration 41, loss = 0.41523203\n",
      "Iteration 42, loss = 0.41514256\n",
      "Iteration 43, loss = 0.41503626\n",
      "Iteration 44, loss = 0.41486384\n",
      "Iteration 45, loss = 0.41470775\n",
      "Iteration 46, loss = 0.41462271\n",
      "Iteration 47, loss = 0.41440965\n",
      "Iteration 48, loss = 0.41427056\n",
      "Iteration 49, loss = 0.41413269\n",
      "Iteration 50, loss = 0.41400061\n",
      "Iteration 51, loss = 0.41394125\n",
      "Iteration 52, loss = 0.41380842\n",
      "Iteration 53, loss = 0.41359244\n",
      "Iteration 54, loss = 0.41355827\n",
      "Iteration 55, loss = 0.41339878\n",
      "Iteration 56, loss = 0.41328062\n",
      "Iteration 57, loss = 0.41324160\n",
      "Iteration 58, loss = 0.41316098\n",
      "Iteration 59, loss = 0.41285654\n",
      "Iteration 60, loss = 0.41273941\n",
      "Iteration 61, loss = 0.41283072\n",
      "Iteration 62, loss = 0.41287205\n",
      "Iteration 63, loss = 0.41257751\n",
      "Iteration 64, loss = 0.41238770\n",
      "Iteration 65, loss = 0.41211520\n",
      "Iteration 66, loss = 0.41209146\n",
      "Iteration 67, loss = 0.41188086\n",
      "Iteration 68, loss = 0.41193537\n",
      "Iteration 69, loss = 0.41166278\n",
      "Iteration 70, loss = 0.41160812\n",
      "Iteration 71, loss = 0.41144096\n",
      "Iteration 72, loss = 0.41136005\n",
      "Iteration 73, loss = 0.41119236\n",
      "Iteration 74, loss = 0.41109465\n",
      "Iteration 75, loss = 0.41098529\n",
      "Iteration 76, loss = 0.41093000\n",
      "Iteration 77, loss = 0.41072549\n",
      "Iteration 78, loss = 0.41071467\n",
      "Iteration 79, loss = 0.41045613\n",
      "Iteration 80, loss = 0.41040753\n",
      "Iteration 81, loss = 0.41031437\n",
      "Iteration 82, loss = 0.41014274\n",
      "Iteration 83, loss = 0.41017575\n",
      "Iteration 84, loss = 0.40996937\n",
      "Iteration 85, loss = 0.40979589\n",
      "Iteration 86, loss = 0.40984899\n",
      "Iteration 87, loss = 0.40957526\n",
      "Iteration 88, loss = 0.40959407\n",
      "Iteration 89, loss = 0.40931871\n",
      "Iteration 90, loss = 0.40924805\n",
      "Iteration 91, loss = 0.40921498\n",
      "Iteration 92, loss = 0.40905426\n",
      "Iteration 93, loss = 0.40886965\n",
      "Iteration 94, loss = 0.40883283\n",
      "Iteration 95, loss = 0.40859470\n",
      "Iteration 96, loss = 0.40880839\n",
      "Iteration 97, loss = 0.40839403\n",
      "Iteration 98, loss = 0.40831894\n",
      "Iteration 99, loss = 0.40816818\n",
      "Iteration 100, loss = 0.40819205\n",
      "Iteration 101, loss = 0.40794648\n",
      "Iteration 102, loss = 0.40785835\n",
      "Iteration 103, loss = 0.40765693\n",
      "Iteration 104, loss = 0.40764303\n",
      "Iteration 105, loss = 0.40744174\n",
      "Iteration 106, loss = 0.40735286\n",
      "Iteration 107, loss = 0.40726093\n",
      "Iteration 108, loss = 0.40729413\n",
      "Iteration 109, loss = 0.40699658\n",
      "Iteration 110, loss = 0.40697029\n",
      "Iteration 111, loss = 0.40678945\n",
      "Iteration 112, loss = 0.40658646\n",
      "Iteration 113, loss = 0.40658342\n",
      "Iteration 114, loss = 0.40634611\n",
      "Iteration 115, loss = 0.40628379\n",
      "Iteration 116, loss = 0.40618372\n",
      "Iteration 117, loss = 0.40598909\n",
      "Iteration 118, loss = 0.40594864\n",
      "Iteration 119, loss = 0.40581182\n",
      "Iteration 120, loss = 0.40566023\n",
      "Iteration 121, loss = 0.40566171\n",
      "Iteration 122, loss = 0.40541536\n",
      "Iteration 123, loss = 0.40540424\n",
      "Iteration 124, loss = 0.40538503\n",
      "Iteration 125, loss = 0.40519908\n",
      "Iteration 126, loss = 0.40496485\n",
      "Iteration 127, loss = 0.40484357\n",
      "Iteration 128, loss = 0.40471636\n",
      "Iteration 129, loss = 0.40458365\n",
      "Iteration 130, loss = 0.40445386\n",
      "Iteration 131, loss = 0.40448260\n",
      "Iteration 132, loss = 0.40419868\n",
      "Iteration 133, loss = 0.40411372\n",
      "Iteration 134, loss = 0.40399646\n",
      "Iteration 135, loss = 0.40382597\n",
      "Iteration 136, loss = 0.40368231\n",
      "Iteration 137, loss = 0.40370798\n",
      "Iteration 138, loss = 0.40344603\n",
      "Iteration 139, loss = 0.40351338\n",
      "Iteration 140, loss = 0.40338249\n",
      "Iteration 141, loss = 0.40314555\n",
      "Iteration 142, loss = 0.40298126\n",
      "Iteration 143, loss = 0.40291376\n",
      "Iteration 144, loss = 0.40273134\n",
      "Iteration 145, loss = 0.40288478\n",
      "Iteration 146, loss = 0.40263410\n",
      "Iteration 147, loss = 0.40238755\n",
      "Iteration 148, loss = 0.40241347\n",
      "Iteration 149, loss = 0.40219673\n",
      "Iteration 150, loss = 0.40201163\n",
      "Iteration 151, loss = 0.40202240\n",
      "Iteration 152, loss = 0.40176151\n",
      "Iteration 153, loss = 0.40166051\n",
      "Iteration 154, loss = 0.40163510\n",
      "Iteration 155, loss = 0.40183553\n",
      "Iteration 156, loss = 0.40121195\n",
      "Iteration 157, loss = 0.40109979\n",
      "Iteration 158, loss = 0.40104201\n",
      "Iteration 159, loss = 0.40087374\n",
      "Iteration 160, loss = 0.40078370\n",
      "Iteration 161, loss = 0.40057832\n",
      "Iteration 162, loss = 0.40061751\n",
      "Iteration 163, loss = 0.40039459\n",
      "Iteration 164, loss = 0.40023920\n",
      "Iteration 165, loss = 0.40023453\n",
      "Iteration 166, loss = 0.40004613\n",
      "Iteration 167, loss = 0.39993002\n",
      "Iteration 168, loss = 0.39972695\n",
      "Iteration 169, loss = 0.39972597\n",
      "Iteration 170, loss = 0.39944778\n",
      "Iteration 171, loss = 0.39928003\n",
      "Iteration 172, loss = 0.39929091\n",
      "Iteration 173, loss = 0.39920402\n",
      "Iteration 174, loss = 0.39894228\n",
      "Iteration 175, loss = 0.39874180\n",
      "Iteration 176, loss = 0.39906631\n",
      "Iteration 177, loss = 0.39868722\n",
      "Iteration 178, loss = 0.39849674\n",
      "Iteration 179, loss = 0.39840216\n",
      "Iteration 180, loss = 0.39816242\n",
      "Iteration 181, loss = 0.39819884\n",
      "Iteration 182, loss = 0.39792698\n",
      "Iteration 183, loss = 0.39790758\n",
      "Iteration 184, loss = 0.39770691\n",
      "Iteration 185, loss = 0.39765468\n",
      "Iteration 186, loss = 0.39742633\n",
      "Iteration 187, loss = 0.39745225\n",
      "Iteration 188, loss = 0.39714101\n",
      "Iteration 189, loss = 0.39701021\n",
      "Iteration 190, loss = 0.39701887\n",
      "Iteration 191, loss = 0.39666710\n",
      "Iteration 192, loss = 0.39667852\n",
      "Iteration 193, loss = 0.39650846\n",
      "Iteration 194, loss = 0.39651310\n",
      "Iteration 195, loss = 0.39642314\n",
      "Iteration 196, loss = 0.39613975\n",
      "Iteration 197, loss = 0.39586352\n",
      "Iteration 198, loss = 0.39588017\n",
      "Iteration 199, loss = 0.39571294\n",
      "Iteration 200, loss = 0.39561707\n",
      "Iteration 1, loss = 0.76370564\n",
      "Iteration 2, loss = 0.71709415\n",
      "Iteration 3, loss = 0.67447676\n",
      "Iteration 4, loss = 0.64084281\n",
      "Iteration 5, loss = 0.61220263\n",
      "Iteration 6, loss = 0.58765287\n",
      "Iteration 7, loss = 0.56580839\n",
      "Iteration 8, loss = 0.54906245\n",
      "Iteration 9, loss = 0.53316446\n",
      "Iteration 10, loss = 0.52021648\n",
      "Iteration 11, loss = 0.50845066\n",
      "Iteration 12, loss = 0.49865077\n",
      "Iteration 13, loss = 0.48924856\n",
      "Iteration 14, loss = 0.48189432\n",
      "Iteration 15, loss = 0.47508564\n",
      "Iteration 16, loss = 0.46934614\n",
      "Iteration 17, loss = 0.46432262\n",
      "Iteration 18, loss = 0.46063720\n",
      "Iteration 19, loss = 0.45693961\n",
      "Iteration 20, loss = 0.45378686\n",
      "Iteration 21, loss = 0.45131613\n",
      "Iteration 22, loss = 0.44909990\n",
      "Iteration 23, loss = 0.44778259\n",
      "Iteration 24, loss = 0.44618625\n",
      "Iteration 25, loss = 0.44522025\n",
      "Iteration 26, loss = 0.44407019\n",
      "Iteration 27, loss = 0.44337731\n",
      "Iteration 28, loss = 0.44272978\n",
      "Iteration 29, loss = 0.44196250\n",
      "Iteration 30, loss = 0.44160958\n",
      "Iteration 31, loss = 0.44103110\n",
      "Iteration 32, loss = 0.44086890\n",
      "Iteration 33, loss = 0.44046749\n",
      "Iteration 34, loss = 0.44009735\n",
      "Iteration 35, loss = 0.43982880\n",
      "Iteration 36, loss = 0.43968061\n",
      "Iteration 37, loss = 0.43937505\n",
      "Iteration 38, loss = 0.43919366\n",
      "Iteration 39, loss = 0.43900726\n",
      "Iteration 40, loss = 0.43893935\n",
      "Iteration 41, loss = 0.43864620\n",
      "Iteration 42, loss = 0.43856740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.43843183\n",
      "Iteration 44, loss = 0.43830775\n",
      "Iteration 45, loss = 0.43808021\n",
      "Iteration 46, loss = 0.43796351\n",
      "Iteration 47, loss = 0.43780104\n",
      "Iteration 48, loss = 0.43771534\n",
      "Iteration 49, loss = 0.43761025\n",
      "Iteration 50, loss = 0.43749093\n",
      "Iteration 51, loss = 0.43735533\n",
      "Iteration 52, loss = 0.43718061\n",
      "Iteration 53, loss = 0.43708045\n",
      "Iteration 54, loss = 0.43698937\n",
      "Iteration 55, loss = 0.43682848\n",
      "Iteration 56, loss = 0.43674106\n",
      "Iteration 57, loss = 0.43660843\n",
      "Iteration 58, loss = 0.43650490\n",
      "Iteration 59, loss = 0.43635708\n",
      "Iteration 60, loss = 0.43629161\n",
      "Iteration 61, loss = 0.43624781\n",
      "Iteration 62, loss = 0.43607239\n",
      "Iteration 63, loss = 0.43590168\n",
      "Iteration 64, loss = 0.43586939\n",
      "Iteration 65, loss = 0.43565957\n",
      "Iteration 66, loss = 0.43563562\n",
      "Iteration 67, loss = 0.43539828\n",
      "Iteration 68, loss = 0.43547204\n",
      "Iteration 69, loss = 0.43532453\n",
      "Iteration 70, loss = 0.43514931\n",
      "Iteration 71, loss = 0.43500393\n",
      "Iteration 72, loss = 0.43483209\n",
      "Iteration 73, loss = 0.43485119\n",
      "Iteration 74, loss = 0.43473042\n",
      "Iteration 75, loss = 0.43455311\n",
      "Iteration 76, loss = 0.43447835\n",
      "Iteration 77, loss = 0.43431553\n",
      "Iteration 78, loss = 0.43421823\n",
      "Iteration 79, loss = 0.43405034\n",
      "Iteration 80, loss = 0.43395452\n",
      "Iteration 81, loss = 0.43406756\n",
      "Iteration 82, loss = 0.43373402\n",
      "Iteration 83, loss = 0.43391456\n",
      "Iteration 84, loss = 0.43357454\n",
      "Iteration 85, loss = 0.43336549\n",
      "Iteration 86, loss = 0.43328145\n",
      "Iteration 87, loss = 0.43321421\n",
      "Iteration 88, loss = 0.43314334\n",
      "Iteration 89, loss = 0.43296736\n",
      "Iteration 90, loss = 0.43287527\n",
      "Iteration 91, loss = 0.43286775\n",
      "Iteration 92, loss = 0.43263755\n",
      "Iteration 93, loss = 0.43248445\n",
      "Iteration 94, loss = 0.43273223\n",
      "Iteration 95, loss = 0.43237590\n",
      "Iteration 96, loss = 0.43227084\n",
      "Iteration 97, loss = 0.43215470\n",
      "Iteration 98, loss = 0.43220400\n",
      "Iteration 99, loss = 0.43191726\n",
      "Iteration 100, loss = 0.43179213\n",
      "Iteration 101, loss = 0.43175279\n",
      "Iteration 102, loss = 0.43161409\n",
      "Iteration 103, loss = 0.43153122\n",
      "Iteration 104, loss = 0.43139348\n",
      "Iteration 105, loss = 0.43144547\n",
      "Iteration 106, loss = 0.43114689\n",
      "Iteration 107, loss = 0.43100645\n",
      "Iteration 108, loss = 0.43094209\n",
      "Iteration 109, loss = 0.43083965\n",
      "Iteration 110, loss = 0.43074941\n",
      "Iteration 111, loss = 0.43067414\n",
      "Iteration 112, loss = 0.43043144\n",
      "Iteration 113, loss = 0.43046305\n",
      "Iteration 114, loss = 0.43023989\n",
      "Iteration 115, loss = 0.43018281\n",
      "Iteration 116, loss = 0.43023037\n",
      "Iteration 117, loss = 0.42997108\n",
      "Iteration 118, loss = 0.42989077\n",
      "Iteration 119, loss = 0.42981471\n",
      "Iteration 120, loss = 0.42960657\n",
      "Iteration 121, loss = 0.42964522\n",
      "Iteration 122, loss = 0.42945746\n",
      "Iteration 123, loss = 0.42932216\n",
      "Iteration 124, loss = 0.42932376\n",
      "Iteration 125, loss = 0.42909829\n",
      "Iteration 126, loss = 0.42904096\n",
      "Iteration 127, loss = 0.42884852\n",
      "Iteration 128, loss = 0.42866661\n",
      "Iteration 129, loss = 0.42876790\n",
      "Iteration 130, loss = 0.42857040\n",
      "Iteration 131, loss = 0.42847196\n",
      "Iteration 132, loss = 0.42828916\n",
      "Iteration 133, loss = 0.42828167\n",
      "Iteration 134, loss = 0.42810543\n",
      "Iteration 135, loss = 0.42801969\n",
      "Iteration 136, loss = 0.42783585\n",
      "Iteration 137, loss = 0.42782225\n",
      "Iteration 138, loss = 0.42761088\n",
      "Iteration 139, loss = 0.42792685\n",
      "Iteration 140, loss = 0.42748231\n",
      "Iteration 141, loss = 0.42753744\n",
      "Iteration 142, loss = 0.42720560\n",
      "Iteration 143, loss = 0.42708941\n",
      "Iteration 144, loss = 0.42701844\n",
      "Iteration 145, loss = 0.42694062\n",
      "Iteration 146, loss = 0.42690995\n",
      "Iteration 147, loss = 0.42671981\n",
      "Iteration 148, loss = 0.42674947\n",
      "Iteration 149, loss = 0.42646796\n",
      "Iteration 150, loss = 0.42625616\n",
      "Iteration 151, loss = 0.42629892\n",
      "Iteration 152, loss = 0.42612020\n",
      "Iteration 153, loss = 0.42591855\n",
      "Iteration 154, loss = 0.42606372\n",
      "Iteration 155, loss = 0.42625295\n",
      "Iteration 156, loss = 0.42570949\n",
      "Iteration 157, loss = 0.42551621\n",
      "Iteration 158, loss = 0.42550949\n",
      "Iteration 159, loss = 0.42557927\n",
      "Iteration 160, loss = 0.42516861\n",
      "Iteration 161, loss = 0.42509294\n",
      "Iteration 162, loss = 0.42512446\n",
      "Iteration 163, loss = 0.42493768\n",
      "Iteration 164, loss = 0.42470283\n",
      "Iteration 165, loss = 0.42466559\n",
      "Iteration 166, loss = 0.42458713\n",
      "Iteration 167, loss = 0.42451829\n",
      "Iteration 168, loss = 0.42427674\n",
      "Iteration 169, loss = 0.42417411\n",
      "Iteration 170, loss = 0.42410026\n",
      "Iteration 171, loss = 0.42408652\n",
      "Iteration 172, loss = 0.42384058\n",
      "Iteration 173, loss = 0.42385026\n",
      "Iteration 174, loss = 0.42366123\n",
      "Iteration 175, loss = 0.42360834\n",
      "Iteration 176, loss = 0.42357580\n",
      "Iteration 177, loss = 0.42331957\n",
      "Iteration 178, loss = 0.42327803\n",
      "Iteration 179, loss = 0.42304769\n",
      "Iteration 180, loss = 0.42300201\n",
      "Iteration 181, loss = 0.42292174\n",
      "Iteration 182, loss = 0.42260962\n",
      "Iteration 183, loss = 0.42252558\n",
      "Iteration 184, loss = 0.42246169\n",
      "Iteration 185, loss = 0.42237358\n",
      "Iteration 186, loss = 0.42218244\n",
      "Iteration 187, loss = 0.42239632\n",
      "Iteration 188, loss = 0.42206526\n",
      "Iteration 189, loss = 0.42192017\n",
      "Iteration 190, loss = 0.42179274\n",
      "Iteration 191, loss = 0.42157694\n",
      "Iteration 192, loss = 0.42156288\n",
      "Iteration 193, loss = 0.42138215\n",
      "Iteration 194, loss = 0.42131296\n",
      "Iteration 195, loss = 0.42133381\n",
      "Iteration 196, loss = 0.42107231\n",
      "Iteration 197, loss = 0.42086224\n",
      "Iteration 198, loss = 0.42077951\n",
      "Iteration 199, loss = 0.42067890\n",
      "Iteration 200, loss = 0.42057830\n",
      "Iteration 1, loss = 0.76461857\n",
      "Iteration 2, loss = 0.71810828\n",
      "Iteration 3, loss = 0.67821531\n",
      "Iteration 4, loss = 0.64461735\n",
      "Iteration 5, loss = 0.61578539\n",
      "Iteration 6, loss = 0.59002665\n",
      "Iteration 7, loss = 0.56944838\n",
      "Iteration 8, loss = 0.55047627\n",
      "Iteration 9, loss = 0.53357105\n",
      "Iteration 10, loss = 0.51876503\n",
      "Iteration 11, loss = 0.50603548\n",
      "Iteration 12, loss = 0.49450402\n",
      "Iteration 13, loss = 0.48463789\n",
      "Iteration 14, loss = 0.47559372\n",
      "Iteration 15, loss = 0.46804171\n",
      "Iteration 16, loss = 0.46178237\n",
      "Iteration 17, loss = 0.45619100\n",
      "Iteration 18, loss = 0.45196519\n",
      "Iteration 19, loss = 0.44778076\n",
      "Iteration 20, loss = 0.44395626\n",
      "Iteration 21, loss = 0.44133373\n",
      "Iteration 22, loss = 0.43882625\n",
      "Iteration 23, loss = 0.43664097\n",
      "Iteration 24, loss = 0.43504595\n",
      "Iteration 25, loss = 0.43366711\n",
      "Iteration 26, loss = 0.43240322\n",
      "Iteration 27, loss = 0.43125083\n",
      "Iteration 28, loss = 0.43057095\n",
      "Iteration 29, loss = 0.42963907\n",
      "Iteration 30, loss = 0.42909245\n",
      "Iteration 31, loss = 0.42862496\n",
      "Iteration 32, loss = 0.42810460\n",
      "Iteration 33, loss = 0.42795619\n",
      "Iteration 34, loss = 0.42732317\n",
      "Iteration 35, loss = 0.42711000\n",
      "Iteration 36, loss = 0.42669122\n",
      "Iteration 37, loss = 0.42653823\n",
      "Iteration 38, loss = 0.42621466\n",
      "Iteration 39, loss = 0.42586925\n",
      "Iteration 40, loss = 0.42577283\n",
      "Iteration 41, loss = 0.42549683\n",
      "Iteration 42, loss = 0.42534871\n",
      "Iteration 43, loss = 0.42501452\n",
      "Iteration 44, loss = 0.42504901\n",
      "Iteration 45, loss = 0.42469804\n",
      "Iteration 46, loss = 0.42447657\n",
      "Iteration 47, loss = 0.42427742\n",
      "Iteration 48, loss = 0.42420435\n",
      "Iteration 49, loss = 0.42397769\n",
      "Iteration 50, loss = 0.42381967\n",
      "Iteration 51, loss = 0.42368837\n",
      "Iteration 52, loss = 0.42358340\n",
      "Iteration 53, loss = 0.42332569\n",
      "Iteration 54, loss = 0.42314010\n",
      "Iteration 55, loss = 0.42299981\n",
      "Iteration 56, loss = 0.42294562\n",
      "Iteration 57, loss = 0.42284822\n",
      "Iteration 58, loss = 0.42264448\n",
      "Iteration 59, loss = 0.42242626\n",
      "Iteration 60, loss = 0.42232399\n",
      "Iteration 61, loss = 0.42217292\n",
      "Iteration 62, loss = 0.42204640\n",
      "Iteration 63, loss = 0.42188497\n",
      "Iteration 64, loss = 0.42180711\n",
      "Iteration 65, loss = 0.42160216\n",
      "Iteration 66, loss = 0.42159175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67, loss = 0.42152331\n",
      "Iteration 68, loss = 0.42128026\n",
      "Iteration 69, loss = 0.42107049\n",
      "Iteration 70, loss = 0.42109927\n",
      "Iteration 71, loss = 0.42083595\n",
      "Iteration 72, loss = 0.42072649\n",
      "Iteration 73, loss = 0.42053675\n",
      "Iteration 74, loss = 0.42044580\n",
      "Iteration 75, loss = 0.42026503\n",
      "Iteration 76, loss = 0.42031478\n",
      "Iteration 77, loss = 0.41995567\n",
      "Iteration 78, loss = 0.41984231\n",
      "Iteration 79, loss = 0.41973070\n",
      "Iteration 80, loss = 0.41968729\n",
      "Iteration 81, loss = 0.41953515\n",
      "Iteration 82, loss = 0.41942351\n",
      "Iteration 83, loss = 0.41925110\n",
      "Iteration 84, loss = 0.41912730\n",
      "Iteration 85, loss = 0.41914651\n",
      "Iteration 86, loss = 0.41900322\n",
      "Iteration 87, loss = 0.41893393\n",
      "Iteration 88, loss = 0.41861335\n",
      "Iteration 89, loss = 0.41850678\n",
      "Iteration 90, loss = 0.41850125\n",
      "Iteration 91, loss = 0.41823829\n",
      "Iteration 92, loss = 0.41845967\n",
      "Iteration 93, loss = 0.41804898\n",
      "Iteration 94, loss = 0.41833040\n",
      "Iteration 95, loss = 0.41776110\n",
      "Iteration 96, loss = 0.41758154\n",
      "Iteration 97, loss = 0.41747768\n",
      "Iteration 98, loss = 0.41729560\n",
      "Iteration 99, loss = 0.41722313\n",
      "Iteration 100, loss = 0.41705125\n",
      "Iteration 101, loss = 0.41725698\n",
      "Iteration 102, loss = 0.41687548\n",
      "Iteration 103, loss = 0.41671286\n",
      "Iteration 104, loss = 0.41672063\n",
      "Iteration 105, loss = 0.41645609\n",
      "Iteration 106, loss = 0.41645978\n",
      "Iteration 107, loss = 0.41625798\n",
      "Iteration 108, loss = 0.41610848\n",
      "Iteration 109, loss = 0.41598295\n",
      "Iteration 110, loss = 0.41580385\n",
      "Iteration 111, loss = 0.41572507\n",
      "Iteration 112, loss = 0.41567374\n",
      "Iteration 113, loss = 0.41542188\n",
      "Iteration 114, loss = 0.41537388\n",
      "Iteration 115, loss = 0.41519348\n",
      "Iteration 116, loss = 0.41497169\n",
      "Iteration 117, loss = 0.41486429\n",
      "Iteration 118, loss = 0.41477897\n",
      "Iteration 119, loss = 0.41481284\n",
      "Iteration 120, loss = 0.41467029\n",
      "Iteration 121, loss = 0.41444488\n",
      "Iteration 122, loss = 0.41431944\n",
      "Iteration 123, loss = 0.41410030\n",
      "Iteration 124, loss = 0.41410607\n",
      "Iteration 125, loss = 0.41399719\n",
      "Iteration 126, loss = 0.41374906\n",
      "Iteration 127, loss = 0.41357569\n",
      "Iteration 128, loss = 0.41350268\n",
      "Iteration 129, loss = 0.41343226\n",
      "Iteration 130, loss = 0.41327965\n",
      "Iteration 131, loss = 0.41307077\n",
      "Iteration 132, loss = 0.41300803\n",
      "Iteration 133, loss = 0.41286378\n",
      "Iteration 134, loss = 0.41272926\n",
      "Iteration 135, loss = 0.41248088\n",
      "Iteration 136, loss = 0.41274969\n",
      "Iteration 137, loss = 0.41222426\n",
      "Iteration 138, loss = 0.41214063\n",
      "Iteration 139, loss = 0.41216520\n",
      "Iteration 140, loss = 0.41188613\n",
      "Iteration 141, loss = 0.41176515\n",
      "Iteration 142, loss = 0.41158875\n",
      "Iteration 143, loss = 0.41147526\n",
      "Iteration 144, loss = 0.41143666\n",
      "Iteration 145, loss = 0.41132196\n",
      "Iteration 146, loss = 0.41112701\n",
      "Iteration 147, loss = 0.41099968\n",
      "Iteration 148, loss = 0.41093325\n",
      "Iteration 149, loss = 0.41062103\n",
      "Iteration 150, loss = 0.41047547\n",
      "Iteration 151, loss = 0.41037057\n",
      "Iteration 152, loss = 0.41028149\n",
      "Iteration 153, loss = 0.41065497\n",
      "Iteration 154, loss = 0.41009754\n",
      "Iteration 155, loss = 0.40968277\n",
      "Iteration 156, loss = 0.40975363\n",
      "Iteration 157, loss = 0.40956158\n",
      "Iteration 158, loss = 0.40939286\n",
      "Iteration 159, loss = 0.40926050\n",
      "Iteration 160, loss = 0.40921385\n",
      "Iteration 161, loss = 0.40903146\n",
      "Iteration 162, loss = 0.40893606\n",
      "Iteration 163, loss = 0.40871478\n",
      "Iteration 164, loss = 0.40872381\n",
      "Iteration 165, loss = 0.40850811\n",
      "Iteration 166, loss = 0.40831779\n",
      "Iteration 167, loss = 0.40805106\n",
      "Iteration 168, loss = 0.40797706\n",
      "Iteration 169, loss = 0.40781695\n",
      "Iteration 170, loss = 0.40783082\n",
      "Iteration 171, loss = 0.40756732\n",
      "Iteration 172, loss = 0.40760012\n",
      "Iteration 173, loss = 0.40742048\n",
      "Iteration 174, loss = 0.40725160\n",
      "Iteration 175, loss = 0.40703586\n",
      "Iteration 176, loss = 0.40692100\n",
      "Iteration 177, loss = 0.40665380\n",
      "Iteration 178, loss = 0.40675444\n",
      "Iteration 179, loss = 0.40643767\n",
      "Iteration 180, loss = 0.40624247\n",
      "Iteration 181, loss = 0.40604709\n",
      "Iteration 182, loss = 0.40596225\n",
      "Iteration 183, loss = 0.40570368\n",
      "Iteration 184, loss = 0.40563960\n",
      "Iteration 185, loss = 0.40571569\n",
      "Iteration 186, loss = 0.40543625\n",
      "Iteration 187, loss = 0.40530603\n",
      "Iteration 188, loss = 0.40512238\n",
      "Iteration 189, loss = 0.40498453\n",
      "Iteration 190, loss = 0.40474299\n",
      "Iteration 191, loss = 0.40465847\n",
      "Iteration 192, loss = 0.40450249\n",
      "Iteration 193, loss = 0.40422998\n",
      "Iteration 194, loss = 0.40431688\n",
      "Iteration 195, loss = 0.40414709\n",
      "Iteration 196, loss = 0.40388955\n",
      "Iteration 197, loss = 0.40399438\n",
      "Iteration 198, loss = 0.40364059\n",
      "Iteration 199, loss = 0.40350445\n",
      "Iteration 200, loss = 0.40315916\n",
      "Iteration 1, loss = 0.76698668\n",
      "Iteration 2, loss = 0.71931575\n",
      "Iteration 3, loss = 0.68047286\n",
      "Iteration 4, loss = 0.64658256\n",
      "Iteration 5, loss = 0.61699206\n",
      "Iteration 6, loss = 0.59184022\n",
      "Iteration 7, loss = 0.57053122\n",
      "Iteration 8, loss = 0.55105828\n",
      "Iteration 9, loss = 0.53387083\n",
      "Iteration 10, loss = 0.51868375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.50582741\n",
      "Iteration 12, loss = 0.49403082\n",
      "Iteration 13, loss = 0.48395043\n",
      "Iteration 14, loss = 0.47457915\n",
      "Iteration 15, loss = 0.46699736\n",
      "Iteration 16, loss = 0.46028293\n",
      "Iteration 17, loss = 0.45471435\n",
      "Iteration 18, loss = 0.45016087\n",
      "Iteration 19, loss = 0.44543484\n",
      "Iteration 20, loss = 0.44204581\n",
      "Iteration 21, loss = 0.43889025\n",
      "Iteration 22, loss = 0.43630381\n",
      "Iteration 23, loss = 0.43387192\n",
      "Iteration 24, loss = 0.43205691\n",
      "Iteration 25, loss = 0.43060946\n",
      "Iteration 26, loss = 0.42914312\n",
      "Iteration 27, loss = 0.42798085\n",
      "Iteration 28, loss = 0.42715581\n",
      "Iteration 29, loss = 0.42628258\n",
      "Iteration 30, loss = 0.42549795\n",
      "Iteration 31, loss = 0.42508747\n",
      "Iteration 32, loss = 0.42460474\n",
      "Iteration 33, loss = 0.42434262\n",
      "Iteration 34, loss = 0.42371759\n",
      "Iteration 35, loss = 0.42331717\n",
      "Iteration 36, loss = 0.42319010\n",
      "Iteration 37, loss = 0.42287232\n",
      "Iteration 38, loss = 0.42251797\n",
      "Iteration 39, loss = 0.42230602\n",
      "Iteration 40, loss = 0.42223711\n",
      "Iteration 41, loss = 0.42193776\n",
      "Iteration 42, loss = 0.42179630\n",
      "Iteration 43, loss = 0.42159182\n",
      "Iteration 44, loss = 0.42152033\n",
      "Iteration 45, loss = 0.42122125\n",
      "Iteration 46, loss = 0.42107361\n",
      "Iteration 47, loss = 0.42088978\n",
      "Iteration 48, loss = 0.42079099\n",
      "Iteration 49, loss = 0.42064651\n",
      "Iteration 50, loss = 0.42046172\n",
      "Iteration 51, loss = 0.42034726\n",
      "Iteration 52, loss = 0.42027875\n",
      "Iteration 53, loss = 0.42007663\n",
      "Iteration 54, loss = 0.41992558\n",
      "Iteration 55, loss = 0.41979882\n",
      "Iteration 56, loss = 0.41966464\n",
      "Iteration 57, loss = 0.41959706\n",
      "Iteration 58, loss = 0.41953825\n",
      "Iteration 59, loss = 0.41929654\n",
      "Iteration 60, loss = 0.41914278\n",
      "Iteration 61, loss = 0.41909925\n",
      "Iteration 62, loss = 0.41893716\n",
      "Iteration 63, loss = 0.41889749\n",
      "Iteration 64, loss = 0.41884792\n",
      "Iteration 65, loss = 0.41864276\n",
      "Iteration 66, loss = 0.41853264\n",
      "Iteration 67, loss = 0.41838397\n",
      "Iteration 68, loss = 0.41821371\n",
      "Iteration 69, loss = 0.41810053\n",
      "Iteration 70, loss = 0.41808058\n",
      "Iteration 71, loss = 0.41789543\n",
      "Iteration 72, loss = 0.41776948\n",
      "Iteration 73, loss = 0.41755704\n",
      "Iteration 74, loss = 0.41754270\n",
      "Iteration 75, loss = 0.41738362\n",
      "Iteration 76, loss = 0.41729181\n",
      "Iteration 77, loss = 0.41716839\n",
      "Iteration 78, loss = 0.41700924\n",
      "Iteration 79, loss = 0.41685867\n",
      "Iteration 80, loss = 0.41678594\n",
      "Iteration 81, loss = 0.41665242\n",
      "Iteration 82, loss = 0.41652722\n",
      "Iteration 83, loss = 0.41639190\n",
      "Iteration 84, loss = 0.41631400\n",
      "Iteration 85, loss = 0.41631639\n",
      "Iteration 86, loss = 0.41602842\n",
      "Iteration 87, loss = 0.41613037\n",
      "Iteration 88, loss = 0.41592346\n",
      "Iteration 89, loss = 0.41580151\n",
      "Iteration 90, loss = 0.41576732\n",
      "Iteration 91, loss = 0.41550207\n",
      "Iteration 92, loss = 0.41565218\n",
      "Iteration 93, loss = 0.41534233\n",
      "Iteration 94, loss = 0.41529679\n",
      "Iteration 95, loss = 0.41504010\n",
      "Iteration 96, loss = 0.41494349\n",
      "Iteration 97, loss = 0.41495101\n",
      "Iteration 98, loss = 0.41471227\n",
      "Iteration 99, loss = 0.41466980\n",
      "Iteration 100, loss = 0.41438963\n",
      "Iteration 101, loss = 0.41461783\n",
      "Iteration 102, loss = 0.41437644\n",
      "Iteration 103, loss = 0.41415427\n",
      "Iteration 104, loss = 0.41415369\n",
      "Iteration 105, loss = 0.41387622\n",
      "Iteration 106, loss = 0.41382177\n",
      "Iteration 107, loss = 0.41374635\n",
      "Iteration 108, loss = 0.41372042\n",
      "Iteration 109, loss = 0.41360979\n",
      "Iteration 110, loss = 0.41340923\n",
      "Iteration 111, loss = 0.41317872\n",
      "Iteration 112, loss = 0.41314603\n",
      "Iteration 113, loss = 0.41300282\n",
      "Iteration 114, loss = 0.41294138\n",
      "Iteration 115, loss = 0.41281349\n",
      "Iteration 116, loss = 0.41261919\n",
      "Iteration 117, loss = 0.41251450\n",
      "Iteration 118, loss = 0.41249845\n",
      "Iteration 119, loss = 0.41247371\n",
      "Iteration 120, loss = 0.41226575\n",
      "Iteration 121, loss = 0.41218178\n",
      "Iteration 122, loss = 0.41204885\n",
      "Iteration 123, loss = 0.41181899\n",
      "Iteration 124, loss = 0.41179997\n",
      "Iteration 125, loss = 0.41171308\n",
      "Iteration 126, loss = 0.41155924\n",
      "Iteration 127, loss = 0.41145862\n",
      "Iteration 128, loss = 0.41128875\n",
      "Iteration 129, loss = 0.41119806\n",
      "Iteration 130, loss = 0.41114917\n",
      "Iteration 131, loss = 0.41089690\n",
      "Iteration 132, loss = 0.41087383\n",
      "Iteration 133, loss = 0.41077691\n",
      "Iteration 134, loss = 0.41062607\n",
      "Iteration 135, loss = 0.41046649\n",
      "Iteration 136, loss = 0.41042915\n",
      "Iteration 137, loss = 0.41054359\n",
      "Iteration 138, loss = 0.41026541\n",
      "Iteration 139, loss = 0.41015867\n",
      "Iteration 140, loss = 0.40998224\n",
      "Iteration 141, loss = 0.40987599\n",
      "Iteration 142, loss = 0.40964099\n",
      "Iteration 143, loss = 0.40947431\n",
      "Iteration 144, loss = 0.40951201\n",
      "Iteration 145, loss = 0.40942992\n",
      "Iteration 146, loss = 0.40937096\n",
      "Iteration 147, loss = 0.40907281\n",
      "Iteration 148, loss = 0.40905138\n",
      "Iteration 149, loss = 0.40880685\n",
      "Iteration 150, loss = 0.40869131\n",
      "Iteration 151, loss = 0.40856757\n",
      "Iteration 152, loss = 0.40864067\n",
      "Iteration 153, loss = 0.40876787\n",
      "Iteration 154, loss = 0.40824185\n",
      "Iteration 155, loss = 0.40808373\n",
      "Iteration 156, loss = 0.40805503\n",
      "Iteration 157, loss = 0.40785075\n",
      "Iteration 158, loss = 0.40773423\n",
      "Iteration 159, loss = 0.40760114\n",
      "Iteration 160, loss = 0.40756470\n",
      "Iteration 161, loss = 0.40747267\n",
      "Iteration 162, loss = 0.40737447\n",
      "Iteration 163, loss = 0.40720250\n",
      "Iteration 164, loss = 0.40707298\n",
      "Iteration 165, loss = 0.40686471\n",
      "Iteration 166, loss = 0.40681090\n",
      "Iteration 167, loss = 0.40660319\n",
      "Iteration 168, loss = 0.40665548\n",
      "Iteration 169, loss = 0.40644347\n",
      "Iteration 170, loss = 0.40638287\n",
      "Iteration 171, loss = 0.40611417\n",
      "Iteration 172, loss = 0.40606214\n",
      "Iteration 173, loss = 0.40612171\n",
      "Iteration 174, loss = 0.40584654\n",
      "Iteration 175, loss = 0.40579657\n",
      "Iteration 176, loss = 0.40555888\n",
      "Iteration 177, loss = 0.40544021\n",
      "Iteration 178, loss = 0.40535636\n",
      "Iteration 179, loss = 0.40522781\n",
      "Iteration 180, loss = 0.40498772\n",
      "Iteration 181, loss = 0.40486773\n",
      "Iteration 182, loss = 0.40475640\n",
      "Iteration 183, loss = 0.40456211\n",
      "Iteration 184, loss = 0.40444738\n",
      "Iteration 185, loss = 0.40442870\n",
      "Iteration 186, loss = 0.40436166\n",
      "Iteration 187, loss = 0.40406319\n",
      "Iteration 188, loss = 0.40403353\n",
      "Iteration 189, loss = 0.40392129\n",
      "Iteration 190, loss = 0.40380519\n",
      "Iteration 191, loss = 0.40353269\n",
      "Iteration 192, loss = 0.40364072\n",
      "Iteration 193, loss = 0.40319085\n",
      "Iteration 194, loss = 0.40324408\n",
      "Iteration 195, loss = 0.40325830\n",
      "Iteration 196, loss = 0.40285143\n",
      "Iteration 197, loss = 0.40282339\n",
      "Iteration 198, loss = 0.40271970\n",
      "Iteration 199, loss = 0.40266201\n",
      "Iteration 200, loss = 0.40230377\n",
      "Iteration 1, loss = 0.75977775\n",
      "Iteration 2, loss = 0.71338118\n",
      "Iteration 3, loss = 0.67397373\n",
      "Iteration 4, loss = 0.64118000\n",
      "Iteration 5, loss = 0.61291609\n",
      "Iteration 6, loss = 0.58912251\n",
      "Iteration 7, loss = 0.56969590\n",
      "Iteration 8, loss = 0.55182183\n",
      "Iteration 9, loss = 0.53666622\n",
      "Iteration 10, loss = 0.52326531\n",
      "Iteration 11, loss = 0.51190507\n",
      "Iteration 12, loss = 0.50220708\n",
      "Iteration 13, loss = 0.49366151\n",
      "Iteration 14, loss = 0.48574896\n",
      "Iteration 15, loss = 0.47969172\n",
      "Iteration 16, loss = 0.47419528\n",
      "Iteration 17, loss = 0.47028418\n",
      "Iteration 18, loss = 0.46626132\n",
      "Iteration 19, loss = 0.46275930\n",
      "Iteration 20, loss = 0.46055236\n",
      "Iteration 21, loss = 0.45839741\n",
      "Iteration 22, loss = 0.45637320\n",
      "Iteration 23, loss = 0.45490479\n",
      "Iteration 24, loss = 0.45373051\n",
      "Iteration 25, loss = 0.45272731\n",
      "Iteration 26, loss = 0.45180684\n",
      "Iteration 27, loss = 0.45112329\n",
      "Iteration 28, loss = 0.45069400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.45010131\n",
      "Iteration 30, loss = 0.44961647\n",
      "Iteration 31, loss = 0.44931499\n",
      "Iteration 32, loss = 0.44904142\n",
      "Iteration 33, loss = 0.44881140\n",
      "Iteration 34, loss = 0.44846938\n",
      "Iteration 35, loss = 0.44815970\n",
      "Iteration 36, loss = 0.44807669\n",
      "Iteration 37, loss = 0.44784330\n",
      "Iteration 38, loss = 0.44757018\n",
      "Iteration 39, loss = 0.44735477\n",
      "Iteration 40, loss = 0.44730593\n",
      "Iteration 41, loss = 0.44705182\n",
      "Iteration 42, loss = 0.44690810\n",
      "Iteration 43, loss = 0.44669258\n",
      "Iteration 44, loss = 0.44658457\n",
      "Iteration 45, loss = 0.44637255\n",
      "Iteration 46, loss = 0.44622999\n",
      "Iteration 47, loss = 0.44600411\n",
      "Iteration 48, loss = 0.44588374\n",
      "Iteration 49, loss = 0.44577344\n",
      "Iteration 50, loss = 0.44569833\n",
      "Iteration 51, loss = 0.44545129\n",
      "Iteration 52, loss = 0.44532419\n",
      "Iteration 53, loss = 0.44520507\n",
      "Iteration 54, loss = 0.44495658\n",
      "Iteration 55, loss = 0.44485045\n",
      "Iteration 56, loss = 0.44461176\n",
      "Iteration 57, loss = 0.44451278\n",
      "Iteration 58, loss = 0.44467440\n",
      "Iteration 59, loss = 0.44422579\n",
      "Iteration 60, loss = 0.44406685\n",
      "Iteration 61, loss = 0.44396955\n",
      "Iteration 62, loss = 0.44375128\n",
      "Iteration 63, loss = 0.44374307\n",
      "Iteration 64, loss = 0.44360201\n",
      "Iteration 65, loss = 0.44348602\n",
      "Iteration 66, loss = 0.44329784\n",
      "Iteration 67, loss = 0.44317036\n",
      "Iteration 68, loss = 0.44304527\n",
      "Iteration 69, loss = 0.44288343\n",
      "Iteration 70, loss = 0.44286259\n",
      "Iteration 71, loss = 0.44255869\n",
      "Iteration 72, loss = 0.44246207\n",
      "Iteration 73, loss = 0.44223908\n",
      "Iteration 74, loss = 0.44218932\n",
      "Iteration 75, loss = 0.44193066\n",
      "Iteration 76, loss = 0.44187541\n",
      "Iteration 77, loss = 0.44179889\n",
      "Iteration 78, loss = 0.44171200\n",
      "Iteration 79, loss = 0.44140243\n",
      "Iteration 80, loss = 0.44131036\n",
      "Iteration 81, loss = 0.44117603\n",
      "Iteration 82, loss = 0.44099335\n",
      "Iteration 83, loss = 0.44089249\n",
      "Iteration 84, loss = 0.44080386\n",
      "Iteration 85, loss = 0.44066637\n",
      "Iteration 86, loss = 0.44045407\n",
      "Iteration 87, loss = 0.44079628\n",
      "Iteration 88, loss = 0.44035768\n",
      "Iteration 89, loss = 0.44010768\n",
      "Iteration 90, loss = 0.43992851\n",
      "Iteration 91, loss = 0.43979248\n",
      "Iteration 92, loss = 0.43982276\n",
      "Iteration 93, loss = 0.43951091\n",
      "Iteration 94, loss = 0.43942954\n",
      "Iteration 95, loss = 0.43924440\n",
      "Iteration 96, loss = 0.43915751\n",
      "Iteration 97, loss = 0.43906413\n",
      "Iteration 98, loss = 0.43880458\n",
      "Iteration 99, loss = 0.43870295\n",
      "Iteration 100, loss = 0.43845478\n",
      "Iteration 101, loss = 0.43877066\n",
      "Iteration 102, loss = 0.43835535\n",
      "Iteration 103, loss = 0.43828169\n",
      "Iteration 104, loss = 0.43807898\n",
      "Iteration 105, loss = 0.43792317\n",
      "Iteration 106, loss = 0.43769519\n",
      "Iteration 107, loss = 0.43767809\n",
      "Iteration 108, loss = 0.43760198\n",
      "Iteration 109, loss = 0.43750622\n",
      "Iteration 110, loss = 0.43733996\n",
      "Iteration 111, loss = 0.43698358\n",
      "Iteration 112, loss = 0.43698888\n",
      "Iteration 113, loss = 0.43670532\n",
      "Iteration 114, loss = 0.43658303\n",
      "Iteration 115, loss = 0.43656366\n",
      "Iteration 116, loss = 0.43638372\n",
      "Iteration 117, loss = 0.43620684\n",
      "Iteration 118, loss = 0.43611030\n",
      "Iteration 119, loss = 0.43609093\n",
      "Iteration 120, loss = 0.43602216\n",
      "Iteration 121, loss = 0.43565880\n",
      "Iteration 122, loss = 0.43556530\n",
      "Iteration 123, loss = 0.43541884\n",
      "Iteration 124, loss = 0.43536628\n",
      "Iteration 125, loss = 0.43512903\n",
      "Iteration 126, loss = 0.43522229\n",
      "Iteration 127, loss = 0.43478009\n",
      "Iteration 128, loss = 0.43470622\n",
      "Iteration 129, loss = 0.43448431\n",
      "Iteration 130, loss = 0.43440538\n",
      "Iteration 131, loss = 0.43424580\n",
      "Iteration 132, loss = 0.43410286\n",
      "Iteration 133, loss = 0.43410689\n",
      "Iteration 134, loss = 0.43381356\n",
      "Iteration 135, loss = 0.43374910\n",
      "Iteration 136, loss = 0.43359445\n",
      "Iteration 137, loss = 0.43350217\n",
      "Iteration 138, loss = 0.43310787\n",
      "Iteration 139, loss = 0.43324521\n",
      "Iteration 140, loss = 0.43302707\n",
      "Iteration 141, loss = 0.43288165\n",
      "Iteration 142, loss = 0.43266535\n",
      "Iteration 143, loss = 0.43261318\n",
      "Iteration 144, loss = 0.43247530\n",
      "Iteration 145, loss = 0.43228384\n",
      "Iteration 146, loss = 0.43204937\n",
      "Iteration 147, loss = 0.43202092\n",
      "Iteration 148, loss = 0.43179755\n",
      "Iteration 149, loss = 0.43159693\n",
      "Iteration 150, loss = 0.43152496\n",
      "Iteration 151, loss = 0.43135089\n",
      "Iteration 152, loss = 0.43131747\n",
      "Iteration 153, loss = 0.43140723\n",
      "Iteration 154, loss = 0.43088123\n",
      "Iteration 155, loss = 0.43093320\n",
      "Iteration 156, loss = 0.43057101\n",
      "Iteration 157, loss = 0.43048034\n",
      "Iteration 158, loss = 0.43038591\n",
      "Iteration 159, loss = 0.43016337\n",
      "Iteration 160, loss = 0.43000459\n",
      "Iteration 161, loss = 0.42980935\n",
      "Iteration 162, loss = 0.42963310\n",
      "Iteration 163, loss = 0.42956065\n",
      "Iteration 164, loss = 0.42942475\n",
      "Iteration 165, loss = 0.42920761\n",
      "Iteration 166, loss = 0.42916893\n",
      "Iteration 167, loss = 0.42896248\n",
      "Iteration 168, loss = 0.42882020\n",
      "Iteration 169, loss = 0.42870889\n",
      "Iteration 170, loss = 0.42857218\n",
      "Iteration 171, loss = 0.42823776\n",
      "Iteration 172, loss = 0.42812462\n",
      "Iteration 173, loss = 0.42803376\n",
      "Iteration 174, loss = 0.42794704\n",
      "Iteration 175, loss = 0.42784307\n",
      "Iteration 176, loss = 0.42763458\n",
      "Iteration 177, loss = 0.42739187\n",
      "Iteration 178, loss = 0.42720017\n",
      "Iteration 179, loss = 0.42713009\n",
      "Iteration 180, loss = 0.42693461\n",
      "Iteration 181, loss = 0.42671417\n",
      "Iteration 182, loss = 0.42661776\n",
      "Iteration 183, loss = 0.42651210\n",
      "Iteration 184, loss = 0.42645375\n",
      "Iteration 185, loss = 0.42636061\n",
      "Iteration 186, loss = 0.42609408\n",
      "Iteration 187, loss = 0.42584718\n",
      "Iteration 188, loss = 0.42565780\n",
      "Iteration 189, loss = 0.42551983\n",
      "Iteration 190, loss = 0.42530550\n",
      "Iteration 191, loss = 0.42520151\n",
      "Iteration 192, loss = 0.42519500\n",
      "Iteration 193, loss = 0.42480160\n",
      "Iteration 194, loss = 0.42477938\n",
      "Iteration 195, loss = 0.42476550\n",
      "Iteration 196, loss = 0.42435930\n",
      "Iteration 197, loss = 0.42449098\n",
      "Iteration 198, loss = 0.42425100\n",
      "Iteration 199, loss = 0.42417529\n",
      "Iteration 200, loss = 0.42403744\n",
      "Iteration 1, loss = 0.75119036\n",
      "Iteration 2, loss = 0.70244548\n",
      "Iteration 3, loss = 0.65776226\n",
      "Iteration 4, loss = 0.62193710\n",
      "Iteration 5, loss = 0.59213843\n",
      "Iteration 6, loss = 0.56640384\n",
      "Iteration 7, loss = 0.54412222\n",
      "Iteration 8, loss = 0.52633545\n",
      "Iteration 9, loss = 0.50965865\n",
      "Iteration 10, loss = 0.49607285\n",
      "Iteration 11, loss = 0.48394682\n",
      "Iteration 12, loss = 0.47372542\n",
      "Iteration 13, loss = 0.46361749\n",
      "Iteration 14, loss = 0.45574916\n",
      "Iteration 15, loss = 0.44866068\n",
      "Iteration 16, loss = 0.44239822\n",
      "Iteration 17, loss = 0.43687950\n",
      "Iteration 18, loss = 0.43303207\n",
      "Iteration 19, loss = 0.42896865\n",
      "Iteration 20, loss = 0.42567933\n",
      "Iteration 21, loss = 0.42310761\n",
      "Iteration 22, loss = 0.42056068\n",
      "Iteration 23, loss = 0.41899675\n",
      "Iteration 24, loss = 0.41733731\n",
      "Iteration 25, loss = 0.41623215\n",
      "Iteration 26, loss = 0.41517294\n",
      "Iteration 27, loss = 0.41420798\n",
      "Iteration 28, loss = 0.41352678\n",
      "Iteration 29, loss = 0.41290442\n",
      "Iteration 30, loss = 0.41236020\n",
      "Iteration 31, loss = 0.41191421\n",
      "Iteration 32, loss = 0.41169502\n",
      "Iteration 33, loss = 0.41133468\n",
      "Iteration 34, loss = 0.41099500\n",
      "Iteration 35, loss = 0.41078443\n",
      "Iteration 36, loss = 0.41059346\n",
      "Iteration 37, loss = 0.41034743\n",
      "Iteration 38, loss = 0.41016852\n",
      "Iteration 39, loss = 0.40999217\n",
      "Iteration 40, loss = 0.40987374\n",
      "Iteration 41, loss = 0.40972533\n",
      "Iteration 42, loss = 0.40963890\n",
      "Iteration 43, loss = 0.40953258\n",
      "Iteration 44, loss = 0.40936160\n",
      "Iteration 45, loss = 0.40920815\n",
      "Iteration 46, loss = 0.40912359\n",
      "Iteration 47, loss = 0.40891048\n",
      "Iteration 48, loss = 0.40877274\n",
      "Iteration 49, loss = 0.40863469\n",
      "Iteration 50, loss = 0.40850314\n",
      "Iteration 51, loss = 0.40844405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.40831000\n",
      "Iteration 53, loss = 0.40809418\n",
      "Iteration 54, loss = 0.40805703\n",
      "Iteration 55, loss = 0.40789685\n",
      "Iteration 56, loss = 0.40777843\n",
      "Iteration 57, loss = 0.40773617\n",
      "Iteration 58, loss = 0.40765441\n",
      "Iteration 59, loss = 0.40734714\n",
      "Iteration 60, loss = 0.40722810\n",
      "Iteration 61, loss = 0.40731651\n",
      "Iteration 62, loss = 0.40735609\n",
      "Iteration 63, loss = 0.40705611\n",
      "Iteration 64, loss = 0.40686461\n",
      "Iteration 65, loss = 0.40658625\n",
      "Iteration 66, loss = 0.40655840\n",
      "Iteration 67, loss = 0.40634327\n",
      "Iteration 68, loss = 0.40639197\n",
      "Iteration 69, loss = 0.40611444\n",
      "Iteration 70, loss = 0.40605642\n",
      "Iteration 71, loss = 0.40588233\n",
      "Iteration 72, loss = 0.40579491\n",
      "Iteration 73, loss = 0.40562268\n",
      "Iteration 74, loss = 0.40551729\n",
      "Iteration 75, loss = 0.40540174\n",
      "Iteration 76, loss = 0.40533982\n",
      "Iteration 77, loss = 0.40512861\n",
      "Iteration 78, loss = 0.40510965\n",
      "Iteration 79, loss = 0.40484282\n",
      "Iteration 80, loss = 0.40478634\n",
      "Iteration 81, loss = 0.40468553\n",
      "Iteration 82, loss = 0.40450501\n",
      "Iteration 83, loss = 0.40452853\n",
      "Iteration 84, loss = 0.40431373\n",
      "Iteration 85, loss = 0.40412978\n",
      "Iteration 86, loss = 0.40417521\n",
      "Iteration 87, loss = 0.40388893\n",
      "Iteration 88, loss = 0.40390006\n",
      "Iteration 89, loss = 0.40361192\n",
      "Iteration 90, loss = 0.40353255\n",
      "Iteration 91, loss = 0.40348599\n",
      "Iteration 92, loss = 0.40331294\n",
      "Iteration 93, loss = 0.40311831\n",
      "Iteration 94, loss = 0.40306952\n",
      "Iteration 95, loss = 0.40281864\n",
      "Iteration 96, loss = 0.40302265\n",
      "Iteration 97, loss = 0.40259151\n",
      "Iteration 98, loss = 0.40250342\n",
      "Iteration 99, loss = 0.40233853\n",
      "Iteration 100, loss = 0.40235451\n",
      "Iteration 101, loss = 0.40209145\n",
      "Iteration 102, loss = 0.40199168\n",
      "Iteration 103, loss = 0.40177305\n",
      "Iteration 104, loss = 0.40174353\n",
      "Iteration 105, loss = 0.40152465\n",
      "Iteration 106, loss = 0.40142222\n",
      "Iteration 107, loss = 0.40131221\n",
      "Iteration 108, loss = 0.40133138\n",
      "Iteration 109, loss = 0.40101792\n",
      "Iteration 110, loss = 0.40097575\n",
      "Iteration 111, loss = 0.40077692\n",
      "Iteration 112, loss = 0.40055444\n",
      "Iteration 113, loss = 0.40053510\n",
      "Iteration 114, loss = 0.40027996\n",
      "Iteration 115, loss = 0.40020192\n",
      "Iteration 116, loss = 0.40007923\n",
      "Iteration 117, loss = 0.39986789\n",
      "Iteration 118, loss = 0.39981067\n",
      "Iteration 119, loss = 0.39965036\n",
      "Iteration 120, loss = 0.39948051\n",
      "Iteration 121, loss = 0.39946303\n",
      "Iteration 122, loss = 0.39919636\n",
      "Iteration 123, loss = 0.39916664\n",
      "Iteration 124, loss = 0.39912801\n",
      "Iteration 125, loss = 0.39892007\n",
      "Iteration 126, loss = 0.39866191\n",
      "Iteration 127, loss = 0.39851836\n",
      "Iteration 128, loss = 0.39836920\n",
      "Iteration 129, loss = 0.39821300\n",
      "Iteration 130, loss = 0.39806474\n",
      "Iteration 131, loss = 0.39807198\n",
      "Iteration 132, loss = 0.39776130\n",
      "Iteration 133, loss = 0.39765272\n",
      "Iteration 134, loss = 0.39751504\n",
      "Iteration 135, loss = 0.39731429\n",
      "Iteration 136, loss = 0.39715037\n",
      "Iteration 137, loss = 0.39714971\n",
      "Iteration 138, loss = 0.39686192\n",
      "Iteration 139, loss = 0.39690699\n",
      "Iteration 140, loss = 0.39674777\n",
      "Iteration 141, loss = 0.39648565\n",
      "Iteration 142, loss = 0.39629397\n",
      "Iteration 143, loss = 0.39620442\n",
      "Iteration 144, loss = 0.39599110\n",
      "Iteration 145, loss = 0.39612421\n",
      "Iteration 146, loss = 0.39583763\n",
      "Iteration 147, loss = 0.39556858\n",
      "Iteration 148, loss = 0.39556715\n",
      "Iteration 149, loss = 0.39532407\n",
      "Iteration 150, loss = 0.39510916\n",
      "Iteration 151, loss = 0.39509200\n",
      "Iteration 152, loss = 0.39480222\n",
      "Iteration 153, loss = 0.39467426\n",
      "Iteration 154, loss = 0.39461496\n",
      "Iteration 155, loss = 0.39479387\n",
      "Iteration 156, loss = 0.39413297\n",
      "Iteration 157, loss = 0.39399333\n",
      "Iteration 158, loss = 0.39390673\n",
      "Iteration 159, loss = 0.39370574\n",
      "Iteration 160, loss = 0.39358935\n",
      "Iteration 161, loss = 0.39334892\n",
      "Iteration 162, loss = 0.39335816\n",
      "Iteration 163, loss = 0.39310039\n",
      "Iteration 164, loss = 0.39291349\n",
      "Iteration 165, loss = 0.39288109\n",
      "Iteration 166, loss = 0.39266014\n",
      "Iteration 167, loss = 0.39251133\n",
      "Iteration 168, loss = 0.39227840\n",
      "Iteration 169, loss = 0.39224802\n",
      "Iteration 170, loss = 0.39193075\n",
      "Iteration 171, loss = 0.39172977\n",
      "Iteration 172, loss = 0.39170812\n",
      "Iteration 173, loss = 0.39159032\n",
      "Iteration 174, loss = 0.39129402\n",
      "Iteration 175, loss = 0.39105600\n",
      "Iteration 176, loss = 0.39135475\n",
      "Iteration 177, loss = 0.39093484\n",
      "Iteration 178, loss = 0.39071448\n",
      "Iteration 179, loss = 0.39058892\n",
      "Iteration 180, loss = 0.39030546\n",
      "Iteration 181, loss = 0.39031989\n",
      "Iteration 182, loss = 0.39000024\n",
      "Iteration 183, loss = 0.38995182\n",
      "Iteration 184, loss = 0.38971307\n",
      "Iteration 185, loss = 0.38963368\n",
      "Iteration 186, loss = 0.38936247\n",
      "Iteration 187, loss = 0.38935814\n",
      "Iteration 188, loss = 0.38900175\n",
      "Iteration 189, loss = 0.38882879\n",
      "Iteration 190, loss = 0.38880803\n",
      "Iteration 191, loss = 0.38841697\n",
      "Iteration 192, loss = 0.38839634\n",
      "Iteration 193, loss = 0.38818780\n",
      "Iteration 194, loss = 0.38815772\n",
      "Iteration 195, loss = 0.38802441\n",
      "Iteration 196, loss = 0.38770687\n",
      "Iteration 197, loss = 0.38738552\n",
      "Iteration 198, loss = 0.38736928\n",
      "Iteration 199, loss = 0.38716175\n",
      "Iteration 200, loss = 0.38702878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75832456\n",
      "Iteration 2, loss = 0.71171039\n",
      "Iteration 3, loss = 0.66908978\n",
      "Iteration 4, loss = 0.63545083\n",
      "Iteration 5, loss = 0.60680628\n",
      "Iteration 6, loss = 0.58225380\n",
      "Iteration 7, loss = 0.56040464\n",
      "Iteration 8, loss = 0.54365459\n",
      "Iteration 9, loss = 0.52775228\n",
      "Iteration 10, loss = 0.51479965\n",
      "Iteration 11, loss = 0.50303278\n",
      "Iteration 12, loss = 0.49322982\n",
      "Iteration 13, loss = 0.48382353\n",
      "Iteration 14, loss = 0.47646565\n",
      "Iteration 15, loss = 0.46965282\n",
      "Iteration 16, loss = 0.46390666\n",
      "Iteration 17, loss = 0.45887594\n",
      "Iteration 18, loss = 0.45518417\n",
      "Iteration 19, loss = 0.45147907\n",
      "Iteration 20, loss = 0.44831786\n",
      "Iteration 21, loss = 0.44584141\n",
      "Iteration 22, loss = 0.44361769\n",
      "Iteration 23, loss = 0.44229591\n",
      "Iteration 24, loss = 0.44069308\n",
      "Iteration 25, loss = 0.43972276\n",
      "Iteration 26, loss = 0.43856770\n",
      "Iteration 27, loss = 0.43787131\n",
      "Iteration 28, loss = 0.43722117\n",
      "Iteration 29, loss = 0.43645116\n",
      "Iteration 30, loss = 0.43609639\n",
      "Iteration 31, loss = 0.43551459\n",
      "Iteration 32, loss = 0.43535283\n",
      "Iteration 33, loss = 0.43495016\n",
      "Iteration 34, loss = 0.43457925\n",
      "Iteration 35, loss = 0.43431085\n",
      "Iteration 36, loss = 0.43416272\n",
      "Iteration 37, loss = 0.43385617\n",
      "Iteration 38, loss = 0.43367446\n",
      "Iteration 39, loss = 0.43348782\n",
      "Iteration 40, loss = 0.43342159\n",
      "Iteration 41, loss = 0.43312504\n",
      "Iteration 42, loss = 0.43304791\n",
      "Iteration 43, loss = 0.43291141\n",
      "Iteration 44, loss = 0.43278707\n",
      "Iteration 45, loss = 0.43255913\n",
      "Iteration 46, loss = 0.43244030\n",
      "Iteration 47, loss = 0.43227622\n",
      "Iteration 48, loss = 0.43218995\n",
      "Iteration 49, loss = 0.43208251\n",
      "Iteration 50, loss = 0.43196295\n",
      "Iteration 51, loss = 0.43182412\n",
      "Iteration 52, loss = 0.43164683\n",
      "Iteration 53, loss = 0.43154447\n",
      "Iteration 54, loss = 0.43144968\n",
      "Iteration 55, loss = 0.43128637\n",
      "Iteration 56, loss = 0.43119653\n",
      "Iteration 57, loss = 0.43105918\n",
      "Iteration 58, loss = 0.43095178\n",
      "Iteration 59, loss = 0.43080071\n",
      "Iteration 60, loss = 0.43073039\n",
      "Iteration 61, loss = 0.43068113\n",
      "Iteration 62, loss = 0.43050218\n",
      "Iteration 63, loss = 0.43032608\n",
      "Iteration 64, loss = 0.43028896\n",
      "Iteration 65, loss = 0.43007265\n",
      "Iteration 66, loss = 0.43004319\n",
      "Iteration 67, loss = 0.42980009\n",
      "Iteration 68, loss = 0.42986655\n",
      "Iteration 69, loss = 0.42971458\n",
      "Iteration 70, loss = 0.42953243\n",
      "Iteration 71, loss = 0.42937826\n",
      "Iteration 72, loss = 0.42919914\n",
      "Iteration 73, loss = 0.42921312\n",
      "Iteration 74, loss = 0.42908183\n",
      "Iteration 75, loss = 0.42889643\n",
      "Iteration 76, loss = 0.42881326\n",
      "Iteration 77, loss = 0.42864137\n",
      "Iteration 78, loss = 0.42853673\n",
      "Iteration 79, loss = 0.42835868\n",
      "Iteration 80, loss = 0.42825301\n",
      "Iteration 81, loss = 0.42835717\n",
      "Iteration 82, loss = 0.42801414\n",
      "Iteration 83, loss = 0.42818806\n",
      "Iteration 84, loss = 0.42783339\n",
      "Iteration 85, loss = 0.42761297\n",
      "Iteration 86, loss = 0.42751908\n",
      "Iteration 87, loss = 0.42744036\n",
      "Iteration 88, loss = 0.42735742\n",
      "Iteration 89, loss = 0.42716826\n",
      "Iteration 90, loss = 0.42706531\n",
      "Iteration 91, loss = 0.42704405\n",
      "Iteration 92, loss = 0.42680241\n",
      "Iteration 93, loss = 0.42663561\n",
      "Iteration 94, loss = 0.42687294\n",
      "Iteration 95, loss = 0.42650003\n",
      "Iteration 96, loss = 0.42638021\n",
      "Iteration 97, loss = 0.42625089\n",
      "Iteration 98, loss = 0.42628804\n",
      "Iteration 99, loss = 0.42598435\n",
      "Iteration 100, loss = 0.42584507\n",
      "Iteration 101, loss = 0.42579035\n",
      "Iteration 102, loss = 0.42563571\n",
      "Iteration 103, loss = 0.42553876\n",
      "Iteration 104, loss = 0.42538356\n",
      "Iteration 105, loss = 0.42541670\n",
      "Iteration 106, loss = 0.42510380\n",
      "Iteration 107, loss = 0.42494520\n",
      "Iteration 108, loss = 0.42486613\n",
      "Iteration 109, loss = 0.42474291\n",
      "Iteration 110, loss = 0.42463552\n",
      "Iteration 111, loss = 0.42454258\n",
      "Iteration 112, loss = 0.42428126\n",
      "Iteration 113, loss = 0.42429561\n",
      "Iteration 114, loss = 0.42405354\n",
      "Iteration 115, loss = 0.42397731\n",
      "Iteration 116, loss = 0.42400499\n",
      "Iteration 117, loss = 0.42372603\n",
      "Iteration 118, loss = 0.42362567\n",
      "Iteration 119, loss = 0.42352972\n",
      "Iteration 120, loss = 0.42329969\n",
      "Iteration 121, loss = 0.42332145\n",
      "Iteration 122, loss = 0.42311123\n",
      "Iteration 123, loss = 0.42295521\n",
      "Iteration 124, loss = 0.42293622\n",
      "Iteration 125, loss = 0.42268475\n",
      "Iteration 126, loss = 0.42260736\n",
      "Iteration 127, loss = 0.42239074\n",
      "Iteration 128, loss = 0.42218532\n",
      "Iteration 129, loss = 0.42227088\n",
      "Iteration 130, loss = 0.42204903\n",
      "Iteration 131, loss = 0.42192339\n",
      "Iteration 132, loss = 0.42171876\n",
      "Iteration 133, loss = 0.42168535\n",
      "Iteration 134, loss = 0.42148389\n",
      "Iteration 135, loss = 0.42137394\n",
      "Iteration 136, loss = 0.42116455\n",
      "Iteration 137, loss = 0.42112799\n",
      "Iteration 138, loss = 0.42088920\n",
      "Iteration 139, loss = 0.42118739\n",
      "Iteration 140, loss = 0.42071229\n",
      "Iteration 141, loss = 0.42074136\n",
      "Iteration 142, loss = 0.42037979\n",
      "Iteration 143, loss = 0.42024110\n",
      "Iteration 144, loss = 0.42013896\n",
      "Iteration 145, loss = 0.42003779\n",
      "Iteration 146, loss = 0.41997541\n",
      "Iteration 147, loss = 0.41976172\n",
      "Iteration 148, loss = 0.41976130\n",
      "Iteration 149, loss = 0.41945519\n",
      "Iteration 150, loss = 0.41921402\n",
      "Iteration 151, loss = 0.41923311\n",
      "Iteration 152, loss = 0.41902584\n",
      "Iteration 153, loss = 0.41879022\n",
      "Iteration 154, loss = 0.41890437\n",
      "Iteration 155, loss = 0.41906926\n",
      "Iteration 156, loss = 0.41849188\n",
      "Iteration 157, loss = 0.41827165\n",
      "Iteration 158, loss = 0.41823477\n",
      "Iteration 159, loss = 0.41828071\n",
      "Iteration 160, loss = 0.41783146\n",
      "Iteration 161, loss = 0.41772492\n",
      "Iteration 162, loss = 0.41772731\n",
      "Iteration 163, loss = 0.41751204\n",
      "Iteration 164, loss = 0.41724267\n",
      "Iteration 165, loss = 0.41717906\n",
      "Iteration 166, loss = 0.41706908\n",
      "Iteration 167, loss = 0.41696108\n",
      "Iteration 168, loss = 0.41668694\n",
      "Iteration 169, loss = 0.41655511\n",
      "Iteration 170, loss = 0.41645120\n",
      "Iteration 171, loss = 0.41639993\n",
      "Iteration 172, loss = 0.41612656\n",
      "Iteration 173, loss = 0.41610258\n",
      "Iteration 174, loss = 0.41588099\n",
      "Iteration 175, loss = 0.41579163\n",
      "Iteration 176, loss = 0.41573125\n",
      "Iteration 177, loss = 0.41543041\n",
      "Iteration 178, loss = 0.41536297\n",
      "Iteration 179, loss = 0.41509700\n",
      "Iteration 180, loss = 0.41501808\n",
      "Iteration 181, loss = 0.41491652\n",
      "Iteration 182, loss = 0.41455405\n",
      "Iteration 183, loss = 0.41444035\n",
      "Iteration 184, loss = 0.41433950\n",
      "Iteration 185, loss = 0.41421429\n",
      "Iteration 186, loss = 0.41399110\n",
      "Iteration 187, loss = 0.41418659\n",
      "Iteration 188, loss = 0.41380807\n",
      "Iteration 189, loss = 0.41361941\n",
      "Iteration 190, loss = 0.41345939\n",
      "Iteration 191, loss = 0.41320752\n",
      "Iteration 192, loss = 0.41316211\n",
      "Iteration 193, loss = 0.41294116\n",
      "Iteration 194, loss = 0.41283627\n",
      "Iteration 195, loss = 0.41281803\n",
      "Iteration 196, loss = 0.41252444\n",
      "Iteration 197, loss = 0.41227496\n",
      "Iteration 198, loss = 0.41215980\n",
      "Iteration 199, loss = 0.41202066\n",
      "Iteration 200, loss = 0.41187958\n",
      "Iteration 1, loss = 0.75925580\n",
      "Iteration 2, loss = 0.71274133\n",
      "Iteration 3, loss = 0.67284115\n",
      "Iteration 4, loss = 0.63924087\n",
      "Iteration 5, loss = 0.61040600\n",
      "Iteration 6, loss = 0.58464224\n",
      "Iteration 7, loss = 0.56405912\n",
      "Iteration 8, loss = 0.54508177\n",
      "Iteration 9, loss = 0.52817072\n",
      "Iteration 10, loss = 0.51336129\n",
      "Iteration 11, loss = 0.50062726\n",
      "Iteration 12, loss = 0.48909053\n",
      "Iteration 13, loss = 0.47921894\n",
      "Iteration 14, loss = 0.47016732\n",
      "Iteration 15, loss = 0.46260794\n",
      "Iteration 16, loss = 0.45634034\n",
      "Iteration 17, loss = 0.45074090\n",
      "Iteration 18, loss = 0.44650673\n",
      "Iteration 19, loss = 0.44231222\n",
      "Iteration 20, loss = 0.43847968\n",
      "Iteration 21, loss = 0.43584833\n",
      "Iteration 22, loss = 0.43333389\n",
      "Iteration 23, loss = 0.43114110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.42953874\n",
      "Iteration 25, loss = 0.42815390\n",
      "Iteration 26, loss = 0.42688468\n",
      "Iteration 27, loss = 0.42572668\n",
      "Iteration 28, loss = 0.42504230\n",
      "Iteration 29, loss = 0.42410608\n",
      "Iteration 30, loss = 0.42355713\n",
      "Iteration 31, loss = 0.42308607\n",
      "Iteration 32, loss = 0.42256403\n",
      "Iteration 33, loss = 0.42241425\n",
      "Iteration 34, loss = 0.42177924\n",
      "Iteration 35, loss = 0.42156506\n",
      "Iteration 36, loss = 0.42114533\n",
      "Iteration 37, loss = 0.42099101\n",
      "Iteration 38, loss = 0.42066656\n",
      "Iteration 39, loss = 0.42032083\n",
      "Iteration 40, loss = 0.42022289\n",
      "Iteration 41, loss = 0.41994678\n",
      "Iteration 42, loss = 0.41979640\n",
      "Iteration 43, loss = 0.41946204\n",
      "Iteration 44, loss = 0.41949628\n",
      "Iteration 45, loss = 0.41914328\n",
      "Iteration 46, loss = 0.41892023\n",
      "Iteration 47, loss = 0.41871969\n",
      "Iteration 48, loss = 0.41864402\n",
      "Iteration 49, loss = 0.41841531\n",
      "Iteration 50, loss = 0.41825405\n",
      "Iteration 51, loss = 0.41812041\n",
      "Iteration 52, loss = 0.41801237\n",
      "Iteration 53, loss = 0.41775094\n",
      "Iteration 54, loss = 0.41756304\n",
      "Iteration 55, loss = 0.41741925\n",
      "Iteration 56, loss = 0.41736088\n",
      "Iteration 57, loss = 0.41725878\n",
      "Iteration 58, loss = 0.41704912\n",
      "Iteration 59, loss = 0.41682713\n",
      "Iteration 60, loss = 0.41672103\n",
      "Iteration 61, loss = 0.41656511\n",
      "Iteration 62, loss = 0.41643321\n",
      "Iteration 63, loss = 0.41626486\n",
      "Iteration 64, loss = 0.41618180\n",
      "Iteration 65, loss = 0.41597012\n",
      "Iteration 66, loss = 0.41595250\n",
      "Iteration 67, loss = 0.41587818\n",
      "Iteration 68, loss = 0.41562745\n",
      "Iteration 69, loss = 0.41540926\n",
      "Iteration 70, loss = 0.41543148\n",
      "Iteration 71, loss = 0.41515984\n",
      "Iteration 72, loss = 0.41504211\n",
      "Iteration 73, loss = 0.41484086\n",
      "Iteration 74, loss = 0.41474301\n",
      "Iteration 75, loss = 0.41455225\n",
      "Iteration 76, loss = 0.41459528\n",
      "Iteration 77, loss = 0.41422415\n",
      "Iteration 78, loss = 0.41410077\n",
      "Iteration 79, loss = 0.41397740\n",
      "Iteration 80, loss = 0.41392502\n",
      "Iteration 81, loss = 0.41376251\n",
      "Iteration 82, loss = 0.41363789\n",
      "Iteration 83, loss = 0.41345589\n",
      "Iteration 84, loss = 0.41331969\n",
      "Iteration 85, loss = 0.41332718\n",
      "Iteration 86, loss = 0.41316897\n",
      "Iteration 87, loss = 0.41309114\n",
      "Iteration 88, loss = 0.41275448\n",
      "Iteration 89, loss = 0.41263294\n",
      "Iteration 90, loss = 0.41261538\n",
      "Iteration 91, loss = 0.41233817\n",
      "Iteration 92, loss = 0.41254134\n",
      "Iteration 93, loss = 0.41211903\n",
      "Iteration 94, loss = 0.41239059\n",
      "Iteration 95, loss = 0.41180218\n",
      "Iteration 96, loss = 0.41160738\n",
      "Iteration 97, loss = 0.41148594\n",
      "Iteration 98, loss = 0.41128848\n",
      "Iteration 99, loss = 0.41119934\n",
      "Iteration 100, loss = 0.41101113\n",
      "Iteration 101, loss = 0.41120389\n",
      "Iteration 102, loss = 0.41080213\n",
      "Iteration 103, loss = 0.41062142\n",
      "Iteration 104, loss = 0.41060648\n",
      "Iteration 105, loss = 0.41032792\n",
      "Iteration 106, loss = 0.41031351\n",
      "Iteration 107, loss = 0.41009344\n",
      "Iteration 108, loss = 0.40992550\n",
      "Iteration 109, loss = 0.40977983\n",
      "Iteration 110, loss = 0.40958057\n",
      "Iteration 111, loss = 0.40948071\n",
      "Iteration 112, loss = 0.40940772\n",
      "Iteration 113, loss = 0.40913886\n",
      "Iteration 114, loss = 0.40906841\n",
      "Iteration 115, loss = 0.40886924\n",
      "Iteration 116, loss = 0.40862454\n",
      "Iteration 117, loss = 0.40849465\n",
      "Iteration 118, loss = 0.40838730\n",
      "Iteration 119, loss = 0.40839855\n",
      "Iteration 120, loss = 0.40823567\n",
      "Iteration 121, loss = 0.40798647\n",
      "Iteration 122, loss = 0.40783871\n",
      "Iteration 123, loss = 0.40759267\n",
      "Iteration 124, loss = 0.40757465\n",
      "Iteration 125, loss = 0.40744161\n",
      "Iteration 126, loss = 0.40716898\n",
      "Iteration 127, loss = 0.40696921\n",
      "Iteration 128, loss = 0.40687113\n",
      "Iteration 129, loss = 0.40677540\n",
      "Iteration 130, loss = 0.40659737\n",
      "Iteration 131, loss = 0.40636099\n",
      "Iteration 132, loss = 0.40627231\n",
      "Iteration 133, loss = 0.40610151\n",
      "Iteration 134, loss = 0.40594270\n",
      "Iteration 135, loss = 0.40566150\n",
      "Iteration 136, loss = 0.40590785\n",
      "Iteration 137, loss = 0.40534965\n",
      "Iteration 138, loss = 0.40523999\n",
      "Iteration 139, loss = 0.40523534\n",
      "Iteration 140, loss = 0.40492602\n",
      "Iteration 141, loss = 0.40477904\n",
      "Iteration 142, loss = 0.40456934\n",
      "Iteration 143, loss = 0.40442457\n",
      "Iteration 144, loss = 0.40436259\n",
      "Iteration 145, loss = 0.40421737\n",
      "Iteration 146, loss = 0.40398994\n",
      "Iteration 147, loss = 0.40383132\n",
      "Iteration 148, loss = 0.40373285\n",
      "Iteration 149, loss = 0.40338958\n",
      "Iteration 150, loss = 0.40321517\n",
      "Iteration 151, loss = 0.40308185\n",
      "Iteration 152, loss = 0.40296181\n",
      "Iteration 153, loss = 0.40330520\n",
      "Iteration 154, loss = 0.40271075\n",
      "Iteration 155, loss = 0.40226210\n",
      "Iteration 156, loss = 0.40230115\n",
      "Iteration 157, loss = 0.40207715\n",
      "Iteration 158, loss = 0.40187453\n",
      "Iteration 159, loss = 0.40170969\n",
      "Iteration 160, loss = 0.40162651\n",
      "Iteration 161, loss = 0.40141410\n",
      "Iteration 162, loss = 0.40128643\n",
      "Iteration 163, loss = 0.40102697\n",
      "Iteration 164, loss = 0.40099237\n",
      "Iteration 165, loss = 0.40074496\n",
      "Iteration 166, loss = 0.40053050\n",
      "Iteration 167, loss = 0.40022328\n",
      "Iteration 168, loss = 0.40011570\n",
      "Iteration 169, loss = 0.39992013\n",
      "Iteration 170, loss = 0.39989971\n",
      "Iteration 171, loss = 0.39959231\n",
      "Iteration 172, loss = 0.39959776\n",
      "Iteration 173, loss = 0.39937254\n",
      "Iteration 174, loss = 0.39916508\n",
      "Iteration 175, loss = 0.39891643\n",
      "Iteration 176, loss = 0.39876338\n",
      "Iteration 177, loss = 0.39845717\n",
      "Iteration 178, loss = 0.39852260\n",
      "Iteration 179, loss = 0.39816377\n",
      "Iteration 180, loss = 0.39793326\n",
      "Iteration 181, loss = 0.39770335\n",
      "Iteration 182, loss = 0.39757392\n",
      "Iteration 183, loss = 0.39727938\n",
      "Iteration 184, loss = 0.39717359\n",
      "Iteration 185, loss = 0.39721291\n",
      "Iteration 186, loss = 0.39690293\n",
      "Iteration 187, loss = 0.39672700\n",
      "Iteration 188, loss = 0.39650343\n",
      "Iteration 189, loss = 0.39633308\n",
      "Iteration 190, loss = 0.39604221\n",
      "Iteration 191, loss = 0.39592522\n",
      "Iteration 192, loss = 0.39573477\n",
      "Iteration 193, loss = 0.39541271\n",
      "Iteration 194, loss = 0.39546959\n",
      "Iteration 195, loss = 0.39526025\n",
      "Iteration 196, loss = 0.39495133\n",
      "Iteration 197, loss = 0.39502069\n",
      "Iteration 198, loss = 0.39462146\n",
      "Iteration 199, loss = 0.39444036\n",
      "Iteration 200, loss = 0.39405456\n",
      "Iteration 1, loss = 0.76162014\n",
      "Iteration 2, loss = 0.71395006\n",
      "Iteration 3, loss = 0.67510773\n",
      "Iteration 4, loss = 0.64121698\n",
      "Iteration 5, loss = 0.61162532\n",
      "Iteration 6, loss = 0.58647122\n",
      "Iteration 7, loss = 0.56515843\n",
      "Iteration 8, loss = 0.54568044\n",
      "Iteration 9, loss = 0.52848800\n",
      "Iteration 10, loss = 0.51329764\n",
      "Iteration 11, loss = 0.50043738\n",
      "Iteration 12, loss = 0.48863707\n",
      "Iteration 13, loss = 0.47855389\n",
      "Iteration 14, loss = 0.46917606\n",
      "Iteration 15, loss = 0.46158790\n",
      "Iteration 16, loss = 0.45486821\n",
      "Iteration 17, loss = 0.44929371\n",
      "Iteration 18, loss = 0.44473472\n",
      "Iteration 19, loss = 0.44000009\n",
      "Iteration 20, loss = 0.43660628\n",
      "Iteration 21, loss = 0.43344395\n",
      "Iteration 22, loss = 0.43085293\n",
      "Iteration 23, loss = 0.42841577\n",
      "Iteration 24, loss = 0.42659647\n",
      "Iteration 25, loss = 0.42514503\n",
      "Iteration 26, loss = 0.42367574\n",
      "Iteration 27, loss = 0.42251075\n",
      "Iteration 28, loss = 0.42168396\n",
      "Iteration 29, loss = 0.42080892\n",
      "Iteration 30, loss = 0.42002364\n",
      "Iteration 31, loss = 0.41961428\n",
      "Iteration 32, loss = 0.41913147\n",
      "Iteration 33, loss = 0.41887045\n",
      "Iteration 34, loss = 0.41824601\n",
      "Iteration 35, loss = 0.41784690\n",
      "Iteration 36, loss = 0.41772247\n",
      "Iteration 37, loss = 0.41740640\n",
      "Iteration 38, loss = 0.41705388\n",
      "Iteration 39, loss = 0.41684399\n",
      "Iteration 40, loss = 0.41677747\n",
      "Iteration 41, loss = 0.41648069\n",
      "Iteration 42, loss = 0.41634041\n",
      "Iteration 43, loss = 0.41613864\n",
      "Iteration 44, loss = 0.41607032\n",
      "Iteration 45, loss = 0.41577224\n",
      "Iteration 46, loss = 0.41562564\n",
      "Iteration 47, loss = 0.41544361\n",
      "Iteration 48, loss = 0.41534689\n",
      "Iteration 49, loss = 0.41520264\n",
      "Iteration 50, loss = 0.41501851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51, loss = 0.41490496\n",
      "Iteration 52, loss = 0.41483717\n",
      "Iteration 53, loss = 0.41463435\n",
      "Iteration 54, loss = 0.41448395\n",
      "Iteration 55, loss = 0.41435733\n",
      "Iteration 56, loss = 0.41422265\n",
      "Iteration 57, loss = 0.41415416\n",
      "Iteration 58, loss = 0.41409455\n",
      "Iteration 59, loss = 0.41385033\n",
      "Iteration 60, loss = 0.41369608\n",
      "Iteration 61, loss = 0.41364965\n",
      "Iteration 62, loss = 0.41348600\n",
      "Iteration 63, loss = 0.41344461\n",
      "Iteration 64, loss = 0.41339266\n",
      "Iteration 65, loss = 0.41318384\n",
      "Iteration 66, loss = 0.41306960\n",
      "Iteration 67, loss = 0.41291770\n",
      "Iteration 68, loss = 0.41274353\n",
      "Iteration 69, loss = 0.41262646\n",
      "Iteration 70, loss = 0.41260336\n",
      "Iteration 71, loss = 0.41241218\n",
      "Iteration 72, loss = 0.41228095\n",
      "Iteration 73, loss = 0.41206310\n",
      "Iteration 74, loss = 0.41204438\n",
      "Iteration 75, loss = 0.41187849\n",
      "Iteration 76, loss = 0.41178018\n",
      "Iteration 77, loss = 0.41165065\n",
      "Iteration 78, loss = 0.41148491\n",
      "Iteration 79, loss = 0.41132729\n",
      "Iteration 80, loss = 0.41124716\n",
      "Iteration 81, loss = 0.41110591\n",
      "Iteration 82, loss = 0.41097227\n",
      "Iteration 83, loss = 0.41082922\n",
      "Iteration 84, loss = 0.41074306\n",
      "Iteration 85, loss = 0.41073770\n",
      "Iteration 86, loss = 0.41043923\n",
      "Iteration 87, loss = 0.41053682\n",
      "Iteration 88, loss = 0.41031485\n",
      "Iteration 89, loss = 0.41018276\n",
      "Iteration 90, loss = 0.41013977\n",
      "Iteration 91, loss = 0.40986338\n",
      "Iteration 92, loss = 0.40999788\n",
      "Iteration 93, loss = 0.40968275\n",
      "Iteration 94, loss = 0.40962614\n",
      "Iteration 95, loss = 0.40935542\n",
      "Iteration 96, loss = 0.40924826\n",
      "Iteration 97, loss = 0.40924337\n",
      "Iteration 98, loss = 0.40899168\n",
      "Iteration 99, loss = 0.40893600\n",
      "Iteration 100, loss = 0.40864166\n",
      "Iteration 101, loss = 0.40885910\n",
      "Iteration 102, loss = 0.40859987\n",
      "Iteration 103, loss = 0.40836482\n",
      "Iteration 104, loss = 0.40834533\n",
      "Iteration 105, loss = 0.40805694\n",
      "Iteration 106, loss = 0.40798833\n",
      "Iteration 107, loss = 0.40789748\n",
      "Iteration 108, loss = 0.40785903\n",
      "Iteration 109, loss = 0.40772875\n",
      "Iteration 110, loss = 0.40751156\n",
      "Iteration 111, loss = 0.40726331\n",
      "Iteration 112, loss = 0.40721151\n",
      "Iteration 113, loss = 0.40705374\n",
      "Iteration 114, loss = 0.40697341\n",
      "Iteration 115, loss = 0.40682594\n",
      "Iteration 116, loss = 0.40661627\n",
      "Iteration 117, loss = 0.40649390\n",
      "Iteration 118, loss = 0.40645542\n",
      "Iteration 119, loss = 0.40641639\n",
      "Iteration 120, loss = 0.40618601\n",
      "Iteration 121, loss = 0.40608514\n",
      "Iteration 122, loss = 0.40592832\n",
      "Iteration 123, loss = 0.40567719\n",
      "Iteration 124, loss = 0.40563873\n",
      "Iteration 125, loss = 0.40553267\n",
      "Iteration 126, loss = 0.40535461\n",
      "Iteration 127, loss = 0.40523251\n",
      "Iteration 128, loss = 0.40503912\n",
      "Iteration 129, loss = 0.40492429\n",
      "Iteration 130, loss = 0.40485407\n",
      "Iteration 131, loss = 0.40457422\n",
      "Iteration 132, loss = 0.40453176\n",
      "Iteration 133, loss = 0.40441106\n",
      "Iteration 134, loss = 0.40423741\n",
      "Iteration 135, loss = 0.40404837\n",
      "Iteration 136, loss = 0.40398508\n",
      "Iteration 137, loss = 0.40407400\n",
      "Iteration 138, loss = 0.40377217\n",
      "Iteration 139, loss = 0.40363388\n",
      "Iteration 140, loss = 0.40343406\n",
      "Iteration 141, loss = 0.40330334\n",
      "Iteration 142, loss = 0.40304042\n",
      "Iteration 143, loss = 0.40284689\n",
      "Iteration 144, loss = 0.40285986\n",
      "Iteration 145, loss = 0.40274835\n",
      "Iteration 146, loss = 0.40266281\n",
      "Iteration 147, loss = 0.40233181\n",
      "Iteration 148, loss = 0.40228323\n",
      "Iteration 149, loss = 0.40201160\n",
      "Iteration 150, loss = 0.40186503\n",
      "Iteration 151, loss = 0.40171049\n",
      "Iteration 152, loss = 0.40175858\n",
      "Iteration 153, loss = 0.40185728\n",
      "Iteration 154, loss = 0.40129325\n",
      "Iteration 155, loss = 0.40110511\n",
      "Iteration 156, loss = 0.40104225\n",
      "Iteration 157, loss = 0.40080889\n",
      "Iteration 158, loss = 0.40066116\n",
      "Iteration 159, loss = 0.40049427\n",
      "Iteration 160, loss = 0.40042652\n",
      "Iteration 161, loss = 0.40029959\n",
      "Iteration 162, loss = 0.40017089\n",
      "Iteration 163, loss = 0.39996527\n",
      "Iteration 164, loss = 0.39979336\n",
      "Iteration 165, loss = 0.39955392\n",
      "Iteration 166, loss = 0.39946992\n",
      "Iteration 167, loss = 0.39922656\n",
      "Iteration 168, loss = 0.39924487\n",
      "Iteration 169, loss = 0.39899522\n",
      "Iteration 170, loss = 0.39889903\n",
      "Iteration 171, loss = 0.39859018\n",
      "Iteration 172, loss = 0.39850296\n",
      "Iteration 173, loss = 0.39852792\n",
      "Iteration 174, loss = 0.39821081\n",
      "Iteration 175, loss = 0.39813231\n",
      "Iteration 176, loss = 0.39785049\n",
      "Iteration 177, loss = 0.39769413\n",
      "Iteration 178, loss = 0.39756950\n",
      "Iteration 179, loss = 0.39740728\n",
      "Iteration 180, loss = 0.39712439\n",
      "Iteration 181, loss = 0.39696699\n",
      "Iteration 182, loss = 0.39681519\n",
      "Iteration 183, loss = 0.39657970\n",
      "Iteration 184, loss = 0.39642274\n",
      "Iteration 185, loss = 0.39636589\n",
      "Iteration 186, loss = 0.39626040\n",
      "Iteration 187, loss = 0.39591510\n",
      "Iteration 188, loss = 0.39584586\n",
      "Iteration 189, loss = 0.39569323\n",
      "Iteration 190, loss = 0.39553429\n",
      "Iteration 191, loss = 0.39522100\n",
      "Iteration 192, loss = 0.39529325\n",
      "Iteration 193, loss = 0.39479157\n",
      "Iteration 194, loss = 0.39480780\n",
      "Iteration 195, loss = 0.39478172\n",
      "Iteration 196, loss = 0.39431907\n",
      "Iteration 197, loss = 0.39425127\n",
      "Iteration 198, loss = 0.39410598\n",
      "Iteration 199, loss = 0.39399821\n",
      "Iteration 200, loss = 0.39359271\n",
      "Iteration 1, loss = 0.75442427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.70803020\n",
      "Iteration 3, loss = 0.66862049\n",
      "Iteration 4, loss = 0.63582109\n",
      "Iteration 5, loss = 0.60755083\n",
      "Iteration 6, loss = 0.58374915\n",
      "Iteration 7, loss = 0.56431470\n",
      "Iteration 8, loss = 0.54643150\n",
      "Iteration 9, loss = 0.53126811\n",
      "Iteration 10, loss = 0.51786176\n",
      "Iteration 11, loss = 0.50649546\n",
      "Iteration 12, loss = 0.49679102\n",
      "Iteration 13, loss = 0.48823918\n",
      "Iteration 14, loss = 0.48031866\n",
      "Iteration 15, loss = 0.47425185\n",
      "Iteration 16, loss = 0.46874799\n",
      "Iteration 17, loss = 0.46482924\n",
      "Iteration 18, loss = 0.46079837\n",
      "Iteration 19, loss = 0.45728654\n",
      "Iteration 20, loss = 0.45507408\n",
      "Iteration 21, loss = 0.45291126\n",
      "Iteration 22, loss = 0.45088138\n",
      "Iteration 23, loss = 0.44940744\n",
      "Iteration 24, loss = 0.44822869\n",
      "Iteration 25, loss = 0.44722240\n",
      "Iteration 26, loss = 0.44629803\n",
      "Iteration 27, loss = 0.44561259\n",
      "Iteration 28, loss = 0.44518183\n",
      "Iteration 29, loss = 0.44458657\n",
      "Iteration 30, loss = 0.44410195\n",
      "Iteration 31, loss = 0.44380084\n",
      "Iteration 32, loss = 0.44352698\n",
      "Iteration 33, loss = 0.44329762\n",
      "Iteration 34, loss = 0.44295503\n",
      "Iteration 35, loss = 0.44264666\n",
      "Iteration 36, loss = 0.44256458\n",
      "Iteration 37, loss = 0.44233219\n",
      "Iteration 38, loss = 0.44205944\n",
      "Iteration 39, loss = 0.44184466\n",
      "Iteration 40, loss = 0.44179700\n",
      "Iteration 41, loss = 0.44154258\n",
      "Iteration 42, loss = 0.44139890\n",
      "Iteration 43, loss = 0.44118367\n",
      "Iteration 44, loss = 0.44107569\n",
      "Iteration 45, loss = 0.44086260\n",
      "Iteration 46, loss = 0.44071880\n",
      "Iteration 47, loss = 0.44049257\n",
      "Iteration 48, loss = 0.44037084\n",
      "Iteration 49, loss = 0.44025909\n",
      "Iteration 50, loss = 0.44018186\n",
      "Iteration 51, loss = 0.43993274\n",
      "Iteration 52, loss = 0.43980323\n",
      "Iteration 53, loss = 0.43968179\n",
      "Iteration 54, loss = 0.43943121\n",
      "Iteration 55, loss = 0.43932192\n",
      "Iteration 56, loss = 0.43907868\n",
      "Iteration 57, loss = 0.43897596\n",
      "Iteration 58, loss = 0.43913512\n",
      "Iteration 59, loss = 0.43867992\n",
      "Iteration 60, loss = 0.43851774\n",
      "Iteration 61, loss = 0.43841581\n",
      "Iteration 62, loss = 0.43819191\n",
      "Iteration 63, loss = 0.43817790\n",
      "Iteration 64, loss = 0.43803119\n",
      "Iteration 65, loss = 0.43790882\n",
      "Iteration 66, loss = 0.43771400\n",
      "Iteration 67, loss = 0.43758214\n",
      "Iteration 68, loss = 0.43744820\n",
      "Iteration 69, loss = 0.43727883\n",
      "Iteration 70, loss = 0.43725240\n",
      "Iteration 71, loss = 0.43693794\n",
      "Iteration 72, loss = 0.43683248\n",
      "Iteration 73, loss = 0.43660157\n",
      "Iteration 74, loss = 0.43654354\n",
      "Iteration 75, loss = 0.43627440\n",
      "Iteration 76, loss = 0.43621073\n",
      "Iteration 77, loss = 0.43612404\n",
      "Iteration 78, loss = 0.43602670\n",
      "Iteration 79, loss = 0.43570727\n",
      "Iteration 80, loss = 0.43560422\n",
      "Iteration 81, loss = 0.43545866\n",
      "Iteration 82, loss = 0.43526386\n",
      "Iteration 83, loss = 0.43515149\n",
      "Iteration 84, loss = 0.43505209\n",
      "Iteration 85, loss = 0.43490346\n",
      "Iteration 86, loss = 0.43467696\n",
      "Iteration 87, loss = 0.43501318\n",
      "Iteration 88, loss = 0.43455529\n",
      "Iteration 89, loss = 0.43429149\n",
      "Iteration 90, loss = 0.43409804\n",
      "Iteration 91, loss = 0.43394665\n",
      "Iteration 92, loss = 0.43396080\n",
      "Iteration 93, loss = 0.43363639\n",
      "Iteration 94, loss = 0.43354060\n",
      "Iteration 95, loss = 0.43333802\n",
      "Iteration 96, loss = 0.43323735\n",
      "Iteration 97, loss = 0.43313029\n",
      "Iteration 98, loss = 0.43285165\n",
      "Iteration 99, loss = 0.43273329\n",
      "Iteration 100, loss = 0.43246640\n",
      "Iteration 101, loss = 0.43277102\n",
      "Iteration 102, loss = 0.43233421\n",
      "Iteration 103, loss = 0.43224102\n",
      "Iteration 104, loss = 0.43201897\n",
      "Iteration 105, loss = 0.43184732\n",
      "Iteration 106, loss = 0.43159943\n",
      "Iteration 107, loss = 0.43156382\n",
      "Iteration 108, loss = 0.43146918\n",
      "Iteration 109, loss = 0.43135155\n",
      "Iteration 110, loss = 0.43116562\n",
      "Iteration 111, loss = 0.43078771\n",
      "Iteration 112, loss = 0.43077023\n",
      "Iteration 113, loss = 0.43046701\n",
      "Iteration 114, loss = 0.43032309\n",
      "Iteration 115, loss = 0.43028046\n",
      "Iteration 116, loss = 0.43008150\n",
      "Iteration 117, loss = 0.42988055\n",
      "Iteration 118, loss = 0.42975883\n",
      "Iteration 119, loss = 0.42972280\n",
      "Iteration 120, loss = 0.42962508\n",
      "Iteration 121, loss = 0.42923943\n",
      "Iteration 122, loss = 0.42912292\n",
      "Iteration 123, loss = 0.42894796\n",
      "Iteration 124, loss = 0.42887856\n",
      "Iteration 125, loss = 0.42861032\n",
      "Iteration 126, loss = 0.42868583\n",
      "Iteration 127, loss = 0.42821252\n",
      "Iteration 128, loss = 0.42811275\n",
      "Iteration 129, loss = 0.42786016\n",
      "Iteration 130, loss = 0.42775674\n",
      "Iteration 131, loss = 0.42757188\n",
      "Iteration 132, loss = 0.42740187\n",
      "Iteration 133, loss = 0.42737500\n",
      "Iteration 134, loss = 0.42705772\n",
      "Iteration 135, loss = 0.42696462\n",
      "Iteration 136, loss = 0.42678116\n",
      "Iteration 137, loss = 0.42666018\n",
      "Iteration 138, loss = 0.42623433\n",
      "Iteration 139, loss = 0.42634011\n",
      "Iteration 140, loss = 0.42609205\n",
      "Iteration 141, loss = 0.42592171\n",
      "Iteration 142, loss = 0.42567153\n",
      "Iteration 143, loss = 0.42558842\n",
      "Iteration 144, loss = 0.42542322\n",
      "Iteration 145, loss = 0.42520055\n",
      "Iteration 146, loss = 0.42493076\n",
      "Iteration 147, loss = 0.42487340\n",
      "Iteration 148, loss = 0.42461854\n",
      "Iteration 149, loss = 0.42438149\n",
      "Iteration 150, loss = 0.42428471\n",
      "Iteration 151, loss = 0.42407388\n",
      "Iteration 152, loss = 0.42401548\n",
      "Iteration 153, loss = 0.42407066\n",
      "Iteration 154, loss = 0.42350426\n",
      "Iteration 155, loss = 0.42353166\n",
      "Iteration 156, loss = 0.42312833\n",
      "Iteration 157, loss = 0.42300443\n",
      "Iteration 158, loss = 0.42288069\n",
      "Iteration 159, loss = 0.42261914\n",
      "Iteration 160, loss = 0.42242586\n",
      "Iteration 161, loss = 0.42218757\n",
      "Iteration 162, loss = 0.42198335\n",
      "Iteration 163, loss = 0.42187465\n",
      "Iteration 164, loss = 0.42170568\n",
      "Iteration 165, loss = 0.42144714\n",
      "Iteration 166, loss = 0.42137737\n",
      "Iteration 167, loss = 0.42113434\n",
      "Iteration 168, loss = 0.42095290\n",
      "Iteration 169, loss = 0.42080987\n",
      "Iteration 170, loss = 0.42063489\n",
      "Iteration 171, loss = 0.42025767\n",
      "Iteration 172, loss = 0.42010644\n",
      "Iteration 173, loss = 0.41997494\n",
      "Iteration 174, loss = 0.41985390\n",
      "Iteration 175, loss = 0.41971105\n",
      "Iteration 176, loss = 0.41946506\n",
      "Iteration 177, loss = 0.41917772\n",
      "Iteration 178, loss = 0.41894615\n",
      "Iteration 179, loss = 0.41884738\n",
      "Iteration 180, loss = 0.41860682\n",
      "Iteration 181, loss = 0.41834385\n",
      "Iteration 182, loss = 0.41820518\n",
      "Iteration 183, loss = 0.41806751\n",
      "Iteration 184, loss = 0.41796441\n",
      "Iteration 185, loss = 0.41782257\n",
      "Iteration 186, loss = 0.41752844\n",
      "Iteration 187, loss = 0.41723619\n",
      "Iteration 188, loss = 0.41700611\n",
      "Iteration 189, loss = 0.41682847\n",
      "Iteration 190, loss = 0.41657302\n",
      "Iteration 191, loss = 0.41642783\n",
      "Iteration 192, loss = 0.41638565\n",
      "Iteration 193, loss = 0.41593795\n",
      "Iteration 194, loss = 0.41588253\n",
      "Iteration 195, loss = 0.41582744\n",
      "Iteration 196, loss = 0.41537603\n",
      "Iteration 197, loss = 0.41547250\n",
      "Iteration 198, loss = 0.41518620\n",
      "Iteration 199, loss = 0.41506482\n",
      "Iteration 200, loss = 0.41488463\n",
      "Iteration 1, loss = 0.75065251\n",
      "Iteration 2, loss = 0.70190708\n",
      "Iteration 3, loss = 0.65722314\n",
      "Iteration 4, loss = 0.62139734\n",
      "Iteration 5, loss = 0.59159805\n",
      "Iteration 6, loss = 0.56586290\n",
      "Iteration 7, loss = 0.54358053\n",
      "Iteration 8, loss = 0.52579304\n",
      "Iteration 9, loss = 0.50911544\n",
      "Iteration 10, loss = 0.49552867\n",
      "Iteration 11, loss = 0.48340195\n",
      "Iteration 12, loss = 0.47317948\n",
      "Iteration 13, loss = 0.46307054\n",
      "Iteration 14, loss = 0.45520104\n",
      "Iteration 15, loss = 0.44811142\n",
      "Iteration 16, loss = 0.44184751\n",
      "Iteration 17, loss = 0.43632745\n",
      "Iteration 18, loss = 0.43247853\n",
      "Iteration 19, loss = 0.42841379\n",
      "Iteration 20, loss = 0.42512293\n",
      "Iteration 21, loss = 0.42254999\n",
      "Iteration 22, loss = 0.42000174\n",
      "Iteration 23, loss = 0.41843672\n",
      "Iteration 24, loss = 0.41677612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.41566999\n",
      "Iteration 26, loss = 0.41460980\n",
      "Iteration 27, loss = 0.41364390\n",
      "Iteration 28, loss = 0.41296190\n",
      "Iteration 29, loss = 0.41233888\n",
      "Iteration 30, loss = 0.41179390\n",
      "Iteration 31, loss = 0.41134723\n",
      "Iteration 32, loss = 0.41112754\n",
      "Iteration 33, loss = 0.41076651\n",
      "Iteration 34, loss = 0.41042644\n",
      "Iteration 35, loss = 0.41021550\n",
      "Iteration 36, loss = 0.41002397\n",
      "Iteration 37, loss = 0.40977743\n",
      "Iteration 38, loss = 0.40959813\n",
      "Iteration 39, loss = 0.40942130\n",
      "Iteration 40, loss = 0.40930245\n",
      "Iteration 41, loss = 0.40915350\n",
      "Iteration 42, loss = 0.40906675\n",
      "Iteration 43, loss = 0.40895976\n",
      "Iteration 44, loss = 0.40878826\n",
      "Iteration 45, loss = 0.40863443\n",
      "Iteration 46, loss = 0.40854925\n",
      "Iteration 47, loss = 0.40833548\n",
      "Iteration 48, loss = 0.40819721\n",
      "Iteration 49, loss = 0.40805847\n",
      "Iteration 50, loss = 0.40792630\n",
      "Iteration 51, loss = 0.40786655\n",
      "Iteration 52, loss = 0.40773170\n",
      "Iteration 53, loss = 0.40751524\n",
      "Iteration 54, loss = 0.40747709\n",
      "Iteration 55, loss = 0.40731615\n",
      "Iteration 56, loss = 0.40719703\n",
      "Iteration 57, loss = 0.40715373\n",
      "Iteration 58, loss = 0.40707119\n",
      "Iteration 59, loss = 0.40676294\n",
      "Iteration 60, loss = 0.40664300\n",
      "Iteration 61, loss = 0.40673042\n",
      "Iteration 62, loss = 0.40676913\n",
      "Iteration 63, loss = 0.40646788\n",
      "Iteration 64, loss = 0.40627556\n",
      "Iteration 65, loss = 0.40599590\n",
      "Iteration 66, loss = 0.40596693\n",
      "Iteration 67, loss = 0.40575065\n",
      "Iteration 68, loss = 0.40579806\n",
      "Iteration 69, loss = 0.40551933\n",
      "Iteration 70, loss = 0.40546027\n",
      "Iteration 71, loss = 0.40528476\n",
      "Iteration 72, loss = 0.40519596\n",
      "Iteration 73, loss = 0.40502257\n",
      "Iteration 74, loss = 0.40491569\n",
      "Iteration 75, loss = 0.40479876\n",
      "Iteration 76, loss = 0.40473545\n",
      "Iteration 77, loss = 0.40452285\n",
      "Iteration 78, loss = 0.40450234\n",
      "Iteration 79, loss = 0.40423391\n",
      "Iteration 80, loss = 0.40417588\n",
      "Iteration 81, loss = 0.40407358\n",
      "Iteration 82, loss = 0.40389140\n",
      "Iteration 83, loss = 0.40391319\n",
      "Iteration 84, loss = 0.40369682\n",
      "Iteration 85, loss = 0.40351104\n",
      "Iteration 86, loss = 0.40355492\n",
      "Iteration 87, loss = 0.40326659\n",
      "Iteration 88, loss = 0.40327617\n",
      "Iteration 89, loss = 0.40298595\n",
      "Iteration 90, loss = 0.40290495\n",
      "Iteration 91, loss = 0.40285621\n",
      "Iteration 92, loss = 0.40268112\n",
      "Iteration 93, loss = 0.40248468\n",
      "Iteration 94, loss = 0.40243388\n",
      "Iteration 95, loss = 0.40218092\n",
      "Iteration 96, loss = 0.40238311\n",
      "Iteration 97, loss = 0.40194947\n",
      "Iteration 98, loss = 0.40185925\n",
      "Iteration 99, loss = 0.40169210\n",
      "Iteration 100, loss = 0.40170648\n",
      "Iteration 101, loss = 0.40144081\n",
      "Iteration 102, loss = 0.40133906\n",
      "Iteration 103, loss = 0.40111780\n",
      "Iteration 104, loss = 0.40108585\n",
      "Iteration 105, loss = 0.40086430\n",
      "Iteration 106, loss = 0.40075966\n",
      "Iteration 107, loss = 0.40064692\n",
      "Iteration 108, loss = 0.40066374\n",
      "Iteration 109, loss = 0.40034788\n",
      "Iteration 110, loss = 0.40030323\n",
      "Iteration 111, loss = 0.40010163\n",
      "Iteration 112, loss = 0.39987627\n",
      "Iteration 113, loss = 0.39985440\n",
      "Iteration 114, loss = 0.39959649\n",
      "Iteration 115, loss = 0.39951598\n",
      "Iteration 116, loss = 0.39938999\n",
      "Iteration 117, loss = 0.39917604\n",
      "Iteration 118, loss = 0.39911618\n",
      "Iteration 119, loss = 0.39895250\n",
      "Iteration 120, loss = 0.39877986\n",
      "Iteration 121, loss = 0.39875948\n",
      "Iteration 122, loss = 0.39848977\n",
      "Iteration 123, loss = 0.39845718\n",
      "Iteration 124, loss = 0.39841563\n",
      "Iteration 125, loss = 0.39820441\n",
      "Iteration 126, loss = 0.39794283\n",
      "Iteration 127, loss = 0.39779596\n",
      "Iteration 128, loss = 0.39764356\n",
      "Iteration 129, loss = 0.39748392\n",
      "Iteration 130, loss = 0.39733281\n",
      "Iteration 131, loss = 0.39733678\n",
      "Iteration 132, loss = 0.39702233\n",
      "Iteration 133, loss = 0.39691027\n",
      "Iteration 134, loss = 0.39676945\n",
      "Iteration 135, loss = 0.39656452\n",
      "Iteration 136, loss = 0.39639750\n",
      "Iteration 137, loss = 0.39639295\n",
      "Iteration 138, loss = 0.39610147\n",
      "Iteration 139, loss = 0.39614315\n",
      "Iteration 140, loss = 0.39597982\n",
      "Iteration 141, loss = 0.39571409\n",
      "Iteration 142, loss = 0.39551846\n",
      "Iteration 143, loss = 0.39542555\n",
      "Iteration 144, loss = 0.39520785\n",
      "Iteration 145, loss = 0.39533783\n",
      "Iteration 146, loss = 0.39504633\n",
      "Iteration 147, loss = 0.39477385\n",
      "Iteration 148, loss = 0.39476845\n",
      "Iteration 149, loss = 0.39452156\n",
      "Iteration 150, loss = 0.39430232\n",
      "Iteration 151, loss = 0.39428112\n",
      "Iteration 152, loss = 0.39398710\n",
      "Iteration 153, loss = 0.39385519\n",
      "Iteration 154, loss = 0.39379120\n",
      "Iteration 155, loss = 0.39396680\n",
      "Iteration 156, loss = 0.39330065\n",
      "Iteration 157, loss = 0.39315700\n",
      "Iteration 158, loss = 0.39306617\n",
      "Iteration 159, loss = 0.39286052\n",
      "Iteration 160, loss = 0.39274022\n",
      "Iteration 161, loss = 0.39249483\n",
      "Iteration 162, loss = 0.39249969\n",
      "Iteration 163, loss = 0.39223698\n",
      "Iteration 164, loss = 0.39204548\n",
      "Iteration 165, loss = 0.39200898\n",
      "Iteration 166, loss = 0.39178336\n",
      "Iteration 167, loss = 0.39162992\n",
      "Iteration 168, loss = 0.39139255\n",
      "Iteration 169, loss = 0.39135773\n",
      "Iteration 170, loss = 0.39103502\n",
      "Iteration 171, loss = 0.39082932\n",
      "Iteration 172, loss = 0.39080287\n",
      "Iteration 173, loss = 0.39068053\n",
      "Iteration 174, loss = 0.39037936\n",
      "Iteration 175, loss = 0.39013598\n",
      "Iteration 176, loss = 0.39043066\n",
      "Iteration 177, loss = 0.39000508\n",
      "Iteration 178, loss = 0.38978035\n",
      "Iteration 179, loss = 0.38965019\n",
      "Iteration 180, loss = 0.38936072\n",
      "Iteration 181, loss = 0.38937140\n",
      "Iteration 182, loss = 0.38904529\n",
      "Iteration 183, loss = 0.38899248\n",
      "Iteration 184, loss = 0.38874829\n",
      "Iteration 185, loss = 0.38866472\n",
      "Iteration 186, loss = 0.38838748\n",
      "Iteration 187, loss = 0.38837854\n",
      "Iteration 188, loss = 0.38801598\n",
      "Iteration 189, loss = 0.38783707\n",
      "Iteration 190, loss = 0.38781202\n",
      "Iteration 191, loss = 0.38741517\n",
      "Iteration 192, loss = 0.38738958\n",
      "Iteration 193, loss = 0.38717561\n",
      "Iteration 194, loss = 0.38714040\n",
      "Iteration 195, loss = 0.38700090\n",
      "Iteration 196, loss = 0.38667841\n",
      "Iteration 197, loss = 0.38635074\n",
      "Iteration 198, loss = 0.38632943\n",
      "Iteration 199, loss = 0.38611620\n",
      "Iteration 200, loss = 0.38597764\n",
      "Iteration 1, loss = 0.75778688\n",
      "Iteration 2, loss = 0.71117249\n",
      "Iteration 3, loss = 0.66855183\n",
      "Iteration 4, loss = 0.63491254\n",
      "Iteration 5, loss = 0.60626754\n",
      "Iteration 6, loss = 0.58171472\n",
      "Iteration 7, loss = 0.55986479\n",
      "Iteration 8, loss = 0.54311391\n",
      "Iteration 9, loss = 0.52721065\n",
      "Iteration 10, loss = 0.51425698\n",
      "Iteration 11, loss = 0.50248936\n",
      "Iteration 12, loss = 0.49268548\n",
      "Iteration 13, loss = 0.48327816\n",
      "Iteration 14, loss = 0.47591923\n",
      "Iteration 15, loss = 0.46910536\n",
      "Iteration 16, loss = 0.46335780\n",
      "Iteration 17, loss = 0.45832567\n",
      "Iteration 18, loss = 0.45463252\n",
      "Iteration 19, loss = 0.45092600\n",
      "Iteration 20, loss = 0.44776320\n",
      "Iteration 21, loss = 0.44528549\n",
      "Iteration 22, loss = 0.44306035\n",
      "Iteration 23, loss = 0.44173745\n",
      "Iteration 24, loss = 0.44013331\n",
      "Iteration 25, loss = 0.43916187\n",
      "Iteration 26, loss = 0.43800563\n",
      "Iteration 27, loss = 0.43730819\n",
      "Iteration 28, loss = 0.43665713\n",
      "Iteration 29, loss = 0.43588618\n",
      "Iteration 30, loss = 0.43553051\n",
      "Iteration 31, loss = 0.43494773\n",
      "Iteration 32, loss = 0.43478533\n",
      "Iteration 33, loss = 0.43438185\n",
      "Iteration 34, loss = 0.43401018\n",
      "Iteration 35, loss = 0.43374113\n",
      "Iteration 36, loss = 0.43359232\n",
      "Iteration 37, loss = 0.43328496\n",
      "Iteration 38, loss = 0.43310258\n",
      "Iteration 39, loss = 0.43291521\n",
      "Iteration 40, loss = 0.43284847\n",
      "Iteration 41, loss = 0.43255086\n",
      "Iteration 42, loss = 0.43247325\n",
      "Iteration 43, loss = 0.43233591\n",
      "Iteration 44, loss = 0.43221085\n",
      "Iteration 45, loss = 0.43198218\n",
      "Iteration 46, loss = 0.43186241\n",
      "Iteration 47, loss = 0.43169749\n",
      "Iteration 48, loss = 0.43161046\n",
      "Iteration 49, loss = 0.43150209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.43138178\n",
      "Iteration 51, loss = 0.43124193\n",
      "Iteration 52, loss = 0.43106365\n",
      "Iteration 53, loss = 0.43096038\n",
      "Iteration 54, loss = 0.43086446\n",
      "Iteration 55, loss = 0.43070022\n",
      "Iteration 56, loss = 0.43060941\n",
      "Iteration 57, loss = 0.43047086\n",
      "Iteration 58, loss = 0.43036234\n",
      "Iteration 59, loss = 0.43021022\n",
      "Iteration 60, loss = 0.43013869\n",
      "Iteration 61, loss = 0.43008812\n",
      "Iteration 62, loss = 0.42990812\n",
      "Iteration 63, loss = 0.42973072\n",
      "Iteration 64, loss = 0.42969239\n",
      "Iteration 65, loss = 0.42947470\n",
      "Iteration 66, loss = 0.42944395\n",
      "Iteration 67, loss = 0.42919955\n",
      "Iteration 68, loss = 0.42926450\n",
      "Iteration 69, loss = 0.42911135\n",
      "Iteration 70, loss = 0.42892779\n",
      "Iteration 71, loss = 0.42877197\n",
      "Iteration 72, loss = 0.42859139\n",
      "Iteration 73, loss = 0.42860411\n",
      "Iteration 74, loss = 0.42847101\n",
      "Iteration 75, loss = 0.42828402\n",
      "Iteration 76, loss = 0.42819927\n",
      "Iteration 77, loss = 0.42802575\n",
      "Iteration 78, loss = 0.42791960\n",
      "Iteration 79, loss = 0.42773974\n",
      "Iteration 80, loss = 0.42763232\n",
      "Iteration 81, loss = 0.42773480\n",
      "Iteration 82, loss = 0.42739007\n",
      "Iteration 83, loss = 0.42756254\n",
      "Iteration 84, loss = 0.42720566\n",
      "Iteration 85, loss = 0.42698328\n",
      "Iteration 86, loss = 0.42688765\n",
      "Iteration 87, loss = 0.42680698\n",
      "Iteration 88, loss = 0.42672201\n",
      "Iteration 89, loss = 0.42653076\n",
      "Iteration 90, loss = 0.42642592\n",
      "Iteration 91, loss = 0.42640247\n",
      "Iteration 92, loss = 0.42615887\n",
      "Iteration 93, loss = 0.42598989\n",
      "Iteration 94, loss = 0.42622531\n",
      "Iteration 95, loss = 0.42584994\n",
      "Iteration 96, loss = 0.42572783\n",
      "Iteration 97, loss = 0.42559629\n",
      "Iteration 98, loss = 0.42563140\n",
      "Iteration 99, loss = 0.42532522\n",
      "Iteration 100, loss = 0.42518370\n",
      "Iteration 101, loss = 0.42512663\n",
      "Iteration 102, loss = 0.42496951\n",
      "Iteration 103, loss = 0.42487026\n",
      "Iteration 104, loss = 0.42471249\n",
      "Iteration 105, loss = 0.42474283\n",
      "Iteration 106, loss = 0.42442780\n",
      "Iteration 107, loss = 0.42426639\n",
      "Iteration 108, loss = 0.42418501\n",
      "Iteration 109, loss = 0.42405880\n",
      "Iteration 110, loss = 0.42394887\n",
      "Iteration 111, loss = 0.42385324\n",
      "Iteration 112, loss = 0.42358916\n",
      "Iteration 113, loss = 0.42360091\n",
      "Iteration 114, loss = 0.42335599\n",
      "Iteration 115, loss = 0.42327700\n",
      "Iteration 116, loss = 0.42330178\n",
      "Iteration 117, loss = 0.42301989\n",
      "Iteration 118, loss = 0.42291657\n",
      "Iteration 119, loss = 0.42281775\n",
      "Iteration 120, loss = 0.42258455\n",
      "Iteration 121, loss = 0.42260372\n",
      "Iteration 122, loss = 0.42239031\n",
      "Iteration 123, loss = 0.42223131\n",
      "Iteration 124, loss = 0.42220930\n",
      "Iteration 125, loss = 0.42195431\n",
      "Iteration 126, loss = 0.42187401\n",
      "Iteration 127, loss = 0.42165394\n",
      "Iteration 128, loss = 0.42144519\n",
      "Iteration 129, loss = 0.42152818\n",
      "Iteration 130, loss = 0.42130294\n",
      "Iteration 131, loss = 0.42117360\n",
      "Iteration 132, loss = 0.42096582\n",
      "Iteration 133, loss = 0.42092870\n",
      "Iteration 134, loss = 0.42072375\n",
      "Iteration 135, loss = 0.42061047\n",
      "Iteration 136, loss = 0.42039752\n",
      "Iteration 137, loss = 0.42035758\n",
      "Iteration 138, loss = 0.42011497\n",
      "Iteration 139, loss = 0.42041031\n",
      "Iteration 140, loss = 0.41993115\n",
      "Iteration 141, loss = 0.41995665\n",
      "Iteration 142, loss = 0.41959108\n",
      "Iteration 143, loss = 0.41944916\n",
      "Iteration 144, loss = 0.41934271\n",
      "Iteration 145, loss = 0.41923818\n",
      "Iteration 146, loss = 0.41917154\n",
      "Iteration 147, loss = 0.41895443\n",
      "Iteration 148, loss = 0.41894987\n",
      "Iteration 149, loss = 0.41864035\n",
      "Iteration 150, loss = 0.41839505\n",
      "Iteration 151, loss = 0.41841066\n",
      "Iteration 152, loss = 0.41819950\n",
      "Iteration 153, loss = 0.41795935\n",
      "Iteration 154, loss = 0.41806929\n",
      "Iteration 155, loss = 0.41823070\n",
      "Iteration 156, loss = 0.41764863\n",
      "Iteration 157, loss = 0.41742473\n",
      "Iteration 158, loss = 0.41738362\n",
      "Iteration 159, loss = 0.41742599\n",
      "Iteration 160, loss = 0.41697177\n",
      "Iteration 161, loss = 0.41686098\n",
      "Iteration 162, loss = 0.41685931\n",
      "Iteration 163, loss = 0.41664010\n",
      "Iteration 164, loss = 0.41636606\n",
      "Iteration 165, loss = 0.41629866\n",
      "Iteration 166, loss = 0.41618434\n",
      "Iteration 167, loss = 0.41607119\n",
      "Iteration 168, loss = 0.41579254\n",
      "Iteration 169, loss = 0.41565664\n",
      "Iteration 170, loss = 0.41554847\n",
      "Iteration 171, loss = 0.41549222\n",
      "Iteration 172, loss = 0.41521486\n",
      "Iteration 173, loss = 0.41518626\n",
      "Iteration 174, loss = 0.41496045\n",
      "Iteration 175, loss = 0.41486594\n",
      "Iteration 176, loss = 0.41480175\n",
      "Iteration 177, loss = 0.41449509\n",
      "Iteration 178, loss = 0.41442368\n",
      "Iteration 179, loss = 0.41415290\n",
      "Iteration 180, loss = 0.41406941\n",
      "Iteration 181, loss = 0.41396441\n",
      "Iteration 182, loss = 0.41359575\n",
      "Iteration 183, loss = 0.41347773\n",
      "Iteration 184, loss = 0.41337185\n",
      "Iteration 185, loss = 0.41324141\n",
      "Iteration 186, loss = 0.41301391\n",
      "Iteration 187, loss = 0.41320637\n",
      "Iteration 188, loss = 0.41282153\n",
      "Iteration 189, loss = 0.41262731\n",
      "Iteration 190, loss = 0.41246272\n",
      "Iteration 191, loss = 0.41220585\n",
      "Iteration 192, loss = 0.41215593\n",
      "Iteration 193, loss = 0.41192965\n",
      "Iteration 194, loss = 0.41181982\n",
      "Iteration 195, loss = 0.41179616\n",
      "Iteration 196, loss = 0.41149825\n",
      "Iteration 197, loss = 0.41124335\n",
      "Iteration 198, loss = 0.41112347\n",
      "Iteration 199, loss = 0.41097907\n",
      "Iteration 200, loss = 0.41083255\n",
      "Iteration 1, loss = 0.75871916\n",
      "Iteration 2, loss = 0.71220444\n",
      "Iteration 3, loss = 0.67230378\n",
      "Iteration 4, loss = 0.63870312\n",
      "Iteration 5, loss = 0.60986775\n",
      "Iteration 6, loss = 0.58410315\n",
      "Iteration 7, loss = 0.56351916\n",
      "Iteration 8, loss = 0.54454088\n",
      "Iteration 9, loss = 0.52762883\n",
      "Iteration 10, loss = 0.51281857\n",
      "Iteration 11, loss = 0.50008362\n",
      "Iteration 12, loss = 0.48854588\n",
      "Iteration 13, loss = 0.47867323\n",
      "Iteration 14, loss = 0.46962033\n",
      "Iteration 15, loss = 0.46205966\n",
      "Iteration 16, loss = 0.45579066\n",
      "Iteration 17, loss = 0.45018981\n",
      "Iteration 18, loss = 0.44595421\n",
      "Iteration 19, loss = 0.44175809\n",
      "Iteration 20, loss = 0.43792413\n",
      "Iteration 21, loss = 0.43529129\n",
      "Iteration 22, loss = 0.43277552\n",
      "Iteration 23, loss = 0.43058135\n",
      "Iteration 24, loss = 0.42897762\n",
      "Iteration 25, loss = 0.42759153\n",
      "Iteration 26, loss = 0.42632114\n",
      "Iteration 27, loss = 0.42516192\n",
      "Iteration 28, loss = 0.42447645\n",
      "Iteration 29, loss = 0.42353914\n",
      "Iteration 30, loss = 0.42298930\n",
      "Iteration 31, loss = 0.42251722\n",
      "Iteration 32, loss = 0.42199436\n",
      "Iteration 33, loss = 0.42184377\n",
      "Iteration 34, loss = 0.42120792\n",
      "Iteration 35, loss = 0.42099297\n",
      "Iteration 36, loss = 0.42057251\n",
      "Iteration 37, loss = 0.42041737\n",
      "Iteration 38, loss = 0.42009218\n",
      "Iteration 39, loss = 0.41974577\n",
      "Iteration 40, loss = 0.41964702\n",
      "Iteration 41, loss = 0.41937021\n",
      "Iteration 42, loss = 0.41921894\n",
      "Iteration 43, loss = 0.41888390\n",
      "Iteration 44, loss = 0.41891743\n",
      "Iteration 45, loss = 0.41856355\n",
      "Iteration 46, loss = 0.41833966\n",
      "Iteration 47, loss = 0.41813831\n",
      "Iteration 48, loss = 0.41806168\n",
      "Iteration 49, loss = 0.41783209\n",
      "Iteration 50, loss = 0.41766982\n",
      "Iteration 51, loss = 0.41753525\n",
      "Iteration 52, loss = 0.41742621\n",
      "Iteration 53, loss = 0.41716371\n",
      "Iteration 54, loss = 0.41697488\n",
      "Iteration 55, loss = 0.41683005\n",
      "Iteration 56, loss = 0.41677054\n",
      "Iteration 57, loss = 0.41666726\n",
      "Iteration 58, loss = 0.41645630\n",
      "Iteration 59, loss = 0.41623324\n",
      "Iteration 60, loss = 0.41612604\n",
      "Iteration 61, loss = 0.41596893\n",
      "Iteration 62, loss = 0.41583576\n",
      "Iteration 63, loss = 0.41566601\n",
      "Iteration 64, loss = 0.41558170\n",
      "Iteration 65, loss = 0.41536863\n",
      "Iteration 66, loss = 0.41534955\n",
      "Iteration 67, loss = 0.41527392\n",
      "Iteration 68, loss = 0.41502168\n",
      "Iteration 69, loss = 0.41480193\n",
      "Iteration 70, loss = 0.41482275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.41454955\n",
      "Iteration 72, loss = 0.41443025\n",
      "Iteration 73, loss = 0.41422710\n",
      "Iteration 74, loss = 0.41412781\n",
      "Iteration 75, loss = 0.41393529\n",
      "Iteration 76, loss = 0.41397693\n",
      "Iteration 77, loss = 0.41360383\n",
      "Iteration 78, loss = 0.41347867\n",
      "Iteration 79, loss = 0.41335338\n",
      "Iteration 80, loss = 0.41329934\n",
      "Iteration 81, loss = 0.41313503\n",
      "Iteration 82, loss = 0.41300833\n",
      "Iteration 83, loss = 0.41282461\n",
      "Iteration 84, loss = 0.41268638\n",
      "Iteration 85, loss = 0.41269193\n",
      "Iteration 86, loss = 0.41253140\n",
      "Iteration 87, loss = 0.41245196\n",
      "Iteration 88, loss = 0.41211288\n",
      "Iteration 89, loss = 0.41198906\n",
      "Iteration 90, loss = 0.41196949\n",
      "Iteration 91, loss = 0.41169006\n",
      "Iteration 92, loss = 0.41189056\n",
      "Iteration 93, loss = 0.41146632\n",
      "Iteration 94, loss = 0.41173608\n",
      "Iteration 95, loss = 0.41114493\n",
      "Iteration 96, loss = 0.41094782\n",
      "Iteration 97, loss = 0.41082378\n",
      "Iteration 98, loss = 0.41062397\n",
      "Iteration 99, loss = 0.41053233\n",
      "Iteration 100, loss = 0.41034165\n",
      "Iteration 101, loss = 0.41053227\n",
      "Iteration 102, loss = 0.41012767\n",
      "Iteration 103, loss = 0.40994429\n",
      "Iteration 104, loss = 0.40992619\n",
      "Iteration 105, loss = 0.40964537\n",
      "Iteration 106, loss = 0.40962830\n",
      "Iteration 107, loss = 0.40940552\n",
      "Iteration 108, loss = 0.40923490\n",
      "Iteration 109, loss = 0.40908632\n",
      "Iteration 110, loss = 0.40888415\n",
      "Iteration 111, loss = 0.40878130\n",
      "Iteration 112, loss = 0.40870524\n",
      "Iteration 113, loss = 0.40843378\n",
      "Iteration 114, loss = 0.40836015\n",
      "Iteration 115, loss = 0.40815820\n",
      "Iteration 116, loss = 0.40791033\n",
      "Iteration 117, loss = 0.40777723\n",
      "Iteration 118, loss = 0.40766671\n",
      "Iteration 119, loss = 0.40767471\n",
      "Iteration 120, loss = 0.40750893\n",
      "Iteration 121, loss = 0.40725637\n",
      "Iteration 122, loss = 0.40710541\n",
      "Iteration 123, loss = 0.40685573\n",
      "Iteration 124, loss = 0.40683436\n",
      "Iteration 125, loss = 0.40669789\n",
      "Iteration 126, loss = 0.40642184\n",
      "Iteration 127, loss = 0.40621840\n",
      "Iteration 128, loss = 0.40611686\n",
      "Iteration 129, loss = 0.40601754\n",
      "Iteration 130, loss = 0.40583600\n",
      "Iteration 131, loss = 0.40559581\n",
      "Iteration 132, loss = 0.40550354\n",
      "Iteration 133, loss = 0.40532897\n",
      "Iteration 134, loss = 0.40516672\n",
      "Iteration 135, loss = 0.40488119\n",
      "Iteration 136, loss = 0.40512425\n",
      "Iteration 137, loss = 0.40456173\n",
      "Iteration 138, loss = 0.40444836\n",
      "Iteration 139, loss = 0.40443969\n",
      "Iteration 140, loss = 0.40412629\n",
      "Iteration 141, loss = 0.40397564\n",
      "Iteration 142, loss = 0.40376148\n",
      "Iteration 143, loss = 0.40361245\n",
      "Iteration 144, loss = 0.40354709\n",
      "Iteration 145, loss = 0.40339766\n",
      "Iteration 146, loss = 0.40316591\n",
      "Iteration 147, loss = 0.40300299\n",
      "Iteration 148, loss = 0.40290015\n",
      "Iteration 149, loss = 0.40255267\n",
      "Iteration 150, loss = 0.40237425\n",
      "Iteration 151, loss = 0.40223694\n",
      "Iteration 152, loss = 0.40211268\n",
      "Iteration 153, loss = 0.40245181\n",
      "Iteration 154, loss = 0.40185247\n",
      "Iteration 155, loss = 0.40139942\n",
      "Iteration 156, loss = 0.40143400\n",
      "Iteration 157, loss = 0.40120563\n",
      "Iteration 158, loss = 0.40099847\n",
      "Iteration 159, loss = 0.40082920\n",
      "Iteration 160, loss = 0.40074116\n",
      "Iteration 161, loss = 0.40052448\n",
      "Iteration 162, loss = 0.40039248\n",
      "Iteration 163, loss = 0.40012789\n",
      "Iteration 164, loss = 0.40008768\n",
      "Iteration 165, loss = 0.39983579\n",
      "Iteration 166, loss = 0.39961773\n",
      "Iteration 167, loss = 0.39930519\n",
      "Iteration 168, loss = 0.39919303\n",
      "Iteration 169, loss = 0.39899260\n",
      "Iteration 170, loss = 0.39896758\n",
      "Iteration 171, loss = 0.39865441\n",
      "Iteration 172, loss = 0.39865581\n",
      "Iteration 173, loss = 0.39842470\n",
      "Iteration 174, loss = 0.39821210\n",
      "Iteration 175, loss = 0.39795887\n",
      "Iteration 176, loss = 0.39780072\n",
      "Iteration 177, loss = 0.39748927\n",
      "Iteration 178, loss = 0.39754993\n",
      "Iteration 179, loss = 0.39718552\n",
      "Iteration 180, loss = 0.39695024\n",
      "Iteration 181, loss = 0.39671556\n",
      "Iteration 182, loss = 0.39658017\n",
      "Iteration 183, loss = 0.39628085\n",
      "Iteration 184, loss = 0.39616948\n",
      "Iteration 185, loss = 0.39620367\n",
      "Iteration 186, loss = 0.39588938\n",
      "Iteration 187, loss = 0.39570745\n",
      "Iteration 188, loss = 0.39547860\n",
      "Iteration 189, loss = 0.39530365\n",
      "Iteration 190, loss = 0.39500644\n",
      "Iteration 191, loss = 0.39488474\n",
      "Iteration 192, loss = 0.39468956\n",
      "Iteration 193, loss = 0.39436103\n",
      "Iteration 194, loss = 0.39441376\n",
      "Iteration 195, loss = 0.39419897\n",
      "Iteration 196, loss = 0.39388338\n",
      "Iteration 197, loss = 0.39394784\n",
      "Iteration 198, loss = 0.39354242\n",
      "Iteration 199, loss = 0.39335544\n",
      "Iteration 200, loss = 0.39296425\n",
      "Iteration 1, loss = 0.76108340\n",
      "Iteration 2, loss = 0.71341317\n",
      "Iteration 3, loss = 0.67457061\n",
      "Iteration 4, loss = 0.64067955\n",
      "Iteration 5, loss = 0.61108747\n",
      "Iteration 6, loss = 0.58593284\n",
      "Iteration 7, loss = 0.56461930\n",
      "Iteration 8, loss = 0.54514041\n",
      "Iteration 9, loss = 0.52794703\n",
      "Iteration 10, loss = 0.51275585\n",
      "Iteration 11, loss = 0.49989471\n",
      "Iteration 12, loss = 0.48809354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.47800957\n",
      "Iteration 14, loss = 0.46863058\n",
      "Iteration 15, loss = 0.46104126\n",
      "Iteration 16, loss = 0.45432051\n",
      "Iteration 17, loss = 0.44874487\n",
      "Iteration 18, loss = 0.44418479\n",
      "Iteration 19, loss = 0.43944874\n",
      "Iteration 20, loss = 0.43605388\n",
      "Iteration 21, loss = 0.43289031\n",
      "Iteration 22, loss = 0.43029823\n",
      "Iteration 23, loss = 0.42785997\n",
      "Iteration 24, loss = 0.42603965\n",
      "Iteration 25, loss = 0.42458720\n",
      "Iteration 26, loss = 0.42311700\n",
      "Iteration 27, loss = 0.42195112\n",
      "Iteration 28, loss = 0.42112353\n",
      "Iteration 29, loss = 0.42024767\n",
      "Iteration 30, loss = 0.41946169\n",
      "Iteration 31, loss = 0.41905178\n",
      "Iteration 32, loss = 0.41856831\n",
      "Iteration 33, loss = 0.41830674\n",
      "Iteration 34, loss = 0.41768171\n",
      "Iteration 35, loss = 0.41728209\n",
      "Iteration 36, loss = 0.41715725\n",
      "Iteration 37, loss = 0.41684070\n",
      "Iteration 38, loss = 0.41648770\n",
      "Iteration 39, loss = 0.41627736\n",
      "Iteration 40, loss = 0.41621040\n",
      "Iteration 41, loss = 0.41591322\n",
      "Iteration 42, loss = 0.41577238\n",
      "Iteration 43, loss = 0.41557021\n",
      "Iteration 44, loss = 0.41550153\n",
      "Iteration 45, loss = 0.41520287\n",
      "Iteration 46, loss = 0.41505570\n",
      "Iteration 47, loss = 0.41487316\n",
      "Iteration 48, loss = 0.41477596\n",
      "Iteration 49, loss = 0.41463104\n",
      "Iteration 50, loss = 0.41444628\n",
      "Iteration 51, loss = 0.41433213\n",
      "Iteration 52, loss = 0.41426371\n",
      "Iteration 53, loss = 0.41406012\n",
      "Iteration 54, loss = 0.41390908\n",
      "Iteration 55, loss = 0.41378177\n",
      "Iteration 56, loss = 0.41364632\n",
      "Iteration 57, loss = 0.41357703\n",
      "Iteration 58, loss = 0.41351662\n",
      "Iteration 59, loss = 0.41327143\n",
      "Iteration 60, loss = 0.41311641\n",
      "Iteration 61, loss = 0.41306896\n",
      "Iteration 62, loss = 0.41290443\n",
      "Iteration 63, loss = 0.41286214\n",
      "Iteration 64, loss = 0.41280921\n",
      "Iteration 65, loss = 0.41259931\n",
      "Iteration 66, loss = 0.41248392\n",
      "Iteration 67, loss = 0.41233095\n",
      "Iteration 68, loss = 0.41215564\n",
      "Iteration 69, loss = 0.41203744\n",
      "Iteration 70, loss = 0.41201329\n",
      "Iteration 71, loss = 0.41182077\n",
      "Iteration 72, loss = 0.41168825\n",
      "Iteration 73, loss = 0.41146910\n",
      "Iteration 74, loss = 0.41144921\n",
      "Iteration 75, loss = 0.41128186\n",
      "Iteration 76, loss = 0.41118214\n",
      "Iteration 77, loss = 0.41105122\n",
      "Iteration 78, loss = 0.41088405\n",
      "Iteration 79, loss = 0.41072497\n",
      "Iteration 80, loss = 0.41064331\n",
      "Iteration 81, loss = 0.41050051\n",
      "Iteration 82, loss = 0.41036522\n",
      "Iteration 83, loss = 0.41022061\n",
      "Iteration 84, loss = 0.41013283\n",
      "Iteration 85, loss = 0.41012587\n",
      "Iteration 86, loss = 0.40982554\n",
      "Iteration 87, loss = 0.40992192\n",
      "Iteration 88, loss = 0.40969761\n",
      "Iteration 89, loss = 0.40956368\n",
      "Iteration 90, loss = 0.40951900\n",
      "Iteration 91, loss = 0.40924069\n",
      "Iteration 92, loss = 0.40937277\n",
      "Iteration 93, loss = 0.40905632\n",
      "Iteration 94, loss = 0.40899776\n",
      "Iteration 95, loss = 0.40872478\n",
      "Iteration 96, loss = 0.40861574\n",
      "Iteration 97, loss = 0.40860875\n",
      "Iteration 98, loss = 0.40835493\n",
      "Iteration 99, loss = 0.40829704\n",
      "Iteration 100, loss = 0.40800042\n",
      "Iteration 101, loss = 0.40821594\n",
      "Iteration 102, loss = 0.40795402\n",
      "Iteration 103, loss = 0.40771682\n",
      "Iteration 104, loss = 0.40769450\n",
      "Iteration 105, loss = 0.40740417\n",
      "Iteration 106, loss = 0.40733324\n",
      "Iteration 107, loss = 0.40723994\n",
      "Iteration 108, loss = 0.40719932\n",
      "Iteration 109, loss = 0.40706615\n",
      "Iteration 110, loss = 0.40684639\n",
      "Iteration 111, loss = 0.40659541\n",
      "Iteration 112, loss = 0.40654074\n",
      "Iteration 113, loss = 0.40638060\n",
      "Iteration 114, loss = 0.40629737\n",
      "Iteration 115, loss = 0.40614700\n",
      "Iteration 116, loss = 0.40593484\n",
      "Iteration 117, loss = 0.40580972\n",
      "Iteration 118, loss = 0.40576800\n",
      "Iteration 119, loss = 0.40572652\n",
      "Iteration 120, loss = 0.40549295\n",
      "Iteration 121, loss = 0.40538940\n",
      "Iteration 122, loss = 0.40522913\n",
      "Iteration 123, loss = 0.40497484\n",
      "Iteration 124, loss = 0.40493342\n",
      "Iteration 125, loss = 0.40482438\n",
      "Iteration 126, loss = 0.40464285\n",
      "Iteration 127, loss = 0.40451759\n",
      "Iteration 128, loss = 0.40432075\n",
      "Iteration 129, loss = 0.40420238\n",
      "Iteration 130, loss = 0.40412902\n",
      "Iteration 131, loss = 0.40384523\n",
      "Iteration 132, loss = 0.40379982\n",
      "Iteration 133, loss = 0.40367557\n",
      "Iteration 134, loss = 0.40349849\n",
      "Iteration 135, loss = 0.40330535\n",
      "Iteration 136, loss = 0.40323830\n",
      "Iteration 137, loss = 0.40332361\n",
      "Iteration 138, loss = 0.40301817\n",
      "Iteration 139, loss = 0.40287553\n",
      "Iteration 140, loss = 0.40267224\n",
      "Iteration 141, loss = 0.40253791\n",
      "Iteration 142, loss = 0.40227093\n",
      "Iteration 143, loss = 0.40207355\n",
      "Iteration 144, loss = 0.40208283\n",
      "Iteration 145, loss = 0.40196714\n",
      "Iteration 146, loss = 0.40187782\n",
      "Iteration 147, loss = 0.40154219\n",
      "Iteration 148, loss = 0.40148966\n",
      "Iteration 149, loss = 0.40121415\n",
      "Iteration 150, loss = 0.40106321\n",
      "Iteration 151, loss = 0.40090424\n",
      "Iteration 152, loss = 0.40094854\n",
      "Iteration 153, loss = 0.40104317\n",
      "Iteration 154, loss = 0.40047393\n",
      "Iteration 155, loss = 0.40028158\n",
      "Iteration 156, loss = 0.40021382\n",
      "Iteration 157, loss = 0.39997633\n",
      "Iteration 158, loss = 0.39982417\n",
      "Iteration 159, loss = 0.39965247\n",
      "Iteration 160, loss = 0.39958023\n",
      "Iteration 161, loss = 0.39944840\n",
      "Iteration 162, loss = 0.39931529\n",
      "Iteration 163, loss = 0.39910502\n",
      "Iteration 164, loss = 0.39892722\n",
      "Iteration 165, loss = 0.39868330\n",
      "Iteration 166, loss = 0.39859486\n",
      "Iteration 167, loss = 0.39834659\n",
      "Iteration 168, loss = 0.39836001\n",
      "Iteration 169, loss = 0.39810504\n",
      "Iteration 170, loss = 0.39800385\n",
      "Iteration 171, loss = 0.39768957\n",
      "Iteration 172, loss = 0.39759731\n",
      "Iteration 173, loss = 0.39761745\n",
      "Iteration 174, loss = 0.39729450\n",
      "Iteration 175, loss = 0.39721164\n",
      "Iteration 176, loss = 0.39692385\n",
      "Iteration 177, loss = 0.39676224\n",
      "Iteration 178, loss = 0.39663201\n",
      "Iteration 179, loss = 0.39646483\n",
      "Iteration 180, loss = 0.39617622\n",
      "Iteration 181, loss = 0.39601349\n",
      "Iteration 182, loss = 0.39585596\n",
      "Iteration 183, loss = 0.39561487\n",
      "Iteration 184, loss = 0.39545208\n",
      "Iteration 185, loss = 0.39538973\n",
      "Iteration 186, loss = 0.39527880\n",
      "Iteration 187, loss = 0.39492719\n",
      "Iteration 188, loss = 0.39485251\n",
      "Iteration 189, loss = 0.39469430\n",
      "Iteration 190, loss = 0.39452911\n",
      "Iteration 191, loss = 0.39421035\n",
      "Iteration 192, loss = 0.39427738\n",
      "Iteration 193, loss = 0.39376883\n",
      "Iteration 194, loss = 0.39377967\n",
      "Iteration 195, loss = 0.39374801\n",
      "Iteration 196, loss = 0.39327794\n",
      "Iteration 197, loss = 0.39320453\n",
      "Iteration 198, loss = 0.39305326\n",
      "Iteration 199, loss = 0.39293876\n",
      "Iteration 200, loss = 0.39252677\n",
      "Iteration 1, loss = 0.75388633\n",
      "Iteration 2, loss = 0.70749206\n",
      "Iteration 3, loss = 0.66808270\n",
      "Iteration 4, loss = 0.63528311\n",
      "Iteration 5, loss = 0.60701258\n",
      "Iteration 6, loss = 0.58321035\n",
      "Iteration 7, loss = 0.56377509\n",
      "Iteration 8, loss = 0.54589088\n",
      "Iteration 9, loss = 0.53072645\n",
      "Iteration 10, loss = 0.51731920\n",
      "Iteration 11, loss = 0.50595180\n",
      "Iteration 12, loss = 0.49624627\n",
      "Iteration 13, loss = 0.48769328\n",
      "Iteration 14, loss = 0.47977143\n",
      "Iteration 15, loss = 0.47370309\n",
      "Iteration 16, loss = 0.46819787\n",
      "Iteration 17, loss = 0.46427774\n",
      "Iteration 18, loss = 0.46024545\n",
      "Iteration 19, loss = 0.45673199\n",
      "Iteration 20, loss = 0.45451833\n",
      "Iteration 21, loss = 0.45235407\n",
      "Iteration 22, loss = 0.45032300\n",
      "Iteration 23, loss = 0.44884784\n",
      "Iteration 24, loss = 0.44766801\n",
      "Iteration 25, loss = 0.44666076\n",
      "Iteration 26, loss = 0.44573534\n",
      "Iteration 27, loss = 0.44504905\n",
      "Iteration 28, loss = 0.44461749\n",
      "Iteration 29, loss = 0.44402130\n",
      "Iteration 30, loss = 0.44353604\n",
      "Iteration 31, loss = 0.44323429\n",
      "Iteration 32, loss = 0.44295971\n",
      "Iteration 33, loss = 0.44272975\n",
      "Iteration 34, loss = 0.44238644\n",
      "Iteration 35, loss = 0.44207753\n",
      "Iteration 36, loss = 0.44199486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.44176190\n",
      "Iteration 38, loss = 0.44148852\n",
      "Iteration 39, loss = 0.44127313\n",
      "Iteration 40, loss = 0.44122490\n",
      "Iteration 41, loss = 0.44096978\n",
      "Iteration 42, loss = 0.44082542\n",
      "Iteration 43, loss = 0.44060953\n",
      "Iteration 44, loss = 0.44050086\n",
      "Iteration 45, loss = 0.44028698\n",
      "Iteration 46, loss = 0.44014236\n",
      "Iteration 47, loss = 0.43991540\n",
      "Iteration 48, loss = 0.43979285\n",
      "Iteration 49, loss = 0.43968023\n",
      "Iteration 50, loss = 0.43960208\n",
      "Iteration 51, loss = 0.43935205\n",
      "Iteration 52, loss = 0.43922159\n",
      "Iteration 53, loss = 0.43909921\n",
      "Iteration 54, loss = 0.43884772\n",
      "Iteration 55, loss = 0.43873741\n",
      "Iteration 56, loss = 0.43849299\n",
      "Iteration 57, loss = 0.43838918\n",
      "Iteration 58, loss = 0.43854736\n",
      "Iteration 59, loss = 0.43809078\n",
      "Iteration 60, loss = 0.43792757\n",
      "Iteration 61, loss = 0.43782443\n",
      "Iteration 62, loss = 0.43759925\n",
      "Iteration 63, loss = 0.43758391\n",
      "Iteration 64, loss = 0.43743590\n",
      "Iteration 65, loss = 0.43731215\n",
      "Iteration 66, loss = 0.43711593\n",
      "Iteration 67, loss = 0.43698287\n",
      "Iteration 68, loss = 0.43684729\n",
      "Iteration 69, loss = 0.43667642\n",
      "Iteration 70, loss = 0.43664871\n",
      "Iteration 71, loss = 0.43633241\n",
      "Iteration 72, loss = 0.43622531\n",
      "Iteration 73, loss = 0.43599286\n",
      "Iteration 74, loss = 0.43593325\n",
      "Iteration 75, loss = 0.43566227\n",
      "Iteration 76, loss = 0.43559704\n",
      "Iteration 77, loss = 0.43550850\n",
      "Iteration 78, loss = 0.43540935\n",
      "Iteration 79, loss = 0.43508817\n",
      "Iteration 80, loss = 0.43498324\n",
      "Iteration 81, loss = 0.43483575\n",
      "Iteration 82, loss = 0.43463894\n",
      "Iteration 83, loss = 0.43452464\n",
      "Iteration 84, loss = 0.43442337\n",
      "Iteration 85, loss = 0.43427281\n",
      "Iteration 86, loss = 0.43404408\n",
      "Iteration 87, loss = 0.43437894\n",
      "Iteration 88, loss = 0.43391827\n",
      "Iteration 89, loss = 0.43365227\n",
      "Iteration 90, loss = 0.43345660\n",
      "Iteration 91, loss = 0.43330284\n",
      "Iteration 92, loss = 0.43331455\n",
      "Iteration 93, loss = 0.43298806\n",
      "Iteration 94, loss = 0.43288999\n",
      "Iteration 95, loss = 0.43268480\n",
      "Iteration 96, loss = 0.43258193\n",
      "Iteration 97, loss = 0.43247267\n",
      "Iteration 98, loss = 0.43219126\n",
      "Iteration 99, loss = 0.43207037\n",
      "Iteration 100, loss = 0.43180074\n",
      "Iteration 101, loss = 0.43210346\n",
      "Iteration 102, loss = 0.43166359\n",
      "Iteration 103, loss = 0.43156755\n",
      "Iteration 104, loss = 0.43134268\n",
      "Iteration 105, loss = 0.43116859\n",
      "Iteration 106, loss = 0.43091780\n",
      "Iteration 107, loss = 0.43087945\n",
      "Iteration 108, loss = 0.43078205\n",
      "Iteration 109, loss = 0.43066133\n",
      "Iteration 110, loss = 0.43047258\n",
      "Iteration 111, loss = 0.43009160\n",
      "Iteration 112, loss = 0.43007089\n",
      "Iteration 113, loss = 0.42976479\n",
      "Iteration 114, loss = 0.42961783\n",
      "Iteration 115, loss = 0.42957188\n",
      "Iteration 116, loss = 0.42937008\n",
      "Iteration 117, loss = 0.42916578\n",
      "Iteration 118, loss = 0.42904058\n",
      "Iteration 119, loss = 0.42900193\n",
      "Iteration 120, loss = 0.42890035\n",
      "Iteration 121, loss = 0.42851155\n",
      "Iteration 122, loss = 0.42839176\n",
      "Iteration 123, loss = 0.42821292\n",
      "Iteration 124, loss = 0.42814092\n",
      "Iteration 125, loss = 0.42786854\n",
      "Iteration 126, loss = 0.42794134\n",
      "Iteration 127, loss = 0.42746391\n",
      "Iteration 128, loss = 0.42736059\n",
      "Iteration 129, loss = 0.42710385\n",
      "Iteration 130, loss = 0.42699705\n",
      "Iteration 131, loss = 0.42680862\n",
      "Iteration 132, loss = 0.42663493\n",
      "Iteration 133, loss = 0.42660387\n",
      "Iteration 134, loss = 0.42628320\n",
      "Iteration 135, loss = 0.42618620\n",
      "Iteration 136, loss = 0.42599874\n",
      "Iteration 137, loss = 0.42587389\n",
      "Iteration 138, loss = 0.42544384\n",
      "Iteration 139, loss = 0.42554535\n",
      "Iteration 140, loss = 0.42529328\n",
      "Iteration 141, loss = 0.42511950\n",
      "Iteration 142, loss = 0.42486468\n",
      "Iteration 143, loss = 0.42477743\n",
      "Iteration 144, loss = 0.42460849\n",
      "Iteration 145, loss = 0.42438159\n",
      "Iteration 146, loss = 0.42410723\n",
      "Iteration 147, loss = 0.42404585\n",
      "Iteration 148, loss = 0.42378673\n",
      "Iteration 149, loss = 0.42354494\n",
      "Iteration 150, loss = 0.42344466\n",
      "Iteration 151, loss = 0.42322895\n",
      "Iteration 152, loss = 0.42316702\n",
      "Iteration 153, loss = 0.42321764\n",
      "Iteration 154, loss = 0.42264602\n",
      "Iteration 155, loss = 0.42266998\n",
      "Iteration 156, loss = 0.42226133\n",
      "Iteration 157, loss = 0.42213302\n",
      "Iteration 158, loss = 0.42200521\n",
      "Iteration 159, loss = 0.42173851\n",
      "Iteration 160, loss = 0.42154065\n",
      "Iteration 161, loss = 0.42129683\n",
      "Iteration 162, loss = 0.42108881\n",
      "Iteration 163, loss = 0.42097539\n",
      "Iteration 164, loss = 0.42080189\n",
      "Iteration 165, loss = 0.42053794\n",
      "Iteration 166, loss = 0.42046392\n",
      "Iteration 167, loss = 0.42021605\n",
      "Iteration 168, loss = 0.42002956\n",
      "Iteration 169, loss = 0.41988228\n",
      "Iteration 170, loss = 0.41970214\n",
      "Iteration 171, loss = 0.41931952\n",
      "Iteration 172, loss = 0.41916324\n",
      "Iteration 173, loss = 0.41902658\n",
      "Iteration 174, loss = 0.41890076\n",
      "Iteration 175, loss = 0.41875271\n",
      "Iteration 176, loss = 0.41850189\n",
      "Iteration 177, loss = 0.41820885\n",
      "Iteration 178, loss = 0.41797215\n",
      "Iteration 179, loss = 0.41786933\n",
      "Iteration 180, loss = 0.41762314\n",
      "Iteration 181, loss = 0.41735449\n",
      "Iteration 182, loss = 0.41721046\n",
      "Iteration 183, loss = 0.41706827\n",
      "Iteration 184, loss = 0.41695937\n",
      "Iteration 185, loss = 0.41681137\n",
      "Iteration 186, loss = 0.41651337\n",
      "Iteration 187, loss = 0.41621537\n",
      "Iteration 188, loss = 0.41598000\n",
      "Iteration 189, loss = 0.41579710\n",
      "Iteration 190, loss = 0.41553623\n",
      "Iteration 191, loss = 0.41538570\n",
      "Iteration 192, loss = 0.41533880\n",
      "Iteration 193, loss = 0.41488422\n",
      "Iteration 194, loss = 0.41482406\n",
      "Iteration 195, loss = 0.41476370\n",
      "Iteration 196, loss = 0.41430657\n",
      "Iteration 197, loss = 0.41439803\n",
      "Iteration 198, loss = 0.41410582\n",
      "Iteration 199, loss = 0.41397873\n",
      "Iteration 200, loss = 0.41379253\n",
      "Iteration 1, loss = 0.75059882\n",
      "Iteration 2, loss = 0.70185336\n",
      "Iteration 3, loss = 0.65716935\n",
      "Iteration 4, loss = 0.62134349\n",
      "Iteration 5, loss = 0.59154412\n",
      "Iteration 6, loss = 0.56580888\n",
      "Iteration 7, loss = 0.54352641\n",
      "Iteration 8, loss = 0.52573882\n",
      "Iteration 9, loss = 0.50906111\n",
      "Iteration 10, loss = 0.49547423\n",
      "Iteration 11, loss = 0.48334743\n",
      "Iteration 12, loss = 0.47312484\n",
      "Iteration 13, loss = 0.46301579\n",
      "Iteration 14, loss = 0.45514618\n",
      "Iteration 15, loss = 0.44805645\n",
      "Iteration 16, loss = 0.44179239\n",
      "Iteration 17, loss = 0.43627220\n",
      "Iteration 18, loss = 0.43242312\n",
      "Iteration 19, loss = 0.42835825\n",
      "Iteration 20, loss = 0.42506724\n",
      "Iteration 21, loss = 0.42249417\n",
      "Iteration 22, loss = 0.41994577\n",
      "Iteration 23, loss = 0.41838065\n",
      "Iteration 24, loss = 0.41671992\n",
      "Iteration 25, loss = 0.41561368\n",
      "Iteration 26, loss = 0.41455339\n",
      "Iteration 27, loss = 0.41358739\n",
      "Iteration 28, loss = 0.41290530\n",
      "Iteration 29, loss = 0.41228221\n",
      "Iteration 30, loss = 0.41173714\n",
      "Iteration 31, loss = 0.41129039\n",
      "Iteration 32, loss = 0.41107065\n",
      "Iteration 33, loss = 0.41070955\n",
      "Iteration 34, loss = 0.41036943\n",
      "Iteration 35, loss = 0.41015844\n",
      "Iteration 36, loss = 0.40996685\n",
      "Iteration 37, loss = 0.40972025\n",
      "Iteration 38, loss = 0.40954090\n",
      "Iteration 39, loss = 0.40936402\n",
      "Iteration 40, loss = 0.40924512\n",
      "Iteration 41, loss = 0.40909611\n",
      "Iteration 42, loss = 0.40900932\n",
      "Iteration 43, loss = 0.40890225\n",
      "Iteration 44, loss = 0.40873070\n",
      "Iteration 45, loss = 0.40857681\n",
      "Iteration 46, loss = 0.40849157\n",
      "Iteration 47, loss = 0.40827772\n",
      "Iteration 48, loss = 0.40813939\n",
      "Iteration 49, loss = 0.40800058\n",
      "Iteration 50, loss = 0.40786834\n",
      "Iteration 51, loss = 0.40780852\n",
      "Iteration 52, loss = 0.40767358\n",
      "Iteration 53, loss = 0.40745704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.40741879\n",
      "Iteration 55, loss = 0.40725777\n",
      "Iteration 56, loss = 0.40713857\n",
      "Iteration 57, loss = 0.40709515\n",
      "Iteration 58, loss = 0.40701253\n",
      "Iteration 59, loss = 0.40670417\n",
      "Iteration 60, loss = 0.40658414\n",
      "Iteration 61, loss = 0.40667145\n",
      "Iteration 62, loss = 0.40671006\n",
      "Iteration 63, loss = 0.40640868\n",
      "Iteration 64, loss = 0.40621627\n",
      "Iteration 65, loss = 0.40593647\n",
      "Iteration 66, loss = 0.40590738\n",
      "Iteration 67, loss = 0.40569098\n",
      "Iteration 68, loss = 0.40573825\n",
      "Iteration 69, loss = 0.40545939\n",
      "Iteration 70, loss = 0.40540022\n",
      "Iteration 71, loss = 0.40522456\n",
      "Iteration 72, loss = 0.40513561\n",
      "Iteration 73, loss = 0.40496209\n",
      "Iteration 74, loss = 0.40485506\n",
      "Iteration 75, loss = 0.40473799\n",
      "Iteration 76, loss = 0.40467452\n",
      "Iteration 77, loss = 0.40446178\n",
      "Iteration 78, loss = 0.40444110\n",
      "Iteration 79, loss = 0.40417251\n",
      "Iteration 80, loss = 0.40411431\n",
      "Iteration 81, loss = 0.40401185\n",
      "Iteration 82, loss = 0.40382950\n",
      "Iteration 83, loss = 0.40385110\n",
      "Iteration 84, loss = 0.40363458\n",
      "Iteration 85, loss = 0.40344860\n",
      "Iteration 86, loss = 0.40349231\n",
      "Iteration 87, loss = 0.40320377\n",
      "Iteration 88, loss = 0.40321318\n",
      "Iteration 89, loss = 0.40292275\n",
      "Iteration 90, loss = 0.40284158\n",
      "Iteration 91, loss = 0.40279261\n",
      "Iteration 92, loss = 0.40261731\n",
      "Iteration 93, loss = 0.40242067\n",
      "Iteration 94, loss = 0.40236966\n",
      "Iteration 95, loss = 0.40211649\n",
      "Iteration 96, loss = 0.40231849\n",
      "Iteration 97, loss = 0.40188459\n",
      "Iteration 98, loss = 0.40179414\n",
      "Iteration 99, loss = 0.40162675\n",
      "Iteration 100, loss = 0.40164096\n",
      "Iteration 101, loss = 0.40137503\n",
      "Iteration 102, loss = 0.40127307\n",
      "Iteration 103, loss = 0.40105153\n",
      "Iteration 104, loss = 0.40101934\n",
      "Iteration 105, loss = 0.40079751\n",
      "Iteration 106, loss = 0.40069263\n",
      "Iteration 107, loss = 0.40057961\n",
      "Iteration 108, loss = 0.40059618\n",
      "Iteration 109, loss = 0.40028007\n",
      "Iteration 110, loss = 0.40023516\n",
      "Iteration 111, loss = 0.40003327\n",
      "Iteration 112, loss = 0.39980762\n",
      "Iteration 113, loss = 0.39978549\n",
      "Iteration 114, loss = 0.39952728\n",
      "Iteration 115, loss = 0.39944652\n",
      "Iteration 116, loss = 0.39932019\n",
      "Iteration 117, loss = 0.39910597\n",
      "Iteration 118, loss = 0.39904583\n",
      "Iteration 119, loss = 0.39888179\n",
      "Iteration 120, loss = 0.39870887\n",
      "Iteration 121, loss = 0.39868819\n",
      "Iteration 122, loss = 0.39841816\n",
      "Iteration 123, loss = 0.39838528\n",
      "Iteration 124, loss = 0.39834342\n",
      "Iteration 125, loss = 0.39813186\n",
      "Iteration 126, loss = 0.39786993\n",
      "Iteration 127, loss = 0.39772271\n",
      "Iteration 128, loss = 0.39756997\n",
      "Iteration 129, loss = 0.39740998\n",
      "Iteration 130, loss = 0.39725857\n",
      "Iteration 131, loss = 0.39726220\n",
      "Iteration 132, loss = 0.39694736\n",
      "Iteration 133, loss = 0.39683494\n",
      "Iteration 134, loss = 0.39669379\n",
      "Iteration 135, loss = 0.39648843\n",
      "Iteration 136, loss = 0.39632109\n",
      "Iteration 137, loss = 0.39631614\n",
      "Iteration 138, loss = 0.39602427\n",
      "Iteration 139, loss = 0.39606560\n",
      "Iteration 140, loss = 0.39590184\n",
      "Iteration 141, loss = 0.39563574\n",
      "Iteration 142, loss = 0.39543970\n",
      "Iteration 143, loss = 0.39534643\n",
      "Iteration 144, loss = 0.39512828\n",
      "Iteration 145, loss = 0.39525794\n",
      "Iteration 146, loss = 0.39496593\n",
      "Iteration 147, loss = 0.39469309\n",
      "Iteration 148, loss = 0.39468728\n",
      "Iteration 149, loss = 0.39444000\n",
      "Iteration 150, loss = 0.39422030\n",
      "Iteration 151, loss = 0.39419869\n",
      "Iteration 152, loss = 0.39390423\n",
      "Iteration 153, loss = 0.39377191\n",
      "Iteration 154, loss = 0.39370743\n",
      "Iteration 155, loss = 0.39388269\n",
      "Iteration 156, loss = 0.39321599\n",
      "Iteration 157, loss = 0.39307193\n",
      "Iteration 158, loss = 0.39298066\n",
      "Iteration 159, loss = 0.39277453\n",
      "Iteration 160, loss = 0.39265382\n",
      "Iteration 161, loss = 0.39240792\n",
      "Iteration 162, loss = 0.39241232\n",
      "Iteration 163, loss = 0.39214911\n",
      "Iteration 164, loss = 0.39195712\n",
      "Iteration 165, loss = 0.39192020\n",
      "Iteration 166, loss = 0.39169409\n",
      "Iteration 167, loss = 0.39154018\n",
      "Iteration 168, loss = 0.39130235\n",
      "Iteration 169, loss = 0.39126706\n",
      "Iteration 170, loss = 0.39094379\n",
      "Iteration 171, loss = 0.39073761\n",
      "Iteration 172, loss = 0.39071065\n",
      "Iteration 173, loss = 0.39058784\n",
      "Iteration 174, loss = 0.39028618\n",
      "Iteration 175, loss = 0.39004224\n",
      "Iteration 176, loss = 0.39033649\n",
      "Iteration 177, loss = 0.38991033\n",
      "Iteration 178, loss = 0.38968514\n",
      "Iteration 179, loss = 0.38955451\n",
      "Iteration 180, loss = 0.38926442\n",
      "Iteration 181, loss = 0.38927470\n",
      "Iteration 182, loss = 0.38894793\n",
      "Iteration 183, loss = 0.38889467\n",
      "Iteration 184, loss = 0.38864991\n",
      "Iteration 185, loss = 0.38856590\n",
      "Iteration 186, loss = 0.38828804\n",
      "Iteration 187, loss = 0.38827862\n",
      "Iteration 188, loss = 0.38791542\n",
      "Iteration 189, loss = 0.38773591\n",
      "Iteration 190, loss = 0.38771040\n",
      "Iteration 191, loss = 0.38731295\n",
      "Iteration 192, loss = 0.38728685\n",
      "Iteration 193, loss = 0.38707231\n",
      "Iteration 194, loss = 0.38703658\n",
      "Iteration 195, loss = 0.38689644\n",
      "Iteration 196, loss = 0.38657344\n",
      "Iteration 197, loss = 0.38624512\n",
      "Iteration 198, loss = 0.38622327\n",
      "Iteration 199, loss = 0.38600946\n",
      "Iteration 200, loss = 0.38587031\n",
      "Iteration 1, loss = 0.75773311\n",
      "Iteration 2, loss = 0.71111868\n",
      "Iteration 3, loss = 0.66849798\n",
      "Iteration 4, loss = 0.63485861\n",
      "Iteration 5, loss = 0.60621354\n",
      "Iteration 6, loss = 0.58166065\n",
      "Iteration 7, loss = 0.55981063\n",
      "Iteration 8, loss = 0.54305966\n",
      "Iteration 9, loss = 0.52715629\n",
      "Iteration 10, loss = 0.51420251\n",
      "Iteration 11, loss = 0.50243482\n",
      "Iteration 12, loss = 0.49263085\n",
      "Iteration 13, loss = 0.48322343\n",
      "Iteration 14, loss = 0.47586439\n",
      "Iteration 15, loss = 0.46905042\n",
      "Iteration 16, loss = 0.46330273\n",
      "Iteration 17, loss = 0.45827046\n",
      "Iteration 18, loss = 0.45457718\n",
      "Iteration 19, loss = 0.45087052\n",
      "Iteration 20, loss = 0.44770756\n",
      "Iteration 21, loss = 0.44522973\n",
      "Iteration 22, loss = 0.44300445\n",
      "Iteration 23, loss = 0.44168143\n",
      "Iteration 24, loss = 0.44007716\n",
      "Iteration 25, loss = 0.43910560\n",
      "Iteration 26, loss = 0.43794924\n",
      "Iteration 27, loss = 0.43725169\n",
      "Iteration 28, loss = 0.43660053\n",
      "Iteration 29, loss = 0.43582948\n",
      "Iteration 30, loss = 0.43547372\n",
      "Iteration 31, loss = 0.43489083\n",
      "Iteration 32, loss = 0.43472836\n",
      "Iteration 33, loss = 0.43432479\n",
      "Iteration 34, loss = 0.43395304\n",
      "Iteration 35, loss = 0.43368391\n",
      "Iteration 36, loss = 0.43353503\n",
      "Iteration 37, loss = 0.43322758\n",
      "Iteration 38, loss = 0.43304513\n",
      "Iteration 39, loss = 0.43285767\n",
      "Iteration 40, loss = 0.43279087\n",
      "Iteration 41, loss = 0.43249315\n",
      "Iteration 42, loss = 0.43241548\n",
      "Iteration 43, loss = 0.43227805\n",
      "Iteration 44, loss = 0.43215291\n",
      "Iteration 45, loss = 0.43192415\n",
      "Iteration 46, loss = 0.43180429\n",
      "Iteration 47, loss = 0.43163927\n",
      "Iteration 48, loss = 0.43155216\n",
      "Iteration 49, loss = 0.43144369\n",
      "Iteration 50, loss = 0.43132329\n",
      "Iteration 51, loss = 0.43118332\n",
      "Iteration 52, loss = 0.43100494\n",
      "Iteration 53, loss = 0.43090156\n",
      "Iteration 54, loss = 0.43080553\n",
      "Iteration 55, loss = 0.43064118\n",
      "Iteration 56, loss = 0.43055027\n",
      "Iteration 57, loss = 0.43041159\n",
      "Iteration 58, loss = 0.43030294\n",
      "Iteration 59, loss = 0.43015071\n",
      "Iteration 60, loss = 0.43007904\n",
      "Iteration 61, loss = 0.43002834\n",
      "Iteration 62, loss = 0.42984823\n",
      "Iteration 63, loss = 0.42967068\n",
      "Iteration 64, loss = 0.42963222\n",
      "Iteration 65, loss = 0.42941439\n",
      "Iteration 66, loss = 0.42938349\n",
      "Iteration 67, loss = 0.42913895\n",
      "Iteration 68, loss = 0.42920374\n",
      "Iteration 69, loss = 0.42905046\n",
      "Iteration 70, loss = 0.42886675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.42871075\n",
      "Iteration 72, loss = 0.42853002\n",
      "Iteration 73, loss = 0.42854260\n",
      "Iteration 74, loss = 0.42840931\n",
      "Iteration 75, loss = 0.42822215\n",
      "Iteration 76, loss = 0.42813723\n",
      "Iteration 77, loss = 0.42796353\n",
      "Iteration 78, loss = 0.42785723\n",
      "Iteration 79, loss = 0.42767717\n",
      "Iteration 80, loss = 0.42756956\n",
      "Iteration 81, loss = 0.42767186\n",
      "Iteration 82, loss = 0.42732695\n",
      "Iteration 83, loss = 0.42749927\n",
      "Iteration 84, loss = 0.42714214\n",
      "Iteration 85, loss = 0.42691956\n",
      "Iteration 86, loss = 0.42682375\n",
      "Iteration 87, loss = 0.42674287\n",
      "Iteration 88, loss = 0.42665768\n",
      "Iteration 89, loss = 0.42646621\n",
      "Iteration 90, loss = 0.42636117\n",
      "Iteration 91, loss = 0.42633748\n",
      "Iteration 92, loss = 0.42609368\n",
      "Iteration 93, loss = 0.42592447\n",
      "Iteration 94, loss = 0.42615968\n",
      "Iteration 95, loss = 0.42578406\n",
      "Iteration 96, loss = 0.42566170\n",
      "Iteration 97, loss = 0.42552992\n",
      "Iteration 98, loss = 0.42556482\n",
      "Iteration 99, loss = 0.42525837\n",
      "Iteration 100, loss = 0.42511661\n",
      "Iteration 101, loss = 0.42505930\n",
      "Iteration 102, loss = 0.42490192\n",
      "Iteration 103, loss = 0.42480243\n",
      "Iteration 104, loss = 0.42464439\n",
      "Iteration 105, loss = 0.42467443\n",
      "Iteration 106, loss = 0.42435917\n",
      "Iteration 107, loss = 0.42419747\n",
      "Iteration 108, loss = 0.42411584\n",
      "Iteration 109, loss = 0.42398931\n",
      "Iteration 110, loss = 0.42387911\n",
      "Iteration 111, loss = 0.42378321\n",
      "Iteration 112, loss = 0.42351883\n",
      "Iteration 113, loss = 0.42353030\n",
      "Iteration 114, loss = 0.42328508\n",
      "Iteration 115, loss = 0.42320580\n",
      "Iteration 116, loss = 0.42323028\n",
      "Iteration 117, loss = 0.42294807\n",
      "Iteration 118, loss = 0.42284444\n",
      "Iteration 119, loss = 0.42274531\n",
      "Iteration 120, loss = 0.42251179\n",
      "Iteration 121, loss = 0.42253068\n",
      "Iteration 122, loss = 0.42231694\n",
      "Iteration 123, loss = 0.42215762\n",
      "Iteration 124, loss = 0.42213529\n",
      "Iteration 125, loss = 0.42187993\n",
      "Iteration 126, loss = 0.42179932\n",
      "Iteration 127, loss = 0.42157889\n",
      "Iteration 128, loss = 0.42136979\n",
      "Iteration 129, loss = 0.42145251\n",
      "Iteration 130, loss = 0.42122691\n",
      "Iteration 131, loss = 0.42109718\n",
      "Iteration 132, loss = 0.42088908\n",
      "Iteration 133, loss = 0.42085156\n",
      "Iteration 134, loss = 0.42064624\n",
      "Iteration 135, loss = 0.42053261\n",
      "Iteration 136, loss = 0.42031929\n",
      "Iteration 137, loss = 0.42027900\n",
      "Iteration 138, loss = 0.42003598\n",
      "Iteration 139, loss = 0.42033103\n",
      "Iteration 140, loss = 0.41985143\n",
      "Iteration 141, loss = 0.41987656\n",
      "Iteration 142, loss = 0.41951057\n",
      "Iteration 143, loss = 0.41936831\n",
      "Iteration 144, loss = 0.41926141\n",
      "Iteration 145, loss = 0.41915652\n",
      "Iteration 146, loss = 0.41908944\n",
      "Iteration 147, loss = 0.41887196\n",
      "Iteration 148, loss = 0.41886697\n",
      "Iteration 149, loss = 0.41855709\n",
      "Iteration 150, loss = 0.41831136\n",
      "Iteration 151, loss = 0.41832659\n",
      "Iteration 152, loss = 0.41811503\n",
      "Iteration 153, loss = 0.41787440\n",
      "Iteration 154, loss = 0.41798390\n",
      "Iteration 155, loss = 0.41814494\n",
      "Iteration 156, loss = 0.41756239\n",
      "Iteration 157, loss = 0.41733810\n",
      "Iteration 158, loss = 0.41729654\n",
      "Iteration 159, loss = 0.41733854\n",
      "Iteration 160, loss = 0.41688380\n",
      "Iteration 161, loss = 0.41677256\n",
      "Iteration 162, loss = 0.41677047\n",
      "Iteration 163, loss = 0.41655084\n",
      "Iteration 164, loss = 0.41627631\n",
      "Iteration 165, loss = 0.41620852\n",
      "Iteration 166, loss = 0.41609374\n",
      "Iteration 167, loss = 0.41598005\n",
      "Iteration 168, loss = 0.41570093\n",
      "Iteration 169, loss = 0.41556460\n",
      "Iteration 170, loss = 0.41545598\n",
      "Iteration 171, loss = 0.41539921\n",
      "Iteration 172, loss = 0.41512143\n",
      "Iteration 173, loss = 0.41509235\n",
      "Iteration 174, loss = 0.41486609\n",
      "Iteration 175, loss = 0.41477105\n",
      "Iteration 176, loss = 0.41470646\n",
      "Iteration 177, loss = 0.41439919\n",
      "Iteration 178, loss = 0.41432736\n",
      "Iteration 179, loss = 0.41405608\n",
      "Iteration 180, loss = 0.41397211\n",
      "Iteration 181, loss = 0.41386674\n",
      "Iteration 182, loss = 0.41349744\n",
      "Iteration 183, loss = 0.41337897\n",
      "Iteration 184, loss = 0.41327256\n",
      "Iteration 185, loss = 0.41314157\n",
      "Iteration 186, loss = 0.41291362\n",
      "Iteration 187, loss = 0.41310576\n",
      "Iteration 188, loss = 0.41272026\n",
      "Iteration 189, loss = 0.41252546\n",
      "Iteration 190, loss = 0.41236039\n",
      "Iteration 191, loss = 0.41210300\n",
      "Iteration 192, loss = 0.41205261\n",
      "Iteration 193, loss = 0.41182578\n",
      "Iteration 194, loss = 0.41171543\n",
      "Iteration 195, loss = 0.41169119\n",
      "Iteration 196, loss = 0.41139283\n",
      "Iteration 197, loss = 0.41113737\n",
      "Iteration 198, loss = 0.41101700\n",
      "Iteration 199, loss = 0.41087205\n",
      "Iteration 200, loss = 0.41072496\n",
      "Iteration 1, loss = 0.75866550\n",
      "Iteration 2, loss = 0.71215075\n",
      "Iteration 3, loss = 0.67225003\n",
      "Iteration 4, loss = 0.63864933\n",
      "Iteration 5, loss = 0.60981390\n",
      "Iteration 6, loss = 0.58404922\n",
      "Iteration 7, loss = 0.56346513\n",
      "Iteration 8, loss = 0.54448675\n",
      "Iteration 9, loss = 0.52757460\n",
      "Iteration 10, loss = 0.51276425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.50002920\n",
      "Iteration 12, loss = 0.48849135\n",
      "Iteration 13, loss = 0.47861860\n",
      "Iteration 14, loss = 0.46956556\n",
      "Iteration 15, loss = 0.46200475\n",
      "Iteration 16, loss = 0.45573561\n",
      "Iteration 17, loss = 0.45013461\n",
      "Iteration 18, loss = 0.44589886\n",
      "Iteration 19, loss = 0.44170258\n",
      "Iteration 20, loss = 0.43786847\n",
      "Iteration 21, loss = 0.43523547\n",
      "Iteration 22, loss = 0.43271957\n",
      "Iteration 23, loss = 0.43052526\n",
      "Iteration 24, loss = 0.42892139\n",
      "Iteration 25, loss = 0.42753517\n",
      "Iteration 26, loss = 0.42626465\n",
      "Iteration 27, loss = 0.42510531\n",
      "Iteration 28, loss = 0.42441971\n",
      "Iteration 29, loss = 0.42348229\n",
      "Iteration 30, loss = 0.42293235\n",
      "Iteration 31, loss = 0.42246017\n",
      "Iteration 32, loss = 0.42193722\n",
      "Iteration 33, loss = 0.42178654\n",
      "Iteration 34, loss = 0.42115060\n",
      "Iteration 35, loss = 0.42093556\n",
      "Iteration 36, loss = 0.42051502\n",
      "Iteration 37, loss = 0.42035980\n",
      "Iteration 38, loss = 0.42003452\n",
      "Iteration 39, loss = 0.41968803\n",
      "Iteration 40, loss = 0.41958919\n",
      "Iteration 41, loss = 0.41931230\n",
      "Iteration 42, loss = 0.41916094\n",
      "Iteration 43, loss = 0.41882581\n",
      "Iteration 44, loss = 0.41885927\n",
      "Iteration 45, loss = 0.41850529\n",
      "Iteration 46, loss = 0.41828131\n",
      "Iteration 47, loss = 0.41807987\n",
      "Iteration 48, loss = 0.41800313\n",
      "Iteration 49, loss = 0.41777344\n",
      "Iteration 50, loss = 0.41761106\n",
      "Iteration 51, loss = 0.41747639\n",
      "Iteration 52, loss = 0.41736725\n",
      "Iteration 53, loss = 0.41710463\n",
      "Iteration 54, loss = 0.41691570\n",
      "Iteration 55, loss = 0.41677075\n",
      "Iteration 56, loss = 0.41671112\n",
      "Iteration 57, loss = 0.41660771\n",
      "Iteration 58, loss = 0.41639661\n",
      "Iteration 59, loss = 0.41617343\n",
      "Iteration 60, loss = 0.41606611\n",
      "Iteration 61, loss = 0.41590887\n",
      "Iteration 62, loss = 0.41577556\n",
      "Iteration 63, loss = 0.41560566\n",
      "Iteration 64, loss = 0.41552122\n",
      "Iteration 65, loss = 0.41530800\n",
      "Iteration 66, loss = 0.41528876\n",
      "Iteration 67, loss = 0.41521300\n",
      "Iteration 68, loss = 0.41496059\n",
      "Iteration 69, loss = 0.41474067\n",
      "Iteration 70, loss = 0.41476135\n",
      "Iteration 71, loss = 0.41448797\n",
      "Iteration 72, loss = 0.41436851\n",
      "Iteration 73, loss = 0.41416515\n",
      "Iteration 74, loss = 0.41406571\n",
      "Iteration 75, loss = 0.41387301\n",
      "Iteration 76, loss = 0.41391449\n",
      "Iteration 77, loss = 0.41354118\n",
      "Iteration 78, loss = 0.41341584\n",
      "Iteration 79, loss = 0.41329034\n",
      "Iteration 80, loss = 0.41323613\n",
      "Iteration 81, loss = 0.41307162\n",
      "Iteration 82, loss = 0.41294470\n",
      "Iteration 83, loss = 0.41276079\n",
      "Iteration 84, loss = 0.41262236\n",
      "Iteration 85, loss = 0.41262769\n",
      "Iteration 86, loss = 0.41246693\n",
      "Iteration 87, loss = 0.41238731\n",
      "Iteration 88, loss = 0.41204798\n",
      "Iteration 89, loss = 0.41192391\n",
      "Iteration 90, loss = 0.41190413\n",
      "Iteration 91, loss = 0.41162447\n",
      "Iteration 92, loss = 0.41182469\n",
      "Iteration 93, loss = 0.41140024\n",
      "Iteration 94, loss = 0.41166981\n",
      "Iteration 95, loss = 0.41107837\n",
      "Iteration 96, loss = 0.41088102\n",
      "Iteration 97, loss = 0.41075670\n",
      "Iteration 98, loss = 0.41055664\n",
      "Iteration 99, loss = 0.41046474\n",
      "Iteration 100, loss = 0.41027380\n",
      "Iteration 101, loss = 0.41046419\n",
      "Iteration 102, loss = 0.41005929\n",
      "Iteration 103, loss = 0.40987564\n",
      "Iteration 104, loss = 0.40985720\n",
      "Iteration 105, loss = 0.40957615\n",
      "Iteration 106, loss = 0.40955880\n",
      "Iteration 107, loss = 0.40933573\n",
      "Iteration 108, loss = 0.40916482\n",
      "Iteration 109, loss = 0.40901594\n",
      "Iteration 110, loss = 0.40881346\n",
      "Iteration 111, loss = 0.40871029\n",
      "Iteration 112, loss = 0.40863391\n",
      "Iteration 113, loss = 0.40836218\n",
      "Iteration 114, loss = 0.40828822\n",
      "Iteration 115, loss = 0.40808597\n",
      "Iteration 116, loss = 0.40783777\n",
      "Iteration 117, loss = 0.40770434\n",
      "Iteration 118, loss = 0.40759348\n",
      "Iteration 119, loss = 0.40760114\n",
      "Iteration 120, loss = 0.40743505\n",
      "Iteration 121, loss = 0.40718214\n",
      "Iteration 122, loss = 0.40703084\n",
      "Iteration 123, loss = 0.40678078\n",
      "Iteration 124, loss = 0.40675907\n",
      "Iteration 125, loss = 0.40662223\n",
      "Iteration 126, loss = 0.40634582\n",
      "Iteration 127, loss = 0.40614200\n",
      "Iteration 128, loss = 0.40604010\n",
      "Iteration 129, loss = 0.40594040\n",
      "Iteration 130, loss = 0.40575849\n",
      "Iteration 131, loss = 0.40551790\n",
      "Iteration 132, loss = 0.40542525\n",
      "Iteration 133, loss = 0.40525029\n",
      "Iteration 134, loss = 0.40508768\n",
      "Iteration 135, loss = 0.40480170\n",
      "Iteration 136, loss = 0.40504441\n",
      "Iteration 137, loss = 0.40448144\n",
      "Iteration 138, loss = 0.40436769\n",
      "Iteration 139, loss = 0.40435860\n",
      "Iteration 140, loss = 0.40404477\n",
      "Iteration 141, loss = 0.40389373\n",
      "Iteration 142, loss = 0.40367911\n",
      "Iteration 143, loss = 0.40352963\n",
      "Iteration 144, loss = 0.40346391\n",
      "Iteration 145, loss = 0.40331404\n",
      "Iteration 146, loss = 0.40308184\n",
      "Iteration 147, loss = 0.40291848\n",
      "Iteration 148, loss = 0.40281518\n",
      "Iteration 149, loss = 0.40246726\n",
      "Iteration 150, loss = 0.40228842\n",
      "Iteration 151, loss = 0.40215069\n",
      "Iteration 152, loss = 0.40202599\n",
      "Iteration 153, loss = 0.40236467\n",
      "Iteration 154, loss = 0.40176482\n",
      "Iteration 155, loss = 0.40131131\n",
      "Iteration 156, loss = 0.40134543\n",
      "Iteration 157, loss = 0.40111660\n",
      "Iteration 158, loss = 0.40090897\n",
      "Iteration 159, loss = 0.40073924\n",
      "Iteration 160, loss = 0.40065069\n",
      "Iteration 161, loss = 0.40043357\n",
      "Iteration 162, loss = 0.40030111\n",
      "Iteration 163, loss = 0.40003598\n",
      "Iteration 164, loss = 0.39999519\n",
      "Iteration 165, loss = 0.39974283\n",
      "Iteration 166, loss = 0.39952440\n",
      "Iteration 167, loss = 0.39921130\n",
      "Iteration 168, loss = 0.39909867\n",
      "Iteration 169, loss = 0.39889773\n",
      "Iteration 170, loss = 0.39887224\n",
      "Iteration 171, loss = 0.39855846\n",
      "Iteration 172, loss = 0.39855944\n",
      "Iteration 173, loss = 0.39832772\n",
      "Iteration 174, loss = 0.39811458\n",
      "Iteration 175, loss = 0.39786087\n",
      "Iteration 176, loss = 0.39770219\n",
      "Iteration 177, loss = 0.39739019\n",
      "Iteration 178, loss = 0.39745035\n",
      "Iteration 179, loss = 0.39708537\n",
      "Iteration 180, loss = 0.39684959\n",
      "Iteration 181, loss = 0.39661441\n",
      "Iteration 182, loss = 0.39647841\n",
      "Iteration 183, loss = 0.39617859\n",
      "Iteration 184, loss = 0.39606664\n",
      "Iteration 185, loss = 0.39610030\n",
      "Iteration 186, loss = 0.39578556\n",
      "Iteration 187, loss = 0.39560301\n",
      "Iteration 188, loss = 0.39537361\n",
      "Iteration 189, loss = 0.39519818\n",
      "Iteration 190, loss = 0.39490031\n",
      "Iteration 191, loss = 0.39477811\n",
      "Iteration 192, loss = 0.39458245\n",
      "Iteration 193, loss = 0.39425324\n",
      "Iteration 194, loss = 0.39430555\n",
      "Iteration 195, loss = 0.39409019\n",
      "Iteration 196, loss = 0.39377391\n",
      "Iteration 197, loss = 0.39383786\n",
      "Iteration 198, loss = 0.39343179\n",
      "Iteration 199, loss = 0.39324421\n",
      "Iteration 200, loss = 0.39285246\n",
      "Iteration 1, loss = 0.76102973\n",
      "Iteration 2, loss = 0.71335948\n",
      "Iteration 3, loss = 0.67451689\n",
      "Iteration 4, loss = 0.64062580\n",
      "Iteration 5, loss = 0.61103367\n",
      "Iteration 6, loss = 0.58587898\n",
      "Iteration 7, loss = 0.56456537\n",
      "Iteration 8, loss = 0.54508638\n",
      "Iteration 9, loss = 0.52789291\n",
      "Iteration 10, loss = 0.51270164\n",
      "Iteration 11, loss = 0.49984041\n",
      "Iteration 12, loss = 0.48803914\n",
      "Iteration 13, loss = 0.47795509\n",
      "Iteration 14, loss = 0.46857597\n",
      "Iteration 15, loss = 0.46098654\n",
      "Iteration 16, loss = 0.45426567\n",
      "Iteration 17, loss = 0.44868992\n",
      "Iteration 18, loss = 0.44412972\n",
      "Iteration 19, loss = 0.43939352\n",
      "Iteration 20, loss = 0.43599855\n",
      "Iteration 21, loss = 0.43283484\n",
      "Iteration 22, loss = 0.43024266\n",
      "Iteration 23, loss = 0.42780428\n",
      "Iteration 24, loss = 0.42598385\n",
      "Iteration 25, loss = 0.42453129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.42306100\n",
      "Iteration 27, loss = 0.42189502\n",
      "Iteration 28, loss = 0.42106734\n",
      "Iteration 29, loss = 0.42019139\n",
      "Iteration 30, loss = 0.41940534\n",
      "Iteration 31, loss = 0.41899537\n",
      "Iteration 32, loss = 0.41851183\n",
      "Iteration 33, loss = 0.41825019\n",
      "Iteration 34, loss = 0.41762510\n",
      "Iteration 35, loss = 0.41722541\n",
      "Iteration 36, loss = 0.41710053\n",
      "Iteration 37, loss = 0.41678392\n",
      "Iteration 38, loss = 0.41643087\n",
      "Iteration 39, loss = 0.41622047\n",
      "Iteration 40, loss = 0.41615346\n",
      "Iteration 41, loss = 0.41585623\n",
      "Iteration 42, loss = 0.41571533\n",
      "Iteration 43, loss = 0.41551311\n",
      "Iteration 44, loss = 0.41544439\n",
      "Iteration 45, loss = 0.41514566\n",
      "Iteration 46, loss = 0.41499843\n",
      "Iteration 47, loss = 0.41481583\n",
      "Iteration 48, loss = 0.41471858\n",
      "Iteration 49, loss = 0.41457358\n",
      "Iteration 50, loss = 0.41438874\n",
      "Iteration 51, loss = 0.41427453\n",
      "Iteration 52, loss = 0.41420604\n",
      "Iteration 53, loss = 0.41400236\n",
      "Iteration 54, loss = 0.41385125\n",
      "Iteration 55, loss = 0.41372386\n",
      "Iteration 56, loss = 0.41358832\n",
      "Iteration 57, loss = 0.41351895\n",
      "Iteration 58, loss = 0.41345845\n",
      "Iteration 59, loss = 0.41321316\n",
      "Iteration 60, loss = 0.41305805\n",
      "Iteration 61, loss = 0.41301049\n",
      "Iteration 62, loss = 0.41284586\n",
      "Iteration 63, loss = 0.41280348\n",
      "Iteration 64, loss = 0.41275043\n",
      "Iteration 65, loss = 0.41254042\n",
      "Iteration 66, loss = 0.41242490\n",
      "Iteration 67, loss = 0.41227182\n",
      "Iteration 68, loss = 0.41209639\n",
      "Iteration 69, loss = 0.41197806\n",
      "Iteration 70, loss = 0.41195380\n",
      "Iteration 71, loss = 0.41176114\n",
      "Iteration 72, loss = 0.41162848\n",
      "Iteration 73, loss = 0.41140919\n",
      "Iteration 74, loss = 0.41138917\n",
      "Iteration 75, loss = 0.41122166\n",
      "Iteration 76, loss = 0.41112180\n",
      "Iteration 77, loss = 0.41099073\n",
      "Iteration 78, loss = 0.41082340\n",
      "Iteration 79, loss = 0.41066416\n",
      "Iteration 80, loss = 0.41058234\n",
      "Iteration 81, loss = 0.41043938\n",
      "Iteration 82, loss = 0.41030391\n",
      "Iteration 83, loss = 0.41015914\n",
      "Iteration 84, loss = 0.41007118\n",
      "Iteration 85, loss = 0.41006405\n",
      "Iteration 86, loss = 0.40976353\n",
      "Iteration 87, loss = 0.40985977\n",
      "Iteration 88, loss = 0.40963521\n",
      "Iteration 89, loss = 0.40950110\n",
      "Iteration 90, loss = 0.40945623\n",
      "Iteration 91, loss = 0.40917772\n",
      "Iteration 92, loss = 0.40930955\n",
      "Iteration 93, loss = 0.40899296\n",
      "Iteration 94, loss = 0.40893419\n",
      "Iteration 95, loss = 0.40866098\n",
      "Iteration 96, loss = 0.40855173\n",
      "Iteration 97, loss = 0.40854452\n",
      "Iteration 98, loss = 0.40829047\n",
      "Iteration 99, loss = 0.40823235\n",
      "Iteration 100, loss = 0.40793549\n",
      "Iteration 101, loss = 0.40815081\n",
      "Iteration 102, loss = 0.40788860\n",
      "Iteration 103, loss = 0.40765118\n",
      "Iteration 104, loss = 0.40762857\n",
      "Iteration 105, loss = 0.40733803\n",
      "Iteration 106, loss = 0.40726686\n",
      "Iteration 107, loss = 0.40717330\n",
      "Iteration 108, loss = 0.40713245\n",
      "Iteration 109, loss = 0.40699898\n",
      "Iteration 110, loss = 0.40677894\n",
      "Iteration 111, loss = 0.40652768\n",
      "Iteration 112, loss = 0.40647271\n",
      "Iteration 113, loss = 0.40631232\n",
      "Iteration 114, loss = 0.40622878\n",
      "Iteration 115, loss = 0.40607812\n",
      "Iteration 116, loss = 0.40586570\n",
      "Iteration 117, loss = 0.40574029\n",
      "Iteration 118, loss = 0.40569823\n",
      "Iteration 119, loss = 0.40565648\n",
      "Iteration 120, loss = 0.40542258\n",
      "Iteration 121, loss = 0.40531875\n",
      "Iteration 122, loss = 0.40515812\n",
      "Iteration 123, loss = 0.40490350\n",
      "Iteration 124, loss = 0.40486177\n",
      "Iteration 125, loss = 0.40475241\n",
      "Iteration 126, loss = 0.40457052\n",
      "Iteration 127, loss = 0.40444494\n",
      "Iteration 128, loss = 0.40424774\n",
      "Iteration 129, loss = 0.40412900\n",
      "Iteration 130, loss = 0.40405530\n",
      "Iteration 131, loss = 0.40377111\n",
      "Iteration 132, loss = 0.40372539\n",
      "Iteration 133, loss = 0.40360076\n",
      "Iteration 134, loss = 0.40342333\n",
      "Iteration 135, loss = 0.40322976\n",
      "Iteration 136, loss = 0.40316232\n",
      "Iteration 137, loss = 0.40324725\n",
      "Iteration 138, loss = 0.40294143\n",
      "Iteration 139, loss = 0.40279834\n",
      "Iteration 140, loss = 0.40259469\n",
      "Iteration 141, loss = 0.40245997\n",
      "Iteration 142, loss = 0.40219258\n",
      "Iteration 143, loss = 0.40199479\n",
      "Iteration 144, loss = 0.40200369\n",
      "Iteration 145, loss = 0.40188756\n",
      "Iteration 146, loss = 0.40179784\n",
      "Iteration 147, loss = 0.40146174\n",
      "Iteration 148, loss = 0.40140879\n",
      "Iteration 149, loss = 0.40113287\n",
      "Iteration 150, loss = 0.40098148\n",
      "Iteration 151, loss = 0.40082205\n",
      "Iteration 152, loss = 0.40086595\n",
      "Iteration 153, loss = 0.40096015\n",
      "Iteration 154, loss = 0.40039037\n",
      "Iteration 155, loss = 0.40019759\n",
      "Iteration 156, loss = 0.40012931\n",
      "Iteration 157, loss = 0.39989140\n",
      "Iteration 158, loss = 0.39973877\n",
      "Iteration 159, loss = 0.39956657\n",
      "Iteration 160, loss = 0.39949386\n",
      "Iteration 161, loss = 0.39936152\n",
      "Iteration 162, loss = 0.39922795\n",
      "Iteration 163, loss = 0.39901719\n",
      "Iteration 164, loss = 0.39883879\n",
      "Iteration 165, loss = 0.39859439\n",
      "Iteration 166, loss = 0.39850550\n",
      "Iteration 167, loss = 0.39825671\n",
      "Iteration 168, loss = 0.39826962\n",
      "Iteration 169, loss = 0.39801410\n",
      "Iteration 170, loss = 0.39791239\n",
      "Iteration 171, loss = 0.39759754\n",
      "Iteration 172, loss = 0.39750475\n",
      "Iteration 173, loss = 0.39752440\n",
      "Iteration 174, loss = 0.39720084\n",
      "Iteration 175, loss = 0.39711753\n",
      "Iteration 176, loss = 0.39682911\n",
      "Iteration 177, loss = 0.39666696\n",
      "Iteration 178, loss = 0.39653614\n",
      "Iteration 179, loss = 0.39636845\n",
      "Iteration 180, loss = 0.39607924\n",
      "Iteration 181, loss = 0.39591596\n",
      "Iteration 182, loss = 0.39575784\n",
      "Iteration 183, loss = 0.39551617\n",
      "Iteration 184, loss = 0.39535277\n",
      "Iteration 185, loss = 0.39528985\n",
      "Iteration 186, loss = 0.39517835\n",
      "Iteration 187, loss = 0.39482609\n",
      "Iteration 188, loss = 0.39475084\n",
      "Iteration 189, loss = 0.39459206\n",
      "Iteration 190, loss = 0.39442622\n",
      "Iteration 191, loss = 0.39410689\n",
      "Iteration 192, loss = 0.39417338\n",
      "Iteration 193, loss = 0.39366412\n",
      "Iteration 194, loss = 0.39367440\n",
      "Iteration 195, loss = 0.39364216\n",
      "Iteration 196, loss = 0.39317132\n",
      "Iteration 197, loss = 0.39309733\n",
      "Iteration 198, loss = 0.39294544\n",
      "Iteration 199, loss = 0.39283024\n",
      "Iteration 200, loss = 0.39241757\n",
      "Iteration 1, loss = 0.75383256\n",
      "Iteration 2, loss = 0.70743824\n",
      "Iteration 3, loss = 0.66802887\n",
      "Iteration 4, loss = 0.63522922\n",
      "Iteration 5, loss = 0.60695864\n",
      "Iteration 6, loss = 0.58315632\n",
      "Iteration 7, loss = 0.56372096\n",
      "Iteration 8, loss = 0.54583664\n",
      "Iteration 9, loss = 0.53067210\n",
      "Iteration 10, loss = 0.51726475\n",
      "Iteration 11, loss = 0.50589725\n",
      "Iteration 12, loss = 0.49619160\n",
      "Iteration 13, loss = 0.48763850\n",
      "Iteration 14, loss = 0.47971652\n",
      "Iteration 15, loss = 0.47364804\n",
      "Iteration 16, loss = 0.46814269\n",
      "Iteration 17, loss = 0.46422242\n",
      "Iteration 18, loss = 0.46019000\n",
      "Iteration 19, loss = 0.45667638\n",
      "Iteration 20, loss = 0.45446260\n",
      "Iteration 21, loss = 0.45229821\n",
      "Iteration 22, loss = 0.45026701\n",
      "Iteration 23, loss = 0.44879174\n",
      "Iteration 24, loss = 0.44761180\n",
      "Iteration 25, loss = 0.44660445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.44567892\n",
      "Iteration 27, loss = 0.44499255\n",
      "Iteration 28, loss = 0.44456090\n",
      "Iteration 29, loss = 0.44396461\n",
      "Iteration 30, loss = 0.44347928\n",
      "Iteration 31, loss = 0.44317747\n",
      "Iteration 32, loss = 0.44290281\n",
      "Iteration 33, loss = 0.44267279\n",
      "Iteration 34, loss = 0.44232939\n",
      "Iteration 35, loss = 0.44202043\n",
      "Iteration 36, loss = 0.44193769\n",
      "Iteration 37, loss = 0.44170467\n",
      "Iteration 38, loss = 0.44143122\n",
      "Iteration 39, loss = 0.44121576\n",
      "Iteration 40, loss = 0.44116747\n",
      "Iteration 41, loss = 0.44091227\n",
      "Iteration 42, loss = 0.44076784\n",
      "Iteration 43, loss = 0.44055188\n",
      "Iteration 44, loss = 0.44044313\n",
      "Iteration 45, loss = 0.44022916\n",
      "Iteration 46, loss = 0.44008445\n",
      "Iteration 47, loss = 0.43985742\n",
      "Iteration 48, loss = 0.43973477\n",
      "Iteration 49, loss = 0.43962207\n",
      "Iteration 50, loss = 0.43954382\n",
      "Iteration 51, loss = 0.43929369\n",
      "Iteration 52, loss = 0.43916313\n",
      "Iteration 53, loss = 0.43904064\n",
      "Iteration 54, loss = 0.43878905\n",
      "Iteration 55, loss = 0.43867864\n",
      "Iteration 56, loss = 0.43843409\n",
      "Iteration 57, loss = 0.43833016\n",
      "Iteration 58, loss = 0.43848824\n",
      "Iteration 59, loss = 0.43803151\n",
      "Iteration 60, loss = 0.43786819\n",
      "Iteration 61, loss = 0.43776493\n",
      "Iteration 62, loss = 0.43753960\n",
      "Iteration 63, loss = 0.43752413\n",
      "Iteration 64, loss = 0.43737598\n",
      "Iteration 65, loss = 0.43725209\n",
      "Iteration 66, loss = 0.43705572\n",
      "Iteration 67, loss = 0.43692254\n",
      "Iteration 68, loss = 0.43678678\n",
      "Iteration 69, loss = 0.43661575\n",
      "Iteration 70, loss = 0.43658790\n",
      "Iteration 71, loss = 0.43627142\n",
      "Iteration 72, loss = 0.43616414\n",
      "Iteration 73, loss = 0.43593153\n",
      "Iteration 74, loss = 0.43587176\n",
      "Iteration 75, loss = 0.43560058\n",
      "Iteration 76, loss = 0.43553519\n",
      "Iteration 77, loss = 0.43544646\n",
      "Iteration 78, loss = 0.43534712\n",
      "Iteration 79, loss = 0.43502575\n",
      "Iteration 80, loss = 0.43492063\n",
      "Iteration 81, loss = 0.43477294\n",
      "Iteration 82, loss = 0.43457592\n",
      "Iteration 83, loss = 0.43446142\n",
      "Iteration 84, loss = 0.43435995\n",
      "Iteration 85, loss = 0.43420919\n",
      "Iteration 86, loss = 0.43398022\n",
      "Iteration 87, loss = 0.43431494\n",
      "Iteration 88, loss = 0.43385399\n",
      "Iteration 89, loss = 0.43358776\n",
      "Iteration 90, loss = 0.43339186\n",
      "Iteration 91, loss = 0.43323785\n",
      "Iteration 92, loss = 0.43324931\n",
      "Iteration 93, loss = 0.43292261\n",
      "Iteration 94, loss = 0.43282430\n",
      "Iteration 95, loss = 0.43261884\n",
      "Iteration 96, loss = 0.43251573\n",
      "Iteration 97, loss = 0.43240625\n",
      "Iteration 98, loss = 0.43212456\n",
      "Iteration 99, loss = 0.43200340\n",
      "Iteration 100, loss = 0.43173349\n",
      "Iteration 101, loss = 0.43203601\n",
      "Iteration 102, loss = 0.43159583\n",
      "Iteration 103, loss = 0.43149950\n",
      "Iteration 104, loss = 0.43127433\n",
      "Iteration 105, loss = 0.43109999\n",
      "Iteration 106, loss = 0.43084890\n",
      "Iteration 107, loss = 0.43081027\n",
      "Iteration 108, loss = 0.43071258\n",
      "Iteration 109, loss = 0.43059155\n",
      "Iteration 110, loss = 0.43040250\n",
      "Iteration 111, loss = 0.43002121\n",
      "Iteration 112, loss = 0.43000016\n",
      "Iteration 113, loss = 0.42969377\n",
      "Iteration 114, loss = 0.42954650\n",
      "Iteration 115, loss = 0.42950020\n",
      "Iteration 116, loss = 0.42929810\n",
      "Iteration 117, loss = 0.42909346\n",
      "Iteration 118, loss = 0.42896791\n",
      "Iteration 119, loss = 0.42892899\n",
      "Iteration 120, loss = 0.42882701\n",
      "Iteration 121, loss = 0.42843789\n",
      "Iteration 122, loss = 0.42831776\n",
      "Iteration 123, loss = 0.42813852\n",
      "Iteration 124, loss = 0.42806625\n",
      "Iteration 125, loss = 0.42779345\n",
      "Iteration 126, loss = 0.42786596\n",
      "Iteration 127, loss = 0.42738812\n",
      "Iteration 128, loss = 0.42728443\n",
      "Iteration 129, loss = 0.42702726\n",
      "Iteration 130, loss = 0.42692011\n",
      "Iteration 131, loss = 0.42673132\n",
      "Iteration 132, loss = 0.42655725\n",
      "Iteration 133, loss = 0.42652576\n",
      "Iteration 134, loss = 0.42620474\n",
      "Iteration 135, loss = 0.42610734\n",
      "Iteration 136, loss = 0.42591947\n",
      "Iteration 137, loss = 0.42579422\n",
      "Iteration 138, loss = 0.42536375\n",
      "Iteration 139, loss = 0.42546481\n",
      "Iteration 140, loss = 0.42521234\n",
      "Iteration 141, loss = 0.42503820\n",
      "Iteration 142, loss = 0.42478291\n",
      "Iteration 143, loss = 0.42469523\n",
      "Iteration 144, loss = 0.42452590\n",
      "Iteration 145, loss = 0.42429857\n",
      "Iteration 146, loss = 0.42402374\n",
      "Iteration 147, loss = 0.42396195\n",
      "Iteration 148, loss = 0.42370239\n",
      "Iteration 149, loss = 0.42346012\n",
      "Iteration 150, loss = 0.42335948\n",
      "Iteration 151, loss = 0.42314327\n",
      "Iteration 152, loss = 0.42308097\n",
      "Iteration 153, loss = 0.42313112\n",
      "Iteration 154, loss = 0.42255897\n",
      "Iteration 155, loss = 0.42258258\n",
      "Iteration 156, loss = 0.42217338\n",
      "Iteration 157, loss = 0.42204462\n",
      "Iteration 158, loss = 0.42191639\n",
      "Iteration 159, loss = 0.42164916\n",
      "Iteration 160, loss = 0.42145083\n",
      "Iteration 161, loss = 0.42120645\n",
      "Iteration 162, loss = 0.42099804\n",
      "Iteration 163, loss = 0.42088414\n",
      "Iteration 164, loss = 0.42071016\n",
      "Iteration 165, loss = 0.42044566\n",
      "Iteration 166, loss = 0.42037120\n",
      "Iteration 167, loss = 0.42012283\n",
      "Iteration 168, loss = 0.41993583\n",
      "Iteration 169, loss = 0.41978811\n",
      "Iteration 170, loss = 0.41960744\n",
      "Iteration 171, loss = 0.41922426\n",
      "Iteration 172, loss = 0.41906747\n",
      "Iteration 173, loss = 0.41893029\n",
      "Iteration 174, loss = 0.41880397\n",
      "Iteration 175, loss = 0.41865538\n",
      "Iteration 176, loss = 0.41840407\n",
      "Iteration 177, loss = 0.41811044\n",
      "Iteration 178, loss = 0.41787321\n",
      "Iteration 179, loss = 0.41776997\n",
      "Iteration 180, loss = 0.41752321\n",
      "Iteration 181, loss = 0.41725398\n",
      "Iteration 182, loss = 0.41710940\n",
      "Iteration 183, loss = 0.41696674\n",
      "Iteration 184, loss = 0.41685724\n",
      "Iteration 185, loss = 0.41670862\n",
      "Iteration 186, loss = 0.41641021\n",
      "Iteration 187, loss = 0.41611161\n",
      "Iteration 188, loss = 0.41587570\n",
      "Iteration 189, loss = 0.41569227\n",
      "Iteration 190, loss = 0.41543083\n",
      "Iteration 191, loss = 0.41527975\n",
      "Iteration 192, loss = 0.41523238\n",
      "Iteration 193, loss = 0.41477708\n",
      "Iteration 194, loss = 0.41471643\n",
      "Iteration 195, loss = 0.41465553\n",
      "Iteration 196, loss = 0.41419781\n",
      "Iteration 197, loss = 0.41428875\n",
      "Iteration 198, loss = 0.41399593\n",
      "Iteration 199, loss = 0.41386826\n",
      "Iteration 200, loss = 0.41368142\n",
      "Iteration 1, loss = 0.75059346\n",
      "Iteration 2, loss = 0.70184799\n",
      "Iteration 3, loss = 0.65716398\n",
      "Iteration 4, loss = 0.62133811\n",
      "Iteration 5, loss = 0.59153873\n",
      "Iteration 6, loss = 0.56580348\n",
      "Iteration 7, loss = 0.54352100\n",
      "Iteration 8, loss = 0.52573340\n",
      "Iteration 9, loss = 0.50905567\n",
      "Iteration 10, loss = 0.49546878\n",
      "Iteration 11, loss = 0.48334197\n",
      "Iteration 12, loss = 0.47311938\n",
      "Iteration 13, loss = 0.46301032\n",
      "Iteration 14, loss = 0.45514069\n",
      "Iteration 15, loss = 0.44805095\n",
      "Iteration 16, loss = 0.44178688\n",
      "Iteration 17, loss = 0.43626667\n",
      "Iteration 18, loss = 0.43241758\n",
      "Iteration 19, loss = 0.42835269\n",
      "Iteration 20, loss = 0.42506166\n",
      "Iteration 21, loss = 0.42248859\n",
      "Iteration 22, loss = 0.41994018\n",
      "Iteration 23, loss = 0.41837504\n",
      "Iteration 24, loss = 0.41671430\n",
      "Iteration 25, loss = 0.41560805\n",
      "Iteration 26, loss = 0.41454775\n",
      "Iteration 27, loss = 0.41358174\n",
      "Iteration 28, loss = 0.41289964\n",
      "Iteration 29, loss = 0.41227654\n",
      "Iteration 30, loss = 0.41173146\n",
      "Iteration 31, loss = 0.41128471\n",
      "Iteration 32, loss = 0.41106496\n",
      "Iteration 33, loss = 0.41070385\n",
      "Iteration 34, loss = 0.41036372\n",
      "Iteration 35, loss = 0.41015273\n",
      "Iteration 36, loss = 0.40996113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.40971453\n",
      "Iteration 38, loss = 0.40953518\n",
      "Iteration 39, loss = 0.40935829\n",
      "Iteration 40, loss = 0.40923938\n",
      "Iteration 41, loss = 0.40909036\n",
      "Iteration 42, loss = 0.40900358\n",
      "Iteration 43, loss = 0.40889650\n",
      "Iteration 44, loss = 0.40872494\n",
      "Iteration 45, loss = 0.40857105\n",
      "Iteration 46, loss = 0.40848580\n",
      "Iteration 47, loss = 0.40827195\n",
      "Iteration 48, loss = 0.40813361\n",
      "Iteration 49, loss = 0.40799478\n",
      "Iteration 50, loss = 0.40786254\n",
      "Iteration 51, loss = 0.40780271\n",
      "Iteration 52, loss = 0.40766776\n",
      "Iteration 53, loss = 0.40745122\n",
      "Iteration 54, loss = 0.40741295\n",
      "Iteration 55, loss = 0.40725193\n",
      "Iteration 56, loss = 0.40713272\n",
      "Iteration 57, loss = 0.40708929\n",
      "Iteration 58, loss = 0.40700666\n",
      "Iteration 59, loss = 0.40669829\n",
      "Iteration 60, loss = 0.40657825\n",
      "Iteration 61, loss = 0.40666555\n",
      "Iteration 62, loss = 0.40670415\n",
      "Iteration 63, loss = 0.40640275\n",
      "Iteration 64, loss = 0.40621033\n",
      "Iteration 65, loss = 0.40593053\n",
      "Iteration 66, loss = 0.40590142\n",
      "Iteration 67, loss = 0.40568501\n",
      "Iteration 68, loss = 0.40573226\n",
      "Iteration 69, loss = 0.40545340\n",
      "Iteration 70, loss = 0.40539421\n",
      "Iteration 71, loss = 0.40521854\n",
      "Iteration 72, loss = 0.40512957\n",
      "Iteration 73, loss = 0.40495604\n",
      "Iteration 74, loss = 0.40484900\n",
      "Iteration 75, loss = 0.40473191\n",
      "Iteration 76, loss = 0.40466843\n",
      "Iteration 77, loss = 0.40445567\n",
      "Iteration 78, loss = 0.40443497\n",
      "Iteration 79, loss = 0.40416636\n",
      "Iteration 80, loss = 0.40410815\n",
      "Iteration 81, loss = 0.40400567\n",
      "Iteration 82, loss = 0.40382331\n",
      "Iteration 83, loss = 0.40384489\n",
      "Iteration 84, loss = 0.40362834\n",
      "Iteration 85, loss = 0.40344235\n",
      "Iteration 86, loss = 0.40348605\n",
      "Iteration 87, loss = 0.40319748\n",
      "Iteration 88, loss = 0.40320688\n",
      "Iteration 89, loss = 0.40291642\n",
      "Iteration 90, loss = 0.40283523\n",
      "Iteration 91, loss = 0.40278625\n",
      "Iteration 92, loss = 0.40261092\n",
      "Iteration 93, loss = 0.40241426\n",
      "Iteration 94, loss = 0.40236323\n",
      "Iteration 95, loss = 0.40211004\n",
      "Iteration 96, loss = 0.40231202\n",
      "Iteration 97, loss = 0.40187809\n",
      "Iteration 98, loss = 0.40178763\n",
      "Iteration 99, loss = 0.40162021\n",
      "Iteration 100, loss = 0.40163441\n",
      "Iteration 101, loss = 0.40136844\n",
      "Iteration 102, loss = 0.40126647\n",
      "Iteration 103, loss = 0.40104490\n",
      "Iteration 104, loss = 0.40101268\n",
      "Iteration 105, loss = 0.40079082\n",
      "Iteration 106, loss = 0.40068592\n",
      "Iteration 107, loss = 0.40057287\n",
      "Iteration 108, loss = 0.40058942\n",
      "Iteration 109, loss = 0.40027328\n",
      "Iteration 110, loss = 0.40022835\n",
      "Iteration 111, loss = 0.40002643\n",
      "Iteration 112, loss = 0.39980074\n",
      "Iteration 113, loss = 0.39977858\n",
      "Iteration 114, loss = 0.39952035\n",
      "Iteration 115, loss = 0.39943957\n",
      "Iteration 116, loss = 0.39931320\n",
      "Iteration 117, loss = 0.39909895\n",
      "Iteration 118, loss = 0.39903879\n",
      "Iteration 119, loss = 0.39887472\n",
      "Iteration 120, loss = 0.39870176\n",
      "Iteration 121, loss = 0.39868105\n",
      "Iteration 122, loss = 0.39841099\n",
      "Iteration 123, loss = 0.39837808\n",
      "Iteration 124, loss = 0.39833619\n",
      "Iteration 125, loss = 0.39812459\n",
      "Iteration 126, loss = 0.39786263\n",
      "Iteration 127, loss = 0.39771537\n",
      "Iteration 128, loss = 0.39756260\n",
      "Iteration 129, loss = 0.39740257\n",
      "Iteration 130, loss = 0.39725113\n",
      "Iteration 131, loss = 0.39725473\n",
      "Iteration 132, loss = 0.39693985\n",
      "Iteration 133, loss = 0.39682740\n",
      "Iteration 134, loss = 0.39668621\n",
      "Iteration 135, loss = 0.39648080\n",
      "Iteration 136, loss = 0.39631344\n",
      "Iteration 137, loss = 0.39630844\n",
      "Iteration 138, loss = 0.39601653\n",
      "Iteration 139, loss = 0.39605783\n",
      "Iteration 140, loss = 0.39589403\n",
      "Iteration 141, loss = 0.39562789\n",
      "Iteration 142, loss = 0.39543181\n",
      "Iteration 143, loss = 0.39533851\n",
      "Iteration 144, loss = 0.39512031\n",
      "Iteration 145, loss = 0.39524994\n",
      "Iteration 146, loss = 0.39495788\n",
      "Iteration 147, loss = 0.39468500\n",
      "Iteration 148, loss = 0.39467915\n",
      "Iteration 149, loss = 0.39443183\n",
      "Iteration 150, loss = 0.39421209\n",
      "Iteration 151, loss = 0.39419043\n",
      "Iteration 152, loss = 0.39389593\n",
      "Iteration 153, loss = 0.39376357\n",
      "Iteration 154, loss = 0.39369904\n",
      "Iteration 155, loss = 0.39387427\n",
      "Iteration 156, loss = 0.39320751\n",
      "Iteration 157, loss = 0.39306341\n",
      "Iteration 158, loss = 0.39297210\n",
      "Iteration 159, loss = 0.39276592\n",
      "Iteration 160, loss = 0.39264516\n",
      "Iteration 161, loss = 0.39239922\n",
      "Iteration 162, loss = 0.39240357\n",
      "Iteration 163, loss = 0.39214030\n",
      "Iteration 164, loss = 0.39194827\n",
      "Iteration 165, loss = 0.39191131\n",
      "Iteration 166, loss = 0.39168515\n",
      "Iteration 167, loss = 0.39153119\n",
      "Iteration 168, loss = 0.39129331\n",
      "Iteration 169, loss = 0.39125798\n",
      "Iteration 170, loss = 0.39093465\n",
      "Iteration 171, loss = 0.39072842\n",
      "Iteration 172, loss = 0.39070141\n",
      "Iteration 173, loss = 0.39057856\n",
      "Iteration 174, loss = 0.39027684\n",
      "Iteration 175, loss = 0.39003285\n",
      "Iteration 176, loss = 0.39032705\n",
      "Iteration 177, loss = 0.38990084\n",
      "Iteration 178, loss = 0.38967560\n",
      "Iteration 179, loss = 0.38954492\n",
      "Iteration 180, loss = 0.38925477\n",
      "Iteration 181, loss = 0.38926501\n",
      "Iteration 182, loss = 0.38893818\n",
      "Iteration 183, loss = 0.38888486\n",
      "Iteration 184, loss = 0.38864005\n",
      "Iteration 185, loss = 0.38855600\n",
      "Iteration 186, loss = 0.38827808\n",
      "Iteration 187, loss = 0.38826861\n",
      "Iteration 188, loss = 0.38790535\n",
      "Iteration 189, loss = 0.38772577\n",
      "Iteration 190, loss = 0.38770022\n",
      "Iteration 191, loss = 0.38730271\n",
      "Iteration 192, loss = 0.38727656\n",
      "Iteration 193, loss = 0.38706196\n",
      "Iteration 194, loss = 0.38702618\n",
      "Iteration 195, loss = 0.38688597\n",
      "Iteration 196, loss = 0.38656292\n",
      "Iteration 197, loss = 0.38623453\n",
      "Iteration 198, loss = 0.38621264\n",
      "Iteration 199, loss = 0.38599877\n",
      "Iteration 200, loss = 0.38585956\n",
      "Iteration 1, loss = 0.75772774\n",
      "Iteration 2, loss = 0.71111330\n",
      "Iteration 3, loss = 0.66849259\n",
      "Iteration 4, loss = 0.63485322\n",
      "Iteration 5, loss = 0.60620814\n",
      "Iteration 6, loss = 0.58165524\n",
      "Iteration 7, loss = 0.55980521\n",
      "Iteration 8, loss = 0.54305423\n",
      "Iteration 9, loss = 0.52715085\n",
      "Iteration 10, loss = 0.51419706\n",
      "Iteration 11, loss = 0.50242937\n",
      "Iteration 12, loss = 0.49262539\n",
      "Iteration 13, loss = 0.48321795\n",
      "Iteration 14, loss = 0.47585890\n",
      "Iteration 15, loss = 0.46904493\n",
      "Iteration 16, loss = 0.46329722\n",
      "Iteration 17, loss = 0.45826493\n",
      "Iteration 18, loss = 0.45457165\n",
      "Iteration 19, loss = 0.45086497\n",
      "Iteration 20, loss = 0.44770200\n",
      "Iteration 21, loss = 0.44522416\n",
      "Iteration 22, loss = 0.44299886\n",
      "Iteration 23, loss = 0.44167583\n",
      "Iteration 24, loss = 0.44007154\n",
      "Iteration 25, loss = 0.43909998\n",
      "Iteration 26, loss = 0.43794360\n",
      "Iteration 27, loss = 0.43724603\n",
      "Iteration 28, loss = 0.43659487\n",
      "Iteration 29, loss = 0.43582381\n",
      "Iteration 30, loss = 0.43546803\n",
      "Iteration 31, loss = 0.43488513\n",
      "Iteration 32, loss = 0.43472266\n",
      "Iteration 33, loss = 0.43431908\n",
      "Iteration 34, loss = 0.43394732\n",
      "Iteration 35, loss = 0.43367819\n",
      "Iteration 36, loss = 0.43352930\n",
      "Iteration 37, loss = 0.43322184\n",
      "Iteration 38, loss = 0.43303938\n",
      "Iteration 39, loss = 0.43285192\n",
      "Iteration 40, loss = 0.43278511\n",
      "Iteration 41, loss = 0.43248738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.43240970\n",
      "Iteration 43, loss = 0.43227226\n",
      "Iteration 44, loss = 0.43214712\n",
      "Iteration 45, loss = 0.43191835\n",
      "Iteration 46, loss = 0.43179847\n",
      "Iteration 47, loss = 0.43163344\n",
      "Iteration 48, loss = 0.43154632\n",
      "Iteration 49, loss = 0.43143784\n",
      "Iteration 50, loss = 0.43131743\n",
      "Iteration 51, loss = 0.43117746\n",
      "Iteration 52, loss = 0.43099906\n",
      "Iteration 53, loss = 0.43089568\n",
      "Iteration 54, loss = 0.43079963\n",
      "Iteration 55, loss = 0.43063528\n",
      "Iteration 56, loss = 0.43054435\n",
      "Iteration 57, loss = 0.43040566\n",
      "Iteration 58, loss = 0.43029700\n",
      "Iteration 59, loss = 0.43014475\n",
      "Iteration 60, loss = 0.43007308\n",
      "Iteration 61, loss = 0.43002236\n",
      "Iteration 62, loss = 0.42984223\n",
      "Iteration 63, loss = 0.42966467\n",
      "Iteration 64, loss = 0.42962619\n",
      "Iteration 65, loss = 0.42940835\n",
      "Iteration 66, loss = 0.42937744\n",
      "Iteration 67, loss = 0.42913288\n",
      "Iteration 68, loss = 0.42919766\n",
      "Iteration 69, loss = 0.42904436\n",
      "Iteration 70, loss = 0.42886064\n",
      "Iteration 71, loss = 0.42870462\n",
      "Iteration 72, loss = 0.42852387\n",
      "Iteration 73, loss = 0.42853644\n",
      "Iteration 74, loss = 0.42840313\n",
      "Iteration 75, loss = 0.42821596\n",
      "Iteration 76, loss = 0.42813102\n",
      "Iteration 77, loss = 0.42795730\n",
      "Iteration 78, loss = 0.42785098\n",
      "Iteration 79, loss = 0.42767090\n",
      "Iteration 80, loss = 0.42756328\n",
      "Iteration 81, loss = 0.42766556\n",
      "Iteration 82, loss = 0.42732063\n",
      "Iteration 83, loss = 0.42749293\n",
      "Iteration 84, loss = 0.42713579\n",
      "Iteration 85, loss = 0.42691318\n",
      "Iteration 86, loss = 0.42681735\n",
      "Iteration 87, loss = 0.42673645\n",
      "Iteration 88, loss = 0.42665124\n",
      "Iteration 89, loss = 0.42645975\n",
      "Iteration 90, loss = 0.42635469\n",
      "Iteration 91, loss = 0.42633098\n",
      "Iteration 92, loss = 0.42608715\n",
      "Iteration 93, loss = 0.42591792\n",
      "Iteration 94, loss = 0.42615311\n",
      "Iteration 95, loss = 0.42577746\n",
      "Iteration 96, loss = 0.42565508\n",
      "Iteration 97, loss = 0.42552328\n",
      "Iteration 98, loss = 0.42555816\n",
      "Iteration 99, loss = 0.42525168\n",
      "Iteration 100, loss = 0.42510990\n",
      "Iteration 101, loss = 0.42505255\n",
      "Iteration 102, loss = 0.42489515\n",
      "Iteration 103, loss = 0.42479563\n",
      "Iteration 104, loss = 0.42463756\n",
      "Iteration 105, loss = 0.42466758\n",
      "Iteration 106, loss = 0.42435229\n",
      "Iteration 107, loss = 0.42419056\n",
      "Iteration 108, loss = 0.42410891\n",
      "Iteration 109, loss = 0.42398235\n",
      "Iteration 110, loss = 0.42387213\n",
      "Iteration 111, loss = 0.42377619\n",
      "Iteration 112, loss = 0.42351179\n",
      "Iteration 113, loss = 0.42352323\n",
      "Iteration 114, loss = 0.42327798\n",
      "Iteration 115, loss = 0.42319867\n",
      "Iteration 116, loss = 0.42322311\n",
      "Iteration 117, loss = 0.42294088\n",
      "Iteration 118, loss = 0.42283722\n",
      "Iteration 119, loss = 0.42273806\n",
      "Iteration 120, loss = 0.42250450\n",
      "Iteration 121, loss = 0.42252336\n",
      "Iteration 122, loss = 0.42230958\n",
      "Iteration 123, loss = 0.42215024\n",
      "Iteration 124, loss = 0.42212787\n",
      "Iteration 125, loss = 0.42187248\n",
      "Iteration 126, loss = 0.42179184\n",
      "Iteration 127, loss = 0.42157137\n",
      "Iteration 128, loss = 0.42136224\n",
      "Iteration 129, loss = 0.42144493\n",
      "Iteration 130, loss = 0.42121929\n",
      "Iteration 131, loss = 0.42108952\n",
      "Iteration 132, loss = 0.42088138\n",
      "Iteration 133, loss = 0.42084383\n",
      "Iteration 134, loss = 0.42063848\n",
      "Iteration 135, loss = 0.42052481\n",
      "Iteration 136, loss = 0.42031145\n",
      "Iteration 137, loss = 0.42027112\n",
      "Iteration 138, loss = 0.42002807\n",
      "Iteration 139, loss = 0.42032308\n",
      "Iteration 140, loss = 0.41984344\n",
      "Iteration 141, loss = 0.41986853\n",
      "Iteration 142, loss = 0.41950250\n",
      "Iteration 143, loss = 0.41936020\n",
      "Iteration 144, loss = 0.41925326\n",
      "Iteration 145, loss = 0.41914834\n",
      "Iteration 146, loss = 0.41908121\n",
      "Iteration 147, loss = 0.41886369\n",
      "Iteration 148, loss = 0.41885866\n",
      "Iteration 149, loss = 0.41854875\n",
      "Iteration 150, loss = 0.41830297\n",
      "Iteration 151, loss = 0.41831817\n",
      "Iteration 152, loss = 0.41810656\n",
      "Iteration 153, loss = 0.41786589\n",
      "Iteration 154, loss = 0.41797534\n",
      "Iteration 155, loss = 0.41813635\n",
      "Iteration 156, loss = 0.41755374\n",
      "Iteration 157, loss = 0.41732941\n",
      "Iteration 158, loss = 0.41728781\n",
      "Iteration 159, loss = 0.41732978\n",
      "Iteration 160, loss = 0.41687498\n",
      "Iteration 161, loss = 0.41676369\n",
      "Iteration 162, loss = 0.41676156\n",
      "Iteration 163, loss = 0.41654190\n",
      "Iteration 164, loss = 0.41626731\n",
      "Iteration 165, loss = 0.41619948\n",
      "Iteration 166, loss = 0.41608466\n",
      "Iteration 167, loss = 0.41597091\n",
      "Iteration 168, loss = 0.41569174\n",
      "Iteration 169, loss = 0.41555538\n",
      "Iteration 170, loss = 0.41544671\n",
      "Iteration 171, loss = 0.41538988\n",
      "Iteration 172, loss = 0.41511206\n",
      "Iteration 173, loss = 0.41508293\n",
      "Iteration 174, loss = 0.41485663\n",
      "Iteration 175, loss = 0.41476153\n",
      "Iteration 176, loss = 0.41469690\n",
      "Iteration 177, loss = 0.41438957\n",
      "Iteration 178, loss = 0.41431770\n",
      "Iteration 179, loss = 0.41404637\n",
      "Iteration 180, loss = 0.41396235\n",
      "Iteration 181, loss = 0.41385695\n",
      "Iteration 182, loss = 0.41348758\n",
      "Iteration 183, loss = 0.41336906\n",
      "Iteration 184, loss = 0.41326260\n",
      "Iteration 185, loss = 0.41313156\n",
      "Iteration 186, loss = 0.41290357\n",
      "Iteration 187, loss = 0.41309567\n",
      "Iteration 188, loss = 0.41271011\n",
      "Iteration 189, loss = 0.41251525\n",
      "Iteration 190, loss = 0.41235013\n",
      "Iteration 191, loss = 0.41209268\n",
      "Iteration 192, loss = 0.41204224\n",
      "Iteration 193, loss = 0.41181536\n",
      "Iteration 194, loss = 0.41170496\n",
      "Iteration 195, loss = 0.41168066\n",
      "Iteration 196, loss = 0.41138226\n",
      "Iteration 197, loss = 0.41112674\n",
      "Iteration 198, loss = 0.41100632\n",
      "Iteration 199, loss = 0.41086131\n",
      "Iteration 200, loss = 0.41071417\n",
      "Iteration 1, loss = 0.75866013\n",
      "Iteration 2, loss = 0.71214538\n",
      "Iteration 3, loss = 0.67224466\n",
      "Iteration 4, loss = 0.63864395\n",
      "Iteration 5, loss = 0.60980852\n",
      "Iteration 6, loss = 0.58404382\n",
      "Iteration 7, loss = 0.56345973\n",
      "Iteration 8, loss = 0.54448134\n",
      "Iteration 9, loss = 0.52756917\n",
      "Iteration 10, loss = 0.51275882\n",
      "Iteration 11, loss = 0.50002376\n",
      "Iteration 12, loss = 0.48848590\n",
      "Iteration 13, loss = 0.47861313\n",
      "Iteration 14, loss = 0.46956008\n",
      "Iteration 15, loss = 0.46199926\n",
      "Iteration 16, loss = 0.45573010\n",
      "Iteration 17, loss = 0.45012909\n",
      "Iteration 18, loss = 0.44589333\n",
      "Iteration 19, loss = 0.44169702\n",
      "Iteration 20, loss = 0.43786290\n",
      "Iteration 21, loss = 0.43522989\n",
      "Iteration 22, loss = 0.43271398\n",
      "Iteration 23, loss = 0.43051965\n",
      "Iteration 24, loss = 0.42891576\n",
      "Iteration 25, loss = 0.42752953\n",
      "Iteration 26, loss = 0.42625900\n",
      "Iteration 27, loss = 0.42509964\n",
      "Iteration 28, loss = 0.42441404\n",
      "Iteration 29, loss = 0.42347660\n",
      "Iteration 30, loss = 0.42292666\n",
      "Iteration 31, loss = 0.42245446\n",
      "Iteration 32, loss = 0.42193151\n",
      "Iteration 33, loss = 0.42178081\n",
      "Iteration 34, loss = 0.42114486\n",
      "Iteration 35, loss = 0.42092982\n",
      "Iteration 36, loss = 0.42050927\n",
      "Iteration 37, loss = 0.42035404\n",
      "Iteration 38, loss = 0.42002875\n",
      "Iteration 39, loss = 0.41968225\n",
      "Iteration 40, loss = 0.41958341\n",
      "Iteration 41, loss = 0.41930651\n",
      "Iteration 42, loss = 0.41915513\n",
      "Iteration 43, loss = 0.41882000\n",
      "Iteration 44, loss = 0.41885345\n",
      "Iteration 45, loss = 0.41849946\n",
      "Iteration 46, loss = 0.41827548\n",
      "Iteration 47, loss = 0.41807402\n",
      "Iteration 48, loss = 0.41799727\n",
      "Iteration 49, loss = 0.41776757\n",
      "Iteration 50, loss = 0.41760519\n",
      "Iteration 51, loss = 0.41747050\n",
      "Iteration 52, loss = 0.41736135\n",
      "Iteration 53, loss = 0.41709871\n",
      "Iteration 54, loss = 0.41690977\n",
      "Iteration 55, loss = 0.41676482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 56, loss = 0.41670518\n",
      "Iteration 57, loss = 0.41660175\n",
      "Iteration 58, loss = 0.41639064\n",
      "Iteration 59, loss = 0.41616744\n",
      "Iteration 60, loss = 0.41606012\n",
      "Iteration 61, loss = 0.41590286\n",
      "Iteration 62, loss = 0.41576954\n",
      "Iteration 63, loss = 0.41559962\n",
      "Iteration 64, loss = 0.41551517\n",
      "Iteration 65, loss = 0.41530193\n",
      "Iteration 66, loss = 0.41528268\n",
      "Iteration 67, loss = 0.41520690\n",
      "Iteration 68, loss = 0.41495448\n",
      "Iteration 69, loss = 0.41473454\n",
      "Iteration 70, loss = 0.41475520\n",
      "Iteration 71, loss = 0.41448181\n",
      "Iteration 72, loss = 0.41436233\n",
      "Iteration 73, loss = 0.41415895\n",
      "Iteration 74, loss = 0.41405949\n",
      "Iteration 75, loss = 0.41386677\n",
      "Iteration 76, loss = 0.41390824\n",
      "Iteration 77, loss = 0.41353491\n",
      "Iteration 78, loss = 0.41340955\n",
      "Iteration 79, loss = 0.41328403\n",
      "Iteration 80, loss = 0.41322980\n",
      "Iteration 81, loss = 0.41306527\n",
      "Iteration 82, loss = 0.41293833\n",
      "Iteration 83, loss = 0.41275441\n",
      "Iteration 84, loss = 0.41261595\n",
      "Iteration 85, loss = 0.41262126\n",
      "Iteration 86, loss = 0.41246047\n",
      "Iteration 87, loss = 0.41238084\n",
      "Iteration 88, loss = 0.41204148\n",
      "Iteration 89, loss = 0.41191739\n",
      "Iteration 90, loss = 0.41189758\n",
      "Iteration 91, loss = 0.41161790\n",
      "Iteration 92, loss = 0.41181809\n",
      "Iteration 93, loss = 0.41139362\n",
      "Iteration 94, loss = 0.41166317\n",
      "Iteration 95, loss = 0.41107171\n",
      "Iteration 96, loss = 0.41087433\n",
      "Iteration 97, loss = 0.41074998\n",
      "Iteration 98, loss = 0.41054990\n",
      "Iteration 99, loss = 0.41045797\n",
      "Iteration 100, loss = 0.41026700\n",
      "Iteration 101, loss = 0.41045737\n",
      "Iteration 102, loss = 0.41005244\n",
      "Iteration 103, loss = 0.40986876\n",
      "Iteration 104, loss = 0.40985029\n",
      "Iteration 105, loss = 0.40956921\n",
      "Iteration 106, loss = 0.40955184\n",
      "Iteration 107, loss = 0.40932874\n",
      "Iteration 108, loss = 0.40915781\n",
      "Iteration 109, loss = 0.40900889\n",
      "Iteration 110, loss = 0.40880638\n",
      "Iteration 111, loss = 0.40870318\n",
      "Iteration 112, loss = 0.40862677\n",
      "Iteration 113, loss = 0.40835500\n",
      "Iteration 114, loss = 0.40828101\n",
      "Iteration 115, loss = 0.40807874\n",
      "Iteration 116, loss = 0.40783050\n",
      "Iteration 117, loss = 0.40769703\n",
      "Iteration 118, loss = 0.40758615\n",
      "Iteration 119, loss = 0.40759377\n",
      "Iteration 120, loss = 0.40742765\n",
      "Iteration 121, loss = 0.40717471\n",
      "Iteration 122, loss = 0.40702337\n",
      "Iteration 123, loss = 0.40677328\n",
      "Iteration 124, loss = 0.40675152\n",
      "Iteration 125, loss = 0.40661465\n",
      "Iteration 126, loss = 0.40633821\n",
      "Iteration 127, loss = 0.40613435\n",
      "Iteration 128, loss = 0.40603241\n",
      "Iteration 129, loss = 0.40593267\n",
      "Iteration 130, loss = 0.40575073\n",
      "Iteration 131, loss = 0.40551010\n",
      "Iteration 132, loss = 0.40541741\n",
      "Iteration 133, loss = 0.40524240\n",
      "Iteration 134, loss = 0.40507976\n",
      "Iteration 135, loss = 0.40479374\n",
      "Iteration 136, loss = 0.40503641\n",
      "Iteration 137, loss = 0.40447339\n",
      "Iteration 138, loss = 0.40435960\n",
      "Iteration 139, loss = 0.40435047\n",
      "Iteration 140, loss = 0.40403660\n",
      "Iteration 141, loss = 0.40388552\n",
      "Iteration 142, loss = 0.40367086\n",
      "Iteration 143, loss = 0.40352133\n",
      "Iteration 144, loss = 0.40345558\n",
      "Iteration 145, loss = 0.40330566\n",
      "Iteration 146, loss = 0.40307342\n",
      "Iteration 147, loss = 0.40291001\n",
      "Iteration 148, loss = 0.40280666\n",
      "Iteration 149, loss = 0.40245870\n",
      "Iteration 150, loss = 0.40227982\n",
      "Iteration 151, loss = 0.40214205\n",
      "Iteration 152, loss = 0.40201730\n",
      "Iteration 153, loss = 0.40235594\n",
      "Iteration 154, loss = 0.40175604\n",
      "Iteration 155, loss = 0.40130248\n",
      "Iteration 156, loss = 0.40133655\n",
      "Iteration 157, loss = 0.40110768\n",
      "Iteration 158, loss = 0.40090000\n",
      "Iteration 159, loss = 0.40073022\n",
      "Iteration 160, loss = 0.40064162\n",
      "Iteration 161, loss = 0.40042445\n",
      "Iteration 162, loss = 0.40029195\n",
      "Iteration 163, loss = 0.40002677\n",
      "Iteration 164, loss = 0.39998592\n",
      "Iteration 165, loss = 0.39973352\n",
      "Iteration 166, loss = 0.39951504\n",
      "Iteration 167, loss = 0.39920189\n",
      "Iteration 168, loss = 0.39908921\n",
      "Iteration 169, loss = 0.39888822\n",
      "Iteration 170, loss = 0.39886268\n",
      "Iteration 171, loss = 0.39854884\n",
      "Iteration 172, loss = 0.39854978\n",
      "Iteration 173, loss = 0.39831799\n",
      "Iteration 174, loss = 0.39810480\n",
      "Iteration 175, loss = 0.39785105\n",
      "Iteration 176, loss = 0.39769231\n",
      "Iteration 177, loss = 0.39738026\n",
      "Iteration 178, loss = 0.39744037\n",
      "Iteration 179, loss = 0.39707533\n",
      "Iteration 180, loss = 0.39683950\n",
      "Iteration 181, loss = 0.39660427\n",
      "Iteration 182, loss = 0.39646821\n",
      "Iteration 183, loss = 0.39616834\n",
      "Iteration 184, loss = 0.39605633\n",
      "Iteration 185, loss = 0.39608993\n",
      "Iteration 186, loss = 0.39577515\n",
      "Iteration 187, loss = 0.39559254\n",
      "Iteration 188, loss = 0.39536308\n",
      "Iteration 189, loss = 0.39518760\n",
      "Iteration 190, loss = 0.39488967\n",
      "Iteration 191, loss = 0.39476742\n",
      "Iteration 192, loss = 0.39457171\n",
      "Iteration 193, loss = 0.39424243\n",
      "Iteration 194, loss = 0.39429470\n",
      "Iteration 195, loss = 0.39407928\n",
      "Iteration 196, loss = 0.39376293\n",
      "Iteration 197, loss = 0.39382683\n",
      "Iteration 198, loss = 0.39342070\n",
      "Iteration 199, loss = 0.39323305\n",
      "Iteration 200, loss = 0.39284125\n",
      "Iteration 1, loss = 0.76102436\n",
      "Iteration 2, loss = 0.71335411\n",
      "Iteration 3, loss = 0.67451152\n",
      "Iteration 4, loss = 0.64062042\n",
      "Iteration 5, loss = 0.61102829\n",
      "Iteration 6, loss = 0.58587360\n",
      "Iteration 7, loss = 0.56455997\n",
      "Iteration 8, loss = 0.54508098\n",
      "Iteration 9, loss = 0.52788750\n",
      "Iteration 10, loss = 0.51269622\n",
      "Iteration 11, loss = 0.49983498\n",
      "Iteration 12, loss = 0.48803370\n",
      "Iteration 13, loss = 0.47794964\n",
      "Iteration 14, loss = 0.46857051\n",
      "Iteration 15, loss = 0.46098107\n",
      "Iteration 16, loss = 0.45426019\n",
      "Iteration 17, loss = 0.44868442\n",
      "Iteration 18, loss = 0.44412421\n",
      "Iteration 19, loss = 0.43938800\n",
      "Iteration 20, loss = 0.43599302\n",
      "Iteration 21, loss = 0.43282930\n",
      "Iteration 22, loss = 0.43023710\n",
      "Iteration 23, loss = 0.42779871\n",
      "Iteration 24, loss = 0.42597827\n",
      "Iteration 25, loss = 0.42452570\n",
      "Iteration 26, loss = 0.42305540\n",
      "Iteration 27, loss = 0.42188941\n",
      "Iteration 28, loss = 0.42106172\n",
      "Iteration 29, loss = 0.42018576\n",
      "Iteration 30, loss = 0.41939970\n",
      "Iteration 31, loss = 0.41898973\n",
      "Iteration 32, loss = 0.41850618\n",
      "Iteration 33, loss = 0.41824454\n",
      "Iteration 34, loss = 0.41761944\n",
      "Iteration 35, loss = 0.41721974\n",
      "Iteration 36, loss = 0.41709486\n",
      "Iteration 37, loss = 0.41677824\n",
      "Iteration 38, loss = 0.41642518\n",
      "Iteration 39, loss = 0.41621478\n",
      "Iteration 40, loss = 0.41614777\n",
      "Iteration 41, loss = 0.41585053\n",
      "Iteration 42, loss = 0.41570963\n",
      "Iteration 43, loss = 0.41550740\n",
      "Iteration 44, loss = 0.41543868\n",
      "Iteration 45, loss = 0.41513994\n",
      "Iteration 46, loss = 0.41499270\n",
      "Iteration 47, loss = 0.41481010\n",
      "Iteration 48, loss = 0.41471284\n",
      "Iteration 49, loss = 0.41456783\n",
      "Iteration 50, loss = 0.41438299\n",
      "Iteration 51, loss = 0.41426877\n",
      "Iteration 52, loss = 0.41420027\n",
      "Iteration 53, loss = 0.41399658\n",
      "Iteration 54, loss = 0.41384546\n",
      "Iteration 55, loss = 0.41371807\n",
      "Iteration 56, loss = 0.41358252\n",
      "Iteration 57, loss = 0.41351314\n",
      "Iteration 58, loss = 0.41345263\n",
      "Iteration 59, loss = 0.41320733\n",
      "Iteration 60, loss = 0.41305221\n",
      "Iteration 61, loss = 0.41300464\n",
      "Iteration 62, loss = 0.41284000\n",
      "Iteration 63, loss = 0.41279760\n",
      "Iteration 64, loss = 0.41274455\n",
      "Iteration 65, loss = 0.41253452\n",
      "Iteration 66, loss = 0.41241900\n",
      "Iteration 67, loss = 0.41226590\n",
      "Iteration 68, loss = 0.41209046\n",
      "Iteration 69, loss = 0.41197212\n",
      "Iteration 70, loss = 0.41194784\n",
      "Iteration 71, loss = 0.41175517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.41162249\n",
      "Iteration 73, loss = 0.41140319\n",
      "Iteration 74, loss = 0.41138316\n",
      "Iteration 75, loss = 0.41121563\n",
      "Iteration 76, loss = 0.41111576\n",
      "Iteration 77, loss = 0.41098467\n",
      "Iteration 78, loss = 0.41081733\n",
      "Iteration 79, loss = 0.41065808\n",
      "Iteration 80, loss = 0.41057624\n",
      "Iteration 81, loss = 0.41043326\n",
      "Iteration 82, loss = 0.41029778\n",
      "Iteration 83, loss = 0.41015298\n",
      "Iteration 84, loss = 0.41006501\n",
      "Iteration 85, loss = 0.41005786\n",
      "Iteration 86, loss = 0.40975732\n",
      "Iteration 87, loss = 0.40985355\n",
      "Iteration 88, loss = 0.40962897\n",
      "Iteration 89, loss = 0.40949483\n",
      "Iteration 90, loss = 0.40944995\n",
      "Iteration 91, loss = 0.40917142\n",
      "Iteration 92, loss = 0.40930322\n",
      "Iteration 93, loss = 0.40898661\n",
      "Iteration 94, loss = 0.40892783\n",
      "Iteration 95, loss = 0.40865459\n",
      "Iteration 96, loss = 0.40854532\n",
      "Iteration 97, loss = 0.40853809\n",
      "Iteration 98, loss = 0.40828402\n",
      "Iteration 99, loss = 0.40822588\n",
      "Iteration 100, loss = 0.40792899\n",
      "Iteration 101, loss = 0.40814429\n",
      "Iteration 102, loss = 0.40788205\n",
      "Iteration 103, loss = 0.40764461\n",
      "Iteration 104, loss = 0.40762197\n",
      "Iteration 105, loss = 0.40733141\n",
      "Iteration 106, loss = 0.40726021\n",
      "Iteration 107, loss = 0.40716662\n",
      "Iteration 108, loss = 0.40712575\n",
      "Iteration 109, loss = 0.40699225\n",
      "Iteration 110, loss = 0.40677219\n",
      "Iteration 111, loss = 0.40652090\n",
      "Iteration 112, loss = 0.40646590\n",
      "Iteration 113, loss = 0.40630549\n",
      "Iteration 114, loss = 0.40622192\n",
      "Iteration 115, loss = 0.40607122\n",
      "Iteration 116, loss = 0.40585877\n",
      "Iteration 117, loss = 0.40573333\n",
      "Iteration 118, loss = 0.40569124\n",
      "Iteration 119, loss = 0.40564947\n",
      "Iteration 120, loss = 0.40541553\n",
      "Iteration 121, loss = 0.40531167\n",
      "Iteration 122, loss = 0.40515101\n",
      "Iteration 123, loss = 0.40489635\n",
      "Iteration 124, loss = 0.40485459\n",
      "Iteration 125, loss = 0.40474521\n",
      "Iteration 126, loss = 0.40456328\n",
      "Iteration 127, loss = 0.40443766\n",
      "Iteration 128, loss = 0.40424042\n",
      "Iteration 129, loss = 0.40412165\n",
      "Iteration 130, loss = 0.40404792\n",
      "Iteration 131, loss = 0.40376369\n",
      "Iteration 132, loss = 0.40371793\n",
      "Iteration 133, loss = 0.40359327\n",
      "Iteration 134, loss = 0.40341580\n",
      "Iteration 135, loss = 0.40322219\n",
      "Iteration 136, loss = 0.40315471\n",
      "Iteration 137, loss = 0.40323960\n",
      "Iteration 138, loss = 0.40293374\n",
      "Iteration 139, loss = 0.40279060\n",
      "Iteration 140, loss = 0.40258692\n",
      "Iteration 141, loss = 0.40245217\n",
      "Iteration 142, loss = 0.40218473\n",
      "Iteration 143, loss = 0.40198690\n",
      "Iteration 144, loss = 0.40199576\n",
      "Iteration 145, loss = 0.40187959\n",
      "Iteration 146, loss = 0.40178983\n",
      "Iteration 147, loss = 0.40145368\n",
      "Iteration 148, loss = 0.40140069\n",
      "Iteration 149, loss = 0.40112473\n",
      "Iteration 150, loss = 0.40097329\n",
      "Iteration 151, loss = 0.40081381\n",
      "Iteration 152, loss = 0.40085767\n",
      "Iteration 153, loss = 0.40095184\n",
      "Iteration 154, loss = 0.40038200\n",
      "Iteration 155, loss = 0.40018917\n",
      "Iteration 156, loss = 0.40012085\n",
      "Iteration 157, loss = 0.39988289\n",
      "Iteration 158, loss = 0.39973022\n",
      "Iteration 159, loss = 0.39955797\n",
      "Iteration 160, loss = 0.39948520\n",
      "Iteration 161, loss = 0.39935282\n",
      "Iteration 162, loss = 0.39921920\n",
      "Iteration 163, loss = 0.39900839\n",
      "Iteration 164, loss = 0.39882993\n",
      "Iteration 165, loss = 0.39858549\n",
      "Iteration 166, loss = 0.39849654\n",
      "Iteration 167, loss = 0.39824771\n",
      "Iteration 168, loss = 0.39826056\n",
      "Iteration 169, loss = 0.39800498\n",
      "Iteration 170, loss = 0.39790323\n",
      "Iteration 171, loss = 0.39758832\n",
      "Iteration 172, loss = 0.39749548\n",
      "Iteration 173, loss = 0.39751508\n",
      "Iteration 174, loss = 0.39719146\n",
      "Iteration 175, loss = 0.39710810\n",
      "Iteration 176, loss = 0.39681962\n",
      "Iteration 177, loss = 0.39665741\n",
      "Iteration 178, loss = 0.39652653\n",
      "Iteration 179, loss = 0.39635879\n",
      "Iteration 180, loss = 0.39606952\n",
      "Iteration 181, loss = 0.39590618\n",
      "Iteration 182, loss = 0.39574800\n",
      "Iteration 183, loss = 0.39550628\n",
      "Iteration 184, loss = 0.39534282\n",
      "Iteration 185, loss = 0.39527984\n",
      "Iteration 186, loss = 0.39516828\n",
      "Iteration 187, loss = 0.39481595\n",
      "Iteration 188, loss = 0.39474065\n",
      "Iteration 189, loss = 0.39458181\n",
      "Iteration 190, loss = 0.39441590\n",
      "Iteration 191, loss = 0.39409652\n",
      "Iteration 192, loss = 0.39416295\n",
      "Iteration 193, loss = 0.39365362\n",
      "Iteration 194, loss = 0.39366385\n",
      "Iteration 195, loss = 0.39363155\n",
      "Iteration 196, loss = 0.39316063\n",
      "Iteration 197, loss = 0.39308658\n",
      "Iteration 198, loss = 0.39293463\n",
      "Iteration 199, loss = 0.39281936\n",
      "Iteration 200, loss = 0.39240663\n",
      "Iteration 1, loss = 0.75382718\n",
      "Iteration 2, loss = 0.70743286\n",
      "Iteration 3, loss = 0.66802348\n",
      "Iteration 4, loss = 0.63522383\n",
      "Iteration 5, loss = 0.60695324\n",
      "Iteration 6, loss = 0.58315092\n",
      "Iteration 7, loss = 0.56371555\n",
      "Iteration 8, loss = 0.54583122\n",
      "Iteration 9, loss = 0.53066666\n",
      "Iteration 10, loss = 0.51725930\n",
      "Iteration 11, loss = 0.50589179\n",
      "Iteration 12, loss = 0.49618613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.48763302\n",
      "Iteration 14, loss = 0.47971103\n",
      "Iteration 15, loss = 0.47364253\n",
      "Iteration 16, loss = 0.46813717\n",
      "Iteration 17, loss = 0.46421689\n",
      "Iteration 18, loss = 0.46018446\n",
      "Iteration 19, loss = 0.45667082\n",
      "Iteration 20, loss = 0.45445703\n",
      "Iteration 21, loss = 0.45229262\n",
      "Iteration 22, loss = 0.45026142\n",
      "Iteration 23, loss = 0.44878613\n",
      "Iteration 24, loss = 0.44760618\n",
      "Iteration 25, loss = 0.44659882\n",
      "Iteration 26, loss = 0.44567328\n",
      "Iteration 27, loss = 0.44498689\n",
      "Iteration 28, loss = 0.44455524\n",
      "Iteration 29, loss = 0.44395894\n",
      "Iteration 30, loss = 0.44347360\n",
      "Iteration 31, loss = 0.44317179\n",
      "Iteration 32, loss = 0.44289712\n",
      "Iteration 33, loss = 0.44266709\n",
      "Iteration 34, loss = 0.44232369\n",
      "Iteration 35, loss = 0.44201472\n",
      "Iteration 36, loss = 0.44193197\n",
      "Iteration 37, loss = 0.44169894\n",
      "Iteration 38, loss = 0.44142549\n",
      "Iteration 39, loss = 0.44121002\n",
      "Iteration 40, loss = 0.44116173\n",
      "Iteration 41, loss = 0.44090651\n",
      "Iteration 42, loss = 0.44076208\n",
      "Iteration 43, loss = 0.44054611\n",
      "Iteration 44, loss = 0.44043736\n",
      "Iteration 45, loss = 0.44022338\n",
      "Iteration 46, loss = 0.44007866\n",
      "Iteration 47, loss = 0.43985162\n",
      "Iteration 48, loss = 0.43972896\n",
      "Iteration 49, loss = 0.43961625\n",
      "Iteration 50, loss = 0.43953799\n",
      "Iteration 51, loss = 0.43928785\n",
      "Iteration 52, loss = 0.43915728\n",
      "Iteration 53, loss = 0.43903478\n",
      "Iteration 54, loss = 0.43878319\n",
      "Iteration 55, loss = 0.43867276\n",
      "Iteration 56, loss = 0.43842819\n",
      "Iteration 57, loss = 0.43832426\n",
      "Iteration 58, loss = 0.43848232\n",
      "Iteration 59, loss = 0.43802558\n",
      "Iteration 60, loss = 0.43786225\n",
      "Iteration 61, loss = 0.43775897\n",
      "Iteration 62, loss = 0.43753364\n",
      "Iteration 63, loss = 0.43751815\n",
      "Iteration 64, loss = 0.43736998\n",
      "Iteration 65, loss = 0.43724607\n",
      "Iteration 66, loss = 0.43704970\n",
      "Iteration 67, loss = 0.43691650\n",
      "Iteration 68, loss = 0.43678073\n",
      "Iteration 69, loss = 0.43660968\n",
      "Iteration 70, loss = 0.43658182\n",
      "Iteration 71, loss = 0.43626531\n",
      "Iteration 72, loss = 0.43615802\n",
      "Iteration 73, loss = 0.43592539\n",
      "Iteration 74, loss = 0.43586560\n",
      "Iteration 75, loss = 0.43559441\n",
      "Iteration 76, loss = 0.43552899\n",
      "Iteration 77, loss = 0.43544025\n",
      "Iteration 78, loss = 0.43534089\n",
      "Iteration 79, loss = 0.43501951\n",
      "Iteration 80, loss = 0.43491436\n",
      "Iteration 81, loss = 0.43476665\n",
      "Iteration 82, loss = 0.43456961\n",
      "Iteration 83, loss = 0.43445509\n",
      "Iteration 84, loss = 0.43435360\n",
      "Iteration 85, loss = 0.43420282\n",
      "Iteration 86, loss = 0.43397383\n",
      "Iteration 87, loss = 0.43430854\n",
      "Iteration 88, loss = 0.43384755\n",
      "Iteration 89, loss = 0.43358130\n",
      "Iteration 90, loss = 0.43338538\n",
      "Iteration 91, loss = 0.43323135\n",
      "Iteration 92, loss = 0.43324278\n",
      "Iteration 93, loss = 0.43291606\n",
      "Iteration 94, loss = 0.43281772\n",
      "Iteration 95, loss = 0.43261224\n",
      "Iteration 96, loss = 0.43250911\n",
      "Iteration 97, loss = 0.43239961\n",
      "Iteration 98, loss = 0.43211788\n",
      "Iteration 99, loss = 0.43199669\n",
      "Iteration 100, loss = 0.43172676\n",
      "Iteration 101, loss = 0.43202926\n",
      "Iteration 102, loss = 0.43158905\n",
      "Iteration 103, loss = 0.43149269\n",
      "Iteration 104, loss = 0.43126749\n",
      "Iteration 105, loss = 0.43109312\n",
      "Iteration 106, loss = 0.43084200\n",
      "Iteration 107, loss = 0.43080334\n",
      "Iteration 108, loss = 0.43070563\n",
      "Iteration 109, loss = 0.43058456\n",
      "Iteration 110, loss = 0.43039549\n",
      "Iteration 111, loss = 0.43001416\n",
      "Iteration 112, loss = 0.42999308\n",
      "Iteration 113, loss = 0.42968666\n",
      "Iteration 114, loss = 0.42953935\n",
      "Iteration 115, loss = 0.42949302\n",
      "Iteration 116, loss = 0.42929090\n",
      "Iteration 117, loss = 0.42908622\n",
      "Iteration 118, loss = 0.42896063\n",
      "Iteration 119, loss = 0.42892168\n",
      "Iteration 120, loss = 0.42881967\n",
      "Iteration 121, loss = 0.42843051\n",
      "Iteration 122, loss = 0.42831035\n",
      "Iteration 123, loss = 0.42813107\n",
      "Iteration 124, loss = 0.42805877\n",
      "Iteration 125, loss = 0.42778593\n",
      "Iteration 126, loss = 0.42785842\n",
      "Iteration 127, loss = 0.42738053\n",
      "Iteration 128, loss = 0.42727680\n",
      "Iteration 129, loss = 0.42701959\n",
      "Iteration 130, loss = 0.42691241\n",
      "Iteration 131, loss = 0.42672358\n",
      "Iteration 132, loss = 0.42654947\n",
      "Iteration 133, loss = 0.42651794\n",
      "Iteration 134, loss = 0.42619688\n",
      "Iteration 135, loss = 0.42609944\n",
      "Iteration 136, loss = 0.42591153\n",
      "Iteration 137, loss = 0.42578625\n",
      "Iteration 138, loss = 0.42535572\n",
      "Iteration 139, loss = 0.42545674\n",
      "Iteration 140, loss = 0.42520423\n",
      "Iteration 141, loss = 0.42503006\n",
      "Iteration 142, loss = 0.42477472\n",
      "Iteration 143, loss = 0.42468699\n",
      "Iteration 144, loss = 0.42451763\n",
      "Iteration 145, loss = 0.42429025\n",
      "Iteration 146, loss = 0.42401538\n",
      "Iteration 147, loss = 0.42395355\n",
      "Iteration 148, loss = 0.42369395\n",
      "Iteration 149, loss = 0.42345163\n",
      "Iteration 150, loss = 0.42335095\n",
      "Iteration 151, loss = 0.42313469\n",
      "Iteration 152, loss = 0.42307235\n",
      "Iteration 153, loss = 0.42312245\n",
      "Iteration 154, loss = 0.42255025\n",
      "Iteration 155, loss = 0.42257382\n",
      "Iteration 156, loss = 0.42216456\n",
      "Iteration 157, loss = 0.42203577\n",
      "Iteration 158, loss = 0.42190749\n",
      "Iteration 159, loss = 0.42164021\n",
      "Iteration 160, loss = 0.42144183\n",
      "Iteration 161, loss = 0.42119740\n",
      "Iteration 162, loss = 0.42098894\n",
      "Iteration 163, loss = 0.42087499\n",
      "Iteration 164, loss = 0.42070097\n",
      "Iteration 165, loss = 0.42043641\n",
      "Iteration 166, loss = 0.42036191\n",
      "Iteration 167, loss = 0.42011349\n",
      "Iteration 168, loss = 0.41992644\n",
      "Iteration 169, loss = 0.41977867\n",
      "Iteration 170, loss = 0.41959795\n",
      "Iteration 171, loss = 0.41921472\n",
      "Iteration 172, loss = 0.41905788\n",
      "Iteration 173, loss = 0.41892064\n",
      "Iteration 174, loss = 0.41879428\n",
      "Iteration 175, loss = 0.41864563\n",
      "Iteration 176, loss = 0.41839427\n",
      "Iteration 177, loss = 0.41810058\n",
      "Iteration 178, loss = 0.41786330\n",
      "Iteration 179, loss = 0.41776002\n",
      "Iteration 180, loss = 0.41751320\n",
      "Iteration 181, loss = 0.41724391\n",
      "Iteration 182, loss = 0.41709927\n",
      "Iteration 183, loss = 0.41695656\n",
      "Iteration 184, loss = 0.41684701\n",
      "Iteration 185, loss = 0.41669832\n",
      "Iteration 186, loss = 0.41639987\n",
      "Iteration 187, loss = 0.41610122\n",
      "Iteration 188, loss = 0.41586525\n",
      "Iteration 189, loss = 0.41568176\n",
      "Iteration 190, loss = 0.41542027\n",
      "Iteration 191, loss = 0.41526914\n",
      "Iteration 192, loss = 0.41522171\n",
      "Iteration 193, loss = 0.41476634\n",
      "Iteration 194, loss = 0.41470564\n",
      "Iteration 195, loss = 0.41464469\n",
      "Iteration 196, loss = 0.41418691\n",
      "Iteration 197, loss = 0.41427780\n",
      "Iteration 198, loss = 0.41398492\n",
      "Iteration 199, loss = 0.41385719\n",
      "Iteration 200, loss = 0.41367029\n",
      "Iteration 1, loss = 0.75059292\n",
      "Iteration 2, loss = 0.70184745\n",
      "Iteration 3, loss = 0.65716344\n",
      "Iteration 4, loss = 0.62133757\n",
      "Iteration 5, loss = 0.59153819\n",
      "Iteration 6, loss = 0.56580294\n",
      "Iteration 7, loss = 0.54352046\n",
      "Iteration 8, loss = 0.52573286\n",
      "Iteration 9, loss = 0.50905513\n",
      "Iteration 10, loss = 0.49546824\n",
      "Iteration 11, loss = 0.48334143\n",
      "Iteration 12, loss = 0.47311883\n",
      "Iteration 13, loss = 0.46300977\n",
      "Iteration 14, loss = 0.45514014\n",
      "Iteration 15, loss = 0.44805040\n",
      "Iteration 16, loss = 0.44178632\n",
      "Iteration 17, loss = 0.43626612\n",
      "Iteration 18, loss = 0.43241703\n",
      "Iteration 19, loss = 0.42835214\n",
      "Iteration 20, loss = 0.42506111\n",
      "Iteration 21, loss = 0.42248803\n",
      "Iteration 22, loss = 0.41993962\n",
      "Iteration 23, loss = 0.41837448\n",
      "Iteration 24, loss = 0.41671374\n",
      "Iteration 25, loss = 0.41560749\n",
      "Iteration 26, loss = 0.41454719\n",
      "Iteration 27, loss = 0.41358117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.41289907\n",
      "Iteration 29, loss = 0.41227597\n",
      "Iteration 30, loss = 0.41173090\n",
      "Iteration 31, loss = 0.41128414\n",
      "Iteration 32, loss = 0.41106439\n",
      "Iteration 33, loss = 0.41070328\n",
      "Iteration 34, loss = 0.41036315\n",
      "Iteration 35, loss = 0.41015216\n",
      "Iteration 36, loss = 0.40996056\n",
      "Iteration 37, loss = 0.40971396\n",
      "Iteration 38, loss = 0.40953461\n",
      "Iteration 39, loss = 0.40935771\n",
      "Iteration 40, loss = 0.40923881\n",
      "Iteration 41, loss = 0.40908979\n",
      "Iteration 42, loss = 0.40900300\n",
      "Iteration 43, loss = 0.40889592\n",
      "Iteration 44, loss = 0.40872436\n",
      "Iteration 45, loss = 0.40857047\n",
      "Iteration 46, loss = 0.40848522\n",
      "Iteration 47, loss = 0.40827137\n",
      "Iteration 48, loss = 0.40813303\n",
      "Iteration 49, loss = 0.40799420\n",
      "Iteration 50, loss = 0.40786196\n",
      "Iteration 51, loss = 0.40780213\n",
      "Iteration 52, loss = 0.40766718\n",
      "Iteration 53, loss = 0.40745064\n",
      "Iteration 54, loss = 0.40741237\n",
      "Iteration 55, loss = 0.40725134\n",
      "Iteration 56, loss = 0.40713213\n",
      "Iteration 57, loss = 0.40708870\n",
      "Iteration 58, loss = 0.40700607\n",
      "Iteration 59, loss = 0.40669770\n",
      "Iteration 60, loss = 0.40657766\n",
      "Iteration 61, loss = 0.40666496\n",
      "Iteration 62, loss = 0.40670356\n",
      "Iteration 63, loss = 0.40640216\n",
      "Iteration 64, loss = 0.40620974\n",
      "Iteration 65, loss = 0.40592993\n",
      "Iteration 66, loss = 0.40590083\n",
      "Iteration 67, loss = 0.40568441\n",
      "Iteration 68, loss = 0.40573166\n",
      "Iteration 69, loss = 0.40545280\n",
      "Iteration 70, loss = 0.40539361\n",
      "Iteration 71, loss = 0.40521793\n",
      "Iteration 72, loss = 0.40512897\n",
      "Iteration 73, loss = 0.40495544\n",
      "Iteration 74, loss = 0.40484839\n",
      "Iteration 75, loss = 0.40473130\n",
      "Iteration 76, loss = 0.40466782\n",
      "Iteration 77, loss = 0.40445506\n",
      "Iteration 78, loss = 0.40443436\n",
      "Iteration 79, loss = 0.40416575\n",
      "Iteration 80, loss = 0.40410753\n",
      "Iteration 81, loss = 0.40400505\n",
      "Iteration 82, loss = 0.40382269\n",
      "Iteration 83, loss = 0.40384427\n",
      "Iteration 84, loss = 0.40362772\n",
      "Iteration 85, loss = 0.40344172\n",
      "Iteration 86, loss = 0.40348542\n",
      "Iteration 87, loss = 0.40319686\n",
      "Iteration 88, loss = 0.40320625\n",
      "Iteration 89, loss = 0.40291579\n",
      "Iteration 90, loss = 0.40283460\n",
      "Iteration 91, loss = 0.40278561\n",
      "Iteration 92, loss = 0.40261028\n",
      "Iteration 93, loss = 0.40241362\n",
      "Iteration 94, loss = 0.40236259\n",
      "Iteration 95, loss = 0.40210939\n",
      "Iteration 96, loss = 0.40231137\n",
      "Iteration 97, loss = 0.40187744\n",
      "Iteration 98, loss = 0.40178697\n",
      "Iteration 99, loss = 0.40161956\n",
      "Iteration 100, loss = 0.40163375\n",
      "Iteration 101, loss = 0.40136778\n",
      "Iteration 102, loss = 0.40126581\n",
      "Iteration 103, loss = 0.40104424\n",
      "Iteration 104, loss = 0.40101201\n",
      "Iteration 105, loss = 0.40079015\n",
      "Iteration 106, loss = 0.40068525\n",
      "Iteration 107, loss = 0.40057219\n",
      "Iteration 108, loss = 0.40058874\n",
      "Iteration 109, loss = 0.40027260\n",
      "Iteration 110, loss = 0.40022767\n",
      "Iteration 111, loss = 0.40002575\n",
      "Iteration 112, loss = 0.39980005\n",
      "Iteration 113, loss = 0.39977789\n",
      "Iteration 114, loss = 0.39951966\n",
      "Iteration 115, loss = 0.39943887\n",
      "Iteration 116, loss = 0.39931250\n",
      "Iteration 117, loss = 0.39909825\n",
      "Iteration 118, loss = 0.39903808\n",
      "Iteration 119, loss = 0.39887401\n",
      "Iteration 120, loss = 0.39870105\n",
      "Iteration 121, loss = 0.39868033\n",
      "Iteration 122, loss = 0.39841027\n",
      "Iteration 123, loss = 0.39837736\n",
      "Iteration 124, loss = 0.39833546\n",
      "Iteration 125, loss = 0.39812387\n",
      "Iteration 126, loss = 0.39786190\n",
      "Iteration 127, loss = 0.39771464\n",
      "Iteration 128, loss = 0.39756186\n",
      "Iteration 129, loss = 0.39740183\n",
      "Iteration 130, loss = 0.39725039\n",
      "Iteration 131, loss = 0.39725399\n",
      "Iteration 132, loss = 0.39693910\n",
      "Iteration 133, loss = 0.39682664\n",
      "Iteration 134, loss = 0.39668545\n",
      "Iteration 135, loss = 0.39648004\n",
      "Iteration 136, loss = 0.39631267\n",
      "Iteration 137, loss = 0.39630767\n",
      "Iteration 138, loss = 0.39601576\n",
      "Iteration 139, loss = 0.39605706\n",
      "Iteration 140, loss = 0.39589325\n",
      "Iteration 141, loss = 0.39562711\n",
      "Iteration 142, loss = 0.39543102\n",
      "Iteration 143, loss = 0.39533772\n",
      "Iteration 144, loss = 0.39511952\n",
      "Iteration 145, loss = 0.39524914\n",
      "Iteration 146, loss = 0.39495707\n",
      "Iteration 147, loss = 0.39468419\n",
      "Iteration 148, loss = 0.39467833\n",
      "Iteration 149, loss = 0.39443101\n",
      "Iteration 150, loss = 0.39421127\n",
      "Iteration 151, loss = 0.39418961\n",
      "Iteration 152, loss = 0.39389510\n",
      "Iteration 153, loss = 0.39376273\n",
      "Iteration 154, loss = 0.39369820\n",
      "Iteration 155, loss = 0.39387342\n",
      "Iteration 156, loss = 0.39320666\n",
      "Iteration 157, loss = 0.39306256\n",
      "Iteration 158, loss = 0.39297124\n",
      "Iteration 159, loss = 0.39276506\n",
      "Iteration 160, loss = 0.39264430\n",
      "Iteration 161, loss = 0.39239835\n",
      "Iteration 162, loss = 0.39240270\n",
      "Iteration 163, loss = 0.39213942\n",
      "Iteration 164, loss = 0.39194739\n",
      "Iteration 165, loss = 0.39191042\n",
      "Iteration 166, loss = 0.39168426\n",
      "Iteration 167, loss = 0.39153029\n",
      "Iteration 168, loss = 0.39129240\n",
      "Iteration 169, loss = 0.39125707\n",
      "Iteration 170, loss = 0.39093374\n",
      "Iteration 171, loss = 0.39072750\n",
      "Iteration 172, loss = 0.39070049\n",
      "Iteration 173, loss = 0.39057763\n",
      "Iteration 174, loss = 0.39027591\n",
      "Iteration 175, loss = 0.39003191\n",
      "Iteration 176, loss = 0.39032611\n",
      "Iteration 177, loss = 0.38989989\n",
      "Iteration 178, loss = 0.38967465\n",
      "Iteration 179, loss = 0.38954396\n",
      "Iteration 180, loss = 0.38925380\n",
      "Iteration 181, loss = 0.38926404\n",
      "Iteration 182, loss = 0.38893720\n",
      "Iteration 183, loss = 0.38888388\n",
      "Iteration 184, loss = 0.38863907\n",
      "Iteration 185, loss = 0.38855501\n",
      "Iteration 186, loss = 0.38827708\n",
      "Iteration 187, loss = 0.38826761\n",
      "Iteration 188, loss = 0.38790434\n",
      "Iteration 189, loss = 0.38772475\n",
      "Iteration 190, loss = 0.38769920\n",
      "Iteration 191, loss = 0.38730169\n",
      "Iteration 192, loss = 0.38727553\n",
      "Iteration 193, loss = 0.38706093\n",
      "Iteration 194, loss = 0.38702514\n",
      "Iteration 195, loss = 0.38688493\n",
      "Iteration 196, loss = 0.38656187\n",
      "Iteration 197, loss = 0.38623348\n",
      "Iteration 198, loss = 0.38621157\n",
      "Iteration 199, loss = 0.38599770\n",
      "Iteration 200, loss = 0.38585848\n",
      "Iteration 1, loss = 0.75772720\n",
      "Iteration 2, loss = 0.71111276\n",
      "Iteration 3, loss = 0.66849205\n",
      "Iteration 4, loss = 0.63485268\n",
      "Iteration 5, loss = 0.60620760\n",
      "Iteration 6, loss = 0.58165470\n",
      "Iteration 7, loss = 0.55980467\n",
      "Iteration 8, loss = 0.54305369\n",
      "Iteration 9, loss = 0.52715031\n",
      "Iteration 10, loss = 0.51419652\n",
      "Iteration 11, loss = 0.50242882\n",
      "Iteration 12, loss = 0.49262484\n",
      "Iteration 13, loss = 0.48321740\n",
      "Iteration 14, loss = 0.47585836\n",
      "Iteration 15, loss = 0.46904438\n",
      "Iteration 16, loss = 0.46329667\n",
      "Iteration 17, loss = 0.45826438\n",
      "Iteration 18, loss = 0.45457110\n",
      "Iteration 19, loss = 0.45086441\n",
      "Iteration 20, loss = 0.44770144\n",
      "Iteration 21, loss = 0.44522360\n",
      "Iteration 22, loss = 0.44299830\n",
      "Iteration 23, loss = 0.44167527\n",
      "Iteration 24, loss = 0.44007098\n",
      "Iteration 25, loss = 0.43909941\n",
      "Iteration 26, loss = 0.43794303\n",
      "Iteration 27, loss = 0.43724547\n",
      "Iteration 28, loss = 0.43659431\n",
      "Iteration 29, loss = 0.43582324\n",
      "Iteration 30, loss = 0.43546747\n",
      "Iteration 31, loss = 0.43488457\n",
      "Iteration 32, loss = 0.43472209\n",
      "Iteration 33, loss = 0.43431851\n",
      "Iteration 34, loss = 0.43394675\n",
      "Iteration 35, loss = 0.43367762\n",
      "Iteration 36, loss = 0.43352873\n",
      "Iteration 37, loss = 0.43322127\n",
      "Iteration 38, loss = 0.43303880\n",
      "Iteration 39, loss = 0.43285134\n",
      "Iteration 40, loss = 0.43278453\n",
      "Iteration 41, loss = 0.43248680\n",
      "Iteration 42, loss = 0.43240912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.43227169\n",
      "Iteration 44, loss = 0.43214654\n",
      "Iteration 45, loss = 0.43191777\n",
      "Iteration 46, loss = 0.43179789\n",
      "Iteration 47, loss = 0.43163286\n",
      "Iteration 48, loss = 0.43154574\n",
      "Iteration 49, loss = 0.43143726\n",
      "Iteration 50, loss = 0.43131685\n",
      "Iteration 51, loss = 0.43117687\n",
      "Iteration 52, loss = 0.43099847\n",
      "Iteration 53, loss = 0.43089509\n",
      "Iteration 54, loss = 0.43079904\n",
      "Iteration 55, loss = 0.43063469\n",
      "Iteration 56, loss = 0.43054376\n",
      "Iteration 57, loss = 0.43040506\n",
      "Iteration 58, loss = 0.43029641\n",
      "Iteration 59, loss = 0.43014416\n",
      "Iteration 60, loss = 0.43007248\n",
      "Iteration 61, loss = 0.43002176\n",
      "Iteration 62, loss = 0.42984163\n",
      "Iteration 63, loss = 0.42966407\n",
      "Iteration 64, loss = 0.42962559\n",
      "Iteration 65, loss = 0.42940775\n",
      "Iteration 66, loss = 0.42937683\n",
      "Iteration 67, loss = 0.42913228\n",
      "Iteration 68, loss = 0.42919705\n",
      "Iteration 69, loss = 0.42904375\n",
      "Iteration 70, loss = 0.42886003\n",
      "Iteration 71, loss = 0.42870401\n",
      "Iteration 72, loss = 0.42852326\n",
      "Iteration 73, loss = 0.42853583\n",
      "Iteration 74, loss = 0.42840251\n",
      "Iteration 75, loss = 0.42821534\n",
      "Iteration 76, loss = 0.42813039\n",
      "Iteration 77, loss = 0.42795668\n",
      "Iteration 78, loss = 0.42785036\n",
      "Iteration 79, loss = 0.42767028\n",
      "Iteration 80, loss = 0.42756265\n",
      "Iteration 81, loss = 0.42766493\n",
      "Iteration 82, loss = 0.42731999\n",
      "Iteration 83, loss = 0.42749230\n",
      "Iteration 84, loss = 0.42713515\n",
      "Iteration 85, loss = 0.42691254\n",
      "Iteration 86, loss = 0.42681671\n",
      "Iteration 87, loss = 0.42673580\n",
      "Iteration 88, loss = 0.42665060\n",
      "Iteration 89, loss = 0.42645910\n",
      "Iteration 90, loss = 0.42635404\n",
      "Iteration 91, loss = 0.42633033\n",
      "Iteration 92, loss = 0.42608650\n",
      "Iteration 93, loss = 0.42591726\n",
      "Iteration 94, loss = 0.42615246\n",
      "Iteration 95, loss = 0.42577680\n",
      "Iteration 96, loss = 0.42565442\n",
      "Iteration 97, loss = 0.42552261\n",
      "Iteration 98, loss = 0.42555749\n",
      "Iteration 99, loss = 0.42525101\n",
      "Iteration 100, loss = 0.42510922\n",
      "Iteration 101, loss = 0.42505188\n",
      "Iteration 102, loss = 0.42489447\n",
      "Iteration 103, loss = 0.42479496\n",
      "Iteration 104, loss = 0.42463688\n",
      "Iteration 105, loss = 0.42466690\n",
      "Iteration 106, loss = 0.42435161\n",
      "Iteration 107, loss = 0.42418987\n",
      "Iteration 108, loss = 0.42410822\n",
      "Iteration 109, loss = 0.42398166\n",
      "Iteration 110, loss = 0.42387143\n",
      "Iteration 111, loss = 0.42377549\n",
      "Iteration 112, loss = 0.42351108\n",
      "Iteration 113, loss = 0.42352252\n",
      "Iteration 114, loss = 0.42327727\n",
      "Iteration 115, loss = 0.42319796\n",
      "Iteration 116, loss = 0.42322240\n",
      "Iteration 117, loss = 0.42294016\n",
      "Iteration 118, loss = 0.42283649\n",
      "Iteration 119, loss = 0.42273733\n",
      "Iteration 120, loss = 0.42250377\n",
      "Iteration 121, loss = 0.42252263\n",
      "Iteration 122, loss = 0.42230885\n",
      "Iteration 123, loss = 0.42214950\n",
      "Iteration 124, loss = 0.42212713\n",
      "Iteration 125, loss = 0.42187173\n",
      "Iteration 126, loss = 0.42179109\n",
      "Iteration 127, loss = 0.42157062\n",
      "Iteration 128, loss = 0.42136148\n",
      "Iteration 129, loss = 0.42144417\n",
      "Iteration 130, loss = 0.42121853\n",
      "Iteration 131, loss = 0.42108876\n",
      "Iteration 132, loss = 0.42088062\n",
      "Iteration 133, loss = 0.42084306\n",
      "Iteration 134, loss = 0.42063770\n",
      "Iteration 135, loss = 0.42052403\n",
      "Iteration 136, loss = 0.42031066\n",
      "Iteration 137, loss = 0.42027034\n",
      "Iteration 138, loss = 0.42002727\n",
      "Iteration 139, loss = 0.42032229\n",
      "Iteration 140, loss = 0.41984264\n",
      "Iteration 141, loss = 0.41986773\n",
      "Iteration 142, loss = 0.41950169\n",
      "Iteration 143, loss = 0.41935939\n",
      "Iteration 144, loss = 0.41925244\n",
      "Iteration 145, loss = 0.41914752\n",
      "Iteration 146, loss = 0.41908039\n",
      "Iteration 147, loss = 0.41886286\n",
      "Iteration 148, loss = 0.41885783\n",
      "Iteration 149, loss = 0.41854791\n",
      "Iteration 150, loss = 0.41830213\n",
      "Iteration 151, loss = 0.41831732\n",
      "Iteration 152, loss = 0.41810571\n",
      "Iteration 153, loss = 0.41786503\n",
      "Iteration 154, loss = 0.41797448\n",
      "Iteration 155, loss = 0.41813549\n",
      "Iteration 156, loss = 0.41755288\n",
      "Iteration 157, loss = 0.41732855\n",
      "Iteration 158, loss = 0.41728694\n",
      "Iteration 159, loss = 0.41732890\n",
      "Iteration 160, loss = 0.41687410\n",
      "Iteration 161, loss = 0.41676281\n",
      "Iteration 162, loss = 0.41676067\n",
      "Iteration 163, loss = 0.41654100\n",
      "Iteration 164, loss = 0.41626641\n",
      "Iteration 165, loss = 0.41619857\n",
      "Iteration 166, loss = 0.41608375\n",
      "Iteration 167, loss = 0.41597000\n",
      "Iteration 168, loss = 0.41569082\n",
      "Iteration 169, loss = 0.41555445\n",
      "Iteration 170, loss = 0.41544578\n",
      "Iteration 171, loss = 0.41538895\n",
      "Iteration 172, loss = 0.41511113\n",
      "Iteration 173, loss = 0.41508199\n",
      "Iteration 174, loss = 0.41485568\n",
      "Iteration 175, loss = 0.41476058\n",
      "Iteration 176, loss = 0.41469595\n",
      "Iteration 177, loss = 0.41438861\n",
      "Iteration 178, loss = 0.41431674\n",
      "Iteration 179, loss = 0.41404540\n",
      "Iteration 180, loss = 0.41396137\n",
      "Iteration 181, loss = 0.41385597\n",
      "Iteration 182, loss = 0.41348660\n",
      "Iteration 183, loss = 0.41336807\n",
      "Iteration 184, loss = 0.41326161\n",
      "Iteration 185, loss = 0.41313056\n",
      "Iteration 186, loss = 0.41290256\n",
      "Iteration 187, loss = 0.41309467\n",
      "Iteration 188, loss = 0.41270909\n",
      "Iteration 189, loss = 0.41251423\n",
      "Iteration 190, loss = 0.41234910\n",
      "Iteration 191, loss = 0.41209165\n",
      "Iteration 192, loss = 0.41204121\n",
      "Iteration 193, loss = 0.41181432\n",
      "Iteration 194, loss = 0.41170391\n",
      "Iteration 195, loss = 0.41167961\n",
      "Iteration 196, loss = 0.41138120\n",
      "Iteration 197, loss = 0.41112568\n",
      "Iteration 198, loss = 0.41100525\n",
      "Iteration 199, loss = 0.41086024\n",
      "Iteration 200, loss = 0.41071309\n",
      "Iteration 1, loss = 0.75865959\n",
      "Iteration 2, loss = 0.71214484\n",
      "Iteration 3, loss = 0.67224412\n",
      "Iteration 4, loss = 0.63864341\n",
      "Iteration 5, loss = 0.60980798\n",
      "Iteration 6, loss = 0.58404328\n",
      "Iteration 7, loss = 0.56345919\n",
      "Iteration 8, loss = 0.54448080\n",
      "Iteration 9, loss = 0.52756863\n",
      "Iteration 10, loss = 0.51275827\n",
      "Iteration 11, loss = 0.50002321\n",
      "Iteration 12, loss = 0.48848535\n",
      "Iteration 13, loss = 0.47861258\n",
      "Iteration 14, loss = 0.46955953\n",
      "Iteration 15, loss = 0.46199871\n",
      "Iteration 16, loss = 0.45572955\n",
      "Iteration 17, loss = 0.45012854\n",
      "Iteration 18, loss = 0.44589278\n",
      "Iteration 19, loss = 0.44169647\n",
      "Iteration 20, loss = 0.43786235\n",
      "Iteration 21, loss = 0.43522933\n",
      "Iteration 22, loss = 0.43271342\n",
      "Iteration 23, loss = 0.43051909\n",
      "Iteration 24, loss = 0.42891520\n",
      "Iteration 25, loss = 0.42752897\n",
      "Iteration 26, loss = 0.42625844\n",
      "Iteration 27, loss = 0.42509908\n",
      "Iteration 28, loss = 0.42441347\n",
      "Iteration 29, loss = 0.42347603\n",
      "Iteration 30, loss = 0.42292609\n",
      "Iteration 31, loss = 0.42245389\n",
      "Iteration 32, loss = 0.42193093\n",
      "Iteration 33, loss = 0.42178024\n",
      "Iteration 34, loss = 0.42114429\n",
      "Iteration 35, loss = 0.42092925\n",
      "Iteration 36, loss = 0.42050869\n",
      "Iteration 37, loss = 0.42035346\n",
      "Iteration 38, loss = 0.42002817\n",
      "Iteration 39, loss = 0.41968167\n",
      "Iteration 40, loss = 0.41958283\n",
      "Iteration 41, loss = 0.41930593\n",
      "Iteration 42, loss = 0.41915455\n",
      "Iteration 43, loss = 0.41881942\n",
      "Iteration 44, loss = 0.41885287\n",
      "Iteration 45, loss = 0.41849888\n",
      "Iteration 46, loss = 0.41827489\n",
      "Iteration 47, loss = 0.41807344\n",
      "Iteration 48, loss = 0.41799669\n",
      "Iteration 49, loss = 0.41776699\n",
      "Iteration 50, loss = 0.41760460\n",
      "Iteration 51, loss = 0.41746991\n",
      "Iteration 52, loss = 0.41736076\n",
      "Iteration 53, loss = 0.41709812\n",
      "Iteration 54, loss = 0.41690918\n",
      "Iteration 55, loss = 0.41676422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.41670458\n",
      "Iteration 57, loss = 0.41660115\n",
      "Iteration 58, loss = 0.41639004\n",
      "Iteration 59, loss = 0.41616684\n",
      "Iteration 60, loss = 0.41605952\n",
      "Iteration 61, loss = 0.41590226\n",
      "Iteration 62, loss = 0.41576894\n",
      "Iteration 63, loss = 0.41559902\n",
      "Iteration 64, loss = 0.41551457\n",
      "Iteration 65, loss = 0.41530132\n",
      "Iteration 66, loss = 0.41528207\n",
      "Iteration 67, loss = 0.41520629\n",
      "Iteration 68, loss = 0.41495387\n",
      "Iteration 69, loss = 0.41473393\n",
      "Iteration 70, loss = 0.41475459\n",
      "Iteration 71, loss = 0.41448119\n",
      "Iteration 72, loss = 0.41436171\n",
      "Iteration 73, loss = 0.41415833\n",
      "Iteration 74, loss = 0.41405887\n",
      "Iteration 75, loss = 0.41386615\n",
      "Iteration 76, loss = 0.41390762\n",
      "Iteration 77, loss = 0.41353429\n",
      "Iteration 78, loss = 0.41340892\n",
      "Iteration 79, loss = 0.41328340\n",
      "Iteration 80, loss = 0.41322916\n",
      "Iteration 81, loss = 0.41306464\n",
      "Iteration 82, loss = 0.41293769\n",
      "Iteration 83, loss = 0.41275377\n",
      "Iteration 84, loss = 0.41261530\n",
      "Iteration 85, loss = 0.41262062\n",
      "Iteration 86, loss = 0.41245983\n",
      "Iteration 87, loss = 0.41238019\n",
      "Iteration 88, loss = 0.41204083\n",
      "Iteration 89, loss = 0.41191674\n",
      "Iteration 90, loss = 0.41189693\n",
      "Iteration 91, loss = 0.41161724\n",
      "Iteration 92, loss = 0.41181743\n",
      "Iteration 93, loss = 0.41139296\n",
      "Iteration 94, loss = 0.41166251\n",
      "Iteration 95, loss = 0.41107104\n",
      "Iteration 96, loss = 0.41087366\n",
      "Iteration 97, loss = 0.41074931\n",
      "Iteration 98, loss = 0.41054922\n",
      "Iteration 99, loss = 0.41045729\n",
      "Iteration 100, loss = 0.41026632\n",
      "Iteration 101, loss = 0.41045669\n",
      "Iteration 102, loss = 0.41005176\n",
      "Iteration 103, loss = 0.40986807\n",
      "Iteration 104, loss = 0.40984960\n",
      "Iteration 105, loss = 0.40956852\n",
      "Iteration 106, loss = 0.40955114\n",
      "Iteration 107, loss = 0.40932804\n",
      "Iteration 108, loss = 0.40915710\n",
      "Iteration 109, loss = 0.40900818\n",
      "Iteration 110, loss = 0.40880567\n",
      "Iteration 111, loss = 0.40870247\n",
      "Iteration 112, loss = 0.40862605\n",
      "Iteration 113, loss = 0.40835429\n",
      "Iteration 114, loss = 0.40828029\n",
      "Iteration 115, loss = 0.40807801\n",
      "Iteration 116, loss = 0.40782977\n",
      "Iteration 117, loss = 0.40769630\n",
      "Iteration 118, loss = 0.40758541\n",
      "Iteration 119, loss = 0.40759303\n",
      "Iteration 120, loss = 0.40742691\n",
      "Iteration 121, loss = 0.40717396\n",
      "Iteration 122, loss = 0.40702262\n",
      "Iteration 123, loss = 0.40677253\n",
      "Iteration 124, loss = 0.40675077\n",
      "Iteration 125, loss = 0.40661389\n",
      "Iteration 126, loss = 0.40633744\n",
      "Iteration 127, loss = 0.40613358\n",
      "Iteration 128, loss = 0.40603164\n",
      "Iteration 129, loss = 0.40593190\n",
      "Iteration 130, loss = 0.40574995\n",
      "Iteration 131, loss = 0.40550932\n",
      "Iteration 132, loss = 0.40541663\n",
      "Iteration 133, loss = 0.40524162\n",
      "Iteration 134, loss = 0.40507897\n",
      "Iteration 135, loss = 0.40479294\n",
      "Iteration 136, loss = 0.40503561\n",
      "Iteration 137, loss = 0.40447259\n",
      "Iteration 138, loss = 0.40435879\n",
      "Iteration 139, loss = 0.40434966\n",
      "Iteration 140, loss = 0.40403578\n",
      "Iteration 141, loss = 0.40388470\n",
      "Iteration 142, loss = 0.40367003\n",
      "Iteration 143, loss = 0.40352050\n",
      "Iteration 144, loss = 0.40345474\n",
      "Iteration 145, loss = 0.40330483\n",
      "Iteration 146, loss = 0.40307257\n",
      "Iteration 147, loss = 0.40290916\n",
      "Iteration 148, loss = 0.40280581\n",
      "Iteration 149, loss = 0.40245784\n",
      "Iteration 150, loss = 0.40227895\n",
      "Iteration 151, loss = 0.40214118\n",
      "Iteration 152, loss = 0.40201643\n",
      "Iteration 153, loss = 0.40235507\n",
      "Iteration 154, loss = 0.40175516\n",
      "Iteration 155, loss = 0.40130160\n",
      "Iteration 156, loss = 0.40133567\n",
      "Iteration 157, loss = 0.40110679\n",
      "Iteration 158, loss = 0.40089910\n",
      "Iteration 159, loss = 0.40072932\n",
      "Iteration 160, loss = 0.40064071\n",
      "Iteration 161, loss = 0.40042354\n",
      "Iteration 162, loss = 0.40029104\n",
      "Iteration 163, loss = 0.40002585\n",
      "Iteration 164, loss = 0.39998499\n",
      "Iteration 165, loss = 0.39973259\n",
      "Iteration 166, loss = 0.39951411\n",
      "Iteration 167, loss = 0.39920095\n",
      "Iteration 168, loss = 0.39908826\n",
      "Iteration 169, loss = 0.39888727\n",
      "Iteration 170, loss = 0.39886172\n",
      "Iteration 171, loss = 0.39854787\n",
      "Iteration 172, loss = 0.39854881\n",
      "Iteration 173, loss = 0.39831702\n",
      "Iteration 174, loss = 0.39810382\n",
      "Iteration 175, loss = 0.39785006\n",
      "Iteration 176, loss = 0.39769132\n",
      "Iteration 177, loss = 0.39737927\n",
      "Iteration 178, loss = 0.39743937\n",
      "Iteration 179, loss = 0.39707433\n",
      "Iteration 180, loss = 0.39683849\n",
      "Iteration 181, loss = 0.39660326\n",
      "Iteration 182, loss = 0.39646719\n",
      "Iteration 183, loss = 0.39616732\n",
      "Iteration 184, loss = 0.39605530\n",
      "Iteration 185, loss = 0.39608889\n",
      "Iteration 186, loss = 0.39577411\n",
      "Iteration 187, loss = 0.39559149\n",
      "Iteration 188, loss = 0.39536203\n",
      "Iteration 189, loss = 0.39518655\n",
      "Iteration 190, loss = 0.39488861\n",
      "Iteration 191, loss = 0.39476636\n",
      "Iteration 192, loss = 0.39457063\n",
      "Iteration 193, loss = 0.39424135\n",
      "Iteration 194, loss = 0.39429361\n",
      "Iteration 195, loss = 0.39407819\n",
      "Iteration 196, loss = 0.39376183\n",
      "Iteration 197, loss = 0.39382573\n",
      "Iteration 198, loss = 0.39341959\n",
      "Iteration 199, loss = 0.39323194\n",
      "Iteration 200, loss = 0.39284013\n",
      "Iteration 1, loss = 0.76102383\n",
      "Iteration 2, loss = 0.71335358\n",
      "Iteration 3, loss = 0.67451099\n",
      "Iteration 4, loss = 0.64061988\n",
      "Iteration 5, loss = 0.61102775\n",
      "Iteration 6, loss = 0.58587306\n",
      "Iteration 7, loss = 0.56455943\n",
      "Iteration 8, loss = 0.54508044\n",
      "Iteration 9, loss = 0.52788695\n",
      "Iteration 10, loss = 0.51269567\n",
      "Iteration 11, loss = 0.49983443\n",
      "Iteration 12, loss = 0.48803315\n",
      "Iteration 13, loss = 0.47794910\n",
      "Iteration 14, loss = 0.46856997\n",
      "Iteration 15, loss = 0.46098052\n",
      "Iteration 16, loss = 0.45425964\n",
      "Iteration 17, loss = 0.44868387\n",
      "Iteration 18, loss = 0.44412366\n",
      "Iteration 19, loss = 0.43938745\n",
      "Iteration 20, loss = 0.43599246\n",
      "Iteration 21, loss = 0.43282874\n",
      "Iteration 22, loss = 0.43023655\n",
      "Iteration 23, loss = 0.42779815\n",
      "Iteration 24, loss = 0.42597771\n",
      "Iteration 25, loss = 0.42452514\n",
      "Iteration 26, loss = 0.42305483\n",
      "Iteration 27, loss = 0.42188885\n",
      "Iteration 28, loss = 0.42106116\n",
      "Iteration 29, loss = 0.42018520\n",
      "Iteration 30, loss = 0.41939913\n",
      "Iteration 31, loss = 0.41898916\n",
      "Iteration 32, loss = 0.41850561\n",
      "Iteration 33, loss = 0.41824397\n",
      "Iteration 34, loss = 0.41761887\n",
      "Iteration 35, loss = 0.41721918\n",
      "Iteration 36, loss = 0.41709429\n",
      "Iteration 37, loss = 0.41677767\n",
      "Iteration 38, loss = 0.41642462\n",
      "Iteration 39, loss = 0.41621421\n",
      "Iteration 40, loss = 0.41614720\n",
      "Iteration 41, loss = 0.41584996\n",
      "Iteration 42, loss = 0.41570905\n",
      "Iteration 43, loss = 0.41550683\n",
      "Iteration 44, loss = 0.41543810\n",
      "Iteration 45, loss = 0.41513937\n",
      "Iteration 46, loss = 0.41499212\n",
      "Iteration 47, loss = 0.41480952\n",
      "Iteration 48, loss = 0.41471226\n",
      "Iteration 49, loss = 0.41456725\n",
      "Iteration 50, loss = 0.41438241\n",
      "Iteration 51, loss = 0.41426819\n",
      "Iteration 52, loss = 0.41419969\n",
      "Iteration 53, loss = 0.41399600\n",
      "Iteration 54, loss = 0.41384488\n",
      "Iteration 55, loss = 0.41371749\n",
      "Iteration 56, loss = 0.41358194\n",
      "Iteration 57, loss = 0.41351256\n",
      "Iteration 58, loss = 0.41345205\n",
      "Iteration 59, loss = 0.41320674\n",
      "Iteration 60, loss = 0.41305163\n",
      "Iteration 61, loss = 0.41300406\n",
      "Iteration 62, loss = 0.41283942\n",
      "Iteration 63, loss = 0.41279702\n",
      "Iteration 64, loss = 0.41274396\n",
      "Iteration 65, loss = 0.41253393\n",
      "Iteration 66, loss = 0.41241841\n",
      "Iteration 67, loss = 0.41226531\n",
      "Iteration 68, loss = 0.41208987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69, loss = 0.41197153\n",
      "Iteration 70, loss = 0.41194725\n",
      "Iteration 71, loss = 0.41175457\n",
      "Iteration 72, loss = 0.41162190\n",
      "Iteration 73, loss = 0.41140259\n",
      "Iteration 74, loss = 0.41138256\n",
      "Iteration 75, loss = 0.41121503\n",
      "Iteration 76, loss = 0.41111515\n",
      "Iteration 77, loss = 0.41098407\n",
      "Iteration 78, loss = 0.41081672\n",
      "Iteration 79, loss = 0.41065747\n",
      "Iteration 80, loss = 0.41057563\n",
      "Iteration 81, loss = 0.41043265\n",
      "Iteration 82, loss = 0.41029716\n",
      "Iteration 83, loss = 0.41015237\n",
      "Iteration 84, loss = 0.41006439\n",
      "Iteration 85, loss = 0.41005724\n",
      "Iteration 86, loss = 0.40975670\n",
      "Iteration 87, loss = 0.40985293\n",
      "Iteration 88, loss = 0.40962834\n",
      "Iteration 89, loss = 0.40949420\n",
      "Iteration 90, loss = 0.40944932\n",
      "Iteration 91, loss = 0.40917079\n",
      "Iteration 92, loss = 0.40930259\n",
      "Iteration 93, loss = 0.40898598\n",
      "Iteration 94, loss = 0.40892719\n",
      "Iteration 95, loss = 0.40865395\n",
      "Iteration 96, loss = 0.40854468\n",
      "Iteration 97, loss = 0.40853745\n",
      "Iteration 98, loss = 0.40828338\n",
      "Iteration 99, loss = 0.40822523\n",
      "Iteration 100, loss = 0.40792834\n",
      "Iteration 101, loss = 0.40814363\n",
      "Iteration 102, loss = 0.40788140\n",
      "Iteration 103, loss = 0.40764395\n",
      "Iteration 104, loss = 0.40762131\n",
      "Iteration 105, loss = 0.40733075\n",
      "Iteration 106, loss = 0.40725955\n",
      "Iteration 107, loss = 0.40716595\n",
      "Iteration 108, loss = 0.40712508\n",
      "Iteration 109, loss = 0.40699158\n",
      "Iteration 110, loss = 0.40677151\n",
      "Iteration 111, loss = 0.40652022\n",
      "Iteration 112, loss = 0.40646522\n",
      "Iteration 113, loss = 0.40630480\n",
      "Iteration 114, loss = 0.40622123\n",
      "Iteration 115, loss = 0.40607053\n",
      "Iteration 116, loss = 0.40585808\n",
      "Iteration 117, loss = 0.40573264\n",
      "Iteration 118, loss = 0.40569054\n",
      "Iteration 119, loss = 0.40564877\n",
      "Iteration 120, loss = 0.40541483\n",
      "Iteration 121, loss = 0.40531096\n",
      "Iteration 122, loss = 0.40515030\n",
      "Iteration 123, loss = 0.40489564\n",
      "Iteration 124, loss = 0.40485387\n",
      "Iteration 125, loss = 0.40474449\n",
      "Iteration 126, loss = 0.40456255\n",
      "Iteration 127, loss = 0.40443693\n",
      "Iteration 128, loss = 0.40423969\n",
      "Iteration 129, loss = 0.40412091\n",
      "Iteration 130, loss = 0.40404718\n",
      "Iteration 131, loss = 0.40376294\n",
      "Iteration 132, loss = 0.40371719\n",
      "Iteration 133, loss = 0.40359252\n",
      "Iteration 134, loss = 0.40341505\n",
      "Iteration 135, loss = 0.40322143\n",
      "Iteration 136, loss = 0.40315395\n",
      "Iteration 137, loss = 0.40323883\n",
      "Iteration 138, loss = 0.40293297\n",
      "Iteration 139, loss = 0.40278983\n",
      "Iteration 140, loss = 0.40258614\n",
      "Iteration 141, loss = 0.40245138\n",
      "Iteration 142, loss = 0.40218394\n",
      "Iteration 143, loss = 0.40198611\n",
      "Iteration 144, loss = 0.40199497\n",
      "Iteration 145, loss = 0.40187879\n",
      "Iteration 146, loss = 0.40178903\n",
      "Iteration 147, loss = 0.40145287\n",
      "Iteration 148, loss = 0.40139988\n",
      "Iteration 149, loss = 0.40112391\n",
      "Iteration 150, loss = 0.40097247\n",
      "Iteration 151, loss = 0.40081299\n",
      "Iteration 152, loss = 0.40085684\n",
      "Iteration 153, loss = 0.40095100\n",
      "Iteration 154, loss = 0.40038116\n",
      "Iteration 155, loss = 0.40018833\n",
      "Iteration 156, loss = 0.40012000\n",
      "Iteration 157, loss = 0.39988204\n",
      "Iteration 158, loss = 0.39972936\n",
      "Iteration 159, loss = 0.39955711\n",
      "Iteration 160, loss = 0.39948434\n",
      "Iteration 161, loss = 0.39935195\n",
      "Iteration 162, loss = 0.39921832\n",
      "Iteration 163, loss = 0.39900751\n",
      "Iteration 164, loss = 0.39882904\n",
      "Iteration 165, loss = 0.39858459\n",
      "Iteration 166, loss = 0.39849564\n",
      "Iteration 167, loss = 0.39824681\n",
      "Iteration 168, loss = 0.39825966\n",
      "Iteration 169, loss = 0.39800407\n",
      "Iteration 170, loss = 0.39790231\n",
      "Iteration 171, loss = 0.39758740\n",
      "Iteration 172, loss = 0.39749455\n",
      "Iteration 173, loss = 0.39751415\n",
      "Iteration 174, loss = 0.39719052\n",
      "Iteration 175, loss = 0.39710715\n",
      "Iteration 176, loss = 0.39681867\n",
      "Iteration 177, loss = 0.39665645\n",
      "Iteration 178, loss = 0.39652557\n",
      "Iteration 179, loss = 0.39635783\n",
      "Iteration 180, loss = 0.39606855\n",
      "Iteration 181, loss = 0.39590521\n",
      "Iteration 182, loss = 0.39574702\n",
      "Iteration 183, loss = 0.39550529\n",
      "Iteration 184, loss = 0.39534182\n",
      "Iteration 185, loss = 0.39527884\n",
      "Iteration 186, loss = 0.39516727\n",
      "Iteration 187, loss = 0.39481494\n",
      "Iteration 188, loss = 0.39473963\n",
      "Iteration 189, loss = 0.39458079\n",
      "Iteration 190, loss = 0.39441487\n",
      "Iteration 191, loss = 0.39409548\n",
      "Iteration 192, loss = 0.39416191\n",
      "Iteration 193, loss = 0.39365257\n",
      "Iteration 194, loss = 0.39366279\n",
      "Iteration 195, loss = 0.39363049\n",
      "Iteration 196, loss = 0.39315957\n",
      "Iteration 197, loss = 0.39308551\n",
      "Iteration 198, loss = 0.39293355\n",
      "Iteration 199, loss = 0.39281827\n",
      "Iteration 200, loss = 0.39240553\n",
      "Iteration 1, loss = 0.75382664\n",
      "Iteration 2, loss = 0.70743232\n",
      "Iteration 3, loss = 0.66802294\n",
      "Iteration 4, loss = 0.63522330\n",
      "Iteration 5, loss = 0.60695270\n",
      "Iteration 6, loss = 0.58315038\n",
      "Iteration 7, loss = 0.56371501\n",
      "Iteration 8, loss = 0.54583068\n",
      "Iteration 9, loss = 0.53066612\n",
      "Iteration 10, loss = 0.51725875\n",
      "Iteration 11, loss = 0.50589124\n",
      "Iteration 12, loss = 0.49618558\n",
      "Iteration 13, loss = 0.48763248\n",
      "Iteration 14, loss = 0.47971048\n",
      "Iteration 15, loss = 0.47364198\n",
      "Iteration 16, loss = 0.46813662\n",
      "Iteration 17, loss = 0.46421634\n",
      "Iteration 18, loss = 0.46018390\n",
      "Iteration 19, loss = 0.45667027\n",
      "Iteration 20, loss = 0.45445647\n",
      "Iteration 21, loss = 0.45229206\n",
      "Iteration 22, loss = 0.45026086\n",
      "Iteration 23, loss = 0.44878557\n",
      "Iteration 24, loss = 0.44760562\n",
      "Iteration 25, loss = 0.44659826\n",
      "Iteration 26, loss = 0.44567271\n",
      "Iteration 27, loss = 0.44498633\n",
      "Iteration 28, loss = 0.44455467\n",
      "Iteration 29, loss = 0.44395837\n",
      "Iteration 30, loss = 0.44347304\n",
      "Iteration 31, loss = 0.44317122\n",
      "Iteration 32, loss = 0.44289655\n",
      "Iteration 33, loss = 0.44266652\n",
      "Iteration 34, loss = 0.44232312\n",
      "Iteration 35, loss = 0.44201415\n",
      "Iteration 36, loss = 0.44193140\n",
      "Iteration 37, loss = 0.44169837\n",
      "Iteration 38, loss = 0.44142491\n",
      "Iteration 39, loss = 0.44120945\n",
      "Iteration 40, loss = 0.44116115\n",
      "Iteration 41, loss = 0.44090594\n",
      "Iteration 42, loss = 0.44076150\n",
      "Iteration 43, loss = 0.44054553\n",
      "Iteration 44, loss = 0.44043678\n",
      "Iteration 45, loss = 0.44022280\n",
      "Iteration 46, loss = 0.44007808\n",
      "Iteration 47, loss = 0.43985104\n",
      "Iteration 48, loss = 0.43972838\n",
      "Iteration 49, loss = 0.43961566\n",
      "Iteration 50, loss = 0.43953741\n",
      "Iteration 51, loss = 0.43928726\n",
      "Iteration 52, loss = 0.43915669\n",
      "Iteration 53, loss = 0.43903420\n",
      "Iteration 54, loss = 0.43878260\n",
      "Iteration 55, loss = 0.43867217\n",
      "Iteration 56, loss = 0.43842760\n",
      "Iteration 57, loss = 0.43832367\n",
      "Iteration 58, loss = 0.43848173\n",
      "Iteration 59, loss = 0.43802499\n",
      "Iteration 60, loss = 0.43786165\n",
      "Iteration 61, loss = 0.43775838\n",
      "Iteration 62, loss = 0.43753304\n",
      "Iteration 63, loss = 0.43751755\n",
      "Iteration 64, loss = 0.43736938\n",
      "Iteration 65, loss = 0.43724547\n",
      "Iteration 66, loss = 0.43704909\n",
      "Iteration 67, loss = 0.43691589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.43678012\n",
      "Iteration 69, loss = 0.43660907\n",
      "Iteration 70, loss = 0.43658121\n",
      "Iteration 71, loss = 0.43626470\n",
      "Iteration 72, loss = 0.43615740\n",
      "Iteration 73, loss = 0.43592477\n",
      "Iteration 74, loss = 0.43586499\n",
      "Iteration 75, loss = 0.43559379\n",
      "Iteration 76, loss = 0.43552838\n",
      "Iteration 77, loss = 0.43543963\n",
      "Iteration 78, loss = 0.43534026\n",
      "Iteration 79, loss = 0.43501888\n",
      "Iteration 80, loss = 0.43491374\n",
      "Iteration 81, loss = 0.43476602\n",
      "Iteration 82, loss = 0.43456898\n",
      "Iteration 83, loss = 0.43445446\n",
      "Iteration 84, loss = 0.43435297\n",
      "Iteration 85, loss = 0.43420218\n",
      "Iteration 86, loss = 0.43397319\n",
      "Iteration 87, loss = 0.43430790\n",
      "Iteration 88, loss = 0.43384691\n",
      "Iteration 89, loss = 0.43358066\n",
      "Iteration 90, loss = 0.43338473\n",
      "Iteration 91, loss = 0.43323070\n",
      "Iteration 92, loss = 0.43324213\n",
      "Iteration 93, loss = 0.43291540\n",
      "Iteration 94, loss = 0.43281706\n",
      "Iteration 95, loss = 0.43261158\n",
      "Iteration 96, loss = 0.43250845\n",
      "Iteration 97, loss = 0.43239894\n",
      "Iteration 98, loss = 0.43211721\n",
      "Iteration 99, loss = 0.43199602\n",
      "Iteration 100, loss = 0.43172609\n",
      "Iteration 101, loss = 0.43202858\n",
      "Iteration 102, loss = 0.43158837\n",
      "Iteration 103, loss = 0.43149200\n",
      "Iteration 104, loss = 0.43126680\n",
      "Iteration 105, loss = 0.43109243\n",
      "Iteration 106, loss = 0.43084131\n",
      "Iteration 107, loss = 0.43080265\n",
      "Iteration 108, loss = 0.43070493\n",
      "Iteration 109, loss = 0.43058386\n",
      "Iteration 110, loss = 0.43039478\n",
      "Iteration 111, loss = 0.43001345\n",
      "Iteration 112, loss = 0.42999237\n",
      "Iteration 113, loss = 0.42968594\n",
      "Iteration 114, loss = 0.42953864\n",
      "Iteration 115, loss = 0.42949230\n",
      "Iteration 116, loss = 0.42929018\n",
      "Iteration 117, loss = 0.42908550\n",
      "Iteration 118, loss = 0.42895991\n",
      "Iteration 119, loss = 0.42892095\n",
      "Iteration 120, loss = 0.42881893\n",
      "Iteration 121, loss = 0.42842977\n",
      "Iteration 122, loss = 0.42830961\n",
      "Iteration 123, loss = 0.42813032\n",
      "Iteration 124, loss = 0.42805802\n",
      "Iteration 125, loss = 0.42778517\n",
      "Iteration 126, loss = 0.42785766\n",
      "Iteration 127, loss = 0.42737977\n",
      "Iteration 128, loss = 0.42727604\n",
      "Iteration 129, loss = 0.42701883\n",
      "Iteration 130, loss = 0.42691164\n",
      "Iteration 131, loss = 0.42672280\n",
      "Iteration 132, loss = 0.42654869\n",
      "Iteration 133, loss = 0.42651716\n",
      "Iteration 134, loss = 0.42619610\n",
      "Iteration 135, loss = 0.42609865\n",
      "Iteration 136, loss = 0.42591074\n",
      "Iteration 137, loss = 0.42578545\n",
      "Iteration 138, loss = 0.42535492\n",
      "Iteration 139, loss = 0.42545594\n",
      "Iteration 140, loss = 0.42520342\n",
      "Iteration 141, loss = 0.42502925\n",
      "Iteration 142, loss = 0.42477390\n",
      "Iteration 143, loss = 0.42468617\n",
      "Iteration 144, loss = 0.42451680\n",
      "Iteration 145, loss = 0.42428942\n",
      "Iteration 146, loss = 0.42401454\n",
      "Iteration 147, loss = 0.42395271\n",
      "Iteration 148, loss = 0.42369310\n",
      "Iteration 149, loss = 0.42345078\n",
      "Iteration 150, loss = 0.42335009\n",
      "Iteration 151, loss = 0.42313383\n",
      "Iteration 152, loss = 0.42307149\n",
      "Iteration 153, loss = 0.42312159\n",
      "Iteration 154, loss = 0.42254938\n",
      "Iteration 155, loss = 0.42257295\n",
      "Iteration 156, loss = 0.42216368\n",
      "Iteration 157, loss = 0.42203488\n",
      "Iteration 158, loss = 0.42190660\n",
      "Iteration 159, loss = 0.42163932\n",
      "Iteration 160, loss = 0.42144093\n",
      "Iteration 161, loss = 0.42119649\n",
      "Iteration 162, loss = 0.42098803\n",
      "Iteration 163, loss = 0.42087408\n",
      "Iteration 164, loss = 0.42070005\n",
      "Iteration 165, loss = 0.42043549\n",
      "Iteration 166, loss = 0.42036098\n",
      "Iteration 167, loss = 0.42011256\n",
      "Iteration 168, loss = 0.41992550\n",
      "Iteration 169, loss = 0.41977773\n",
      "Iteration 170, loss = 0.41959700\n",
      "Iteration 171, loss = 0.41921376\n",
      "Iteration 172, loss = 0.41905692\n",
      "Iteration 173, loss = 0.41891967\n",
      "Iteration 174, loss = 0.41879331\n",
      "Iteration 175, loss = 0.41864466\n",
      "Iteration 176, loss = 0.41839328\n",
      "Iteration 177, loss = 0.41809959\n",
      "Iteration 178, loss = 0.41786231\n",
      "Iteration 179, loss = 0.41775902\n",
      "Iteration 180, loss = 0.41751220\n",
      "Iteration 181, loss = 0.41724290\n",
      "Iteration 182, loss = 0.41709826\n",
      "Iteration 183, loss = 0.41695555\n",
      "Iteration 184, loss = 0.41684599\n",
      "Iteration 185, loss = 0.41669729\n",
      "Iteration 186, loss = 0.41639884\n",
      "Iteration 187, loss = 0.41610018\n",
      "Iteration 188, loss = 0.41586421\n",
      "Iteration 189, loss = 0.41568071\n",
      "Iteration 190, loss = 0.41541921\n",
      "Iteration 191, loss = 0.41526807\n",
      "Iteration 192, loss = 0.41522065\n",
      "Iteration 193, loss = 0.41476527\n",
      "Iteration 194, loss = 0.41470456\n",
      "Iteration 195, loss = 0.41464360\n",
      "Iteration 196, loss = 0.41418582\n",
      "Iteration 197, loss = 0.41427671\n",
      "Iteration 198, loss = 0.41398382\n",
      "Iteration 199, loss = 0.41385608\n",
      "Iteration 200, loss = 0.41366918\n",
      "Iteration 1, loss = 0.79669760\n",
      "Iteration 2, loss = 0.75557697\n",
      "Iteration 3, loss = 0.71469950\n",
      "Iteration 4, loss = 0.67924961\n",
      "Iteration 5, loss = 0.64721967\n",
      "Iteration 6, loss = 0.61889362\n",
      "Iteration 7, loss = 0.59331554\n",
      "Iteration 8, loss = 0.57202345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.55230062\n",
      "Iteration 10, loss = 0.53594039\n",
      "Iteration 11, loss = 0.52158961\n",
      "Iteration 12, loss = 0.50934520\n",
      "Iteration 13, loss = 0.49746176\n",
      "Iteration 14, loss = 0.48858625\n",
      "Iteration 15, loss = 0.48011948\n",
      "Iteration 16, loss = 0.47240743\n",
      "Iteration 17, loss = 0.46530660\n",
      "Iteration 18, loss = 0.45971048\n",
      "Iteration 19, loss = 0.45392696\n",
      "Iteration 20, loss = 0.44880472\n",
      "Iteration 21, loss = 0.44408580\n",
      "Iteration 22, loss = 0.43977611\n",
      "Iteration 23, loss = 0.43610622\n",
      "Iteration 24, loss = 0.43251204\n",
      "Iteration 25, loss = 0.42926684\n",
      "Iteration 26, loss = 0.42635471\n",
      "Iteration 27, loss = 0.42354868\n",
      "Iteration 28, loss = 0.42124048\n",
      "Iteration 29, loss = 0.41885753\n",
      "Iteration 30, loss = 0.41671698\n",
      "Iteration 31, loss = 0.41484184\n",
      "Iteration 32, loss = 0.41322445\n",
      "Iteration 33, loss = 0.41150185\n",
      "Iteration 34, loss = 0.41005960\n",
      "Iteration 35, loss = 0.40865096\n",
      "Iteration 36, loss = 0.40745207\n",
      "Iteration 37, loss = 0.40618268\n",
      "Iteration 38, loss = 0.40511460\n",
      "Iteration 39, loss = 0.40407324\n",
      "Iteration 40, loss = 0.40308614\n",
      "Iteration 41, loss = 0.40214118\n",
      "Iteration 42, loss = 0.40125116\n",
      "Iteration 43, loss = 0.40040948\n",
      "Iteration 44, loss = 0.39969445\n",
      "Iteration 45, loss = 0.39881351\n",
      "Iteration 46, loss = 0.39815272\n",
      "Iteration 47, loss = 0.39742329\n",
      "Iteration 48, loss = 0.39667854\n",
      "Iteration 49, loss = 0.39600425\n",
      "Iteration 50, loss = 0.39535771\n",
      "Iteration 51, loss = 0.39479927\n",
      "Iteration 52, loss = 0.39412193\n",
      "Iteration 53, loss = 0.39348737\n",
      "Iteration 54, loss = 0.39298633\n",
      "Iteration 55, loss = 0.39237622\n",
      "Iteration 56, loss = 0.39184025\n",
      "Iteration 57, loss = 0.39134928\n",
      "Iteration 58, loss = 0.39079303\n",
      "Iteration 59, loss = 0.39020004\n",
      "Iteration 60, loss = 0.38970487\n",
      "Iteration 61, loss = 0.38935210\n",
      "Iteration 62, loss = 0.38893147\n",
      "Iteration 63, loss = 0.38841325\n",
      "Iteration 64, loss = 0.38787025\n",
      "Iteration 65, loss = 0.38731909\n",
      "Iteration 66, loss = 0.38695092\n",
      "Iteration 67, loss = 0.38641755\n",
      "Iteration 68, loss = 0.38614709\n",
      "Iteration 69, loss = 0.38556306\n",
      "Iteration 70, loss = 0.38520689\n",
      "Iteration 71, loss = 0.38479027\n",
      "Iteration 72, loss = 0.38431671\n",
      "Iteration 73, loss = 0.38396334\n",
      "Iteration 74, loss = 0.38353864\n",
      "Iteration 75, loss = 0.38317797\n",
      "Iteration 76, loss = 0.38280910\n",
      "Iteration 77, loss = 0.38236642\n",
      "Iteration 78, loss = 0.38203892\n",
      "Iteration 79, loss = 0.38160361\n",
      "Iteration 80, loss = 0.38126295\n",
      "Iteration 81, loss = 0.38100087\n",
      "Iteration 82, loss = 0.38051987\n",
      "Iteration 83, loss = 0.38027118\n",
      "Iteration 84, loss = 0.37985466\n",
      "Iteration 85, loss = 0.37944812\n",
      "Iteration 86, loss = 0.37922483\n",
      "Iteration 87, loss = 0.37879295\n",
      "Iteration 88, loss = 0.37860399\n",
      "Iteration 89, loss = 0.37813731\n",
      "Iteration 90, loss = 0.37783048\n",
      "Iteration 91, loss = 0.37760975\n",
      "Iteration 92, loss = 0.37726407\n",
      "Iteration 93, loss = 0.37688092\n",
      "Iteration 94, loss = 0.37664709\n",
      "Iteration 95, loss = 0.37627011\n",
      "Iteration 96, loss = 0.37617697\n",
      "Iteration 97, loss = 0.37570618\n",
      "Iteration 98, loss = 0.37546580\n",
      "Iteration 99, loss = 0.37513636\n",
      "Iteration 100, loss = 0.37492585\n",
      "Iteration 101, loss = 0.37455688\n",
      "Iteration 102, loss = 0.37433807\n",
      "Iteration 103, loss = 0.37398203\n",
      "Iteration 104, loss = 0.37378894\n",
      "Iteration 105, loss = 0.37337225\n",
      "Iteration 106, loss = 0.37315392\n",
      "Iteration 107, loss = 0.37288594\n",
      "Iteration 108, loss = 0.37268326\n",
      "Iteration 109, loss = 0.37234294\n",
      "Iteration 110, loss = 0.37211377\n",
      "Iteration 111, loss = 0.37180141\n",
      "Iteration 112, loss = 0.37146582\n",
      "Iteration 113, loss = 0.37127249\n",
      "Iteration 114, loss = 0.37098043\n",
      "Iteration 115, loss = 0.37079108\n",
      "Iteration 116, loss = 0.37053570\n",
      "Iteration 117, loss = 0.37021577\n",
      "Iteration 118, loss = 0.37000437\n",
      "Iteration 119, loss = 0.36971978\n",
      "Iteration 120, loss = 0.36944006\n",
      "Iteration 121, loss = 0.36926522\n",
      "Iteration 122, loss = 0.36897057\n",
      "Iteration 123, loss = 0.36872537\n",
      "Iteration 124, loss = 0.36854034\n",
      "Iteration 125, loss = 0.36827624\n",
      "Iteration 126, loss = 0.36796157\n",
      "Iteration 127, loss = 0.36780650\n",
      "Iteration 128, loss = 0.36749135\n",
      "Iteration 129, loss = 0.36725852\n",
      "Iteration 130, loss = 0.36701838\n",
      "Iteration 131, loss = 0.36686883\n",
      "Iteration 132, loss = 0.36658430\n",
      "Iteration 133, loss = 0.36631909\n",
      "Iteration 134, loss = 0.36612163\n",
      "Iteration 135, loss = 0.36583841\n",
      "Iteration 136, loss = 0.36564619\n",
      "Iteration 137, loss = 0.36553809\n",
      "Iteration 138, loss = 0.36516578\n",
      "Iteration 139, loss = 0.36505663\n",
      "Iteration 140, loss = 0.36480934\n",
      "Iteration 141, loss = 0.36456529\n",
      "Iteration 142, loss = 0.36434083\n",
      "Iteration 143, loss = 0.36416045\n",
      "Iteration 144, loss = 0.36389162\n",
      "Iteration 145, loss = 0.36388653\n",
      "Iteration 146, loss = 0.36362555\n",
      "Iteration 147, loss = 0.36329666\n",
      "Iteration 148, loss = 0.36324799\n",
      "Iteration 149, loss = 0.36293384\n",
      "Iteration 150, loss = 0.36266341\n",
      "Iteration 151, loss = 0.36258638\n",
      "Iteration 152, loss = 0.36227713\n",
      "Iteration 153, loss = 0.36207737\n",
      "Iteration 154, loss = 0.36203305\n",
      "Iteration 155, loss = 0.36189197\n",
      "Iteration 156, loss = 0.36144865\n",
      "Iteration 157, loss = 0.36125486\n",
      "Iteration 158, loss = 0.36111263\n",
      "Iteration 159, loss = 0.36087217\n",
      "Iteration 160, loss = 0.36073282\n",
      "Iteration 161, loss = 0.36049298\n",
      "Iteration 162, loss = 0.36036918\n",
      "Iteration 163, loss = 0.36009390\n",
      "Iteration 164, loss = 0.35994984\n",
      "Iteration 165, loss = 0.35981267\n",
      "Iteration 166, loss = 0.35963608\n",
      "Iteration 167, loss = 0.35944360\n",
      "Iteration 168, loss = 0.35916915\n",
      "Iteration 169, loss = 0.35903233\n",
      "Iteration 170, loss = 0.35880193\n",
      "Iteration 171, loss = 0.35857206\n",
      "Iteration 172, loss = 0.35856606\n",
      "Iteration 173, loss = 0.35842148\n",
      "Iteration 174, loss = 0.35803099\n",
      "Iteration 175, loss = 0.35789799\n",
      "Iteration 176, loss = 0.35798087\n",
      "Iteration 177, loss = 0.35769945\n",
      "Iteration 178, loss = 0.35738800\n",
      "Iteration 179, loss = 0.35731097\n",
      "Iteration 180, loss = 0.35696026\n",
      "Iteration 181, loss = 0.35706948\n",
      "Iteration 182, loss = 0.35666672\n",
      "Iteration 183, loss = 0.35655904\n",
      "Iteration 184, loss = 0.35638839\n",
      "Iteration 185, loss = 0.35624235\n",
      "Iteration 186, loss = 0.35599433\n",
      "Iteration 187, loss = 0.35595252\n",
      "Iteration 188, loss = 0.35558002\n",
      "Iteration 189, loss = 0.35542146\n",
      "Iteration 190, loss = 0.35537418\n",
      "Iteration 191, loss = 0.35502664\n",
      "Iteration 192, loss = 0.35496094\n",
      "Iteration 193, loss = 0.35479334\n",
      "Iteration 194, loss = 0.35465569\n",
      "Iteration 195, loss = 0.35448446\n",
      "Iteration 196, loss = 0.35428383\n",
      "Iteration 197, loss = 0.35403354\n",
      "Iteration 198, loss = 0.35396345\n",
      "Iteration 199, loss = 0.35378990\n",
      "Iteration 200, loss = 0.35360000\n",
      "Iteration 1, loss = 0.80032318\n",
      "Iteration 2, loss = 0.75976639\n",
      "Iteration 3, loss = 0.72007953\n",
      "Iteration 4, loss = 0.68679759\n",
      "Iteration 5, loss = 0.65667709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.63014574\n",
      "Iteration 7, loss = 0.60602193\n",
      "Iteration 8, loss = 0.58675447\n",
      "Iteration 9, loss = 0.56866078\n",
      "Iteration 10, loss = 0.55328420\n",
      "Iteration 11, loss = 0.54021222\n",
      "Iteration 12, loss = 0.52914660\n",
      "Iteration 13, loss = 0.51867313\n",
      "Iteration 14, loss = 0.51011968\n",
      "Iteration 15, loss = 0.50232890\n",
      "Iteration 16, loss = 0.49517305\n",
      "Iteration 17, loss = 0.48863727\n",
      "Iteration 18, loss = 0.48306034\n",
      "Iteration 19, loss = 0.47753875\n",
      "Iteration 20, loss = 0.47250611\n",
      "Iteration 21, loss = 0.46788387\n",
      "Iteration 22, loss = 0.46372481\n",
      "Iteration 23, loss = 0.46032271\n",
      "Iteration 24, loss = 0.45670468\n",
      "Iteration 25, loss = 0.45361849\n",
      "Iteration 26, loss = 0.45058892\n",
      "Iteration 27, loss = 0.44799808\n",
      "Iteration 28, loss = 0.44581345\n",
      "Iteration 29, loss = 0.44343880\n",
      "Iteration 30, loss = 0.44144084\n",
      "Iteration 31, loss = 0.43942618\n",
      "Iteration 32, loss = 0.43798873\n",
      "Iteration 33, loss = 0.43641247\n",
      "Iteration 34, loss = 0.43482102\n",
      "Iteration 35, loss = 0.43350932\n",
      "Iteration 36, loss = 0.43240488\n",
      "Iteration 37, loss = 0.43120499\n",
      "Iteration 38, loss = 0.43023281\n",
      "Iteration 39, loss = 0.42926244\n",
      "Iteration 40, loss = 0.42849106\n",
      "Iteration 41, loss = 0.42749611\n",
      "Iteration 42, loss = 0.42665644\n",
      "Iteration 43, loss = 0.42592306\n",
      "Iteration 44, loss = 0.42530831\n",
      "Iteration 45, loss = 0.42451361\n",
      "Iteration 46, loss = 0.42389757\n",
      "Iteration 47, loss = 0.42323684\n",
      "Iteration 48, loss = 0.42267969\n",
      "Iteration 49, loss = 0.42205685\n",
      "Iteration 50, loss = 0.42149392\n",
      "Iteration 51, loss = 0.42098444\n",
      "Iteration 52, loss = 0.42037886\n",
      "Iteration 53, loss = 0.41981918\n",
      "Iteration 54, loss = 0.41943426\n",
      "Iteration 55, loss = 0.41886234\n",
      "Iteration 56, loss = 0.41837771\n",
      "Iteration 57, loss = 0.41793320\n",
      "Iteration 58, loss = 0.41746145\n",
      "Iteration 59, loss = 0.41700926\n",
      "Iteration 60, loss = 0.41656322\n",
      "Iteration 61, loss = 0.41617811\n",
      "Iteration 62, loss = 0.41571665\n",
      "Iteration 63, loss = 0.41535120\n",
      "Iteration 64, loss = 0.41488929\n",
      "Iteration 65, loss = 0.41441820\n",
      "Iteration 66, loss = 0.41413584\n",
      "Iteration 67, loss = 0.41361771\n",
      "Iteration 68, loss = 0.41338540\n",
      "Iteration 69, loss = 0.41291625\n",
      "Iteration 70, loss = 0.41253687\n",
      "Iteration 71, loss = 0.41215049\n",
      "Iteration 72, loss = 0.41171926\n",
      "Iteration 73, loss = 0.41146914\n",
      "Iteration 74, loss = 0.41108628\n",
      "Iteration 75, loss = 0.41065874\n",
      "Iteration 76, loss = 0.41039356\n",
      "Iteration 77, loss = 0.40995878\n",
      "Iteration 78, loss = 0.40962124\n",
      "Iteration 79, loss = 0.40923533\n",
      "Iteration 80, loss = 0.40891009\n",
      "Iteration 81, loss = 0.40871838\n",
      "Iteration 82, loss = 0.40822652\n",
      "Iteration 83, loss = 0.40809714\n",
      "Iteration 84, loss = 0.40765774\n",
      "Iteration 85, loss = 0.40726300\n",
      "Iteration 86, loss = 0.40695050\n",
      "Iteration 87, loss = 0.40667775\n",
      "Iteration 88, loss = 0.40643198\n",
      "Iteration 89, loss = 0.40610790\n",
      "Iteration 90, loss = 0.40574674\n",
      "Iteration 91, loss = 0.40558307\n",
      "Iteration 92, loss = 0.40520609\n",
      "Iteration 93, loss = 0.40484669\n",
      "Iteration 94, loss = 0.40474494\n",
      "Iteration 95, loss = 0.40436676\n",
      "Iteration 96, loss = 0.40412636\n",
      "Iteration 97, loss = 0.40387385\n",
      "Iteration 98, loss = 0.40364960\n",
      "Iteration 99, loss = 0.40328893\n",
      "Iteration 100, loss = 0.40304844\n",
      "Iteration 101, loss = 0.40276922\n",
      "Iteration 102, loss = 0.40250746\n",
      "Iteration 103, loss = 0.40227864\n",
      "Iteration 104, loss = 0.40201945\n",
      "Iteration 105, loss = 0.40177757\n",
      "Iteration 106, loss = 0.40144172\n",
      "Iteration 107, loss = 0.40117154\n",
      "Iteration 108, loss = 0.40095290\n",
      "Iteration 109, loss = 0.40067975\n",
      "Iteration 110, loss = 0.40050726\n",
      "Iteration 111, loss = 0.40023780\n",
      "Iteration 112, loss = 0.39987382\n",
      "Iteration 113, loss = 0.39973072\n",
      "Iteration 114, loss = 0.39946597\n",
      "Iteration 115, loss = 0.39921500\n",
      "Iteration 116, loss = 0.39910204\n",
      "Iteration 117, loss = 0.39873420\n",
      "Iteration 118, loss = 0.39857290\n",
      "Iteration 119, loss = 0.39832810\n",
      "Iteration 120, loss = 0.39805925\n",
      "Iteration 121, loss = 0.39785005\n",
      "Iteration 122, loss = 0.39759383\n",
      "Iteration 123, loss = 0.39735952\n",
      "Iteration 124, loss = 0.39713534\n",
      "Iteration 125, loss = 0.39692517\n",
      "Iteration 126, loss = 0.39669790\n",
      "Iteration 127, loss = 0.39648269\n",
      "Iteration 128, loss = 0.39619930\n",
      "Iteration 129, loss = 0.39602993\n",
      "Iteration 130, loss = 0.39581474\n",
      "Iteration 131, loss = 0.39557949\n",
      "Iteration 132, loss = 0.39534469\n",
      "Iteration 133, loss = 0.39519113\n",
      "Iteration 134, loss = 0.39494041\n",
      "Iteration 135, loss = 0.39475876\n",
      "Iteration 136, loss = 0.39442328\n",
      "Iteration 137, loss = 0.39438685\n",
      "Iteration 138, loss = 0.39406145\n",
      "Iteration 139, loss = 0.39408111\n",
      "Iteration 140, loss = 0.39366329\n",
      "Iteration 141, loss = 0.39353858\n",
      "Iteration 142, loss = 0.39325451\n",
      "Iteration 143, loss = 0.39301168\n",
      "Iteration 144, loss = 0.39285984\n",
      "Iteration 145, loss = 0.39263645\n",
      "Iteration 146, loss = 0.39250215\n",
      "Iteration 147, loss = 0.39230080\n",
      "Iteration 148, loss = 0.39216412\n",
      "Iteration 149, loss = 0.39180574\n",
      "Iteration 150, loss = 0.39152310\n",
      "Iteration 151, loss = 0.39140330\n",
      "Iteration 152, loss = 0.39113151\n",
      "Iteration 153, loss = 0.39091997\n",
      "Iteration 154, loss = 0.39088507\n",
      "Iteration 155, loss = 0.39080718\n",
      "Iteration 156, loss = 0.39037204\n",
      "Iteration 157, loss = 0.39006531\n",
      "Iteration 158, loss = 0.38995149\n",
      "Iteration 159, loss = 0.38986005\n",
      "Iteration 160, loss = 0.38949399\n",
      "Iteration 161, loss = 0.38934452\n",
      "Iteration 162, loss = 0.38922719\n",
      "Iteration 163, loss = 0.38893604\n",
      "Iteration 164, loss = 0.38870073\n",
      "Iteration 165, loss = 0.38856129\n",
      "Iteration 166, loss = 0.38849039\n",
      "Iteration 167, loss = 0.38826949\n",
      "Iteration 168, loss = 0.38801831\n",
      "Iteration 169, loss = 0.38776021\n",
      "Iteration 170, loss = 0.38761988\n",
      "Iteration 171, loss = 0.38749014\n",
      "Iteration 172, loss = 0.38727701\n",
      "Iteration 173, loss = 0.38713354\n",
      "Iteration 174, loss = 0.38693221\n",
      "Iteration 175, loss = 0.38672449\n",
      "Iteration 176, loss = 0.38661425\n",
      "Iteration 177, loss = 0.38639276\n",
      "Iteration 178, loss = 0.38613264\n",
      "Iteration 179, loss = 0.38596233\n",
      "Iteration 180, loss = 0.38575075\n",
      "Iteration 181, loss = 0.38571537\n",
      "Iteration 182, loss = 0.38532967\n",
      "Iteration 183, loss = 0.38509408\n",
      "Iteration 184, loss = 0.38498433\n",
      "Iteration 185, loss = 0.38482933\n",
      "Iteration 186, loss = 0.38460072\n",
      "Iteration 187, loss = 0.38465255\n",
      "Iteration 188, loss = 0.38426048\n",
      "Iteration 189, loss = 0.38404663\n",
      "Iteration 190, loss = 0.38393551\n",
      "Iteration 191, loss = 0.38362485\n",
      "Iteration 192, loss = 0.38357388\n",
      "Iteration 193, loss = 0.38328454\n",
      "Iteration 194, loss = 0.38318795\n",
      "Iteration 195, loss = 0.38300627\n",
      "Iteration 196, loss = 0.38280720\n",
      "Iteration 197, loss = 0.38257733\n",
      "Iteration 198, loss = 0.38239391\n",
      "Iteration 199, loss = 0.38226339\n",
      "Iteration 200, loss = 0.38205623\n",
      "Iteration 1, loss = 0.79907644\n",
      "Iteration 2, loss = 0.75805898\n",
      "Iteration 3, loss = 0.72018387\n",
      "Iteration 4, loss = 0.68639335\n",
      "Iteration 5, loss = 0.65642308\n",
      "Iteration 6, loss = 0.62903665\n",
      "Iteration 7, loss = 0.60615117\n",
      "Iteration 8, loss = 0.58594438\n",
      "Iteration 9, loss = 0.56812910\n",
      "Iteration 10, loss = 0.55190390\n",
      "Iteration 11, loss = 0.53903109\n",
      "Iteration 12, loss = 0.52691609\n",
      "Iteration 13, loss = 0.51668254\n",
      "Iteration 14, loss = 0.50702108\n",
      "Iteration 15, loss = 0.49856563\n",
      "Iteration 16, loss = 0.49108096\n",
      "Iteration 17, loss = 0.48403912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.47814778\n",
      "Iteration 19, loss = 0.47217755\n",
      "Iteration 20, loss = 0.46639673\n",
      "Iteration 21, loss = 0.46172279\n",
      "Iteration 22, loss = 0.45713825\n",
      "Iteration 23, loss = 0.45275449\n",
      "Iteration 24, loss = 0.44901364\n",
      "Iteration 25, loss = 0.44539194\n",
      "Iteration 26, loss = 0.44209888\n",
      "Iteration 27, loss = 0.43887782\n",
      "Iteration 28, loss = 0.43628618\n",
      "Iteration 29, loss = 0.43351934\n",
      "Iteration 30, loss = 0.43131012\n",
      "Iteration 31, loss = 0.42919823\n",
      "Iteration 32, loss = 0.42719047\n",
      "Iteration 33, loss = 0.42544984\n",
      "Iteration 34, loss = 0.42361015\n",
      "Iteration 35, loss = 0.42200641\n",
      "Iteration 36, loss = 0.42057655\n",
      "Iteration 37, loss = 0.41925375\n",
      "Iteration 38, loss = 0.41786294\n",
      "Iteration 39, loss = 0.41660394\n",
      "Iteration 40, loss = 0.41546555\n",
      "Iteration 41, loss = 0.41435744\n",
      "Iteration 42, loss = 0.41329999\n",
      "Iteration 43, loss = 0.41227568\n",
      "Iteration 44, loss = 0.41140488\n",
      "Iteration 45, loss = 0.41038995\n",
      "Iteration 46, loss = 0.40951959\n",
      "Iteration 47, loss = 0.40860714\n",
      "Iteration 48, loss = 0.40779882\n",
      "Iteration 49, loss = 0.40703217\n",
      "Iteration 50, loss = 0.40625949\n",
      "Iteration 51, loss = 0.40546440\n",
      "Iteration 52, loss = 0.40476449\n",
      "Iteration 53, loss = 0.40406787\n",
      "Iteration 54, loss = 0.40334053\n",
      "Iteration 55, loss = 0.40266435\n",
      "Iteration 56, loss = 0.40206028\n",
      "Iteration 57, loss = 0.40138958\n",
      "Iteration 58, loss = 0.40072007\n",
      "Iteration 59, loss = 0.40006803\n",
      "Iteration 60, loss = 0.39950722\n",
      "Iteration 61, loss = 0.39887802\n",
      "Iteration 62, loss = 0.39833742\n",
      "Iteration 63, loss = 0.39769359\n",
      "Iteration 64, loss = 0.39724174\n",
      "Iteration 65, loss = 0.39664007\n",
      "Iteration 66, loss = 0.39618612\n",
      "Iteration 67, loss = 0.39575190\n",
      "Iteration 68, loss = 0.39513518\n",
      "Iteration 69, loss = 0.39453915\n",
      "Iteration 70, loss = 0.39413291\n",
      "Iteration 71, loss = 0.39354475\n",
      "Iteration 72, loss = 0.39306893\n",
      "Iteration 73, loss = 0.39258797\n",
      "Iteration 74, loss = 0.39211574\n",
      "Iteration 75, loss = 0.39166646\n",
      "Iteration 76, loss = 0.39125783\n",
      "Iteration 77, loss = 0.39068452\n",
      "Iteration 78, loss = 0.39031588\n",
      "Iteration 79, loss = 0.38986191\n",
      "Iteration 80, loss = 0.38944770\n",
      "Iteration 81, loss = 0.38903644\n",
      "Iteration 82, loss = 0.38865562\n",
      "Iteration 83, loss = 0.38820144\n",
      "Iteration 84, loss = 0.38780887\n",
      "Iteration 85, loss = 0.38743463\n",
      "Iteration 86, loss = 0.38701820\n",
      "Iteration 87, loss = 0.38663366\n",
      "Iteration 88, loss = 0.38619021\n",
      "Iteration 89, loss = 0.38578021\n",
      "Iteration 90, loss = 0.38549202\n",
      "Iteration 91, loss = 0.38502333\n",
      "Iteration 92, loss = 0.38493979\n",
      "Iteration 93, loss = 0.38436316\n",
      "Iteration 94, loss = 0.38430085\n",
      "Iteration 95, loss = 0.38367839\n",
      "Iteration 96, loss = 0.38330895\n",
      "Iteration 97, loss = 0.38301127\n",
      "Iteration 98, loss = 0.38263127\n",
      "Iteration 99, loss = 0.38236187\n",
      "Iteration 100, loss = 0.38200640\n",
      "Iteration 101, loss = 0.38187145\n",
      "Iteration 102, loss = 0.38147831\n",
      "Iteration 103, loss = 0.38113749\n",
      "Iteration 104, loss = 0.38097677\n",
      "Iteration 105, loss = 0.38055190\n",
      "Iteration 106, loss = 0.38029760\n",
      "Iteration 107, loss = 0.38000038\n",
      "Iteration 108, loss = 0.37974422\n",
      "Iteration 109, loss = 0.37940781\n",
      "Iteration 110, loss = 0.37911636\n",
      "Iteration 111, loss = 0.37883583\n",
      "Iteration 112, loss = 0.37862346\n",
      "Iteration 113, loss = 0.37826678\n",
      "Iteration 114, loss = 0.37805372\n",
      "Iteration 115, loss = 0.37777286\n",
      "Iteration 116, loss = 0.37741169\n",
      "Iteration 117, loss = 0.37715191\n",
      "Iteration 118, loss = 0.37690157\n",
      "Iteration 119, loss = 0.37670578\n",
      "Iteration 120, loss = 0.37643872\n",
      "Iteration 121, loss = 0.37614539\n",
      "Iteration 122, loss = 0.37590021\n",
      "Iteration 123, loss = 0.37556818\n",
      "Iteration 124, loss = 0.37530833\n",
      "Iteration 125, loss = 0.37512692\n",
      "Iteration 126, loss = 0.37476898\n",
      "Iteration 127, loss = 0.37445931\n",
      "Iteration 128, loss = 0.37431085\n",
      "Iteration 129, loss = 0.37417113\n",
      "Iteration 130, loss = 0.37375989\n",
      "Iteration 131, loss = 0.37355737\n",
      "Iteration 132, loss = 0.37330773\n",
      "Iteration 133, loss = 0.37307451\n",
      "Iteration 134, loss = 0.37284956\n",
      "Iteration 135, loss = 0.37251164\n",
      "Iteration 136, loss = 0.37249172\n",
      "Iteration 137, loss = 0.37204122\n",
      "Iteration 138, loss = 0.37188750\n",
      "Iteration 139, loss = 0.37173124\n",
      "Iteration 140, loss = 0.37147067\n",
      "Iteration 141, loss = 0.37119997\n",
      "Iteration 142, loss = 0.37104365\n",
      "Iteration 143, loss = 0.37080188\n",
      "Iteration 144, loss = 0.37061092\n",
      "Iteration 145, loss = 0.37042669\n",
      "Iteration 146, loss = 0.37014665\n",
      "Iteration 147, loss = 0.36998422\n",
      "Iteration 148, loss = 0.36979836\n",
      "Iteration 149, loss = 0.36949399\n",
      "Iteration 150, loss = 0.36923376\n",
      "Iteration 151, loss = 0.36907191\n",
      "Iteration 152, loss = 0.36899548\n",
      "Iteration 153, loss = 0.36895418\n",
      "Iteration 154, loss = 0.36859515\n",
      "Iteration 155, loss = 0.36815311\n",
      "Iteration 156, loss = 0.36807487\n",
      "Iteration 157, loss = 0.36786126\n",
      "Iteration 158, loss = 0.36762121\n",
      "Iteration 159, loss = 0.36746306\n",
      "Iteration 160, loss = 0.36727836\n",
      "Iteration 161, loss = 0.36707719\n",
      "Iteration 162, loss = 0.36686642\n",
      "Iteration 163, loss = 0.36662459\n",
      "Iteration 164, loss = 0.36649936\n",
      "Iteration 165, loss = 0.36633995\n",
      "Iteration 166, loss = 0.36608039\n",
      "Iteration 167, loss = 0.36578096\n",
      "Iteration 168, loss = 0.36560837\n",
      "Iteration 169, loss = 0.36545597\n",
      "Iteration 170, loss = 0.36537406\n",
      "Iteration 171, loss = 0.36503972\n",
      "Iteration 172, loss = 0.36501001\n",
      "Iteration 173, loss = 0.36480095\n",
      "Iteration 174, loss = 0.36456612\n",
      "Iteration 175, loss = 0.36434114\n",
      "Iteration 176, loss = 0.36415501\n",
      "Iteration 177, loss = 0.36387636\n",
      "Iteration 178, loss = 0.36392722\n",
      "Iteration 179, loss = 0.36353426\n",
      "Iteration 180, loss = 0.36337139\n",
      "Iteration 181, loss = 0.36317386\n",
      "Iteration 182, loss = 0.36300553\n",
      "Iteration 183, loss = 0.36272549\n",
      "Iteration 184, loss = 0.36266731\n",
      "Iteration 185, loss = 0.36251521\n",
      "Iteration 186, loss = 0.36231080\n",
      "Iteration 187, loss = 0.36213774\n",
      "Iteration 188, loss = 0.36189020\n",
      "Iteration 189, loss = 0.36173569\n",
      "Iteration 190, loss = 0.36151063\n",
      "Iteration 191, loss = 0.36135795\n",
      "Iteration 192, loss = 0.36113279\n",
      "Iteration 193, loss = 0.36094593\n",
      "Iteration 194, loss = 0.36091881\n",
      "Iteration 195, loss = 0.36070830\n",
      "Iteration 196, loss = 0.36038865\n",
      "Iteration 197, loss = 0.36038877\n",
      "Iteration 198, loss = 0.36003514\n",
      "Iteration 199, loss = 0.35992339\n",
      "Iteration 200, loss = 0.35958614\n",
      "Iteration 1, loss = 0.79433606\n",
      "Iteration 2, loss = 0.75464226\n",
      "Iteration 3, loss = 0.71766889\n",
      "Iteration 4, loss = 0.68463338\n",
      "Iteration 5, loss = 0.65437914\n",
      "Iteration 6, loss = 0.62761933\n",
      "Iteration 7, loss = 0.60417337\n",
      "Iteration 8, loss = 0.58312254\n",
      "Iteration 9, loss = 0.56478461\n",
      "Iteration 10, loss = 0.54808187\n",
      "Iteration 11, loss = 0.53447950\n",
      "Iteration 12, loss = 0.52191300\n",
      "Iteration 13, loss = 0.51130674\n",
      "Iteration 14, loss = 0.50103004\n",
      "Iteration 15, loss = 0.49235603\n",
      "Iteration 16, loss = 0.48443740\n",
      "Iteration 17, loss = 0.47727989\n",
      "Iteration 18, loss = 0.47109976\n",
      "Iteration 19, loss = 0.46462814\n",
      "Iteration 20, loss = 0.45911666\n",
      "Iteration 21, loss = 0.45393085\n",
      "Iteration 22, loss = 0.44925347\n",
      "Iteration 23, loss = 0.44470152\n",
      "Iteration 24, loss = 0.44079814\n",
      "Iteration 25, loss = 0.43709199\n",
      "Iteration 26, loss = 0.43366157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.43052879\n",
      "Iteration 28, loss = 0.42781290\n",
      "Iteration 29, loss = 0.42506948\n",
      "Iteration 30, loss = 0.42270514\n",
      "Iteration 31, loss = 0.42065215\n",
      "Iteration 32, loss = 0.41858876\n",
      "Iteration 33, loss = 0.41681935\n",
      "Iteration 34, loss = 0.41498452\n",
      "Iteration 35, loss = 0.41330910\n",
      "Iteration 36, loss = 0.41204567\n",
      "Iteration 37, loss = 0.41062982\n",
      "Iteration 38, loss = 0.40921699\n",
      "Iteration 39, loss = 0.40806217\n",
      "Iteration 40, loss = 0.40692912\n",
      "Iteration 41, loss = 0.40578435\n",
      "Iteration 42, loss = 0.40477487\n",
      "Iteration 43, loss = 0.40385330\n",
      "Iteration 44, loss = 0.40293777\n",
      "Iteration 45, loss = 0.40198079\n",
      "Iteration 46, loss = 0.40103449\n",
      "Iteration 47, loss = 0.40020283\n",
      "Iteration 48, loss = 0.39941825\n",
      "Iteration 49, loss = 0.39864542\n",
      "Iteration 50, loss = 0.39781984\n",
      "Iteration 51, loss = 0.39702271\n",
      "Iteration 52, loss = 0.39639426\n",
      "Iteration 53, loss = 0.39570737\n",
      "Iteration 54, loss = 0.39493956\n",
      "Iteration 55, loss = 0.39432095\n",
      "Iteration 56, loss = 0.39369678\n",
      "Iteration 57, loss = 0.39308553\n",
      "Iteration 58, loss = 0.39247959\n",
      "Iteration 59, loss = 0.39184569\n",
      "Iteration 60, loss = 0.39125995\n",
      "Iteration 61, loss = 0.39073307\n",
      "Iteration 62, loss = 0.39016577\n",
      "Iteration 63, loss = 0.38962582\n",
      "Iteration 64, loss = 0.38923067\n",
      "Iteration 65, loss = 0.38858417\n",
      "Iteration 66, loss = 0.38809010\n",
      "Iteration 67, loss = 0.38759003\n",
      "Iteration 68, loss = 0.38706479\n",
      "Iteration 69, loss = 0.38659932\n",
      "Iteration 70, loss = 0.38611212\n",
      "Iteration 71, loss = 0.38562052\n",
      "Iteration 72, loss = 0.38515813\n",
      "Iteration 73, loss = 0.38465774\n",
      "Iteration 74, loss = 0.38424921\n",
      "Iteration 75, loss = 0.38380628\n",
      "Iteration 76, loss = 0.38333877\n",
      "Iteration 77, loss = 0.38289066\n",
      "Iteration 78, loss = 0.38253571\n",
      "Iteration 79, loss = 0.38199209\n",
      "Iteration 80, loss = 0.38161118\n",
      "Iteration 81, loss = 0.38118700\n",
      "Iteration 82, loss = 0.38078723\n",
      "Iteration 83, loss = 0.38045605\n",
      "Iteration 84, loss = 0.38003710\n",
      "Iteration 85, loss = 0.37967016\n",
      "Iteration 86, loss = 0.37915533\n",
      "Iteration 87, loss = 0.37892138\n",
      "Iteration 88, loss = 0.37847173\n",
      "Iteration 89, loss = 0.37812333\n",
      "Iteration 90, loss = 0.37786183\n",
      "Iteration 91, loss = 0.37736253\n",
      "Iteration 92, loss = 0.37723483\n",
      "Iteration 93, loss = 0.37670701\n",
      "Iteration 94, loss = 0.37640183\n",
      "Iteration 95, loss = 0.37593140\n",
      "Iteration 96, loss = 0.37560722\n",
      "Iteration 97, loss = 0.37535723\n",
      "Iteration 98, loss = 0.37494003\n",
      "Iteration 99, loss = 0.37465343\n",
      "Iteration 100, loss = 0.37420412\n",
      "Iteration 101, loss = 0.37411449\n",
      "Iteration 102, loss = 0.37378781\n",
      "Iteration 103, loss = 0.37340677\n",
      "Iteration 104, loss = 0.37318301\n",
      "Iteration 105, loss = 0.37272326\n",
      "Iteration 106, loss = 0.37247578\n",
      "Iteration 107, loss = 0.37222242\n",
      "Iteration 108, loss = 0.37197531\n",
      "Iteration 109, loss = 0.37166365\n",
      "Iteration 110, loss = 0.37130886\n",
      "Iteration 111, loss = 0.37096685\n",
      "Iteration 112, loss = 0.37077366\n",
      "Iteration 113, loss = 0.37043450\n",
      "Iteration 114, loss = 0.37021366\n",
      "Iteration 115, loss = 0.36992025\n",
      "Iteration 116, loss = 0.36956378\n",
      "Iteration 117, loss = 0.36931472\n",
      "Iteration 118, loss = 0.36910877\n",
      "Iteration 119, loss = 0.36890452\n",
      "Iteration 120, loss = 0.36861274\n",
      "Iteration 121, loss = 0.36834769\n",
      "Iteration 122, loss = 0.36807546\n",
      "Iteration 123, loss = 0.36776660\n",
      "Iteration 124, loss = 0.36757687\n",
      "Iteration 125, loss = 0.36731202\n",
      "Iteration 126, loss = 0.36703358\n",
      "Iteration 127, loss = 0.36680311\n",
      "Iteration 128, loss = 0.36657365\n",
      "Iteration 129, loss = 0.36638662\n",
      "Iteration 130, loss = 0.36610702\n",
      "Iteration 131, loss = 0.36578660\n",
      "Iteration 132, loss = 0.36561328\n",
      "Iteration 133, loss = 0.36535602\n",
      "Iteration 134, loss = 0.36506741\n",
      "Iteration 135, loss = 0.36482083\n",
      "Iteration 136, loss = 0.36461660\n",
      "Iteration 137, loss = 0.36446915\n",
      "Iteration 138, loss = 0.36427369\n",
      "Iteration 139, loss = 0.36395060\n",
      "Iteration 140, loss = 0.36367244\n",
      "Iteration 141, loss = 0.36344627\n",
      "Iteration 142, loss = 0.36322087\n",
      "Iteration 143, loss = 0.36288847\n",
      "Iteration 144, loss = 0.36269024\n",
      "Iteration 145, loss = 0.36252910\n",
      "Iteration 146, loss = 0.36232630\n",
      "Iteration 147, loss = 0.36204204\n",
      "Iteration 148, loss = 0.36175744\n",
      "Iteration 149, loss = 0.36150897\n",
      "Iteration 150, loss = 0.36128574\n",
      "Iteration 151, loss = 0.36104612\n",
      "Iteration 152, loss = 0.36098407\n",
      "Iteration 153, loss = 0.36091059\n",
      "Iteration 154, loss = 0.36045178\n",
      "Iteration 155, loss = 0.36023247\n",
      "Iteration 156, loss = 0.36003901\n",
      "Iteration 157, loss = 0.35979222\n",
      "Iteration 158, loss = 0.35956884\n",
      "Iteration 159, loss = 0.35931837\n",
      "Iteration 160, loss = 0.35921634\n",
      "Iteration 161, loss = 0.35898171\n",
      "Iteration 162, loss = 0.35873506\n",
      "Iteration 163, loss = 0.35857521\n",
      "Iteration 164, loss = 0.35831635\n",
      "Iteration 165, loss = 0.35810003\n",
      "Iteration 166, loss = 0.35792137\n",
      "Iteration 167, loss = 0.35766792\n",
      "Iteration 168, loss = 0.35754666\n",
      "Iteration 169, loss = 0.35727813\n",
      "Iteration 170, loss = 0.35713009\n",
      "Iteration 171, loss = 0.35681643\n",
      "Iteration 172, loss = 0.35670442\n",
      "Iteration 173, loss = 0.35654512\n",
      "Iteration 174, loss = 0.35636358\n",
      "Iteration 175, loss = 0.35611796\n",
      "Iteration 176, loss = 0.35585630\n",
      "Iteration 177, loss = 0.35561683\n",
      "Iteration 178, loss = 0.35551322\n",
      "Iteration 179, loss = 0.35524922\n",
      "Iteration 180, loss = 0.35502081\n",
      "Iteration 181, loss = 0.35481801\n",
      "Iteration 182, loss = 0.35465812\n",
      "Iteration 183, loss = 0.35440606\n",
      "Iteration 184, loss = 0.35420402\n",
      "Iteration 185, loss = 0.35407125\n",
      "Iteration 186, loss = 0.35390294\n",
      "Iteration 187, loss = 0.35367945\n",
      "Iteration 188, loss = 0.35351126\n",
      "Iteration 189, loss = 0.35333928\n",
      "Iteration 190, loss = 0.35312693\n",
      "Iteration 191, loss = 0.35287650\n",
      "Iteration 192, loss = 0.35276777\n",
      "Iteration 193, loss = 0.35249827\n",
      "Iteration 194, loss = 0.35243721\n",
      "Iteration 195, loss = 0.35229063\n",
      "Iteration 196, loss = 0.35192900\n",
      "Iteration 197, loss = 0.35194078\n",
      "Iteration 198, loss = 0.35170984\n",
      "Iteration 199, loss = 0.35165955\n",
      "Iteration 200, loss = 0.35125538\n",
      "Iteration 1, loss = 0.79395106\n",
      "Iteration 2, loss = 0.75421289\n",
      "Iteration 3, loss = 0.71713602\n",
      "Iteration 4, loss = 0.68453685\n",
      "Iteration 5, loss = 0.65529408\n",
      "Iteration 6, loss = 0.62982410\n",
      "Iteration 7, loss = 0.60738284\n",
      "Iteration 8, loss = 0.58743655\n",
      "Iteration 9, loss = 0.57013560\n",
      "Iteration 10, loss = 0.55462790\n",
      "Iteration 11, loss = 0.54179184\n",
      "Iteration 12, loss = 0.53080996\n",
      "Iteration 13, loss = 0.52133752\n",
      "Iteration 14, loss = 0.51200902\n",
      "Iteration 15, loss = 0.50460571\n",
      "Iteration 16, loss = 0.49773151\n",
      "Iteration 17, loss = 0.49190009\n",
      "Iteration 18, loss = 0.48611127\n",
      "Iteration 19, loss = 0.48076618\n",
      "Iteration 20, loss = 0.47631531\n",
      "Iteration 21, loss = 0.47206355\n",
      "Iteration 22, loss = 0.46810433\n",
      "Iteration 23, loss = 0.46437129\n",
      "Iteration 24, loss = 0.46115295\n",
      "Iteration 25, loss = 0.45807083\n",
      "Iteration 26, loss = 0.45537111\n",
      "Iteration 27, loss = 0.45284940\n",
      "Iteration 28, loss = 0.45054349\n",
      "Iteration 29, loss = 0.44840487\n",
      "Iteration 30, loss = 0.44639784\n",
      "Iteration 31, loss = 0.44465146\n",
      "Iteration 32, loss = 0.44300194\n",
      "Iteration 33, loss = 0.44148906\n",
      "Iteration 34, loss = 0.43993574\n",
      "Iteration 35, loss = 0.43856423\n",
      "Iteration 36, loss = 0.43744594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.43616993\n",
      "Iteration 38, loss = 0.43505815\n",
      "Iteration 39, loss = 0.43400366\n",
      "Iteration 40, loss = 0.43308945\n",
      "Iteration 41, loss = 0.43209766\n",
      "Iteration 42, loss = 0.43119726\n",
      "Iteration 43, loss = 0.43032380\n",
      "Iteration 44, loss = 0.42950762\n",
      "Iteration 45, loss = 0.42870102\n",
      "Iteration 46, loss = 0.42787165\n",
      "Iteration 47, loss = 0.42714409\n",
      "Iteration 48, loss = 0.42630294\n",
      "Iteration 49, loss = 0.42563191\n",
      "Iteration 50, loss = 0.42491675\n",
      "Iteration 51, loss = 0.42413926\n",
      "Iteration 52, loss = 0.42350255\n",
      "Iteration 53, loss = 0.42284151\n",
      "Iteration 54, loss = 0.42212519\n",
      "Iteration 55, loss = 0.42145449\n",
      "Iteration 56, loss = 0.42082092\n",
      "Iteration 57, loss = 0.42019162\n",
      "Iteration 58, loss = 0.41966147\n",
      "Iteration 59, loss = 0.41893169\n",
      "Iteration 60, loss = 0.41832641\n",
      "Iteration 61, loss = 0.41774603\n",
      "Iteration 62, loss = 0.41714679\n",
      "Iteration 63, loss = 0.41661245\n",
      "Iteration 64, loss = 0.41607209\n",
      "Iteration 65, loss = 0.41555915\n",
      "Iteration 66, loss = 0.41500459\n",
      "Iteration 67, loss = 0.41442610\n",
      "Iteration 68, loss = 0.41390774\n",
      "Iteration 69, loss = 0.41344160\n",
      "Iteration 70, loss = 0.41287246\n",
      "Iteration 71, loss = 0.41227416\n",
      "Iteration 72, loss = 0.41176426\n",
      "Iteration 73, loss = 0.41118662\n",
      "Iteration 74, loss = 0.41075578\n",
      "Iteration 75, loss = 0.41016354\n",
      "Iteration 76, loss = 0.40972713\n",
      "Iteration 77, loss = 0.40925481\n",
      "Iteration 78, loss = 0.40888280\n",
      "Iteration 79, loss = 0.40822396\n",
      "Iteration 80, loss = 0.40782144\n",
      "Iteration 81, loss = 0.40737002\n",
      "Iteration 82, loss = 0.40685941\n",
      "Iteration 83, loss = 0.40651216\n",
      "Iteration 84, loss = 0.40599181\n",
      "Iteration 85, loss = 0.40556906\n",
      "Iteration 86, loss = 0.40506412\n",
      "Iteration 87, loss = 0.40483554\n",
      "Iteration 88, loss = 0.40431604\n",
      "Iteration 89, loss = 0.40378357\n",
      "Iteration 90, loss = 0.40343715\n",
      "Iteration 91, loss = 0.40299823\n",
      "Iteration 92, loss = 0.40270380\n",
      "Iteration 93, loss = 0.40215068\n",
      "Iteration 94, loss = 0.40180836\n",
      "Iteration 95, loss = 0.40136121\n",
      "Iteration 96, loss = 0.40100923\n",
      "Iteration 97, loss = 0.40068687\n",
      "Iteration 98, loss = 0.40022674\n",
      "Iteration 99, loss = 0.39986511\n",
      "Iteration 100, loss = 0.39944527\n",
      "Iteration 101, loss = 0.39933382\n",
      "Iteration 102, loss = 0.39881913\n",
      "Iteration 103, loss = 0.39851591\n",
      "Iteration 104, loss = 0.39814505\n",
      "Iteration 105, loss = 0.39775537\n",
      "Iteration 106, loss = 0.39735712\n",
      "Iteration 107, loss = 0.39704814\n",
      "Iteration 108, loss = 0.39674108\n",
      "Iteration 109, loss = 0.39646066\n",
      "Iteration 110, loss = 0.39599234\n",
      "Iteration 111, loss = 0.39559821\n",
      "Iteration 112, loss = 0.39536383\n",
      "Iteration 113, loss = 0.39497271\n",
      "Iteration 114, loss = 0.39463761\n",
      "Iteration 115, loss = 0.39438249\n",
      "Iteration 116, loss = 0.39401268\n",
      "Iteration 117, loss = 0.39370154\n",
      "Iteration 118, loss = 0.39339186\n",
      "Iteration 119, loss = 0.39314455\n",
      "Iteration 120, loss = 0.39296489\n",
      "Iteration 121, loss = 0.39245870\n",
      "Iteration 122, loss = 0.39221193\n",
      "Iteration 123, loss = 0.39188330\n",
      "Iteration 124, loss = 0.39169324\n",
      "Iteration 125, loss = 0.39127917\n",
      "Iteration 126, loss = 0.39113684\n",
      "Iteration 127, loss = 0.39069672\n",
      "Iteration 128, loss = 0.39041545\n",
      "Iteration 129, loss = 0.39013181\n",
      "Iteration 130, loss = 0.38987298\n",
      "Iteration 131, loss = 0.38952590\n",
      "Iteration 132, loss = 0.38924106\n",
      "Iteration 133, loss = 0.38897437\n",
      "Iteration 134, loss = 0.38864971\n",
      "Iteration 135, loss = 0.38841295\n",
      "Iteration 136, loss = 0.38812454\n",
      "Iteration 137, loss = 0.38786802\n",
      "Iteration 138, loss = 0.38746891\n",
      "Iteration 139, loss = 0.38739600\n",
      "Iteration 140, loss = 0.38707995\n",
      "Iteration 141, loss = 0.38671443\n",
      "Iteration 142, loss = 0.38646943\n",
      "Iteration 143, loss = 0.38617697\n",
      "Iteration 144, loss = 0.38593139\n",
      "Iteration 145, loss = 0.38567833\n",
      "Iteration 146, loss = 0.38536030\n",
      "Iteration 147, loss = 0.38520293\n",
      "Iteration 148, loss = 0.38479538\n",
      "Iteration 149, loss = 0.38453102\n",
      "Iteration 150, loss = 0.38423184\n",
      "Iteration 151, loss = 0.38399454\n",
      "Iteration 152, loss = 0.38379433\n",
      "Iteration 153, loss = 0.38373400\n",
      "Iteration 154, loss = 0.38321586\n",
      "Iteration 155, loss = 0.38301533\n",
      "Iteration 156, loss = 0.38266709\n",
      "Iteration 157, loss = 0.38244044\n",
      "Iteration 158, loss = 0.38214889\n",
      "Iteration 159, loss = 0.38185910\n",
      "Iteration 160, loss = 0.38169377\n",
      "Iteration 161, loss = 0.38131349\n",
      "Iteration 162, loss = 0.38101156\n",
      "Iteration 163, loss = 0.38082487\n",
      "Iteration 164, loss = 0.38059182\n",
      "Iteration 165, loss = 0.38029621\n",
      "Iteration 166, loss = 0.38022048\n",
      "Iteration 167, loss = 0.37990520\n",
      "Iteration 168, loss = 0.37955962\n",
      "Iteration 169, loss = 0.37937812\n",
      "Iteration 170, loss = 0.37911504\n",
      "Iteration 171, loss = 0.37876842\n",
      "Iteration 172, loss = 0.37853881\n",
      "Iteration 173, loss = 0.37829790\n",
      "Iteration 174, loss = 0.37807621\n",
      "Iteration 175, loss = 0.37784514\n",
      "Iteration 176, loss = 0.37754842\n",
      "Iteration 177, loss = 0.37720565\n",
      "Iteration 178, loss = 0.37692177\n",
      "Iteration 179, loss = 0.37674444\n",
      "Iteration 180, loss = 0.37648443\n",
      "Iteration 181, loss = 0.37617779\n",
      "Iteration 182, loss = 0.37598964\n",
      "Iteration 183, loss = 0.37582627\n",
      "Iteration 184, loss = 0.37552478\n",
      "Iteration 185, loss = 0.37534763\n",
      "Iteration 186, loss = 0.37495961\n",
      "Iteration 187, loss = 0.37470180\n",
      "Iteration 188, loss = 0.37444987\n",
      "Iteration 189, loss = 0.37414414\n",
      "Iteration 190, loss = 0.37388442\n",
      "Iteration 191, loss = 0.37363398\n",
      "Iteration 192, loss = 0.37351242\n",
      "Iteration 193, loss = 0.37314267\n",
      "Iteration 194, loss = 0.37304073\n",
      "Iteration 195, loss = 0.37281307\n",
      "Iteration 196, loss = 0.37247259\n",
      "Iteration 197, loss = 0.37242076\n",
      "Iteration 198, loss = 0.37209261\n",
      "Iteration 199, loss = 0.37195067\n",
      "Iteration 200, loss = 0.37183058\n",
      "Iteration 1, loss = 0.79133751\n",
      "Iteration 2, loss = 0.75021921\n",
      "Iteration 3, loss = 0.70933755\n",
      "Iteration 4, loss = 0.67387716\n",
      "Iteration 5, loss = 0.64183440\n",
      "Iteration 6, loss = 0.61349471\n",
      "Iteration 7, loss = 0.58789971\n",
      "Iteration 8, loss = 0.56659064\n",
      "Iteration 9, loss = 0.54684879\n",
      "Iteration 10, loss = 0.53046746\n",
      "Iteration 11, loss = 0.51609590\n",
      "Iteration 12, loss = 0.50382646\n",
      "Iteration 13, loss = 0.49191861\n",
      "Iteration 14, loss = 0.48301614\n",
      "Iteration 15, loss = 0.47452323\n",
      "Iteration 16, loss = 0.46678905\n",
      "Iteration 17, loss = 0.45965925\n",
      "Iteration 18, loss = 0.45404288\n",
      "Iteration 19, loss = 0.44823299\n",
      "Iteration 20, loss = 0.44308620\n",
      "Iteration 21, loss = 0.43833941\n",
      "Iteration 22, loss = 0.43400276\n",
      "Iteration 23, loss = 0.43030987\n",
      "Iteration 24, loss = 0.42668996\n",
      "Iteration 25, loss = 0.42342089\n",
      "Iteration 26, loss = 0.42048711\n",
      "Iteration 27, loss = 0.41766305\n",
      "Iteration 28, loss = 0.41533415\n",
      "Iteration 29, loss = 0.41293011\n",
      "Iteration 30, loss = 0.41076878\n",
      "Iteration 31, loss = 0.40887231\n",
      "Iteration 32, loss = 0.40723548\n",
      "Iteration 33, loss = 0.40548897\n",
      "Iteration 34, loss = 0.40402663\n",
      "Iteration 35, loss = 0.40259876\n",
      "Iteration 36, loss = 0.40138473\n",
      "Iteration 37, loss = 0.40009935\n",
      "Iteration 38, loss = 0.39901923\n",
      "Iteration 39, loss = 0.39796394\n",
      "Iteration 40, loss = 0.39696018\n",
      "Iteration 41, loss = 0.39600002\n",
      "Iteration 42, loss = 0.39509477\n",
      "Iteration 43, loss = 0.39423316\n",
      "Iteration 44, loss = 0.39351010\n",
      "Iteration 45, loss = 0.39261347\n",
      "Iteration 46, loss = 0.39194041\n",
      "Iteration 47, loss = 0.39119736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.39043730\n",
      "Iteration 49, loss = 0.38974823\n",
      "Iteration 50, loss = 0.38909076\n",
      "Iteration 51, loss = 0.38851778\n",
      "Iteration 52, loss = 0.38781970\n",
      "Iteration 53, loss = 0.38716433\n",
      "Iteration 54, loss = 0.38664207\n",
      "Iteration 55, loss = 0.38601388\n",
      "Iteration 56, loss = 0.38545899\n",
      "Iteration 57, loss = 0.38494308\n",
      "Iteration 58, loss = 0.38436859\n",
      "Iteration 59, loss = 0.38375489\n",
      "Iteration 60, loss = 0.38324015\n",
      "Iteration 61, loss = 0.38287330\n",
      "Iteration 62, loss = 0.38243555\n",
      "Iteration 63, loss = 0.38189466\n",
      "Iteration 64, loss = 0.38132728\n",
      "Iteration 65, loss = 0.38075276\n",
      "Iteration 66, loss = 0.38036947\n",
      "Iteration 67, loss = 0.37981887\n",
      "Iteration 68, loss = 0.37952791\n",
      "Iteration 69, loss = 0.37892052\n",
      "Iteration 70, loss = 0.37854733\n",
      "Iteration 71, loss = 0.37811474\n",
      "Iteration 72, loss = 0.37761751\n",
      "Iteration 73, loss = 0.37724211\n",
      "Iteration 74, loss = 0.37679514\n",
      "Iteration 75, loss = 0.37641278\n",
      "Iteration 76, loss = 0.37602741\n",
      "Iteration 77, loss = 0.37556741\n",
      "Iteration 78, loss = 0.37521572\n",
      "Iteration 79, loss = 0.37476055\n",
      "Iteration 80, loss = 0.37439359\n",
      "Iteration 81, loss = 0.37411059\n",
      "Iteration 82, loss = 0.37360515\n",
      "Iteration 83, loss = 0.37333515\n",
      "Iteration 84, loss = 0.37288710\n",
      "Iteration 85, loss = 0.37245768\n",
      "Iteration 86, loss = 0.37221739\n",
      "Iteration 87, loss = 0.37177074\n",
      "Iteration 88, loss = 0.37156177\n",
      "Iteration 89, loss = 0.37107477\n",
      "Iteration 90, loss = 0.37074959\n",
      "Iteration 91, loss = 0.37051238\n",
      "Iteration 92, loss = 0.37015345\n",
      "Iteration 93, loss = 0.36974368\n",
      "Iteration 94, loss = 0.36949289\n",
      "Iteration 95, loss = 0.36910013\n",
      "Iteration 96, loss = 0.36898497\n",
      "Iteration 97, loss = 0.36850128\n",
      "Iteration 98, loss = 0.36823816\n",
      "Iteration 99, loss = 0.36789009\n",
      "Iteration 100, loss = 0.36766101\n",
      "Iteration 101, loss = 0.36728064\n",
      "Iteration 102, loss = 0.36704972\n",
      "Iteration 103, loss = 0.36668348\n",
      "Iteration 104, loss = 0.36648548\n",
      "Iteration 105, loss = 0.36605334\n",
      "Iteration 106, loss = 0.36582996\n",
      "Iteration 107, loss = 0.36554329\n",
      "Iteration 108, loss = 0.36532712\n",
      "Iteration 109, loss = 0.36496752\n",
      "Iteration 110, loss = 0.36471920\n",
      "Iteration 111, loss = 0.36438918\n",
      "Iteration 112, loss = 0.36403483\n",
      "Iteration 113, loss = 0.36382467\n",
      "Iteration 114, loss = 0.36351137\n",
      "Iteration 115, loss = 0.36331155\n",
      "Iteration 116, loss = 0.36302702\n",
      "Iteration 117, loss = 0.36269799\n",
      "Iteration 118, loss = 0.36246353\n",
      "Iteration 119, loss = 0.36215401\n",
      "Iteration 120, loss = 0.36185783\n",
      "Iteration 121, loss = 0.36166412\n",
      "Iteration 122, loss = 0.36135392\n",
      "Iteration 123, loss = 0.36108600\n",
      "Iteration 124, loss = 0.36088211\n",
      "Iteration 125, loss = 0.36059465\n",
      "Iteration 126, loss = 0.36026131\n",
      "Iteration 127, loss = 0.36007925\n",
      "Iteration 128, loss = 0.35972864\n",
      "Iteration 129, loss = 0.35946656\n",
      "Iteration 130, loss = 0.35920022\n",
      "Iteration 131, loss = 0.35902321\n",
      "Iteration 132, loss = 0.35871488\n",
      "Iteration 133, loss = 0.35841803\n",
      "Iteration 134, loss = 0.35819415\n",
      "Iteration 135, loss = 0.35788559\n",
      "Iteration 136, loss = 0.35767285\n",
      "Iteration 137, loss = 0.35754061\n",
      "Iteration 138, loss = 0.35714181\n",
      "Iteration 139, loss = 0.35700310\n",
      "Iteration 140, loss = 0.35673139\n",
      "Iteration 141, loss = 0.35645877\n",
      "Iteration 142, loss = 0.35619942\n",
      "Iteration 143, loss = 0.35599264\n",
      "Iteration 144, loss = 0.35569202\n",
      "Iteration 145, loss = 0.35564832\n",
      "Iteration 146, loss = 0.35536354\n",
      "Iteration 147, loss = 0.35500461\n",
      "Iteration 148, loss = 0.35493424\n",
      "Iteration 149, loss = 0.35457969\n",
      "Iteration 150, loss = 0.35429440\n",
      "Iteration 151, loss = 0.35419311\n",
      "Iteration 152, loss = 0.35385283\n",
      "Iteration 153, loss = 0.35361832\n",
      "Iteration 154, loss = 0.35355557\n",
      "Iteration 155, loss = 0.35338663\n",
      "Iteration 156, loss = 0.35291242\n",
      "Iteration 157, loss = 0.35270304\n",
      "Iteration 158, loss = 0.35254115\n",
      "Iteration 159, loss = 0.35228078\n",
      "Iteration 160, loss = 0.35212517\n",
      "Iteration 161, loss = 0.35184551\n",
      "Iteration 162, loss = 0.35170602\n",
      "Iteration 163, loss = 0.35140505\n",
      "Iteration 164, loss = 0.35123534\n",
      "Iteration 165, loss = 0.35108322\n",
      "Iteration 166, loss = 0.35088016\n",
      "Iteration 167, loss = 0.35066705\n",
      "Iteration 168, loss = 0.35037188\n",
      "Iteration 169, loss = 0.35020008\n",
      "Iteration 170, loss = 0.34995498\n",
      "Iteration 171, loss = 0.34970450\n",
      "Iteration 172, loss = 0.34967366\n",
      "Iteration 173, loss = 0.34951632\n",
      "Iteration 174, loss = 0.34910550\n",
      "Iteration 175, loss = 0.34895299\n",
      "Iteration 176, loss = 0.34903414\n",
      "Iteration 177, loss = 0.34873338\n",
      "Iteration 178, loss = 0.34840190\n",
      "Iteration 179, loss = 0.34830247\n",
      "Iteration 180, loss = 0.34792471\n",
      "Iteration 181, loss = 0.34803552\n",
      "Iteration 182, loss = 0.34761268\n",
      "Iteration 183, loss = 0.34748144\n",
      "Iteration 184, loss = 0.34728780\n",
      "Iteration 185, loss = 0.34712021\n",
      "Iteration 186, loss = 0.34684550\n",
      "Iteration 187, loss = 0.34679531\n",
      "Iteration 188, loss = 0.34639547\n",
      "Iteration 189, loss = 0.34620360\n",
      "Iteration 190, loss = 0.34610855\n",
      "Iteration 191, loss = 0.34574627\n",
      "Iteration 192, loss = 0.34565542\n",
      "Iteration 193, loss = 0.34545397\n",
      "Iteration 194, loss = 0.34528603\n",
      "Iteration 195, loss = 0.34508667\n",
      "Iteration 196, loss = 0.34486905\n",
      "Iteration 197, loss = 0.34457903\n",
      "Iteration 198, loss = 0.34449174\n",
      "Iteration 199, loss = 0.34428513\n",
      "Iteration 200, loss = 0.34407077\n",
      "Iteration 1, loss = 0.79495178\n",
      "Iteration 2, loss = 0.75439763\n",
      "Iteration 3, loss = 0.71470615\n",
      "Iteration 4, loss = 0.68141833\n",
      "Iteration 5, loss = 0.65128674\n",
      "Iteration 6, loss = 0.62474260\n",
      "Iteration 7, loss = 0.60060353\n",
      "Iteration 8, loss = 0.58131721\n",
      "Iteration 9, loss = 0.56320566\n",
      "Iteration 10, loss = 0.54780497\n",
      "Iteration 11, loss = 0.53470953\n",
      "Iteration 12, loss = 0.52362192\n",
      "Iteration 13, loss = 0.51312125\n",
      "Iteration 14, loss = 0.50454352\n",
      "Iteration 15, loss = 0.49672683\n",
      "Iteration 16, loss = 0.48954239\n",
      "Iteration 17, loss = 0.48297699\n",
      "Iteration 18, loss = 0.47737725\n",
      "Iteration 19, loss = 0.47182412\n",
      "Iteration 20, loss = 0.46676597\n",
      "Iteration 21, loss = 0.46212072\n",
      "Iteration 22, loss = 0.45794146\n",
      "Iteration 23, loss = 0.45451584\n",
      "Iteration 24, loss = 0.45087297\n",
      "Iteration 25, loss = 0.44776413\n",
      "Iteration 26, loss = 0.44471431\n",
      "Iteration 27, loss = 0.44209542\n",
      "Iteration 28, loss = 0.43988682\n",
      "Iteration 29, loss = 0.43749276\n",
      "Iteration 30, loss = 0.43547649\n",
      "Iteration 31, loss = 0.43344560\n",
      "Iteration 32, loss = 0.43198891\n",
      "Iteration 33, loss = 0.43039597\n",
      "Iteration 34, loss = 0.42879163\n",
      "Iteration 35, loss = 0.42745913\n",
      "Iteration 36, loss = 0.42633684\n",
      "Iteration 37, loss = 0.42511806\n",
      "Iteration 38, loss = 0.42412674\n",
      "Iteration 39, loss = 0.42313457\n",
      "Iteration 40, loss = 0.42234338\n",
      "Iteration 41, loss = 0.42133140\n",
      "Iteration 42, loss = 0.42047467\n",
      "Iteration 43, loss = 0.41972641\n",
      "Iteration 44, loss = 0.41909778\n",
      "Iteration 45, loss = 0.41828081\n",
      "Iteration 46, loss = 0.41764254\n",
      "Iteration 47, loss = 0.41695406\n",
      "Iteration 48, loss = 0.41637734\n",
      "Iteration 49, loss = 0.41573112\n",
      "Iteration 50, loss = 0.41514373\n",
      "Iteration 51, loss = 0.41461312\n",
      "Iteration 52, loss = 0.41397793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.41339410\n",
      "Iteration 54, loss = 0.41299775\n",
      "Iteration 55, loss = 0.41240278\n",
      "Iteration 56, loss = 0.41190717\n",
      "Iteration 57, loss = 0.41144921\n",
      "Iteration 58, loss = 0.41096014\n",
      "Iteration 59, loss = 0.41048975\n",
      "Iteration 60, loss = 0.41001518\n",
      "Iteration 61, loss = 0.40961746\n",
      "Iteration 62, loss = 0.40914561\n",
      "Iteration 63, loss = 0.40875751\n",
      "Iteration 64, loss = 0.40827948\n",
      "Iteration 65, loss = 0.40779316\n",
      "Iteration 66, loss = 0.40750252\n",
      "Iteration 67, loss = 0.40697120\n",
      "Iteration 68, loss = 0.40672234\n",
      "Iteration 69, loss = 0.40624740\n",
      "Iteration 70, loss = 0.40585645\n",
      "Iteration 71, loss = 0.40545372\n",
      "Iteration 72, loss = 0.40500583\n",
      "Iteration 73, loss = 0.40474116\n",
      "Iteration 74, loss = 0.40433935\n",
      "Iteration 75, loss = 0.40390019\n",
      "Iteration 76, loss = 0.40361864\n",
      "Iteration 77, loss = 0.40316451\n",
      "Iteration 78, loss = 0.40279837\n",
      "Iteration 79, loss = 0.40240004\n",
      "Iteration 80, loss = 0.40205508\n",
      "Iteration 81, loss = 0.40184142\n",
      "Iteration 82, loss = 0.40132222\n",
      "Iteration 83, loss = 0.40117177\n",
      "Iteration 84, loss = 0.40070660\n",
      "Iteration 85, loss = 0.40028155\n",
      "Iteration 86, loss = 0.39994693\n",
      "Iteration 87, loss = 0.39964706\n",
      "Iteration 88, loss = 0.39937828\n",
      "Iteration 89, loss = 0.39903904\n",
      "Iteration 90, loss = 0.39865530\n",
      "Iteration 91, loss = 0.39846338\n",
      "Iteration 92, loss = 0.39806203\n",
      "Iteration 93, loss = 0.39766550\n",
      "Iteration 94, loss = 0.39754691\n",
      "Iteration 95, loss = 0.39713847\n",
      "Iteration 96, loss = 0.39687784\n",
      "Iteration 97, loss = 0.39660261\n",
      "Iteration 98, loss = 0.39635719\n",
      "Iteration 99, loss = 0.39596774\n",
      "Iteration 100, loss = 0.39571458\n",
      "Iteration 101, loss = 0.39540908\n",
      "Iteration 102, loss = 0.39513065\n",
      "Iteration 103, loss = 0.39487480\n",
      "Iteration 104, loss = 0.39459912\n",
      "Iteration 105, loss = 0.39434343\n",
      "Iteration 106, loss = 0.39398928\n",
      "Iteration 107, loss = 0.39370124\n",
      "Iteration 108, loss = 0.39346559\n",
      "Iteration 109, loss = 0.39317260\n",
      "Iteration 110, loss = 0.39298318\n",
      "Iteration 111, loss = 0.39268791\n",
      "Iteration 112, loss = 0.39228895\n",
      "Iteration 113, loss = 0.39211373\n",
      "Iteration 114, loss = 0.39182591\n",
      "Iteration 115, loss = 0.39155090\n",
      "Iteration 116, loss = 0.39140543\n",
      "Iteration 117, loss = 0.39100998\n",
      "Iteration 118, loss = 0.39083325\n",
      "Iteration 119, loss = 0.39056380\n",
      "Iteration 120, loss = 0.39026854\n",
      "Iteration 121, loss = 0.39004111\n",
      "Iteration 122, loss = 0.38975606\n",
      "Iteration 123, loss = 0.38950227\n",
      "Iteration 124, loss = 0.38925100\n",
      "Iteration 125, loss = 0.38901975\n",
      "Iteration 126, loss = 0.38875939\n",
      "Iteration 127, loss = 0.38852308\n",
      "Iteration 128, loss = 0.38820696\n",
      "Iteration 129, loss = 0.38801407\n",
      "Iteration 130, loss = 0.38777867\n",
      "Iteration 131, loss = 0.38752313\n",
      "Iteration 132, loss = 0.38727238\n",
      "Iteration 133, loss = 0.38709805\n",
      "Iteration 134, loss = 0.38682903\n",
      "Iteration 135, loss = 0.38663155\n",
      "Iteration 136, loss = 0.38627719\n",
      "Iteration 137, loss = 0.38622412\n",
      "Iteration 138, loss = 0.38588196\n",
      "Iteration 139, loss = 0.38587131\n",
      "Iteration 140, loss = 0.38542766\n",
      "Iteration 141, loss = 0.38527539\n",
      "Iteration 142, loss = 0.38496509\n",
      "Iteration 143, loss = 0.38469789\n",
      "Iteration 144, loss = 0.38452454\n",
      "Iteration 145, loss = 0.38427880\n",
      "Iteration 146, loss = 0.38412343\n",
      "Iteration 147, loss = 0.38390404\n",
      "Iteration 148, loss = 0.38375168\n",
      "Iteration 149, loss = 0.38337730\n",
      "Iteration 150, loss = 0.38306998\n",
      "Iteration 151, loss = 0.38293735\n",
      "Iteration 152, loss = 0.38264560\n",
      "Iteration 153, loss = 0.38241763\n",
      "Iteration 154, loss = 0.38236767\n",
      "Iteration 155, loss = 0.38226292\n",
      "Iteration 156, loss = 0.38180715\n",
      "Iteration 157, loss = 0.38148620\n",
      "Iteration 158, loss = 0.38134782\n",
      "Iteration 159, loss = 0.38123577\n",
      "Iteration 160, loss = 0.38084863\n",
      "Iteration 161, loss = 0.38067233\n",
      "Iteration 162, loss = 0.38053721\n",
      "Iteration 163, loss = 0.38021470\n",
      "Iteration 164, loss = 0.37995283\n",
      "Iteration 165, loss = 0.37978113\n",
      "Iteration 166, loss = 0.37968710\n",
      "Iteration 167, loss = 0.37943550\n",
      "Iteration 168, loss = 0.37915663\n",
      "Iteration 169, loss = 0.37887768\n",
      "Iteration 170, loss = 0.37871145\n",
      "Iteration 171, loss = 0.37855633\n",
      "Iteration 172, loss = 0.37833500\n",
      "Iteration 173, loss = 0.37817262\n",
      "Iteration 174, loss = 0.37792797\n",
      "Iteration 175, loss = 0.37770752\n",
      "Iteration 176, loss = 0.37757431\n",
      "Iteration 177, loss = 0.37734145\n",
      "Iteration 178, loss = 0.37705524\n",
      "Iteration 179, loss = 0.37686489\n",
      "Iteration 180, loss = 0.37664098\n",
      "Iteration 181, loss = 0.37659291\n",
      "Iteration 182, loss = 0.37618647\n",
      "Iteration 183, loss = 0.37592948\n",
      "Iteration 184, loss = 0.37580088\n",
      "Iteration 185, loss = 0.37562636\n",
      "Iteration 186, loss = 0.37538268\n",
      "Iteration 187, loss = 0.37541983\n",
      "Iteration 188, loss = 0.37501005\n",
      "Iteration 189, loss = 0.37477881\n",
      "Iteration 190, loss = 0.37464632\n",
      "Iteration 191, loss = 0.37431818\n",
      "Iteration 192, loss = 0.37425720\n",
      "Iteration 193, loss = 0.37394340\n",
      "Iteration 194, loss = 0.37382247\n",
      "Iteration 195, loss = 0.37361488\n",
      "Iteration 196, loss = 0.37339426\n",
      "Iteration 197, loss = 0.37315158\n",
      "Iteration 198, loss = 0.37295186\n",
      "Iteration 199, loss = 0.37279127\n",
      "Iteration 200, loss = 0.37255751\n",
      "Iteration 1, loss = 0.79371532\n",
      "Iteration 2, loss = 0.75270072\n",
      "Iteration 3, loss = 0.71482239\n",
      "Iteration 4, loss = 0.68102282\n",
      "Iteration 5, loss = 0.65104071\n",
      "Iteration 6, loss = 0.62363908\n",
      "Iteration 7, loss = 0.60073824\n",
      "Iteration 8, loss = 0.58051631\n",
      "Iteration 9, loss = 0.56268171\n",
      "Iteration 10, loss = 0.54643624\n",
      "Iteration 11, loss = 0.53354307\n",
      "Iteration 12, loss = 0.52140244\n",
      "Iteration 13, loss = 0.51114147\n",
      "Iteration 14, loss = 0.50145172\n",
      "Iteration 15, loss = 0.49296632\n",
      "Iteration 16, loss = 0.48545202\n",
      "Iteration 17, loss = 0.47838215\n",
      "Iteration 18, loss = 0.47246373\n",
      "Iteration 19, loss = 0.46646351\n",
      "Iteration 20, loss = 0.46065436\n",
      "Iteration 21, loss = 0.45595082\n",
      "Iteration 22, loss = 0.45134016\n",
      "Iteration 23, loss = 0.44692748\n",
      "Iteration 24, loss = 0.44315808\n",
      "Iteration 25, loss = 0.43950548\n",
      "Iteration 26, loss = 0.43618095\n",
      "Iteration 27, loss = 0.43292835\n",
      "Iteration 28, loss = 0.43030290\n",
      "Iteration 29, loss = 0.42750702\n",
      "Iteration 30, loss = 0.42527338\n",
      "Iteration 31, loss = 0.42313343\n",
      "Iteration 32, loss = 0.42109507\n",
      "Iteration 33, loss = 0.41933382\n",
      "Iteration 34, loss = 0.41746556\n",
      "Iteration 35, loss = 0.41583753\n",
      "Iteration 36, loss = 0.41438398\n",
      "Iteration 37, loss = 0.41303519\n",
      "Iteration 38, loss = 0.41162751\n",
      "Iteration 39, loss = 0.41035037\n",
      "Iteration 40, loss = 0.40918879\n",
      "Iteration 41, loss = 0.40806036\n",
      "Iteration 42, loss = 0.40698069\n",
      "Iteration 43, loss = 0.40593948\n",
      "Iteration 44, loss = 0.40504655\n",
      "Iteration 45, loss = 0.40400965\n",
      "Iteration 46, loss = 0.40311783\n",
      "Iteration 47, loss = 0.40218605\n",
      "Iteration 48, loss = 0.40135837\n",
      "Iteration 49, loss = 0.40057256\n",
      "Iteration 50, loss = 0.39978623\n",
      "Iteration 51, loss = 0.39896935\n",
      "Iteration 52, loss = 0.39824421\n",
      "Iteration 53, loss = 0.39751945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.39677242\n",
      "Iteration 55, loss = 0.39607699\n",
      "Iteration 56, loss = 0.39545707\n",
      "Iteration 57, loss = 0.39476536\n",
      "Iteration 58, loss = 0.39407802\n",
      "Iteration 59, loss = 0.39341618\n",
      "Iteration 60, loss = 0.39284499\n",
      "Iteration 61, loss = 0.39219810\n",
      "Iteration 62, loss = 0.39164949\n",
      "Iteration 63, loss = 0.39098713\n",
      "Iteration 64, loss = 0.39051471\n",
      "Iteration 65, loss = 0.38990001\n",
      "Iteration 66, loss = 0.38942455\n",
      "Iteration 67, loss = 0.38898169\n",
      "Iteration 68, loss = 0.38834782\n",
      "Iteration 69, loss = 0.38773107\n",
      "Iteration 70, loss = 0.38730408\n",
      "Iteration 71, loss = 0.38669136\n",
      "Iteration 72, loss = 0.38618555\n",
      "Iteration 73, loss = 0.38569200\n",
      "Iteration 74, loss = 0.38520078\n",
      "Iteration 75, loss = 0.38473598\n",
      "Iteration 76, loss = 0.38431711\n",
      "Iteration 77, loss = 0.38372786\n",
      "Iteration 78, loss = 0.38335243\n",
      "Iteration 79, loss = 0.38288812\n",
      "Iteration 80, loss = 0.38245485\n",
      "Iteration 81, loss = 0.38202967\n",
      "Iteration 82, loss = 0.38163181\n",
      "Iteration 83, loss = 0.38115257\n",
      "Iteration 84, loss = 0.38074341\n",
      "Iteration 85, loss = 0.38034731\n",
      "Iteration 86, loss = 0.37990635\n",
      "Iteration 87, loss = 0.37949965\n",
      "Iteration 88, loss = 0.37904508\n",
      "Iteration 89, loss = 0.37860686\n",
      "Iteration 90, loss = 0.37829012\n",
      "Iteration 91, loss = 0.37779878\n",
      "Iteration 92, loss = 0.37767885\n",
      "Iteration 93, loss = 0.37707164\n",
      "Iteration 94, loss = 0.37697139\n",
      "Iteration 95, loss = 0.37630937\n",
      "Iteration 96, loss = 0.37589313\n",
      "Iteration 97, loss = 0.37555577\n",
      "Iteration 98, loss = 0.37514886\n",
      "Iteration 99, loss = 0.37484391\n",
      "Iteration 100, loss = 0.37445409\n",
      "Iteration 101, loss = 0.37428749\n",
      "Iteration 102, loss = 0.37386937\n",
      "Iteration 103, loss = 0.37349544\n",
      "Iteration 104, loss = 0.37331097\n",
      "Iteration 105, loss = 0.37286156\n",
      "Iteration 106, loss = 0.37257904\n",
      "Iteration 107, loss = 0.37226132\n",
      "Iteration 108, loss = 0.37197803\n",
      "Iteration 109, loss = 0.37160596\n",
      "Iteration 110, loss = 0.37128269\n",
      "Iteration 111, loss = 0.37096924\n",
      "Iteration 112, loss = 0.37072875\n",
      "Iteration 113, loss = 0.37034156\n",
      "Iteration 114, loss = 0.37010160\n",
      "Iteration 115, loss = 0.36979479\n",
      "Iteration 116, loss = 0.36940542\n",
      "Iteration 117, loss = 0.36911791\n",
      "Iteration 118, loss = 0.36884308\n",
      "Iteration 119, loss = 0.36863016\n",
      "Iteration 120, loss = 0.36833364\n",
      "Iteration 121, loss = 0.36801872\n",
      "Iteration 122, loss = 0.36774273\n",
      "Iteration 123, loss = 0.36739360\n",
      "Iteration 124, loss = 0.36710736\n",
      "Iteration 125, loss = 0.36689891\n",
      "Iteration 126, loss = 0.36651985\n",
      "Iteration 127, loss = 0.36618522\n",
      "Iteration 128, loss = 0.36601174\n",
      "Iteration 129, loss = 0.36583478\n",
      "Iteration 130, loss = 0.36538746\n",
      "Iteration 131, loss = 0.36515461\n",
      "Iteration 132, loss = 0.36486995\n",
      "Iteration 133, loss = 0.36458845\n",
      "Iteration 134, loss = 0.36433787\n",
      "Iteration 135, loss = 0.36396561\n",
      "Iteration 136, loss = 0.36392487\n",
      "Iteration 137, loss = 0.36344200\n",
      "Iteration 138, loss = 0.36326260\n",
      "Iteration 139, loss = 0.36308468\n",
      "Iteration 140, loss = 0.36280156\n",
      "Iteration 141, loss = 0.36249566\n",
      "Iteration 142, loss = 0.36232148\n",
      "Iteration 143, loss = 0.36204747\n",
      "Iteration 144, loss = 0.36183448\n",
      "Iteration 145, loss = 0.36162014\n",
      "Iteration 146, loss = 0.36131627\n",
      "Iteration 147, loss = 0.36113489\n",
      "Iteration 148, loss = 0.36091940\n",
      "Iteration 149, loss = 0.36059291\n",
      "Iteration 150, loss = 0.36030066\n",
      "Iteration 151, loss = 0.36011006\n",
      "Iteration 152, loss = 0.36002676\n",
      "Iteration 153, loss = 0.35993898\n",
      "Iteration 154, loss = 0.35955431\n",
      "Iteration 155, loss = 0.35907882\n",
      "Iteration 156, loss = 0.35896970\n",
      "Iteration 157, loss = 0.35872929\n",
      "Iteration 158, loss = 0.35846342\n",
      "Iteration 159, loss = 0.35829602\n",
      "Iteration 160, loss = 0.35809060\n",
      "Iteration 161, loss = 0.35786445\n",
      "Iteration 162, loss = 0.35763993\n",
      "Iteration 163, loss = 0.35738988\n",
      "Iteration 164, loss = 0.35725086\n",
      "Iteration 165, loss = 0.35708815\n",
      "Iteration 166, loss = 0.35682843\n",
      "Iteration 167, loss = 0.35652449\n",
      "Iteration 168, loss = 0.35633566\n",
      "Iteration 169, loss = 0.35617669\n",
      "Iteration 170, loss = 0.35608207\n",
      "Iteration 171, loss = 0.35573614\n",
      "Iteration 172, loss = 0.35570451\n",
      "Iteration 173, loss = 0.35547301\n",
      "Iteration 174, loss = 0.35522387\n",
      "Iteration 175, loss = 0.35499510\n",
      "Iteration 176, loss = 0.35478123\n",
      "Iteration 177, loss = 0.35448863\n",
      "Iteration 178, loss = 0.35453152\n",
      "Iteration 179, loss = 0.35410676\n",
      "Iteration 180, loss = 0.35392440\n",
      "Iteration 181, loss = 0.35371085\n",
      "Iteration 182, loss = 0.35351684\n",
      "Iteration 183, loss = 0.35321577\n",
      "Iteration 184, loss = 0.35313347\n",
      "Iteration 185, loss = 0.35296965\n",
      "Iteration 186, loss = 0.35273838\n",
      "Iteration 187, loss = 0.35254091\n",
      "Iteration 188, loss = 0.35226278\n",
      "Iteration 189, loss = 0.35208596\n",
      "Iteration 190, loss = 0.35183965\n",
      "Iteration 191, loss = 0.35166663\n",
      "Iteration 192, loss = 0.35141515\n",
      "Iteration 193, loss = 0.35120557\n",
      "Iteration 194, loss = 0.35114649\n",
      "Iteration 195, loss = 0.35091467\n",
      "Iteration 196, loss = 0.35055894\n",
      "Iteration 197, loss = 0.35054803\n",
      "Iteration 198, loss = 0.35015887\n",
      "Iteration 199, loss = 0.35003473\n",
      "Iteration 200, loss = 0.34967679\n",
      "Iteration 1, loss = 0.78896725\n",
      "Iteration 2, loss = 0.74927512\n",
      "Iteration 3, loss = 0.71229637\n",
      "Iteration 4, loss = 0.67925173\n",
      "Iteration 5, loss = 0.64898551\n",
      "Iteration 6, loss = 0.62220968\n",
      "Iteration 7, loss = 0.59874681\n",
      "Iteration 8, loss = 0.57767724\n",
      "Iteration 9, loss = 0.55931882\n",
      "Iteration 10, loss = 0.54259321\n",
      "Iteration 11, loss = 0.52896453\n",
      "Iteration 12, loss = 0.51637242\n",
      "Iteration 13, loss = 0.50573934\n",
      "Iteration 14, loss = 0.49543367\n",
      "Iteration 15, loss = 0.48673230\n",
      "Iteration 16, loss = 0.47878739\n",
      "Iteration 17, loss = 0.47160258\n",
      "Iteration 18, loss = 0.46540011\n",
      "Iteration 19, loss = 0.45890470\n",
      "Iteration 20, loss = 0.45336571\n",
      "Iteration 21, loss = 0.44815292\n",
      "Iteration 22, loss = 0.44345108\n",
      "Iteration 23, loss = 0.43887289\n",
      "Iteration 24, loss = 0.43494748\n",
      "Iteration 25, loss = 0.43122015\n",
      "Iteration 26, loss = 0.42776633\n",
      "Iteration 27, loss = 0.42461068\n",
      "Iteration 28, loss = 0.42187270\n",
      "Iteration 29, loss = 0.41910495\n",
      "Iteration 30, loss = 0.41671667\n",
      "Iteration 31, loss = 0.41464314\n",
      "Iteration 32, loss = 0.41255594\n",
      "Iteration 33, loss = 0.41076229\n",
      "Iteration 34, loss = 0.40890506\n",
      "Iteration 35, loss = 0.40720578\n",
      "Iteration 36, loss = 0.40592453\n",
      "Iteration 37, loss = 0.40449176\n",
      "Iteration 38, loss = 0.40306239\n",
      "Iteration 39, loss = 0.40188955\n",
      "Iteration 40, loss = 0.40073540\n",
      "Iteration 41, loss = 0.39957365\n",
      "Iteration 42, loss = 0.39853877\n",
      "Iteration 43, loss = 0.39759823\n",
      "Iteration 44, loss = 0.39666565\n",
      "Iteration 45, loss = 0.39569813\n",
      "Iteration 46, loss = 0.39474499\n",
      "Iteration 47, loss = 0.39389918\n",
      "Iteration 48, loss = 0.39310348\n",
      "Iteration 49, loss = 0.39231551\n",
      "Iteration 50, loss = 0.39146987\n",
      "Iteration 51, loss = 0.39065354\n",
      "Iteration 52, loss = 0.39001264\n",
      "Iteration 53, loss = 0.38930878\n",
      "Iteration 54, loss = 0.38851831\n",
      "Iteration 55, loss = 0.38787372\n",
      "Iteration 56, loss = 0.38722210\n",
      "Iteration 57, loss = 0.38657877\n",
      "Iteration 58, loss = 0.38594300\n",
      "Iteration 59, loss = 0.38528485\n",
      "Iteration 60, loss = 0.38467894\n",
      "Iteration 61, loss = 0.38413218\n",
      "Iteration 62, loss = 0.38354319\n",
      "Iteration 63, loss = 0.38298412\n",
      "Iteration 64, loss = 0.38256855\n",
      "Iteration 65, loss = 0.38190007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.38138872\n",
      "Iteration 67, loss = 0.38086585\n",
      "Iteration 68, loss = 0.38032657\n",
      "Iteration 69, loss = 0.37984529\n",
      "Iteration 70, loss = 0.37934214\n",
      "Iteration 71, loss = 0.37883660\n",
      "Iteration 72, loss = 0.37835531\n",
      "Iteration 73, loss = 0.37783551\n",
      "Iteration 74, loss = 0.37740500\n",
      "Iteration 75, loss = 0.37693784\n",
      "Iteration 76, loss = 0.37644394\n",
      "Iteration 77, loss = 0.37596899\n",
      "Iteration 78, loss = 0.37559127\n",
      "Iteration 79, loss = 0.37503037\n",
      "Iteration 80, loss = 0.37463482\n",
      "Iteration 81, loss = 0.37419201\n",
      "Iteration 82, loss = 0.37377054\n",
      "Iteration 83, loss = 0.37341739\n",
      "Iteration 84, loss = 0.37297004\n",
      "Iteration 85, loss = 0.37257962\n",
      "Iteration 86, loss = 0.37203941\n",
      "Iteration 87, loss = 0.37177525\n",
      "Iteration 88, loss = 0.37129760\n",
      "Iteration 89, loss = 0.37093087\n",
      "Iteration 90, loss = 0.37064099\n",
      "Iteration 91, loss = 0.37012332\n",
      "Iteration 92, loss = 0.36997997\n",
      "Iteration 93, loss = 0.36942772\n",
      "Iteration 94, loss = 0.36909401\n",
      "Iteration 95, loss = 0.36859993\n",
      "Iteration 96, loss = 0.36825521\n",
      "Iteration 97, loss = 0.36798055\n",
      "Iteration 98, loss = 0.36753745\n",
      "Iteration 99, loss = 0.36722251\n",
      "Iteration 100, loss = 0.36674642\n",
      "Iteration 101, loss = 0.36662682\n",
      "Iteration 102, loss = 0.36627213\n",
      "Iteration 103, loss = 0.36586878\n",
      "Iteration 104, loss = 0.36562055\n",
      "Iteration 105, loss = 0.36513227\n",
      "Iteration 106, loss = 0.36486038\n",
      "Iteration 107, loss = 0.36458148\n",
      "Iteration 108, loss = 0.36431238\n",
      "Iteration 109, loss = 0.36398063\n",
      "Iteration 110, loss = 0.36359591\n",
      "Iteration 111, loss = 0.36323447\n",
      "Iteration 112, loss = 0.36301592\n",
      "Iteration 113, loss = 0.36265310\n",
      "Iteration 114, loss = 0.36240652\n",
      "Iteration 115, loss = 0.36207620\n",
      "Iteration 116, loss = 0.36169641\n",
      "Iteration 117, loss = 0.36142651\n",
      "Iteration 118, loss = 0.36119556\n",
      "Iteration 119, loss = 0.36097016\n",
      "Iteration 120, loss = 0.36065180\n",
      "Iteration 121, loss = 0.36035547\n",
      "Iteration 122, loss = 0.36005130\n",
      "Iteration 123, loss = 0.35971566\n",
      "Iteration 124, loss = 0.35949734\n",
      "Iteration 125, loss = 0.35919610\n",
      "Iteration 126, loss = 0.35887151\n",
      "Iteration 127, loss = 0.35860769\n",
      "Iteration 128, loss = 0.35834553\n",
      "Iteration 129, loss = 0.35813172\n",
      "Iteration 130, loss = 0.35781723\n",
      "Iteration 131, loss = 0.35746183\n",
      "Iteration 132, loss = 0.35725380\n",
      "Iteration 133, loss = 0.35696616\n",
      "Iteration 134, loss = 0.35664733\n",
      "Iteration 135, loss = 0.35637140\n",
      "Iteration 136, loss = 0.35614011\n",
      "Iteration 137, loss = 0.35596058\n",
      "Iteration 138, loss = 0.35574869\n",
      "Iteration 139, loss = 0.35537679\n",
      "Iteration 140, loss = 0.35507455\n",
      "Iteration 141, loss = 0.35480581\n",
      "Iteration 142, loss = 0.35456552\n",
      "Iteration 143, loss = 0.35418831\n",
      "Iteration 144, loss = 0.35396723\n",
      "Iteration 145, loss = 0.35376761\n",
      "Iteration 146, loss = 0.35354205\n",
      "Iteration 147, loss = 0.35322070\n",
      "Iteration 148, loss = 0.35289780\n",
      "Iteration 149, loss = 0.35262886\n",
      "Iteration 150, loss = 0.35237608\n",
      "Iteration 151, loss = 0.35211155\n",
      "Iteration 152, loss = 0.35202198\n",
      "Iteration 153, loss = 0.35191634\n",
      "Iteration 154, loss = 0.35140822\n",
      "Iteration 155, loss = 0.35114657\n",
      "Iteration 156, loss = 0.35091618\n",
      "Iteration 157, loss = 0.35064187\n",
      "Iteration 158, loss = 0.35038233\n",
      "Iteration 159, loss = 0.35010439\n",
      "Iteration 160, loss = 0.34997677\n",
      "Iteration 161, loss = 0.34971173\n",
      "Iteration 162, loss = 0.34943885\n",
      "Iteration 163, loss = 0.34925282\n",
      "Iteration 164, loss = 0.34896681\n",
      "Iteration 165, loss = 0.34873396\n",
      "Iteration 166, loss = 0.34853430\n",
      "Iteration 167, loss = 0.34826398\n",
      "Iteration 168, loss = 0.34811870\n",
      "Iteration 169, loss = 0.34783397\n",
      "Iteration 170, loss = 0.34766705\n",
      "Iteration 171, loss = 0.34732342\n",
      "Iteration 172, loss = 0.34719463\n",
      "Iteration 173, loss = 0.34701061\n",
      "Iteration 174, loss = 0.34681494\n",
      "Iteration 175, loss = 0.34655196\n",
      "Iteration 176, loss = 0.34626872\n",
      "Iteration 177, loss = 0.34599970\n",
      "Iteration 178, loss = 0.34587368\n",
      "Iteration 179, loss = 0.34559010\n",
      "Iteration 180, loss = 0.34535138\n",
      "Iteration 181, loss = 0.34512649\n",
      "Iteration 182, loss = 0.34493856\n",
      "Iteration 183, loss = 0.34467211\n",
      "Iteration 184, loss = 0.34444054\n",
      "Iteration 185, loss = 0.34428291\n",
      "Iteration 186, loss = 0.34410469\n",
      "Iteration 187, loss = 0.34385362\n",
      "Iteration 188, loss = 0.34367183\n",
      "Iteration 189, loss = 0.34347943\n",
      "Iteration 190, loss = 0.34323890\n",
      "Iteration 191, loss = 0.34296787\n",
      "Iteration 192, loss = 0.34284178\n",
      "Iteration 193, loss = 0.34255648\n",
      "Iteration 194, loss = 0.34247821\n",
      "Iteration 195, loss = 0.34230887\n",
      "Iteration 196, loss = 0.34190396\n",
      "Iteration 197, loss = 0.34189125\n",
      "Iteration 198, loss = 0.34163823\n",
      "Iteration 199, loss = 0.34157018\n",
      "Iteration 200, loss = 0.34113224\n",
      "Iteration 1, loss = 0.78857255\n",
      "Iteration 2, loss = 0.74883510\n",
      "Iteration 3, loss = 0.71175405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.67914934\n",
      "Iteration 5, loss = 0.64989440\n",
      "Iteration 6, loss = 0.62441450\n",
      "Iteration 7, loss = 0.60196110\n",
      "Iteration 8, loss = 0.58200348\n",
      "Iteration 9, loss = 0.56468373\n",
      "Iteration 10, loss = 0.54915676\n",
      "Iteration 11, loss = 0.53629303\n",
      "Iteration 12, loss = 0.52528531\n",
      "Iteration 13, loss = 0.51578820\n",
      "Iteration 14, loss = 0.50643471\n",
      "Iteration 15, loss = 0.49900994\n",
      "Iteration 16, loss = 0.49211084\n",
      "Iteration 17, loss = 0.48625408\n",
      "Iteration 18, loss = 0.48044106\n",
      "Iteration 19, loss = 0.47506829\n",
      "Iteration 20, loss = 0.47059276\n",
      "Iteration 21, loss = 0.46631862\n",
      "Iteration 22, loss = 0.46233677\n",
      "Iteration 23, loss = 0.45857954\n",
      "Iteration 24, loss = 0.45533580\n",
      "Iteration 25, loss = 0.45222960\n",
      "Iteration 26, loss = 0.44950322\n",
      "Iteration 27, loss = 0.44695535\n",
      "Iteration 28, loss = 0.44461835\n",
      "Iteration 29, loss = 0.44245596\n",
      "Iteration 30, loss = 0.44042726\n",
      "Iteration 31, loss = 0.43865950\n",
      "Iteration 32, loss = 0.43698492\n",
      "Iteration 33, loss = 0.43544893\n",
      "Iteration 34, loss = 0.43387262\n",
      "Iteration 35, loss = 0.43248208\n",
      "Iteration 36, loss = 0.43134514\n",
      "Iteration 37, loss = 0.43005054\n",
      "Iteration 38, loss = 0.42892100\n",
      "Iteration 39, loss = 0.42784124\n",
      "Iteration 40, loss = 0.42690398\n",
      "Iteration 41, loss = 0.42589070\n",
      "Iteration 42, loss = 0.42497012\n",
      "Iteration 43, loss = 0.42407821\n",
      "Iteration 44, loss = 0.42324465\n",
      "Iteration 45, loss = 0.42241956\n",
      "Iteration 46, loss = 0.42156774\n",
      "Iteration 47, loss = 0.42081858\n",
      "Iteration 48, loss = 0.41995990\n",
      "Iteration 49, loss = 0.41927195\n",
      "Iteration 50, loss = 0.41853543\n",
      "Iteration 51, loss = 0.41774314\n",
      "Iteration 52, loss = 0.41708890\n",
      "Iteration 53, loss = 0.41640803\n",
      "Iteration 54, loss = 0.41567687\n",
      "Iteration 55, loss = 0.41498684\n",
      "Iteration 56, loss = 0.41433372\n",
      "Iteration 57, loss = 0.41368233\n",
      "Iteration 58, loss = 0.41312886\n",
      "Iteration 59, loss = 0.41237100\n",
      "Iteration 60, loss = 0.41174765\n",
      "Iteration 61, loss = 0.41114821\n",
      "Iteration 62, loss = 0.41053686\n",
      "Iteration 63, loss = 0.40998268\n",
      "Iteration 64, loss = 0.40942117\n",
      "Iteration 65, loss = 0.40889290\n",
      "Iteration 66, loss = 0.40831750\n",
      "Iteration 67, loss = 0.40771533\n",
      "Iteration 68, loss = 0.40718006\n",
      "Iteration 69, loss = 0.40669666\n",
      "Iteration 70, loss = 0.40610505\n",
      "Iteration 71, loss = 0.40548541\n",
      "Iteration 72, loss = 0.40495155\n",
      "Iteration 73, loss = 0.40434959\n",
      "Iteration 74, loss = 0.40389073\n",
      "Iteration 75, loss = 0.40326892\n",
      "Iteration 76, loss = 0.40280579\n",
      "Iteration 77, loss = 0.40230084\n",
      "Iteration 78, loss = 0.40190080\n",
      "Iteration 79, loss = 0.40121133\n",
      "Iteration 80, loss = 0.40077629\n",
      "Iteration 81, loss = 0.40029608\n",
      "Iteration 82, loss = 0.39976326\n",
      "Iteration 83, loss = 0.39938811\n",
      "Iteration 84, loss = 0.39883648\n",
      "Iteration 85, loss = 0.39839553\n",
      "Iteration 86, loss = 0.39787558\n",
      "Iteration 87, loss = 0.39762758\n",
      "Iteration 88, loss = 0.39708989\n",
      "Iteration 89, loss = 0.39652938\n",
      "Iteration 90, loss = 0.39615912\n",
      "Iteration 91, loss = 0.39570200\n",
      "Iteration 92, loss = 0.39538450\n",
      "Iteration 93, loss = 0.39480207\n",
      "Iteration 94, loss = 0.39443061\n",
      "Iteration 95, loss = 0.39395755\n",
      "Iteration 96, loss = 0.39357453\n",
      "Iteration 97, loss = 0.39322130\n",
      "Iteration 98, loss = 0.39272744\n",
      "Iteration 99, loss = 0.39233467\n",
      "Iteration 100, loss = 0.39188340\n",
      "Iteration 101, loss = 0.39174992\n",
      "Iteration 102, loss = 0.39120498\n",
      "Iteration 103, loss = 0.39087344\n",
      "Iteration 104, loss = 0.39047730\n",
      "Iteration 105, loss = 0.39005031\n",
      "Iteration 106, loss = 0.38962117\n",
      "Iteration 107, loss = 0.38928914\n",
      "Iteration 108, loss = 0.38896987\n",
      "Iteration 109, loss = 0.38866175\n",
      "Iteration 110, loss = 0.38816343\n",
      "Iteration 111, loss = 0.38774261\n",
      "Iteration 112, loss = 0.38747226\n",
      "Iteration 113, loss = 0.38705271\n",
      "Iteration 114, loss = 0.38669749\n",
      "Iteration 115, loss = 0.38640915\n",
      "Iteration 116, loss = 0.38601058\n",
      "Iteration 117, loss = 0.38567940\n",
      "Iteration 118, loss = 0.38534845\n",
      "Iteration 119, loss = 0.38507792\n",
      "Iteration 120, loss = 0.38487138\n",
      "Iteration 121, loss = 0.38432753\n",
      "Iteration 122, loss = 0.38405879\n",
      "Iteration 123, loss = 0.38368730\n",
      "Iteration 124, loss = 0.38348262\n",
      "Iteration 125, loss = 0.38302591\n",
      "Iteration 126, loss = 0.38284959\n",
      "Iteration 127, loss = 0.38238487\n",
      "Iteration 128, loss = 0.38206706\n",
      "Iteration 129, loss = 0.38175777\n",
      "Iteration 130, loss = 0.38146888\n",
      "Iteration 131, loss = 0.38108463\n",
      "Iteration 132, loss = 0.38077689\n",
      "Iteration 133, loss = 0.38047065\n",
      "Iteration 134, loss = 0.38012086\n",
      "Iteration 135, loss = 0.37985391\n",
      "Iteration 136, loss = 0.37952048\n",
      "Iteration 137, loss = 0.37923048\n",
      "Iteration 138, loss = 0.37879321\n",
      "Iteration 139, loss = 0.37870527\n",
      "Iteration 140, loss = 0.37833609\n",
      "Iteration 141, loss = 0.37792753\n",
      "Iteration 142, loss = 0.37764614\n",
      "Iteration 143, loss = 0.37731028\n",
      "Iteration 144, loss = 0.37703043\n",
      "Iteration 145, loss = 0.37674595\n",
      "Iteration 146, loss = 0.37638012\n",
      "Iteration 147, loss = 0.37617772\n",
      "Iteration 148, loss = 0.37571897\n",
      "Iteration 149, loss = 0.37541550\n",
      "Iteration 150, loss = 0.37507441\n",
      "Iteration 151, loss = 0.37481043\n",
      "Iteration 152, loss = 0.37457239\n",
      "Iteration 153, loss = 0.37447735\n",
      "Iteration 154, loss = 0.37391558\n",
      "Iteration 155, loss = 0.37368133\n",
      "Iteration 156, loss = 0.37329829\n",
      "Iteration 157, loss = 0.37304439\n",
      "Iteration 158, loss = 0.37269921\n",
      "Iteration 159, loss = 0.37236346\n",
      "Iteration 160, loss = 0.37217308\n",
      "Iteration 161, loss = 0.37173221\n",
      "Iteration 162, loss = 0.37139388\n",
      "Iteration 163, loss = 0.37116214\n",
      "Iteration 164, loss = 0.37087943\n",
      "Iteration 165, loss = 0.37054715\n",
      "Iteration 166, loss = 0.37043737\n",
      "Iteration 167, loss = 0.37006564\n",
      "Iteration 168, loss = 0.36967624\n",
      "Iteration 169, loss = 0.36946316\n",
      "Iteration 170, loss = 0.36914817\n",
      "Iteration 171, loss = 0.36876236\n",
      "Iteration 172, loss = 0.36849568\n",
      "Iteration 173, loss = 0.36823026\n",
      "Iteration 174, loss = 0.36797157\n",
      "Iteration 175, loss = 0.36771374\n",
      "Iteration 176, loss = 0.36737360\n",
      "Iteration 177, loss = 0.36699200\n",
      "Iteration 178, loss = 0.36666684\n",
      "Iteration 179, loss = 0.36645433\n",
      "Iteration 180, loss = 0.36615922\n",
      "Iteration 181, loss = 0.36582200\n",
      "Iteration 182, loss = 0.36558853\n",
      "Iteration 183, loss = 0.36538947\n",
      "Iteration 184, loss = 0.36506489\n",
      "Iteration 185, loss = 0.36483653\n",
      "Iteration 186, loss = 0.36443620\n",
      "Iteration 187, loss = 0.36415067\n",
      "Iteration 188, loss = 0.36386968\n",
      "Iteration 189, loss = 0.36354560\n",
      "Iteration 190, loss = 0.36325406\n",
      "Iteration 191, loss = 0.36297617\n",
      "Iteration 192, loss = 0.36282188\n",
      "Iteration 193, loss = 0.36241341\n",
      "Iteration 194, loss = 0.36228424\n",
      "Iteration 195, loss = 0.36200701\n",
      "Iteration 196, loss = 0.36163494\n",
      "Iteration 197, loss = 0.36155050\n",
      "Iteration 198, loss = 0.36118085\n",
      "Iteration 199, loss = 0.36101085\n",
      "Iteration 200, loss = 0.36086075\n",
      "Iteration 1, loss = 0.79080143\n",
      "Iteration 2, loss = 0.74968284\n",
      "Iteration 3, loss = 0.70880006\n",
      "Iteration 4, loss = 0.67333844\n",
      "Iteration 5, loss = 0.64129410\n",
      "Iteration 6, loss = 0.61295273\n",
      "Iteration 7, loss = 0.58735557\n",
      "Iteration 8, loss = 0.56604448\n",
      "Iteration 9, loss = 0.54630041\n",
      "Iteration 10, loss = 0.52991677\n",
      "Iteration 11, loss = 0.51554296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.50327129\n",
      "Iteration 13, loss = 0.49136103\n",
      "Iteration 14, loss = 0.48245593\n",
      "Iteration 15, loss = 0.47395988\n",
      "Iteration 16, loss = 0.46622290\n",
      "Iteration 17, loss = 0.45908993\n",
      "Iteration 18, loss = 0.45347083\n",
      "Iteration 19, loss = 0.44765814\n",
      "Iteration 20, loss = 0.44250822\n",
      "Iteration 21, loss = 0.43775932\n",
      "Iteration 22, loss = 0.43342101\n",
      "Iteration 23, loss = 0.42972645\n",
      "Iteration 24, loss = 0.42610394\n",
      "Iteration 25, loss = 0.42283218\n",
      "Iteration 26, loss = 0.41989534\n",
      "Iteration 27, loss = 0.41706846\n",
      "Iteration 28, loss = 0.41473720\n",
      "Iteration 29, loss = 0.41232916\n",
      "Iteration 30, loss = 0.41016456\n",
      "Iteration 31, loss = 0.40826618\n",
      "Iteration 32, loss = 0.40662447\n",
      "Iteration 33, loss = 0.40487499\n",
      "Iteration 34, loss = 0.40341031\n",
      "Iteration 35, loss = 0.40197939\n",
      "Iteration 36, loss = 0.40076144\n",
      "Iteration 37, loss = 0.39947335\n",
      "Iteration 38, loss = 0.39838945\n",
      "Iteration 39, loss = 0.39733278\n",
      "Iteration 40, loss = 0.39632512\n",
      "Iteration 41, loss = 0.39536324\n",
      "Iteration 42, loss = 0.39445492\n",
      "Iteration 43, loss = 0.39358959\n",
      "Iteration 44, loss = 0.39286609\n",
      "Iteration 45, loss = 0.39196666\n",
      "Iteration 46, loss = 0.39129301\n",
      "Iteration 47, loss = 0.39054942\n",
      "Iteration 48, loss = 0.38978847\n",
      "Iteration 49, loss = 0.38910037\n",
      "Iteration 50, loss = 0.38844259\n",
      "Iteration 51, loss = 0.38786934\n",
      "Iteration 52, loss = 0.38716962\n",
      "Iteration 53, loss = 0.38651249\n",
      "Iteration 54, loss = 0.38598525\n",
      "Iteration 55, loss = 0.38535527\n",
      "Iteration 56, loss = 0.38479936\n",
      "Iteration 57, loss = 0.38428232\n",
      "Iteration 58, loss = 0.38370362\n",
      "Iteration 59, loss = 0.38308537\n",
      "Iteration 60, loss = 0.38256740\n",
      "Iteration 61, loss = 0.38219751\n",
      "Iteration 62, loss = 0.38175795\n",
      "Iteration 63, loss = 0.38121508\n",
      "Iteration 64, loss = 0.38064507\n",
      "Iteration 65, loss = 0.38006543\n",
      "Iteration 66, loss = 0.37967819\n",
      "Iteration 67, loss = 0.37912118\n",
      "Iteration 68, loss = 0.37882247\n",
      "Iteration 69, loss = 0.37821014\n",
      "Iteration 70, loss = 0.37783309\n",
      "Iteration 71, loss = 0.37739368\n",
      "Iteration 72, loss = 0.37689245\n",
      "Iteration 73, loss = 0.37651473\n",
      "Iteration 74, loss = 0.37606085\n",
      "Iteration 75, loss = 0.37567703\n",
      "Iteration 76, loss = 0.37528552\n",
      "Iteration 77, loss = 0.37481971\n",
      "Iteration 78, loss = 0.37446066\n",
      "Iteration 79, loss = 0.37399690\n",
      "Iteration 80, loss = 0.37362702\n",
      "Iteration 81, loss = 0.37333739\n",
      "Iteration 82, loss = 0.37282537\n",
      "Iteration 83, loss = 0.37254996\n",
      "Iteration 84, loss = 0.37210431\n",
      "Iteration 85, loss = 0.37167206\n",
      "Iteration 86, loss = 0.37142871\n",
      "Iteration 87, loss = 0.37097672\n",
      "Iteration 88, loss = 0.37076411\n",
      "Iteration 89, loss = 0.37027722\n",
      "Iteration 90, loss = 0.36994995\n",
      "Iteration 91, loss = 0.36970851\n",
      "Iteration 92, loss = 0.36934770\n",
      "Iteration 93, loss = 0.36893025\n",
      "Iteration 94, loss = 0.36867983\n",
      "Iteration 95, loss = 0.36827985\n",
      "Iteration 96, loss = 0.36816222\n",
      "Iteration 97, loss = 0.36767316\n",
      "Iteration 98, loss = 0.36740547\n",
      "Iteration 99, loss = 0.36704777\n",
      "Iteration 100, loss = 0.36681869\n",
      "Iteration 101, loss = 0.36642816\n",
      "Iteration 102, loss = 0.36619028\n",
      "Iteration 103, loss = 0.36582337\n",
      "Iteration 104, loss = 0.36561790\n",
      "Iteration 105, loss = 0.36517459\n",
      "Iteration 106, loss = 0.36494164\n",
      "Iteration 107, loss = 0.36465113\n",
      "Iteration 108, loss = 0.36442058\n",
      "Iteration 109, loss = 0.36405674\n",
      "Iteration 110, loss = 0.36379637\n",
      "Iteration 111, loss = 0.36345909\n",
      "Iteration 112, loss = 0.36309829\n",
      "Iteration 113, loss = 0.36288171\n",
      "Iteration 114, loss = 0.36256214\n",
      "Iteration 115, loss = 0.36235082\n",
      "Iteration 116, loss = 0.36206154\n",
      "Iteration 117, loss = 0.36171887\n",
      "Iteration 118, loss = 0.36148411\n",
      "Iteration 119, loss = 0.36116641\n",
      "Iteration 120, loss = 0.36086409\n",
      "Iteration 121, loss = 0.36066515\n",
      "Iteration 122, loss = 0.36034518\n",
      "Iteration 123, loss = 0.36007522\n",
      "Iteration 124, loss = 0.35987027\n",
      "Iteration 125, loss = 0.35957560\n",
      "Iteration 126, loss = 0.35923735\n",
      "Iteration 127, loss = 0.35904922\n",
      "Iteration 128, loss = 0.35870344\n",
      "Iteration 129, loss = 0.35843743\n",
      "Iteration 130, loss = 0.35816957\n",
      "Iteration 131, loss = 0.35798706\n",
      "Iteration 132, loss = 0.35767548\n",
      "Iteration 133, loss = 0.35737708\n",
      "Iteration 134, loss = 0.35715503\n",
      "Iteration 135, loss = 0.35684026\n",
      "Iteration 136, loss = 0.35662477\n",
      "Iteration 137, loss = 0.35648853\n",
      "Iteration 138, loss = 0.35608306\n",
      "Iteration 139, loss = 0.35593433\n",
      "Iteration 140, loss = 0.35565926\n",
      "Iteration 141, loss = 0.35538380\n",
      "Iteration 142, loss = 0.35511966\n",
      "Iteration 143, loss = 0.35491310\n",
      "Iteration 144, loss = 0.35460330\n",
      "Iteration 145, loss = 0.35455598\n",
      "Iteration 146, loss = 0.35426229\n",
      "Iteration 147, loss = 0.35389967\n",
      "Iteration 148, loss = 0.35382904\n",
      "Iteration 149, loss = 0.35347781\n",
      "Iteration 150, loss = 0.35318557\n",
      "Iteration 151, loss = 0.35308196\n",
      "Iteration 152, loss = 0.35273594\n",
      "Iteration 153, loss = 0.35250395\n",
      "Iteration 154, loss = 0.35244244\n",
      "Iteration 155, loss = 0.35226787\n",
      "Iteration 156, loss = 0.35178933\n",
      "Iteration 157, loss = 0.35157112\n",
      "Iteration 158, loss = 0.35140635\n",
      "Iteration 159, loss = 0.35113496\n",
      "Iteration 160, loss = 0.35097502\n",
      "Iteration 161, loss = 0.35069492\n",
      "Iteration 162, loss = 0.35054850\n",
      "Iteration 163, loss = 0.35024970\n",
      "Iteration 164, loss = 0.35007608\n",
      "Iteration 165, loss = 0.34992127\n",
      "Iteration 166, loss = 0.34970312\n",
      "Iteration 167, loss = 0.34948404\n",
      "Iteration 168, loss = 0.34918344\n",
      "Iteration 169, loss = 0.34902128\n",
      "Iteration 170, loss = 0.34877473\n",
      "Iteration 171, loss = 0.34851387\n",
      "Iteration 172, loss = 0.34848238\n",
      "Iteration 173, loss = 0.34832710\n",
      "Iteration 174, loss = 0.34791356\n",
      "Iteration 175, loss = 0.34774870\n",
      "Iteration 176, loss = 0.34783256\n",
      "Iteration 177, loss = 0.34752997\n",
      "Iteration 178, loss = 0.34719641\n",
      "Iteration 179, loss = 0.34709780\n",
      "Iteration 180, loss = 0.34672433\n",
      "Iteration 181, loss = 0.34682544\n",
      "Iteration 182, loss = 0.34639444\n",
      "Iteration 183, loss = 0.34627279\n",
      "Iteration 184, loss = 0.34608102\n",
      "Iteration 185, loss = 0.34591629\n",
      "Iteration 186, loss = 0.34564345\n",
      "Iteration 187, loss = 0.34558286\n",
      "Iteration 188, loss = 0.34518698\n",
      "Iteration 189, loss = 0.34500673\n",
      "Iteration 190, loss = 0.34492234\n",
      "Iteration 191, loss = 0.34455994\n",
      "Iteration 192, loss = 0.34448184\n",
      "Iteration 193, loss = 0.34429388\n",
      "Iteration 194, loss = 0.34413381\n",
      "Iteration 195, loss = 0.34393632\n",
      "Iteration 196, loss = 0.34371224\n",
      "Iteration 197, loss = 0.34342814\n",
      "Iteration 198, loss = 0.34334046\n",
      "Iteration 199, loss = 0.34313266\n",
      "Iteration 200, loss = 0.34291233\n",
      "Iteration 1, loss = 0.79441434\n",
      "Iteration 2, loss = 0.75386012\n",
      "Iteration 3, loss = 0.71416805\n",
      "Iteration 4, loss = 0.68087946\n",
      "Iteration 5, loss = 0.65074661\n",
      "Iteration 6, loss = 0.62420093\n",
      "Iteration 7, loss = 0.60006018\n",
      "Iteration 8, loss = 0.58077156\n",
      "Iteration 9, loss = 0.56265798\n",
      "Iteration 10, loss = 0.54725417\n",
      "Iteration 11, loss = 0.53415561\n",
      "Iteration 12, loss = 0.52306537\n",
      "Iteration 13, loss = 0.51256160\n",
      "Iteration 14, loss = 0.50398123\n",
      "Iteration 15, loss = 0.49616162\n",
      "Iteration 16, loss = 0.48897414\n",
      "Iteration 17, loss = 0.48240398\n",
      "Iteration 18, loss = 0.47680180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.47124536\n",
      "Iteration 20, loss = 0.46618479\n",
      "Iteration 21, loss = 0.46153769\n",
      "Iteration 22, loss = 0.45735710\n",
      "Iteration 23, loss = 0.45393162\n",
      "Iteration 24, loss = 0.45028969\n",
      "Iteration 25, loss = 0.44718145\n",
      "Iteration 26, loss = 0.44413019\n",
      "Iteration 27, loss = 0.44150701\n",
      "Iteration 28, loss = 0.43929591\n",
      "Iteration 29, loss = 0.43689950\n",
      "Iteration 30, loss = 0.43488260\n",
      "Iteration 31, loss = 0.43285032\n",
      "Iteration 32, loss = 0.43139178\n",
      "Iteration 33, loss = 0.42979575\n",
      "Iteration 34, loss = 0.42819164\n",
      "Iteration 35, loss = 0.42685991\n",
      "Iteration 36, loss = 0.42573338\n",
      "Iteration 37, loss = 0.42451076\n",
      "Iteration 38, loss = 0.42351473\n",
      "Iteration 39, loss = 0.42251673\n",
      "Iteration 40, loss = 0.42172161\n",
      "Iteration 41, loss = 0.42070285\n",
      "Iteration 42, loss = 0.41983956\n",
      "Iteration 43, loss = 0.41908552\n",
      "Iteration 44, loss = 0.41845272\n",
      "Iteration 45, loss = 0.41763039\n",
      "Iteration 46, loss = 0.41698880\n",
      "Iteration 47, loss = 0.41629786\n",
      "Iteration 48, loss = 0.41571968\n",
      "Iteration 49, loss = 0.41507239\n",
      "Iteration 50, loss = 0.41448912\n",
      "Iteration 51, loss = 0.41395708\n",
      "Iteration 52, loss = 0.41332703\n",
      "Iteration 53, loss = 0.41274063\n",
      "Iteration 54, loss = 0.41234366\n",
      "Iteration 55, loss = 0.41174254\n",
      "Iteration 56, loss = 0.41124468\n",
      "Iteration 57, loss = 0.41078228\n",
      "Iteration 58, loss = 0.41028740\n",
      "Iteration 59, loss = 0.40981457\n",
      "Iteration 60, loss = 0.40933469\n",
      "Iteration 61, loss = 0.40893627\n",
      "Iteration 62, loss = 0.40845968\n",
      "Iteration 63, loss = 0.40806650\n",
      "Iteration 64, loss = 0.40758487\n",
      "Iteration 65, loss = 0.40709267\n",
      "Iteration 66, loss = 0.40679597\n",
      "Iteration 67, loss = 0.40625522\n",
      "Iteration 68, loss = 0.40599860\n",
      "Iteration 69, loss = 0.40551894\n",
      "Iteration 70, loss = 0.40512647\n",
      "Iteration 71, loss = 0.40471780\n",
      "Iteration 72, loss = 0.40426128\n",
      "Iteration 73, loss = 0.40399266\n",
      "Iteration 74, loss = 0.40358131\n",
      "Iteration 75, loss = 0.40313226\n",
      "Iteration 76, loss = 0.40285323\n",
      "Iteration 77, loss = 0.40239102\n",
      "Iteration 78, loss = 0.40202152\n",
      "Iteration 79, loss = 0.40161766\n",
      "Iteration 80, loss = 0.40126546\n",
      "Iteration 81, loss = 0.40104782\n",
      "Iteration 82, loss = 0.40052496\n",
      "Iteration 83, loss = 0.40037080\n",
      "Iteration 84, loss = 0.39990641\n",
      "Iteration 85, loss = 0.39948175\n",
      "Iteration 86, loss = 0.39914832\n",
      "Iteration 87, loss = 0.39884826\n",
      "Iteration 88, loss = 0.39857721\n",
      "Iteration 89, loss = 0.39823484\n",
      "Iteration 90, loss = 0.39784452\n",
      "Iteration 91, loss = 0.39765399\n",
      "Iteration 92, loss = 0.39725811\n",
      "Iteration 93, loss = 0.39686158\n",
      "Iteration 94, loss = 0.39673656\n",
      "Iteration 95, loss = 0.39632873\n",
      "Iteration 96, loss = 0.39606603\n",
      "Iteration 97, loss = 0.39578445\n",
      "Iteration 98, loss = 0.39553566\n",
      "Iteration 99, loss = 0.39514294\n",
      "Iteration 100, loss = 0.39488752\n",
      "Iteration 101, loss = 0.39457433\n",
      "Iteration 102, loss = 0.39429541\n",
      "Iteration 103, loss = 0.39403565\n",
      "Iteration 104, loss = 0.39375548\n",
      "Iteration 105, loss = 0.39349584\n",
      "Iteration 106, loss = 0.39313280\n",
      "Iteration 107, loss = 0.39283818\n",
      "Iteration 108, loss = 0.39260236\n",
      "Iteration 109, loss = 0.39230208\n",
      "Iteration 110, loss = 0.39211153\n",
      "Iteration 111, loss = 0.39181117\n",
      "Iteration 112, loss = 0.39140870\n",
      "Iteration 113, loss = 0.39123716\n",
      "Iteration 114, loss = 0.39094812\n",
      "Iteration 115, loss = 0.39066950\n",
      "Iteration 116, loss = 0.39052509\n",
      "Iteration 117, loss = 0.39012468\n",
      "Iteration 118, loss = 0.38994573\n",
      "Iteration 119, loss = 0.38967175\n",
      "Iteration 120, loss = 0.38937543\n",
      "Iteration 121, loss = 0.38913845\n",
      "Iteration 122, loss = 0.38885176\n",
      "Iteration 123, loss = 0.38859312\n",
      "Iteration 124, loss = 0.38833807\n",
      "Iteration 125, loss = 0.38810556\n",
      "Iteration 126, loss = 0.38784053\n",
      "Iteration 127, loss = 0.38760469\n",
      "Iteration 128, loss = 0.38729122\n",
      "Iteration 129, loss = 0.38709522\n",
      "Iteration 130, loss = 0.38685490\n",
      "Iteration 131, loss = 0.38659777\n",
      "Iteration 132, loss = 0.38634373\n",
      "Iteration 133, loss = 0.38616678\n",
      "Iteration 134, loss = 0.38589569\n",
      "Iteration 135, loss = 0.38569842\n",
      "Iteration 136, loss = 0.38534147\n",
      "Iteration 137, loss = 0.38528639\n",
      "Iteration 138, loss = 0.38493565\n",
      "Iteration 139, loss = 0.38492335\n",
      "Iteration 140, loss = 0.38448023\n",
      "Iteration 141, loss = 0.38433279\n",
      "Iteration 142, loss = 0.38401900\n",
      "Iteration 143, loss = 0.38374647\n",
      "Iteration 144, loss = 0.38357325\n",
      "Iteration 145, loss = 0.38332462\n",
      "Iteration 146, loss = 0.38316908\n",
      "Iteration 147, loss = 0.38294822\n",
      "Iteration 148, loss = 0.38279388\n",
      "Iteration 149, loss = 0.38241806\n",
      "Iteration 150, loss = 0.38211043\n",
      "Iteration 151, loss = 0.38198260\n",
      "Iteration 152, loss = 0.38168538\n",
      "Iteration 153, loss = 0.38145181\n",
      "Iteration 154, loss = 0.38139420\n",
      "Iteration 155, loss = 0.38129101\n",
      "Iteration 156, loss = 0.38083849\n",
      "Iteration 157, loss = 0.38052005\n",
      "Iteration 158, loss = 0.38037677\n",
      "Iteration 159, loss = 0.38025808\n",
      "Iteration 160, loss = 0.37985989\n",
      "Iteration 161, loss = 0.37967997\n",
      "Iteration 162, loss = 0.37953805\n",
      "Iteration 163, loss = 0.37922371\n",
      "Iteration 164, loss = 0.37895501\n",
      "Iteration 165, loss = 0.37878176\n",
      "Iteration 166, loss = 0.37869355\n",
      "Iteration 167, loss = 0.37843158\n",
      "Iteration 168, loss = 0.37816118\n",
      "Iteration 169, loss = 0.37788429\n",
      "Iteration 170, loss = 0.37772135\n",
      "Iteration 171, loss = 0.37756834\n",
      "Iteration 172, loss = 0.37734055\n",
      "Iteration 173, loss = 0.37717260\n",
      "Iteration 174, loss = 0.37692897\n",
      "Iteration 175, loss = 0.37669303\n",
      "Iteration 176, loss = 0.37657572\n",
      "Iteration 177, loss = 0.37633403\n",
      "Iteration 178, loss = 0.37604389\n",
      "Iteration 179, loss = 0.37586232\n",
      "Iteration 180, loss = 0.37562288\n",
      "Iteration 181, loss = 0.37556954\n",
      "Iteration 182, loss = 0.37516062\n",
      "Iteration 183, loss = 0.37490005\n",
      "Iteration 184, loss = 0.37476904\n",
      "Iteration 185, loss = 0.37459331\n",
      "Iteration 186, loss = 0.37434765\n",
      "Iteration 187, loss = 0.37436987\n",
      "Iteration 188, loss = 0.37395905\n",
      "Iteration 189, loss = 0.37372177\n",
      "Iteration 190, loss = 0.37359299\n",
      "Iteration 191, loss = 0.37325853\n",
      "Iteration 192, loss = 0.37320242\n",
      "Iteration 193, loss = 0.37288546\n",
      "Iteration 194, loss = 0.37276570\n",
      "Iteration 195, loss = 0.37255387\n",
      "Iteration 196, loss = 0.37233692\n",
      "Iteration 197, loss = 0.37208310\n",
      "Iteration 198, loss = 0.37188515\n",
      "Iteration 199, loss = 0.37172186\n",
      "Iteration 200, loss = 0.37148121\n",
      "Iteration 1, loss = 0.79317864\n",
      "Iteration 2, loss = 0.75216413\n",
      "Iteration 3, loss = 0.71428505\n",
      "Iteration 4, loss = 0.68048446\n",
      "Iteration 5, loss = 0.65050095\n",
      "Iteration 6, loss = 0.62309759\n",
      "Iteration 7, loss = 0.60019486\n",
      "Iteration 8, loss = 0.57997105\n",
      "Iteration 9, loss = 0.56213423\n",
      "Iteration 10, loss = 0.54588665\n",
      "Iteration 11, loss = 0.53299137\n",
      "Iteration 12, loss = 0.52084830\n",
      "Iteration 13, loss = 0.51058442\n",
      "Iteration 14, loss = 0.50089212\n",
      "Iteration 15, loss = 0.49240377\n",
      "Iteration 16, loss = 0.48488592\n",
      "Iteration 17, loss = 0.47781257\n",
      "Iteration 18, loss = 0.47189191\n",
      "Iteration 19, loss = 0.46588838\n",
      "Iteration 20, loss = 0.46007491\n",
      "Iteration 21, loss = 0.45536803\n",
      "Iteration 22, loss = 0.45075555\n",
      "Iteration 23, loss = 0.44634052\n",
      "Iteration 24, loss = 0.44256934\n",
      "Iteration 25, loss = 0.43891314\n",
      "Iteration 26, loss = 0.43558656\n",
      "Iteration 27, loss = 0.43233096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.42970336\n",
      "Iteration 29, loss = 0.42690397\n",
      "Iteration 30, loss = 0.42466555\n",
      "Iteration 31, loss = 0.42252171\n",
      "Iteration 32, loss = 0.42048074\n",
      "Iteration 33, loss = 0.41871530\n",
      "Iteration 34, loss = 0.41684306\n",
      "Iteration 35, loss = 0.41521234\n",
      "Iteration 36, loss = 0.41375567\n",
      "Iteration 37, loss = 0.41240254\n",
      "Iteration 38, loss = 0.41099168\n",
      "Iteration 39, loss = 0.40971151\n",
      "Iteration 40, loss = 0.40854485\n",
      "Iteration 41, loss = 0.40741165\n",
      "Iteration 42, loss = 0.40632718\n",
      "Iteration 43, loss = 0.40528271\n",
      "Iteration 44, loss = 0.40438366\n",
      "Iteration 45, loss = 0.40334281\n",
      "Iteration 46, loss = 0.40244942\n",
      "Iteration 47, loss = 0.40151676\n",
      "Iteration 48, loss = 0.40068639\n",
      "Iteration 49, loss = 0.39989907\n",
      "Iteration 50, loss = 0.39911064\n",
      "Iteration 51, loss = 0.39829475\n",
      "Iteration 52, loss = 0.39756808\n",
      "Iteration 53, loss = 0.39684880\n",
      "Iteration 54, loss = 0.39609927\n",
      "Iteration 55, loss = 0.39540195\n",
      "Iteration 56, loss = 0.39478347\n",
      "Iteration 57, loss = 0.39408836\n",
      "Iteration 58, loss = 0.39339886\n",
      "Iteration 59, loss = 0.39273463\n",
      "Iteration 60, loss = 0.39216170\n",
      "Iteration 61, loss = 0.39151473\n",
      "Iteration 62, loss = 0.39096127\n",
      "Iteration 63, loss = 0.39029625\n",
      "Iteration 64, loss = 0.38982223\n",
      "Iteration 65, loss = 0.38919874\n",
      "Iteration 66, loss = 0.38872036\n",
      "Iteration 67, loss = 0.38827352\n",
      "Iteration 68, loss = 0.38763917\n",
      "Iteration 69, loss = 0.38701890\n",
      "Iteration 70, loss = 0.38658683\n",
      "Iteration 71, loss = 0.38597111\n",
      "Iteration 72, loss = 0.38546304\n",
      "Iteration 73, loss = 0.38496651\n",
      "Iteration 74, loss = 0.38446785\n",
      "Iteration 75, loss = 0.38400007\n",
      "Iteration 76, loss = 0.38357918\n",
      "Iteration 77, loss = 0.38298918\n",
      "Iteration 78, loss = 0.38260973\n",
      "Iteration 79, loss = 0.38214490\n",
      "Iteration 80, loss = 0.38171016\n",
      "Iteration 81, loss = 0.38128757\n",
      "Iteration 82, loss = 0.38089094\n",
      "Iteration 83, loss = 0.38041065\n",
      "Iteration 84, loss = 0.37999539\n",
      "Iteration 85, loss = 0.37960206\n",
      "Iteration 86, loss = 0.37915744\n",
      "Iteration 87, loss = 0.37874920\n",
      "Iteration 88, loss = 0.37829330\n",
      "Iteration 89, loss = 0.37785327\n",
      "Iteration 90, loss = 0.37752817\n",
      "Iteration 91, loss = 0.37703875\n",
      "Iteration 92, loss = 0.37692270\n",
      "Iteration 93, loss = 0.37631306\n",
      "Iteration 94, loss = 0.37621524\n",
      "Iteration 95, loss = 0.37554496\n",
      "Iteration 96, loss = 0.37512741\n",
      "Iteration 97, loss = 0.37478110\n",
      "Iteration 98, loss = 0.37436879\n",
      "Iteration 99, loss = 0.37405818\n",
      "Iteration 100, loss = 0.37365731\n",
      "Iteration 101, loss = 0.37348587\n",
      "Iteration 102, loss = 0.37306486\n",
      "Iteration 103, loss = 0.37268240\n",
      "Iteration 104, loss = 0.37248838\n",
      "Iteration 105, loss = 0.37203400\n",
      "Iteration 106, loss = 0.37174704\n",
      "Iteration 107, loss = 0.37141941\n",
      "Iteration 108, loss = 0.37113465\n",
      "Iteration 109, loss = 0.37075534\n",
      "Iteration 110, loss = 0.37043384\n",
      "Iteration 111, loss = 0.37011460\n",
      "Iteration 112, loss = 0.36986967\n",
      "Iteration 113, loss = 0.36947546\n",
      "Iteration 114, loss = 0.36922736\n",
      "Iteration 115, loss = 0.36891490\n",
      "Iteration 116, loss = 0.36851689\n",
      "Iteration 117, loss = 0.36822149\n",
      "Iteration 118, loss = 0.36794165\n",
      "Iteration 119, loss = 0.36772411\n",
      "Iteration 120, loss = 0.36742717\n",
      "Iteration 121, loss = 0.36710868\n",
      "Iteration 122, loss = 0.36683393\n",
      "Iteration 123, loss = 0.36647964\n",
      "Iteration 124, loss = 0.36618917\n",
      "Iteration 125, loss = 0.36598215\n",
      "Iteration 126, loss = 0.36560714\n",
      "Iteration 127, loss = 0.36526737\n",
      "Iteration 128, loss = 0.36509198\n",
      "Iteration 129, loss = 0.36491433\n",
      "Iteration 130, loss = 0.36446980\n",
      "Iteration 131, loss = 0.36423228\n",
      "Iteration 132, loss = 0.36395006\n",
      "Iteration 133, loss = 0.36367184\n",
      "Iteration 134, loss = 0.36342078\n",
      "Iteration 135, loss = 0.36304566\n",
      "Iteration 136, loss = 0.36299960\n",
      "Iteration 137, loss = 0.36251314\n",
      "Iteration 138, loss = 0.36232727\n",
      "Iteration 139, loss = 0.36214740\n",
      "Iteration 140, loss = 0.36185202\n",
      "Iteration 141, loss = 0.36153790\n",
      "Iteration 142, loss = 0.36135870\n",
      "Iteration 143, loss = 0.36108844\n",
      "Iteration 144, loss = 0.36086773\n",
      "Iteration 145, loss = 0.36064592\n",
      "Iteration 146, loss = 0.36034641\n",
      "Iteration 147, loss = 0.36015584\n",
      "Iteration 148, loss = 0.35994719\n",
      "Iteration 149, loss = 0.35961321\n",
      "Iteration 150, loss = 0.35931167\n",
      "Iteration 151, loss = 0.35911565\n",
      "Iteration 152, loss = 0.35902599\n",
      "Iteration 153, loss = 0.35893585\n",
      "Iteration 154, loss = 0.35855418\n",
      "Iteration 155, loss = 0.35807084\n",
      "Iteration 156, loss = 0.35795624\n",
      "Iteration 157, loss = 0.35771633\n",
      "Iteration 158, loss = 0.35744666\n",
      "Iteration 159, loss = 0.35727978\n",
      "Iteration 160, loss = 0.35707345\n",
      "Iteration 161, loss = 0.35684335\n",
      "Iteration 162, loss = 0.35661259\n",
      "Iteration 163, loss = 0.35635452\n",
      "Iteration 164, loss = 0.35622291\n",
      "Iteration 165, loss = 0.35604968\n",
      "Iteration 166, loss = 0.35577436\n",
      "Iteration 167, loss = 0.35546678\n",
      "Iteration 168, loss = 0.35528498\n",
      "Iteration 169, loss = 0.35512540\n",
      "Iteration 170, loss = 0.35502138\n",
      "Iteration 171, loss = 0.35467512\n",
      "Iteration 172, loss = 0.35463198\n",
      "Iteration 173, loss = 0.35440776\n",
      "Iteration 174, loss = 0.35415564\n",
      "Iteration 175, loss = 0.35391878\n",
      "Iteration 176, loss = 0.35369355\n",
      "Iteration 177, loss = 0.35339450\n",
      "Iteration 178, loss = 0.35343093\n",
      "Iteration 179, loss = 0.35300119\n",
      "Iteration 180, loss = 0.35280644\n",
      "Iteration 181, loss = 0.35258572\n",
      "Iteration 182, loss = 0.35239001\n",
      "Iteration 183, loss = 0.35207697\n",
      "Iteration 184, loss = 0.35199072\n",
      "Iteration 185, loss = 0.35182019\n",
      "Iteration 186, loss = 0.35158409\n",
      "Iteration 187, loss = 0.35138113\n",
      "Iteration 188, loss = 0.35110131\n",
      "Iteration 189, loss = 0.35092622\n",
      "Iteration 190, loss = 0.35067620\n",
      "Iteration 191, loss = 0.35049012\n",
      "Iteration 192, loss = 0.35023442\n",
      "Iteration 193, loss = 0.35001939\n",
      "Iteration 194, loss = 0.34996571\n",
      "Iteration 195, loss = 0.34973458\n",
      "Iteration 196, loss = 0.34937730\n",
      "Iteration 197, loss = 0.34936946\n",
      "Iteration 198, loss = 0.34897248\n",
      "Iteration 199, loss = 0.34883529\n",
      "Iteration 200, loss = 0.34846813\n",
      "Iteration 1, loss = 0.78843068\n",
      "Iteration 2, loss = 0.74873844\n",
      "Iteration 3, loss = 0.71175907\n",
      "Iteration 4, loss = 0.67871337\n",
      "Iteration 5, loss = 0.64844559\n",
      "Iteration 6, loss = 0.62166785\n",
      "Iteration 7, loss = 0.59820285\n",
      "Iteration 8, loss = 0.57713129\n",
      "Iteration 9, loss = 0.55877039\n",
      "Iteration 10, loss = 0.54204163\n",
      "Iteration 11, loss = 0.52840985\n",
      "Iteration 12, loss = 0.51581499\n",
      "Iteration 13, loss = 0.50517890\n",
      "Iteration 14, loss = 0.49486985\n",
      "Iteration 15, loss = 0.48616567\n",
      "Iteration 16, loss = 0.47821822\n",
      "Iteration 17, loss = 0.47103030\n",
      "Iteration 18, loss = 0.46482523\n",
      "Iteration 19, loss = 0.45832696\n",
      "Iteration 20, loss = 0.45278479\n",
      "Iteration 21, loss = 0.44756894\n",
      "Iteration 22, loss = 0.44286400\n",
      "Iteration 23, loss = 0.43828268\n",
      "Iteration 24, loss = 0.43435431\n",
      "Iteration 25, loss = 0.43062431\n",
      "Iteration 26, loss = 0.42716767\n",
      "Iteration 27, loss = 0.42400917\n",
      "Iteration 28, loss = 0.42126931\n",
      "Iteration 29, loss = 0.41849873\n",
      "Iteration 30, loss = 0.41610729\n",
      "Iteration 31, loss = 0.41403065\n",
      "Iteration 32, loss = 0.41194074\n",
      "Iteration 33, loss = 0.41014412\n",
      "Iteration 34, loss = 0.40828355\n",
      "Iteration 35, loss = 0.40657842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.40529692\n",
      "Iteration 37, loss = 0.40386592\n",
      "Iteration 38, loss = 0.40243707\n",
      "Iteration 39, loss = 0.40126738\n",
      "Iteration 40, loss = 0.40011412\n",
      "Iteration 41, loss = 0.39895432\n",
      "Iteration 42, loss = 0.39792356\n",
      "Iteration 43, loss = 0.39698572\n",
      "Iteration 44, loss = 0.39605320\n",
      "Iteration 45, loss = 0.39509386\n",
      "Iteration 46, loss = 0.39414267\n",
      "Iteration 47, loss = 0.39330281\n",
      "Iteration 48, loss = 0.39251083\n",
      "Iteration 49, loss = 0.39172786\n",
      "Iteration 50, loss = 0.39087896\n",
      "Iteration 51, loss = 0.39006468\n",
      "Iteration 52, loss = 0.38942220\n",
      "Iteration 53, loss = 0.38871930\n",
      "Iteration 54, loss = 0.38793261\n",
      "Iteration 55, loss = 0.38729163\n",
      "Iteration 56, loss = 0.38663820\n",
      "Iteration 57, loss = 0.38599929\n",
      "Iteration 58, loss = 0.38536021\n",
      "Iteration 59, loss = 0.38470192\n",
      "Iteration 60, loss = 0.38409623\n",
      "Iteration 61, loss = 0.38354875\n",
      "Iteration 62, loss = 0.38295526\n",
      "Iteration 63, loss = 0.38239223\n",
      "Iteration 64, loss = 0.38197072\n",
      "Iteration 65, loss = 0.38130222\n",
      "Iteration 66, loss = 0.38079000\n",
      "Iteration 67, loss = 0.38026503\n",
      "Iteration 68, loss = 0.37972334\n",
      "Iteration 69, loss = 0.37924132\n",
      "Iteration 70, loss = 0.37873323\n",
      "Iteration 71, loss = 0.37822454\n",
      "Iteration 72, loss = 0.37773920\n",
      "Iteration 73, loss = 0.37721681\n",
      "Iteration 74, loss = 0.37678571\n",
      "Iteration 75, loss = 0.37631090\n",
      "Iteration 76, loss = 0.37581251\n",
      "Iteration 77, loss = 0.37533669\n",
      "Iteration 78, loss = 0.37495784\n",
      "Iteration 79, loss = 0.37439721\n",
      "Iteration 80, loss = 0.37399436\n",
      "Iteration 81, loss = 0.37354829\n",
      "Iteration 82, loss = 0.37311948\n",
      "Iteration 83, loss = 0.37277109\n",
      "Iteration 84, loss = 0.37232299\n",
      "Iteration 85, loss = 0.37192867\n",
      "Iteration 86, loss = 0.37138748\n",
      "Iteration 87, loss = 0.37111621\n",
      "Iteration 88, loss = 0.37063085\n",
      "Iteration 89, loss = 0.37025274\n",
      "Iteration 90, loss = 0.36995959\n",
      "Iteration 91, loss = 0.36943755\n",
      "Iteration 92, loss = 0.36928626\n",
      "Iteration 93, loss = 0.36872958\n",
      "Iteration 94, loss = 0.36839042\n",
      "Iteration 95, loss = 0.36789442\n",
      "Iteration 96, loss = 0.36754439\n",
      "Iteration 97, loss = 0.36726426\n",
      "Iteration 98, loss = 0.36681339\n",
      "Iteration 99, loss = 0.36649649\n",
      "Iteration 100, loss = 0.36601817\n",
      "Iteration 101, loss = 0.36590042\n",
      "Iteration 102, loss = 0.36554147\n",
      "Iteration 103, loss = 0.36513290\n",
      "Iteration 104, loss = 0.36487957\n",
      "Iteration 105, loss = 0.36439127\n",
      "Iteration 106, loss = 0.36411708\n",
      "Iteration 107, loss = 0.36383675\n",
      "Iteration 108, loss = 0.36356764\n",
      "Iteration 109, loss = 0.36323152\n",
      "Iteration 110, loss = 0.36284593\n",
      "Iteration 111, loss = 0.36247068\n",
      "Iteration 112, loss = 0.36224877\n",
      "Iteration 113, loss = 0.36188146\n",
      "Iteration 114, loss = 0.36163630\n",
      "Iteration 115, loss = 0.36130673\n",
      "Iteration 116, loss = 0.36092418\n",
      "Iteration 117, loss = 0.36064817\n",
      "Iteration 118, loss = 0.36041418\n",
      "Iteration 119, loss = 0.36019347\n",
      "Iteration 120, loss = 0.35987526\n",
      "Iteration 121, loss = 0.35958123\n",
      "Iteration 122, loss = 0.35926697\n",
      "Iteration 123, loss = 0.35893283\n",
      "Iteration 124, loss = 0.35871365\n",
      "Iteration 125, loss = 0.35841086\n",
      "Iteration 126, loss = 0.35807999\n",
      "Iteration 127, loss = 0.35781003\n",
      "Iteration 128, loss = 0.35754241\n",
      "Iteration 129, loss = 0.35732124\n",
      "Iteration 130, loss = 0.35700379\n",
      "Iteration 131, loss = 0.35664108\n",
      "Iteration 132, loss = 0.35642518\n",
      "Iteration 133, loss = 0.35612373\n",
      "Iteration 134, loss = 0.35580077\n",
      "Iteration 135, loss = 0.35552516\n",
      "Iteration 136, loss = 0.35528820\n",
      "Iteration 137, loss = 0.35509727\n",
      "Iteration 138, loss = 0.35487786\n",
      "Iteration 139, loss = 0.35450348\n",
      "Iteration 140, loss = 0.35419889\n",
      "Iteration 141, loss = 0.35393256\n",
      "Iteration 142, loss = 0.35368379\n",
      "Iteration 143, loss = 0.35330534\n",
      "Iteration 144, loss = 0.35308897\n",
      "Iteration 145, loss = 0.35289138\n",
      "Iteration 146, loss = 0.35265749\n",
      "Iteration 147, loss = 0.35233239\n",
      "Iteration 148, loss = 0.35200578\n",
      "Iteration 149, loss = 0.35172775\n",
      "Iteration 150, loss = 0.35147454\n",
      "Iteration 151, loss = 0.35119921\n",
      "Iteration 152, loss = 0.35110850\n",
      "Iteration 153, loss = 0.35101137\n",
      "Iteration 154, loss = 0.35049666\n",
      "Iteration 155, loss = 0.35022222\n",
      "Iteration 156, loss = 0.34998830\n",
      "Iteration 157, loss = 0.34971422\n",
      "Iteration 158, loss = 0.34944617\n",
      "Iteration 159, loss = 0.34916826\n",
      "Iteration 160, loss = 0.34903126\n",
      "Iteration 161, loss = 0.34875790\n",
      "Iteration 162, loss = 0.34848847\n",
      "Iteration 163, loss = 0.34828913\n",
      "Iteration 164, loss = 0.34799033\n",
      "Iteration 165, loss = 0.34775642\n",
      "Iteration 166, loss = 0.34755550\n",
      "Iteration 167, loss = 0.34728723\n",
      "Iteration 168, loss = 0.34713721\n",
      "Iteration 169, loss = 0.34684355\n",
      "Iteration 170, loss = 0.34667051\n",
      "Iteration 171, loss = 0.34631926\n",
      "Iteration 172, loss = 0.34619095\n",
      "Iteration 173, loss = 0.34599830\n",
      "Iteration 174, loss = 0.34580106\n",
      "Iteration 175, loss = 0.34553561\n",
      "Iteration 176, loss = 0.34524108\n",
      "Iteration 177, loss = 0.34496696\n",
      "Iteration 178, loss = 0.34483803\n",
      "Iteration 179, loss = 0.34455369\n",
      "Iteration 180, loss = 0.34430051\n",
      "Iteration 181, loss = 0.34407259\n",
      "Iteration 182, loss = 0.34388625\n",
      "Iteration 183, loss = 0.34361753\n",
      "Iteration 184, loss = 0.34338025\n",
      "Iteration 185, loss = 0.34322219\n",
      "Iteration 186, loss = 0.34304010\n",
      "Iteration 187, loss = 0.34278848\n",
      "Iteration 188, loss = 0.34260309\n",
      "Iteration 189, loss = 0.34240130\n",
      "Iteration 190, loss = 0.34216181\n",
      "Iteration 191, loss = 0.34188347\n",
      "Iteration 192, loss = 0.34175005\n",
      "Iteration 193, loss = 0.34146484\n",
      "Iteration 194, loss = 0.34138419\n",
      "Iteration 195, loss = 0.34121465\n",
      "Iteration 196, loss = 0.34080426\n",
      "Iteration 197, loss = 0.34078429\n",
      "Iteration 198, loss = 0.34052595\n",
      "Iteration 199, loss = 0.34044531\n",
      "Iteration 200, loss = 0.34001432\n",
      "Iteration 1, loss = 0.78803468\n",
      "Iteration 2, loss = 0.74829686\n",
      "Iteration 3, loss = 0.71121488\n",
      "Iteration 4, loss = 0.67860927\n",
      "Iteration 5, loss = 0.64935283\n",
      "Iteration 6, loss = 0.62387156\n",
      "Iteration 7, loss = 0.60141621\n",
      "Iteration 8, loss = 0.58145660\n",
      "Iteration 9, loss = 0.56413445\n",
      "Iteration 10, loss = 0.54860555\n",
      "Iteration 11, loss = 0.53573854\n",
      "Iteration 12, loss = 0.52472787\n",
      "Iteration 13, loss = 0.51522822\n",
      "Iteration 14, loss = 0.50587304\n",
      "Iteration 15, loss = 0.49844499\n",
      "Iteration 16, loss = 0.49154308\n",
      "Iteration 17, loss = 0.48568328\n",
      "Iteration 18, loss = 0.47986690\n",
      "Iteration 19, loss = 0.47449069\n",
      "Iteration 20, loss = 0.47001187\n",
      "Iteration 21, loss = 0.46573472\n",
      "Iteration 22, loss = 0.46175020\n",
      "Iteration 23, loss = 0.45798983\n",
      "Iteration 24, loss = 0.45474276\n",
      "Iteration 25, loss = 0.45163323\n",
      "Iteration 26, loss = 0.44890344\n",
      "Iteration 27, loss = 0.44635228\n",
      "Iteration 28, loss = 0.44401259\n",
      "Iteration 29, loss = 0.44184776\n",
      "Iteration 30, loss = 0.43981573\n",
      "Iteration 31, loss = 0.43804652\n",
      "Iteration 32, loss = 0.43636839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.43482955\n",
      "Iteration 34, loss = 0.43325000\n",
      "Iteration 35, loss = 0.43185558\n",
      "Iteration 36, loss = 0.43071761\n",
      "Iteration 37, loss = 0.42942125\n",
      "Iteration 38, loss = 0.42828875\n",
      "Iteration 39, loss = 0.42720721\n",
      "Iteration 40, loss = 0.42626713\n",
      "Iteration 41, loss = 0.42525098\n",
      "Iteration 42, loss = 0.42432757\n",
      "Iteration 43, loss = 0.42343388\n",
      "Iteration 44, loss = 0.42259813\n",
      "Iteration 45, loss = 0.42177129\n",
      "Iteration 46, loss = 0.42091609\n",
      "Iteration 47, loss = 0.42016454\n",
      "Iteration 48, loss = 0.41930149\n",
      "Iteration 49, loss = 0.41861145\n",
      "Iteration 50, loss = 0.41787204\n",
      "Iteration 51, loss = 0.41707488\n",
      "Iteration 52, loss = 0.41641680\n",
      "Iteration 53, loss = 0.41573099\n",
      "Iteration 54, loss = 0.41499607\n",
      "Iteration 55, loss = 0.41430468\n",
      "Iteration 56, loss = 0.41365071\n",
      "Iteration 57, loss = 0.41299432\n",
      "Iteration 58, loss = 0.41243745\n",
      "Iteration 59, loss = 0.41167614\n",
      "Iteration 60, loss = 0.41105277\n",
      "Iteration 61, loss = 0.41045333\n",
      "Iteration 62, loss = 0.40983793\n",
      "Iteration 63, loss = 0.40928149\n",
      "Iteration 64, loss = 0.40871561\n",
      "Iteration 65, loss = 0.40818648\n",
      "Iteration 66, loss = 0.40761185\n",
      "Iteration 67, loss = 0.40700593\n",
      "Iteration 68, loss = 0.40647095\n",
      "Iteration 69, loss = 0.40598266\n",
      "Iteration 70, loss = 0.40538897\n",
      "Iteration 71, loss = 0.40476781\n",
      "Iteration 72, loss = 0.40422844\n",
      "Iteration 73, loss = 0.40362497\n",
      "Iteration 74, loss = 0.40316402\n",
      "Iteration 75, loss = 0.40253847\n",
      "Iteration 76, loss = 0.40207170\n",
      "Iteration 77, loss = 0.40156671\n",
      "Iteration 78, loss = 0.40116481\n",
      "Iteration 79, loss = 0.40047000\n",
      "Iteration 80, loss = 0.40002995\n",
      "Iteration 81, loss = 0.39954659\n",
      "Iteration 82, loss = 0.39901109\n",
      "Iteration 83, loss = 0.39863897\n",
      "Iteration 84, loss = 0.39808633\n",
      "Iteration 85, loss = 0.39764750\n",
      "Iteration 86, loss = 0.39712799\n",
      "Iteration 87, loss = 0.39687808\n",
      "Iteration 88, loss = 0.39634093\n",
      "Iteration 89, loss = 0.39577720\n",
      "Iteration 90, loss = 0.39540417\n",
      "Iteration 91, loss = 0.39494383\n",
      "Iteration 92, loss = 0.39462518\n",
      "Iteration 93, loss = 0.39404281\n",
      "Iteration 94, loss = 0.39367215\n",
      "Iteration 95, loss = 0.39319425\n",
      "Iteration 96, loss = 0.39280593\n",
      "Iteration 97, loss = 0.39244640\n",
      "Iteration 98, loss = 0.39194964\n",
      "Iteration 99, loss = 0.39155676\n",
      "Iteration 100, loss = 0.39109904\n",
      "Iteration 101, loss = 0.39096687\n",
      "Iteration 102, loss = 0.39041930\n",
      "Iteration 103, loss = 0.39008339\n",
      "Iteration 104, loss = 0.38968804\n",
      "Iteration 105, loss = 0.38925681\n",
      "Iteration 106, loss = 0.38882461\n",
      "Iteration 107, loss = 0.38848696\n",
      "Iteration 108, loss = 0.38816434\n",
      "Iteration 109, loss = 0.38785328\n",
      "Iteration 110, loss = 0.38735187\n",
      "Iteration 111, loss = 0.38692188\n",
      "Iteration 112, loss = 0.38665494\n",
      "Iteration 113, loss = 0.38623233\n",
      "Iteration 114, loss = 0.38586855\n",
      "Iteration 115, loss = 0.38557961\n",
      "Iteration 116, loss = 0.38517882\n",
      "Iteration 117, loss = 0.38484788\n",
      "Iteration 118, loss = 0.38450913\n",
      "Iteration 119, loss = 0.38423442\n",
      "Iteration 120, loss = 0.38402145\n",
      "Iteration 121, loss = 0.38346392\n",
      "Iteration 122, loss = 0.38318967\n",
      "Iteration 123, loss = 0.38281204\n",
      "Iteration 124, loss = 0.38259544\n",
      "Iteration 125, loss = 0.38214012\n",
      "Iteration 126, loss = 0.38195792\n",
      "Iteration 127, loss = 0.38148690\n",
      "Iteration 128, loss = 0.38115920\n",
      "Iteration 129, loss = 0.38084851\n",
      "Iteration 130, loss = 0.38055747\n",
      "Iteration 131, loss = 0.38016836\n",
      "Iteration 132, loss = 0.37985787\n",
      "Iteration 133, loss = 0.37954893\n",
      "Iteration 134, loss = 0.37918916\n",
      "Iteration 135, loss = 0.37891659\n",
      "Iteration 136, loss = 0.37857942\n",
      "Iteration 137, loss = 0.37828952\n",
      "Iteration 138, loss = 0.37784364\n",
      "Iteration 139, loss = 0.37774223\n",
      "Iteration 140, loss = 0.37737666\n",
      "Iteration 141, loss = 0.37697241\n",
      "Iteration 142, loss = 0.37668499\n",
      "Iteration 143, loss = 0.37633780\n",
      "Iteration 144, loss = 0.37604593\n",
      "Iteration 145, loss = 0.37575513\n",
      "Iteration 146, loss = 0.37538731\n",
      "Iteration 147, loss = 0.37518426\n",
      "Iteration 148, loss = 0.37472002\n",
      "Iteration 149, loss = 0.37441211\n",
      "Iteration 150, loss = 0.37407009\n",
      "Iteration 151, loss = 0.37379708\n",
      "Iteration 152, loss = 0.37355280\n",
      "Iteration 153, loss = 0.37344369\n",
      "Iteration 154, loss = 0.37287862\n",
      "Iteration 155, loss = 0.37264190\n",
      "Iteration 156, loss = 0.37224257\n",
      "Iteration 157, loss = 0.37198768\n",
      "Iteration 158, loss = 0.37164364\n",
      "Iteration 159, loss = 0.37130963\n",
      "Iteration 160, loss = 0.37111201\n",
      "Iteration 161, loss = 0.37066562\n",
      "Iteration 162, loss = 0.37032891\n",
      "Iteration 163, loss = 0.37008587\n",
      "Iteration 164, loss = 0.36979934\n",
      "Iteration 165, loss = 0.36945405\n",
      "Iteration 166, loss = 0.36934960\n",
      "Iteration 167, loss = 0.36896983\n",
      "Iteration 168, loss = 0.36857778\n",
      "Iteration 169, loss = 0.36835737\n",
      "Iteration 170, loss = 0.36804437\n",
      "Iteration 171, loss = 0.36765682\n",
      "Iteration 172, loss = 0.36738624\n",
      "Iteration 173, loss = 0.36711586\n",
      "Iteration 174, loss = 0.36685261\n",
      "Iteration 175, loss = 0.36658983\n",
      "Iteration 176, loss = 0.36625071\n",
      "Iteration 177, loss = 0.36586806\n",
      "Iteration 178, loss = 0.36553716\n",
      "Iteration 179, loss = 0.36532770\n",
      "Iteration 180, loss = 0.36502713\n",
      "Iteration 181, loss = 0.36468733\n",
      "Iteration 182, loss = 0.36446377\n",
      "Iteration 183, loss = 0.36427007\n",
      "Iteration 184, loss = 0.36393878\n",
      "Iteration 185, loss = 0.36370652\n",
      "Iteration 186, loss = 0.36329694\n",
      "Iteration 187, loss = 0.36301744\n",
      "Iteration 188, loss = 0.36273272\n",
      "Iteration 189, loss = 0.36239635\n",
      "Iteration 190, loss = 0.36210261\n",
      "Iteration 191, loss = 0.36181605\n",
      "Iteration 192, loss = 0.36167093\n",
      "Iteration 193, loss = 0.36124757\n",
      "Iteration 194, loss = 0.36111884\n",
      "Iteration 195, loss = 0.36084532\n",
      "Iteration 196, loss = 0.36046478\n",
      "Iteration 197, loss = 0.36036556\n",
      "Iteration 198, loss = 0.35999094\n",
      "Iteration 199, loss = 0.35981103\n",
      "Iteration 200, loss = 0.35967059\n",
      "Iteration 1, loss = 0.79074773\n",
      "Iteration 2, loss = 0.74962911\n",
      "Iteration 3, loss = 0.70874622\n",
      "Iteration 4, loss = 0.67328448\n",
      "Iteration 5, loss = 0.64124000\n",
      "Iteration 6, loss = 0.61289846\n",
      "Iteration 7, loss = 0.58730109\n",
      "Iteration 8, loss = 0.56598979\n",
      "Iteration 9, loss = 0.54624551\n",
      "Iteration 10, loss = 0.52986160\n",
      "Iteration 11, loss = 0.51548757\n",
      "Iteration 12, loss = 0.50321566\n",
      "Iteration 13, loss = 0.49130512\n",
      "Iteration 14, loss = 0.48239965\n",
      "Iteration 15, loss = 0.47390341\n",
      "Iteration 16, loss = 0.46616613\n",
      "Iteration 17, loss = 0.45903286\n",
      "Iteration 18, loss = 0.45341346\n",
      "Iteration 19, loss = 0.44760043\n",
      "Iteration 20, loss = 0.44245008\n",
      "Iteration 21, loss = 0.43770091\n",
      "Iteration 22, loss = 0.43336229\n",
      "Iteration 23, loss = 0.42966739\n",
      "Iteration 24, loss = 0.42604453\n",
      "Iteration 25, loss = 0.42277238\n",
      "Iteration 26, loss = 0.41983531\n",
      "Iteration 27, loss = 0.41700819\n",
      "Iteration 28, loss = 0.41467675\n",
      "Iteration 29, loss = 0.41226838\n",
      "Iteration 30, loss = 0.41010388\n",
      "Iteration 31, loss = 0.40820505\n",
      "Iteration 32, loss = 0.40656323\n",
      "Iteration 33, loss = 0.40481341\n",
      "Iteration 34, loss = 0.40334852\n",
      "Iteration 35, loss = 0.40191722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.40069923\n",
      "Iteration 37, loss = 0.39941064\n",
      "Iteration 38, loss = 0.39832657\n",
      "Iteration 39, loss = 0.39726953\n",
      "Iteration 40, loss = 0.39626214\n",
      "Iteration 41, loss = 0.39529980\n",
      "Iteration 42, loss = 0.39439155\n",
      "Iteration 43, loss = 0.39352665\n",
      "Iteration 44, loss = 0.39280265\n",
      "Iteration 45, loss = 0.39190317\n",
      "Iteration 46, loss = 0.39122954\n",
      "Iteration 47, loss = 0.39048561\n",
      "Iteration 48, loss = 0.38972594\n",
      "Iteration 49, loss = 0.38903765\n",
      "Iteration 50, loss = 0.38837952\n",
      "Iteration 51, loss = 0.38780658\n",
      "Iteration 52, loss = 0.38710671\n",
      "Iteration 53, loss = 0.38644873\n",
      "Iteration 54, loss = 0.38592070\n",
      "Iteration 55, loss = 0.38529159\n",
      "Iteration 56, loss = 0.38473592\n",
      "Iteration 57, loss = 0.38421811\n",
      "Iteration 58, loss = 0.38363952\n",
      "Iteration 59, loss = 0.38302118\n",
      "Iteration 60, loss = 0.38250270\n",
      "Iteration 61, loss = 0.38213325\n",
      "Iteration 62, loss = 0.38169235\n",
      "Iteration 63, loss = 0.38115000\n",
      "Iteration 64, loss = 0.38058076\n",
      "Iteration 65, loss = 0.38000154\n",
      "Iteration 66, loss = 0.37961456\n",
      "Iteration 67, loss = 0.37905712\n",
      "Iteration 68, loss = 0.37875815\n",
      "Iteration 69, loss = 0.37814547\n",
      "Iteration 70, loss = 0.37776854\n",
      "Iteration 71, loss = 0.37732911\n",
      "Iteration 72, loss = 0.37682759\n",
      "Iteration 73, loss = 0.37644911\n",
      "Iteration 74, loss = 0.37599533\n",
      "Iteration 75, loss = 0.37561047\n",
      "Iteration 76, loss = 0.37521842\n",
      "Iteration 77, loss = 0.37475214\n",
      "Iteration 78, loss = 0.37439298\n",
      "Iteration 79, loss = 0.37393021\n",
      "Iteration 80, loss = 0.37356014\n",
      "Iteration 81, loss = 0.37327082\n",
      "Iteration 82, loss = 0.37275656\n",
      "Iteration 83, loss = 0.37248068\n",
      "Iteration 84, loss = 0.37203186\n",
      "Iteration 85, loss = 0.37159958\n",
      "Iteration 86, loss = 0.37135613\n",
      "Iteration 87, loss = 0.37090495\n",
      "Iteration 88, loss = 0.37069439\n",
      "Iteration 89, loss = 0.37020384\n",
      "Iteration 90, loss = 0.36987552\n",
      "Iteration 91, loss = 0.36963432\n",
      "Iteration 92, loss = 0.36927520\n",
      "Iteration 93, loss = 0.36886019\n",
      "Iteration 94, loss = 0.36860726\n",
      "Iteration 95, loss = 0.36820669\n",
      "Iteration 96, loss = 0.36808800\n",
      "Iteration 97, loss = 0.36759790\n",
      "Iteration 98, loss = 0.36732950\n",
      "Iteration 99, loss = 0.36697389\n",
      "Iteration 100, loss = 0.36674408\n",
      "Iteration 101, loss = 0.36635612\n",
      "Iteration 102, loss = 0.36611571\n",
      "Iteration 103, loss = 0.36574912\n",
      "Iteration 104, loss = 0.36554315\n",
      "Iteration 105, loss = 0.36510020\n",
      "Iteration 106, loss = 0.36487053\n",
      "Iteration 107, loss = 0.36457887\n",
      "Iteration 108, loss = 0.36435053\n",
      "Iteration 109, loss = 0.36398618\n",
      "Iteration 110, loss = 0.36372832\n",
      "Iteration 111, loss = 0.36338601\n",
      "Iteration 112, loss = 0.36302612\n",
      "Iteration 113, loss = 0.36281100\n",
      "Iteration 114, loss = 0.36249324\n",
      "Iteration 115, loss = 0.36228401\n",
      "Iteration 116, loss = 0.36199620\n",
      "Iteration 117, loss = 0.36165769\n",
      "Iteration 118, loss = 0.36142061\n",
      "Iteration 119, loss = 0.36110460\n",
      "Iteration 120, loss = 0.36080200\n",
      "Iteration 121, loss = 0.36060372\n",
      "Iteration 122, loss = 0.36028616\n",
      "Iteration 123, loss = 0.36001334\n",
      "Iteration 124, loss = 0.35980601\n",
      "Iteration 125, loss = 0.35951231\n",
      "Iteration 126, loss = 0.35918044\n",
      "Iteration 127, loss = 0.35899575\n",
      "Iteration 128, loss = 0.35864736\n",
      "Iteration 129, loss = 0.35837767\n",
      "Iteration 130, loss = 0.35811519\n",
      "Iteration 131, loss = 0.35793507\n",
      "Iteration 132, loss = 0.35762009\n",
      "Iteration 133, loss = 0.35732057\n",
      "Iteration 134, loss = 0.35710012\n",
      "Iteration 135, loss = 0.35678574\n",
      "Iteration 136, loss = 0.35656861\n",
      "Iteration 137, loss = 0.35642914\n",
      "Iteration 138, loss = 0.35602660\n",
      "Iteration 139, loss = 0.35588554\n",
      "Iteration 140, loss = 0.35560325\n",
      "Iteration 141, loss = 0.35532703\n",
      "Iteration 142, loss = 0.35506357\n",
      "Iteration 143, loss = 0.35485771\n",
      "Iteration 144, loss = 0.35454652\n",
      "Iteration 145, loss = 0.35450216\n",
      "Iteration 146, loss = 0.35420990\n",
      "Iteration 147, loss = 0.35384375\n",
      "Iteration 148, loss = 0.35377047\n",
      "Iteration 149, loss = 0.35341031\n",
      "Iteration 150, loss = 0.35312323\n",
      "Iteration 151, loss = 0.35302468\n",
      "Iteration 152, loss = 0.35268233\n",
      "Iteration 153, loss = 0.35244863\n",
      "Iteration 154, loss = 0.35238056\n",
      "Iteration 155, loss = 0.35219932\n",
      "Iteration 156, loss = 0.35172807\n",
      "Iteration 157, loss = 0.35151003\n",
      "Iteration 158, loss = 0.35134838\n",
      "Iteration 159, loss = 0.35108263\n",
      "Iteration 160, loss = 0.35091959\n",
      "Iteration 161, loss = 0.35064066\n",
      "Iteration 162, loss = 0.35049585\n",
      "Iteration 163, loss = 0.35019410\n",
      "Iteration 164, loss = 0.35002179\n",
      "Iteration 165, loss = 0.34987110\n",
      "Iteration 166, loss = 0.34965194\n",
      "Iteration 167, loss = 0.34943250\n",
      "Iteration 168, loss = 0.34913006\n",
      "Iteration 169, loss = 0.34896824\n",
      "Iteration 170, loss = 0.34871915\n",
      "Iteration 171, loss = 0.34846239\n",
      "Iteration 172, loss = 0.34842860\n",
      "Iteration 173, loss = 0.34828082\n",
      "Iteration 174, loss = 0.34786488\n",
      "Iteration 175, loss = 0.34770711\n",
      "Iteration 176, loss = 0.34778158\n",
      "Iteration 177, loss = 0.34747746\n",
      "Iteration 178, loss = 0.34714476\n",
      "Iteration 179, loss = 0.34704437\n",
      "Iteration 180, loss = 0.34667371\n",
      "Iteration 181, loss = 0.34677834\n",
      "Iteration 182, loss = 0.34634524\n",
      "Iteration 183, loss = 0.34622230\n",
      "Iteration 184, loss = 0.34602834\n",
      "Iteration 185, loss = 0.34586600\n",
      "Iteration 186, loss = 0.34559037\n",
      "Iteration 187, loss = 0.34553106\n",
      "Iteration 188, loss = 0.34513422\n",
      "Iteration 189, loss = 0.34495607\n",
      "Iteration 190, loss = 0.34487108\n",
      "Iteration 191, loss = 0.34450853\n",
      "Iteration 192, loss = 0.34443414\n",
      "Iteration 193, loss = 0.34424974\n",
      "Iteration 194, loss = 0.34408782\n",
      "Iteration 195, loss = 0.34389158\n",
      "Iteration 196, loss = 0.34366653\n",
      "Iteration 197, loss = 0.34338730\n",
      "Iteration 198, loss = 0.34330193\n",
      "Iteration 199, loss = 0.34309104\n",
      "Iteration 200, loss = 0.34287282\n",
      "Iteration 1, loss = 0.79436059\n",
      "Iteration 2, loss = 0.75380636\n",
      "Iteration 3, loss = 0.71411421\n",
      "Iteration 4, loss = 0.68082551\n",
      "Iteration 5, loss = 0.65069252\n",
      "Iteration 6, loss = 0.62414682\n",
      "Iteration 7, loss = 0.60000597\n",
      "Iteration 8, loss = 0.58071688\n",
      "Iteration 9, loss = 0.56260274\n",
      "Iteration 10, loss = 0.54719857\n",
      "Iteration 11, loss = 0.53409979\n",
      "Iteration 12, loss = 0.52300938\n",
      "Iteration 13, loss = 0.51250533\n",
      "Iteration 14, loss = 0.50392475\n",
      "Iteration 15, loss = 0.49610467\n",
      "Iteration 16, loss = 0.48891667\n",
      "Iteration 17, loss = 0.48234622\n",
      "Iteration 18, loss = 0.47674356\n",
      "Iteration 19, loss = 0.47118678\n",
      "Iteration 20, loss = 0.46612548\n",
      "Iteration 21, loss = 0.46147772\n",
      "Iteration 22, loss = 0.45729663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.45387073\n",
      "Iteration 24, loss = 0.45022826\n",
      "Iteration 25, loss = 0.44712002\n",
      "Iteration 26, loss = 0.44406821\n",
      "Iteration 27, loss = 0.44144484\n",
      "Iteration 28, loss = 0.43923360\n",
      "Iteration 29, loss = 0.43683761\n",
      "Iteration 30, loss = 0.43482078\n",
      "Iteration 31, loss = 0.43278844\n",
      "Iteration 32, loss = 0.43133017\n",
      "Iteration 33, loss = 0.42973372\n",
      "Iteration 34, loss = 0.42812981\n",
      "Iteration 35, loss = 0.42679799\n",
      "Iteration 36, loss = 0.42567076\n",
      "Iteration 37, loss = 0.42444835\n",
      "Iteration 38, loss = 0.42345199\n",
      "Iteration 39, loss = 0.42245429\n",
      "Iteration 40, loss = 0.42165929\n",
      "Iteration 41, loss = 0.42064075\n",
      "Iteration 42, loss = 0.41977791\n",
      "Iteration 43, loss = 0.41902324\n",
      "Iteration 44, loss = 0.41839040\n",
      "Iteration 45, loss = 0.41756727\n",
      "Iteration 46, loss = 0.41692529\n",
      "Iteration 47, loss = 0.41623415\n",
      "Iteration 48, loss = 0.41565493\n",
      "Iteration 49, loss = 0.41500702\n",
      "Iteration 50, loss = 0.41442123\n",
      "Iteration 51, loss = 0.41388777\n",
      "Iteration 52, loss = 0.41325528\n",
      "Iteration 53, loss = 0.41266746\n",
      "Iteration 54, loss = 0.41226939\n",
      "Iteration 55, loss = 0.41166795\n",
      "Iteration 56, loss = 0.41116768\n",
      "Iteration 57, loss = 0.41070559\n",
      "Iteration 58, loss = 0.41021084\n",
      "Iteration 59, loss = 0.40973936\n",
      "Iteration 60, loss = 0.40925896\n",
      "Iteration 61, loss = 0.40886268\n",
      "Iteration 62, loss = 0.40838726\n",
      "Iteration 63, loss = 0.40799350\n",
      "Iteration 64, loss = 0.40751317\n",
      "Iteration 65, loss = 0.40702183\n",
      "Iteration 66, loss = 0.40672437\n",
      "Iteration 67, loss = 0.40618495\n",
      "Iteration 68, loss = 0.40592838\n",
      "Iteration 69, loss = 0.40544844\n",
      "Iteration 70, loss = 0.40505202\n",
      "Iteration 71, loss = 0.40464342\n",
      "Iteration 72, loss = 0.40418839\n",
      "Iteration 73, loss = 0.40391963\n",
      "Iteration 74, loss = 0.40350756\n",
      "Iteration 75, loss = 0.40305769\n",
      "Iteration 76, loss = 0.40277455\n",
      "Iteration 77, loss = 0.40231365\n",
      "Iteration 78, loss = 0.40194343\n",
      "Iteration 79, loss = 0.40154031\n",
      "Iteration 80, loss = 0.40118896\n",
      "Iteration 81, loss = 0.40097102\n",
      "Iteration 82, loss = 0.40044857\n",
      "Iteration 83, loss = 0.40029214\n",
      "Iteration 84, loss = 0.39982910\n",
      "Iteration 85, loss = 0.39940391\n",
      "Iteration 86, loss = 0.39907033\n",
      "Iteration 87, loss = 0.39877315\n",
      "Iteration 88, loss = 0.39850361\n",
      "Iteration 89, loss = 0.39816550\n",
      "Iteration 90, loss = 0.39777835\n",
      "Iteration 91, loss = 0.39759098\n",
      "Iteration 92, loss = 0.39719197\n",
      "Iteration 93, loss = 0.39679397\n",
      "Iteration 94, loss = 0.39667412\n",
      "Iteration 95, loss = 0.39626580\n",
      "Iteration 96, loss = 0.39600217\n",
      "Iteration 97, loss = 0.39572187\n",
      "Iteration 98, loss = 0.39547464\n",
      "Iteration 99, loss = 0.39508161\n",
      "Iteration 100, loss = 0.39482569\n",
      "Iteration 101, loss = 0.39450979\n",
      "Iteration 102, loss = 0.39423004\n",
      "Iteration 103, loss = 0.39396730\n",
      "Iteration 104, loss = 0.39368694\n",
      "Iteration 105, loss = 0.39342611\n",
      "Iteration 106, loss = 0.39306214\n",
      "Iteration 107, loss = 0.39276838\n",
      "Iteration 108, loss = 0.39253116\n",
      "Iteration 109, loss = 0.39223756\n",
      "Iteration 110, loss = 0.39204943\n",
      "Iteration 111, loss = 0.39174645\n",
      "Iteration 112, loss = 0.39134921\n",
      "Iteration 113, loss = 0.39117546\n",
      "Iteration 114, loss = 0.39088702\n",
      "Iteration 115, loss = 0.39060992\n",
      "Iteration 116, loss = 0.39046012\n",
      "Iteration 117, loss = 0.39006157\n",
      "Iteration 118, loss = 0.38987971\n",
      "Iteration 119, loss = 0.38961281\n",
      "Iteration 120, loss = 0.38931063\n",
      "Iteration 121, loss = 0.38907687\n",
      "Iteration 122, loss = 0.38878890\n",
      "Iteration 123, loss = 0.38853092\n",
      "Iteration 124, loss = 0.38827763\n",
      "Iteration 125, loss = 0.38804463\n",
      "Iteration 126, loss = 0.38778031\n",
      "Iteration 127, loss = 0.38753812\n",
      "Iteration 128, loss = 0.38722351\n",
      "Iteration 129, loss = 0.38702542\n",
      "Iteration 130, loss = 0.38678664\n",
      "Iteration 131, loss = 0.38653435\n",
      "Iteration 132, loss = 0.38627843\n",
      "Iteration 133, loss = 0.38610222\n",
      "Iteration 134, loss = 0.38582619\n",
      "Iteration 135, loss = 0.38562456\n",
      "Iteration 136, loss = 0.38526903\n",
      "Iteration 137, loss = 0.38521097\n",
      "Iteration 138, loss = 0.38486233\n",
      "Iteration 139, loss = 0.38485331\n",
      "Iteration 140, loss = 0.38440529\n",
      "Iteration 141, loss = 0.38425167\n",
      "Iteration 142, loss = 0.38393253\n",
      "Iteration 143, loss = 0.38366456\n",
      "Iteration 144, loss = 0.38348747\n",
      "Iteration 145, loss = 0.38323815\n",
      "Iteration 146, loss = 0.38307826\n",
      "Iteration 147, loss = 0.38285443\n",
      "Iteration 148, loss = 0.38270558\n",
      "Iteration 149, loss = 0.38232220\n",
      "Iteration 150, loss = 0.38201635\n",
      "Iteration 151, loss = 0.38188249\n",
      "Iteration 152, loss = 0.38158715\n",
      "Iteration 153, loss = 0.38135883\n",
      "Iteration 154, loss = 0.38130319\n",
      "Iteration 155, loss = 0.38119383\n",
      "Iteration 156, loss = 0.38074449\n",
      "Iteration 157, loss = 0.38042455\n",
      "Iteration 158, loss = 0.38028350\n",
      "Iteration 159, loss = 0.38017007\n",
      "Iteration 160, loss = 0.37977205\n",
      "Iteration 161, loss = 0.37959640\n",
      "Iteration 162, loss = 0.37945776\n",
      "Iteration 163, loss = 0.37913171\n",
      "Iteration 164, loss = 0.37886321\n",
      "Iteration 165, loss = 0.37869326\n",
      "Iteration 166, loss = 0.37860218\n",
      "Iteration 167, loss = 0.37834297\n",
      "Iteration 168, loss = 0.37806821\n",
      "Iteration 169, loss = 0.37778584\n",
      "Iteration 170, loss = 0.37762052\n",
      "Iteration 171, loss = 0.37747024\n",
      "Iteration 172, loss = 0.37724517\n",
      "Iteration 173, loss = 0.37708104\n",
      "Iteration 174, loss = 0.37683822\n",
      "Iteration 175, loss = 0.37660591\n",
      "Iteration 176, loss = 0.37647966\n",
      "Iteration 177, loss = 0.37625335\n",
      "Iteration 178, loss = 0.37595961\n",
      "Iteration 179, loss = 0.37577895\n",
      "Iteration 180, loss = 0.37554696\n",
      "Iteration 181, loss = 0.37549912\n",
      "Iteration 182, loss = 0.37508797\n",
      "Iteration 183, loss = 0.37482611\n",
      "Iteration 184, loss = 0.37469670\n",
      "Iteration 185, loss = 0.37451794\n",
      "Iteration 186, loss = 0.37427076\n",
      "Iteration 187, loss = 0.37429261\n",
      "Iteration 188, loss = 0.37388510\n",
      "Iteration 189, loss = 0.37365607\n",
      "Iteration 190, loss = 0.37352926\n",
      "Iteration 191, loss = 0.37318841\n",
      "Iteration 192, loss = 0.37313435\n",
      "Iteration 193, loss = 0.37282050\n",
      "Iteration 194, loss = 0.37270044\n",
      "Iteration 195, loss = 0.37249192\n",
      "Iteration 196, loss = 0.37227523\n",
      "Iteration 197, loss = 0.37202174\n",
      "Iteration 198, loss = 0.37182005\n",
      "Iteration 199, loss = 0.37165032\n",
      "Iteration 200, loss = 0.37141188\n",
      "Iteration 1, loss = 0.79312493\n",
      "Iteration 2, loss = 0.75211045\n",
      "Iteration 3, loss = 0.71423130\n",
      "Iteration 4, loss = 0.68043060\n",
      "Iteration 5, loss = 0.65044695\n",
      "Iteration 6, loss = 0.62304345\n",
      "Iteration 7, loss = 0.60014056\n",
      "Iteration 8, loss = 0.57991659\n",
      "Iteration 9, loss = 0.56207954\n",
      "Iteration 10, loss = 0.54583173\n",
      "Iteration 11, loss = 0.53293618\n",
      "Iteration 12, loss = 0.52079292\n",
      "Iteration 13, loss = 0.51052883\n",
      "Iteration 14, loss = 0.50083632\n",
      "Iteration 15, loss = 0.49234746\n",
      "Iteration 16, loss = 0.48482909\n",
      "Iteration 17, loss = 0.47775522\n",
      "Iteration 18, loss = 0.47183455\n",
      "Iteration 19, loss = 0.46583058\n",
      "Iteration 20, loss = 0.46001654\n",
      "Iteration 21, loss = 0.45530924\n",
      "Iteration 22, loss = 0.45069643\n",
      "Iteration 23, loss = 0.44628107\n",
      "Iteration 24, loss = 0.44250971\n",
      "Iteration 25, loss = 0.43885335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.43552665\n",
      "Iteration 27, loss = 0.43227024\n",
      "Iteration 28, loss = 0.42964250\n",
      "Iteration 29, loss = 0.42684297\n",
      "Iteration 30, loss = 0.42460444\n",
      "Iteration 31, loss = 0.42246039\n",
      "Iteration 32, loss = 0.42041942\n",
      "Iteration 33, loss = 0.41865375\n",
      "Iteration 34, loss = 0.41678105\n",
      "Iteration 35, loss = 0.41515003\n",
      "Iteration 36, loss = 0.41369303\n",
      "Iteration 37, loss = 0.41233893\n",
      "Iteration 38, loss = 0.41092901\n",
      "Iteration 39, loss = 0.40964930\n",
      "Iteration 40, loss = 0.40848276\n",
      "Iteration 41, loss = 0.40734926\n",
      "Iteration 42, loss = 0.40626533\n",
      "Iteration 43, loss = 0.40522264\n",
      "Iteration 44, loss = 0.40432473\n",
      "Iteration 45, loss = 0.40328248\n",
      "Iteration 46, loss = 0.40238908\n",
      "Iteration 47, loss = 0.40145581\n",
      "Iteration 48, loss = 0.40062440\n",
      "Iteration 49, loss = 0.39983746\n",
      "Iteration 50, loss = 0.39904942\n",
      "Iteration 51, loss = 0.39823255\n",
      "Iteration 52, loss = 0.39750571\n",
      "Iteration 53, loss = 0.39678346\n",
      "Iteration 54, loss = 0.39603389\n",
      "Iteration 55, loss = 0.39533737\n",
      "Iteration 56, loss = 0.39471886\n",
      "Iteration 57, loss = 0.39402184\n",
      "Iteration 58, loss = 0.39332958\n",
      "Iteration 59, loss = 0.39266623\n",
      "Iteration 60, loss = 0.39209384\n",
      "Iteration 61, loss = 0.39144715\n",
      "Iteration 62, loss = 0.39089423\n",
      "Iteration 63, loss = 0.39022940\n",
      "Iteration 64, loss = 0.38975455\n",
      "Iteration 65, loss = 0.38913015\n",
      "Iteration 66, loss = 0.38865277\n",
      "Iteration 67, loss = 0.38820643\n",
      "Iteration 68, loss = 0.38756994\n",
      "Iteration 69, loss = 0.38694602\n",
      "Iteration 70, loss = 0.38651467\n",
      "Iteration 71, loss = 0.38589831\n",
      "Iteration 72, loss = 0.38539072\n",
      "Iteration 73, loss = 0.38489739\n",
      "Iteration 74, loss = 0.38439778\n",
      "Iteration 75, loss = 0.38393057\n",
      "Iteration 76, loss = 0.38350979\n",
      "Iteration 77, loss = 0.38291623\n",
      "Iteration 78, loss = 0.38253765\n",
      "Iteration 79, loss = 0.38207321\n",
      "Iteration 80, loss = 0.38163895\n",
      "Iteration 81, loss = 0.38121769\n",
      "Iteration 82, loss = 0.38082170\n",
      "Iteration 83, loss = 0.38034043\n",
      "Iteration 84, loss = 0.37992764\n",
      "Iteration 85, loss = 0.37953489\n",
      "Iteration 86, loss = 0.37909197\n",
      "Iteration 87, loss = 0.37868373\n",
      "Iteration 88, loss = 0.37822712\n",
      "Iteration 89, loss = 0.37778577\n",
      "Iteration 90, loss = 0.37746246\n",
      "Iteration 91, loss = 0.37696749\n",
      "Iteration 92, loss = 0.37685436\n",
      "Iteration 93, loss = 0.37624368\n",
      "Iteration 94, loss = 0.37614708\n",
      "Iteration 95, loss = 0.37547475\n",
      "Iteration 96, loss = 0.37505956\n",
      "Iteration 97, loss = 0.37471322\n",
      "Iteration 98, loss = 0.37430207\n",
      "Iteration 99, loss = 0.37399192\n",
      "Iteration 100, loss = 0.37359231\n",
      "Iteration 101, loss = 0.37342278\n",
      "Iteration 102, loss = 0.37300225\n",
      "Iteration 103, loss = 0.37261912\n",
      "Iteration 104, loss = 0.37242557\n",
      "Iteration 105, loss = 0.37197095\n",
      "Iteration 106, loss = 0.37168286\n",
      "Iteration 107, loss = 0.37135530\n",
      "Iteration 108, loss = 0.37107505\n",
      "Iteration 109, loss = 0.37069952\n",
      "Iteration 110, loss = 0.37037202\n",
      "Iteration 111, loss = 0.37004879\n",
      "Iteration 112, loss = 0.36980703\n",
      "Iteration 113, loss = 0.36941202\n",
      "Iteration 114, loss = 0.36916187\n",
      "Iteration 115, loss = 0.36885019\n",
      "Iteration 116, loss = 0.36845152\n",
      "Iteration 117, loss = 0.36815421\n",
      "Iteration 118, loss = 0.36787522\n",
      "Iteration 119, loss = 0.36765338\n",
      "Iteration 120, loss = 0.36735119\n",
      "Iteration 121, loss = 0.36703658\n",
      "Iteration 122, loss = 0.36675498\n",
      "Iteration 123, loss = 0.36640076\n",
      "Iteration 124, loss = 0.36610956\n",
      "Iteration 125, loss = 0.36590564\n",
      "Iteration 126, loss = 0.36552978\n",
      "Iteration 127, loss = 0.36519254\n",
      "Iteration 128, loss = 0.36501615\n",
      "Iteration 129, loss = 0.36483340\n",
      "Iteration 130, loss = 0.36439285\n",
      "Iteration 131, loss = 0.36414852\n",
      "Iteration 132, loss = 0.36387179\n",
      "Iteration 133, loss = 0.36359356\n",
      "Iteration 134, loss = 0.36334811\n",
      "Iteration 135, loss = 0.36297045\n",
      "Iteration 136, loss = 0.36291925\n",
      "Iteration 137, loss = 0.36243122\n",
      "Iteration 138, loss = 0.36224657\n",
      "Iteration 139, loss = 0.36206693\n",
      "Iteration 140, loss = 0.36177744\n",
      "Iteration 141, loss = 0.36146278\n",
      "Iteration 142, loss = 0.36128435\n",
      "Iteration 143, loss = 0.36100816\n",
      "Iteration 144, loss = 0.36078474\n",
      "Iteration 145, loss = 0.36056295\n",
      "Iteration 146, loss = 0.36025855\n",
      "Iteration 147, loss = 0.36006590\n",
      "Iteration 148, loss = 0.35984685\n",
      "Iteration 149, loss = 0.35951850\n",
      "Iteration 150, loss = 0.35921803\n",
      "Iteration 151, loss = 0.35902101\n",
      "Iteration 152, loss = 0.35893552\n",
      "Iteration 153, loss = 0.35884717\n",
      "Iteration 154, loss = 0.35845678\n",
      "Iteration 155, loss = 0.35797087\n",
      "Iteration 156, loss = 0.35785644\n",
      "Iteration 157, loss = 0.35761895\n",
      "Iteration 158, loss = 0.35734896\n",
      "Iteration 159, loss = 0.35717420\n",
      "Iteration 160, loss = 0.35696785\n",
      "Iteration 161, loss = 0.35673847\n",
      "Iteration 162, loss = 0.35650494\n",
      "Iteration 163, loss = 0.35624277\n",
      "Iteration 164, loss = 0.35612131\n",
      "Iteration 165, loss = 0.35594052\n",
      "Iteration 166, loss = 0.35566496\n",
      "Iteration 167, loss = 0.35535609\n",
      "Iteration 168, loss = 0.35517131\n",
      "Iteration 169, loss = 0.35501314\n",
      "Iteration 170, loss = 0.35490857\n",
      "Iteration 171, loss = 0.35455736\n",
      "Iteration 172, loss = 0.35451448\n",
      "Iteration 173, loss = 0.35429129\n",
      "Iteration 174, loss = 0.35403573\n",
      "Iteration 175, loss = 0.35379740\n",
      "Iteration 176, loss = 0.35357426\n",
      "Iteration 177, loss = 0.35327536\n",
      "Iteration 178, loss = 0.35331316\n",
      "Iteration 179, loss = 0.35287320\n",
      "Iteration 180, loss = 0.35268140\n",
      "Iteration 181, loss = 0.35245750\n",
      "Iteration 182, loss = 0.35226491\n",
      "Iteration 183, loss = 0.35194829\n",
      "Iteration 184, loss = 0.35185689\n",
      "Iteration 185, loss = 0.35168127\n",
      "Iteration 186, loss = 0.35144763\n",
      "Iteration 187, loss = 0.35124164\n",
      "Iteration 188, loss = 0.35096140\n",
      "Iteration 189, loss = 0.35077792\n",
      "Iteration 190, loss = 0.35051848\n",
      "Iteration 191, loss = 0.35033937\n",
      "Iteration 192, loss = 0.35008252\n",
      "Iteration 193, loss = 0.34987586\n",
      "Iteration 194, loss = 0.34982001\n",
      "Iteration 195, loss = 0.34958340\n",
      "Iteration 196, loss = 0.34923065\n",
      "Iteration 197, loss = 0.34921547\n",
      "Iteration 198, loss = 0.34882641\n",
      "Iteration 199, loss = 0.34868970\n",
      "Iteration 200, loss = 0.34832275\n",
      "Iteration 1, loss = 0.78837702\n",
      "Iteration 2, loss = 0.74868476\n",
      "Iteration 3, loss = 0.71170530\n",
      "Iteration 4, loss = 0.67865947\n",
      "Iteration 5, loss = 0.64839154\n",
      "Iteration 6, loss = 0.62161359\n",
      "Iteration 7, loss = 0.59814838\n",
      "Iteration 8, loss = 0.57707661\n",
      "Iteration 9, loss = 0.55871543\n",
      "Iteration 10, loss = 0.54198635\n",
      "Iteration 11, loss = 0.52835427\n",
      "Iteration 12, loss = 0.51575909\n",
      "Iteration 13, loss = 0.50512269\n",
      "Iteration 14, loss = 0.49481333\n",
      "Iteration 15, loss = 0.48610883\n",
      "Iteration 16, loss = 0.47816107\n",
      "Iteration 17, loss = 0.47097280\n",
      "Iteration 18, loss = 0.46476742\n",
      "Iteration 19, loss = 0.45826890\n",
      "Iteration 20, loss = 0.45272608\n",
      "Iteration 21, loss = 0.44750982\n",
      "Iteration 22, loss = 0.44280471\n",
      "Iteration 23, loss = 0.43822321\n",
      "Iteration 24, loss = 0.43429443\n",
      "Iteration 25, loss = 0.43056432\n",
      "Iteration 26, loss = 0.42710766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.42394904\n",
      "Iteration 28, loss = 0.42120954\n",
      "Iteration 29, loss = 0.41843874\n",
      "Iteration 30, loss = 0.41604742\n",
      "Iteration 31, loss = 0.41396998\n",
      "Iteration 32, loss = 0.41187734\n",
      "Iteration 33, loss = 0.41008011\n",
      "Iteration 34, loss = 0.40821877\n",
      "Iteration 35, loss = 0.40651217\n",
      "Iteration 36, loss = 0.40523241\n",
      "Iteration 37, loss = 0.40380204\n",
      "Iteration 38, loss = 0.40237274\n",
      "Iteration 39, loss = 0.40120351\n",
      "Iteration 40, loss = 0.40005071\n",
      "Iteration 41, loss = 0.39889066\n",
      "Iteration 42, loss = 0.39785912\n",
      "Iteration 43, loss = 0.39692079\n",
      "Iteration 44, loss = 0.39598845\n",
      "Iteration 45, loss = 0.39502813\n",
      "Iteration 46, loss = 0.39407758\n",
      "Iteration 47, loss = 0.39323706\n",
      "Iteration 48, loss = 0.39244416\n",
      "Iteration 49, loss = 0.39165962\n",
      "Iteration 50, loss = 0.39081056\n",
      "Iteration 51, loss = 0.38999631\n",
      "Iteration 52, loss = 0.38935323\n",
      "Iteration 53, loss = 0.38865059\n",
      "Iteration 54, loss = 0.38786428\n",
      "Iteration 55, loss = 0.38722245\n",
      "Iteration 56, loss = 0.38657039\n",
      "Iteration 57, loss = 0.38593509\n",
      "Iteration 58, loss = 0.38529618\n",
      "Iteration 59, loss = 0.38463652\n",
      "Iteration 60, loss = 0.38402920\n",
      "Iteration 61, loss = 0.38347935\n",
      "Iteration 62, loss = 0.38288698\n",
      "Iteration 63, loss = 0.38232437\n",
      "Iteration 64, loss = 0.38190271\n",
      "Iteration 65, loss = 0.38123362\n",
      "Iteration 66, loss = 0.38072039\n",
      "Iteration 67, loss = 0.38019398\n",
      "Iteration 68, loss = 0.37965159\n",
      "Iteration 69, loss = 0.37916855\n",
      "Iteration 70, loss = 0.37866089\n",
      "Iteration 71, loss = 0.37815130\n",
      "Iteration 72, loss = 0.37766485\n",
      "Iteration 73, loss = 0.37714494\n",
      "Iteration 74, loss = 0.37671702\n",
      "Iteration 75, loss = 0.37624228\n",
      "Iteration 76, loss = 0.37574357\n",
      "Iteration 77, loss = 0.37526717\n",
      "Iteration 78, loss = 0.37488752\n",
      "Iteration 79, loss = 0.37432653\n",
      "Iteration 80, loss = 0.37392642\n",
      "Iteration 81, loss = 0.37347918\n",
      "Iteration 82, loss = 0.37305120\n",
      "Iteration 83, loss = 0.37269747\n",
      "Iteration 84, loss = 0.37224966\n",
      "Iteration 85, loss = 0.37185468\n",
      "Iteration 86, loss = 0.37131108\n",
      "Iteration 87, loss = 0.37104229\n",
      "Iteration 88, loss = 0.37055588\n",
      "Iteration 89, loss = 0.37017555\n",
      "Iteration 90, loss = 0.36988077\n",
      "Iteration 91, loss = 0.36936085\n",
      "Iteration 92, loss = 0.36921055\n",
      "Iteration 93, loss = 0.36865637\n",
      "Iteration 94, loss = 0.36831506\n",
      "Iteration 95, loss = 0.36781826\n",
      "Iteration 96, loss = 0.36746893\n",
      "Iteration 97, loss = 0.36718657\n",
      "Iteration 98, loss = 0.36673663\n",
      "Iteration 99, loss = 0.36642141\n",
      "Iteration 100, loss = 0.36594073\n",
      "Iteration 101, loss = 0.36582055\n",
      "Iteration 102, loss = 0.36546173\n",
      "Iteration 103, loss = 0.36504729\n",
      "Iteration 104, loss = 0.36479536\n",
      "Iteration 105, loss = 0.36430998\n",
      "Iteration 106, loss = 0.36403502\n",
      "Iteration 107, loss = 0.36375585\n",
      "Iteration 108, loss = 0.36348655\n",
      "Iteration 109, loss = 0.36314858\n",
      "Iteration 110, loss = 0.36276676\n",
      "Iteration 111, loss = 0.36239053\n",
      "Iteration 112, loss = 0.36216970\n",
      "Iteration 113, loss = 0.36180219\n",
      "Iteration 114, loss = 0.36155461\n",
      "Iteration 115, loss = 0.36122034\n",
      "Iteration 116, loss = 0.36083883\n",
      "Iteration 117, loss = 0.36056288\n",
      "Iteration 118, loss = 0.36032776\n",
      "Iteration 119, loss = 0.36011292\n",
      "Iteration 120, loss = 0.35979590\n",
      "Iteration 121, loss = 0.35950078\n",
      "Iteration 122, loss = 0.35919191\n",
      "Iteration 123, loss = 0.35885874\n",
      "Iteration 124, loss = 0.35863493\n",
      "Iteration 125, loss = 0.35832877\n",
      "Iteration 126, loss = 0.35800074\n",
      "Iteration 127, loss = 0.35773453\n",
      "Iteration 128, loss = 0.35746887\n",
      "Iteration 129, loss = 0.35724731\n",
      "Iteration 130, loss = 0.35692338\n",
      "Iteration 131, loss = 0.35656124\n",
      "Iteration 132, loss = 0.35634599\n",
      "Iteration 133, loss = 0.35604419\n",
      "Iteration 134, loss = 0.35572005\n",
      "Iteration 135, loss = 0.35544393\n",
      "Iteration 136, loss = 0.35520905\n",
      "Iteration 137, loss = 0.35501883\n",
      "Iteration 138, loss = 0.35479509\n",
      "Iteration 139, loss = 0.35441277\n",
      "Iteration 140, loss = 0.35411320\n",
      "Iteration 141, loss = 0.35385000\n",
      "Iteration 142, loss = 0.35360671\n",
      "Iteration 143, loss = 0.35322407\n",
      "Iteration 144, loss = 0.35300089\n",
      "Iteration 145, loss = 0.35280739\n",
      "Iteration 146, loss = 0.35257073\n",
      "Iteration 147, loss = 0.35224718\n",
      "Iteration 148, loss = 0.35191206\n",
      "Iteration 149, loss = 0.35163989\n",
      "Iteration 150, loss = 0.35137863\n",
      "Iteration 151, loss = 0.35110995\n",
      "Iteration 152, loss = 0.35101028\n",
      "Iteration 153, loss = 0.35090073\n",
      "Iteration 154, loss = 0.35038594\n",
      "Iteration 155, loss = 0.35012097\n",
      "Iteration 156, loss = 0.34988579\n",
      "Iteration 157, loss = 0.34960979\n",
      "Iteration 158, loss = 0.34933751\n",
      "Iteration 159, loss = 0.34906045\n",
      "Iteration 160, loss = 0.34892144\n",
      "Iteration 161, loss = 0.34864985\n",
      "Iteration 162, loss = 0.34837484\n",
      "Iteration 163, loss = 0.34818308\n",
      "Iteration 164, loss = 0.34788787\n",
      "Iteration 165, loss = 0.34765254\n",
      "Iteration 166, loss = 0.34744513\n",
      "Iteration 167, loss = 0.34717309\n",
      "Iteration 168, loss = 0.34702808\n",
      "Iteration 169, loss = 0.34673559\n",
      "Iteration 170, loss = 0.34656763\n",
      "Iteration 171, loss = 0.34621613\n",
      "Iteration 172, loss = 0.34608890\n",
      "Iteration 173, loss = 0.34589400\n",
      "Iteration 174, loss = 0.34568819\n",
      "Iteration 175, loss = 0.34543854\n",
      "Iteration 176, loss = 0.34513920\n",
      "Iteration 177, loss = 0.34486343\n",
      "Iteration 178, loss = 0.34473291\n",
      "Iteration 179, loss = 0.34445297\n",
      "Iteration 180, loss = 0.34420229\n",
      "Iteration 181, loss = 0.34396835\n",
      "Iteration 182, loss = 0.34378117\n",
      "Iteration 183, loss = 0.34351718\n",
      "Iteration 184, loss = 0.34327778\n",
      "Iteration 185, loss = 0.34311966\n",
      "Iteration 186, loss = 0.34293583\n",
      "Iteration 187, loss = 0.34267635\n",
      "Iteration 188, loss = 0.34248879\n",
      "Iteration 189, loss = 0.34228234\n",
      "Iteration 190, loss = 0.34204683\n",
      "Iteration 191, loss = 0.34176662\n",
      "Iteration 192, loss = 0.34162953\n",
      "Iteration 193, loss = 0.34133934\n",
      "Iteration 194, loss = 0.34125197\n",
      "Iteration 195, loss = 0.34108924\n",
      "Iteration 196, loss = 0.34067119\n",
      "Iteration 197, loss = 0.34065803\n",
      "Iteration 198, loss = 0.34039841\n",
      "Iteration 199, loss = 0.34031966\n",
      "Iteration 200, loss = 0.33988661\n",
      "Iteration 1, loss = 0.78798058\n",
      "Iteration 2, loss = 0.74824285\n",
      "Iteration 3, loss = 0.71116083\n",
      "Iteration 4, loss = 0.67855513\n",
      "Iteration 5, loss = 0.64929856\n",
      "Iteration 6, loss = 0.62381713\n",
      "Iteration 7, loss = 0.60136161\n",
      "Iteration 8, loss = 0.58140178\n",
      "Iteration 9, loss = 0.56407935\n",
      "Iteration 10, loss = 0.54855021\n",
      "Iteration 11, loss = 0.53568283\n",
      "Iteration 12, loss = 0.52467184\n",
      "Iteration 13, loss = 0.51517186\n",
      "Iteration 14, loss = 0.50581629\n",
      "Iteration 15, loss = 0.49838791\n",
      "Iteration 16, loss = 0.49148567\n",
      "Iteration 17, loss = 0.48562549\n",
      "Iteration 18, loss = 0.47980854\n",
      "Iteration 19, loss = 0.47443199\n",
      "Iteration 20, loss = 0.46995285\n",
      "Iteration 21, loss = 0.46567540\n",
      "Iteration 22, loss = 0.46169072\n",
      "Iteration 23, loss = 0.45793022\n",
      "Iteration 24, loss = 0.45468283\n",
      "Iteration 25, loss = 0.45157317\n",
      "Iteration 26, loss = 0.44884311\n",
      "Iteration 27, loss = 0.44629202\n",
      "Iteration 28, loss = 0.44395204\n",
      "Iteration 29, loss = 0.44178715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.43975490\n",
      "Iteration 31, loss = 0.43798586\n",
      "Iteration 32, loss = 0.43630750\n",
      "Iteration 33, loss = 0.43476802\n",
      "Iteration 34, loss = 0.43318786\n",
      "Iteration 35, loss = 0.43179317\n",
      "Iteration 36, loss = 0.43065497\n",
      "Iteration 37, loss = 0.42935832\n",
      "Iteration 38, loss = 0.42822571\n",
      "Iteration 39, loss = 0.42714387\n",
      "Iteration 40, loss = 0.42620351\n",
      "Iteration 41, loss = 0.42518724\n",
      "Iteration 42, loss = 0.42426351\n",
      "Iteration 43, loss = 0.42336962\n",
      "Iteration 44, loss = 0.42253372\n",
      "Iteration 45, loss = 0.42170658\n",
      "Iteration 46, loss = 0.42085051\n",
      "Iteration 47, loss = 0.42009819\n",
      "Iteration 48, loss = 0.41923551\n",
      "Iteration 49, loss = 0.41854537\n",
      "Iteration 50, loss = 0.41780682\n",
      "Iteration 51, loss = 0.41700783\n",
      "Iteration 52, loss = 0.41634983\n",
      "Iteration 53, loss = 0.41566422\n",
      "Iteration 54, loss = 0.41492882\n",
      "Iteration 55, loss = 0.41423642\n",
      "Iteration 56, loss = 0.41358276\n",
      "Iteration 57, loss = 0.41292581\n",
      "Iteration 58, loss = 0.41236868\n",
      "Iteration 59, loss = 0.41160757\n",
      "Iteration 60, loss = 0.41098369\n",
      "Iteration 61, loss = 0.41038250\n",
      "Iteration 62, loss = 0.40976725\n",
      "Iteration 63, loss = 0.40920954\n",
      "Iteration 64, loss = 0.40864348\n",
      "Iteration 65, loss = 0.40811423\n",
      "Iteration 66, loss = 0.40753864\n",
      "Iteration 67, loss = 0.40693129\n",
      "Iteration 68, loss = 0.40639558\n",
      "Iteration 69, loss = 0.40590801\n",
      "Iteration 70, loss = 0.40531442\n",
      "Iteration 71, loss = 0.40469227\n",
      "Iteration 72, loss = 0.40415379\n",
      "Iteration 73, loss = 0.40354948\n",
      "Iteration 74, loss = 0.40308923\n",
      "Iteration 75, loss = 0.40246276\n",
      "Iteration 76, loss = 0.40199671\n",
      "Iteration 77, loss = 0.40149028\n",
      "Iteration 78, loss = 0.40108763\n",
      "Iteration 79, loss = 0.40039410\n",
      "Iteration 80, loss = 0.39995483\n",
      "Iteration 81, loss = 0.39947271\n",
      "Iteration 82, loss = 0.39893583\n",
      "Iteration 83, loss = 0.39856351\n",
      "Iteration 84, loss = 0.39801085\n",
      "Iteration 85, loss = 0.39757266\n",
      "Iteration 86, loss = 0.39705017\n",
      "Iteration 87, loss = 0.39680030\n",
      "Iteration 88, loss = 0.39625799\n",
      "Iteration 89, loss = 0.39569410\n",
      "Iteration 90, loss = 0.39532285\n",
      "Iteration 91, loss = 0.39486357\n",
      "Iteration 92, loss = 0.39454522\n",
      "Iteration 93, loss = 0.39396256\n",
      "Iteration 94, loss = 0.39359045\n",
      "Iteration 95, loss = 0.39311206\n",
      "Iteration 96, loss = 0.39272508\n",
      "Iteration 97, loss = 0.39236760\n",
      "Iteration 98, loss = 0.39187093\n",
      "Iteration 99, loss = 0.39147631\n",
      "Iteration 100, loss = 0.39101940\n",
      "Iteration 101, loss = 0.39088792\n",
      "Iteration 102, loss = 0.39033807\n",
      "Iteration 103, loss = 0.39000033\n",
      "Iteration 104, loss = 0.38960632\n",
      "Iteration 105, loss = 0.38917446\n",
      "Iteration 106, loss = 0.38874050\n",
      "Iteration 107, loss = 0.38840550\n",
      "Iteration 108, loss = 0.38808075\n",
      "Iteration 109, loss = 0.38776970\n",
      "Iteration 110, loss = 0.38726791\n",
      "Iteration 111, loss = 0.38684038\n",
      "Iteration 112, loss = 0.38656720\n",
      "Iteration 113, loss = 0.38614587\n",
      "Iteration 114, loss = 0.38578801\n",
      "Iteration 115, loss = 0.38549721\n",
      "Iteration 116, loss = 0.38509465\n",
      "Iteration 117, loss = 0.38476418\n",
      "Iteration 118, loss = 0.38442738\n",
      "Iteration 119, loss = 0.38414939\n",
      "Iteration 120, loss = 0.38394020\n",
      "Iteration 121, loss = 0.38338161\n",
      "Iteration 122, loss = 0.38310713\n",
      "Iteration 123, loss = 0.38272606\n",
      "Iteration 124, loss = 0.38251570\n",
      "Iteration 125, loss = 0.38206163\n",
      "Iteration 126, loss = 0.38186974\n",
      "Iteration 127, loss = 0.38140186\n",
      "Iteration 128, loss = 0.38107485\n",
      "Iteration 129, loss = 0.38076257\n",
      "Iteration 130, loss = 0.38047057\n",
      "Iteration 131, loss = 0.38008859\n",
      "Iteration 132, loss = 0.37977611\n",
      "Iteration 133, loss = 0.37946513\n",
      "Iteration 134, loss = 0.37910396\n",
      "Iteration 135, loss = 0.37882934\n",
      "Iteration 136, loss = 0.37850343\n",
      "Iteration 137, loss = 0.37820908\n",
      "Iteration 138, loss = 0.37776490\n",
      "Iteration 139, loss = 0.37767199\n",
      "Iteration 140, loss = 0.37730009\n",
      "Iteration 141, loss = 0.37688900\n",
      "Iteration 142, loss = 0.37659934\n",
      "Iteration 143, loss = 0.37625640\n",
      "Iteration 144, loss = 0.37596954\n",
      "Iteration 145, loss = 0.37567707\n",
      "Iteration 146, loss = 0.37530418\n",
      "Iteration 147, loss = 0.37510097\n",
      "Iteration 148, loss = 0.37463874\n",
      "Iteration 149, loss = 0.37432887\n",
      "Iteration 150, loss = 0.37398789\n",
      "Iteration 151, loss = 0.37371547\n",
      "Iteration 152, loss = 0.37346749\n",
      "Iteration 153, loss = 0.37336707\n",
      "Iteration 154, loss = 0.37279875\n",
      "Iteration 155, loss = 0.37256543\n",
      "Iteration 156, loss = 0.37216611\n",
      "Iteration 157, loss = 0.37191102\n",
      "Iteration 158, loss = 0.37156468\n",
      "Iteration 159, loss = 0.37122985\n",
      "Iteration 160, loss = 0.37102856\n",
      "Iteration 161, loss = 0.37057074\n",
      "Iteration 162, loss = 0.37023440\n",
      "Iteration 163, loss = 0.36999449\n",
      "Iteration 164, loss = 0.36971436\n",
      "Iteration 165, loss = 0.36936372\n",
      "Iteration 166, loss = 0.36925869\n",
      "Iteration 167, loss = 0.36887872\n",
      "Iteration 168, loss = 0.36848940\n",
      "Iteration 169, loss = 0.36826732\n",
      "Iteration 170, loss = 0.36794916\n",
      "Iteration 171, loss = 0.36756588\n",
      "Iteration 172, loss = 0.36729303\n",
      "Iteration 173, loss = 0.36702778\n",
      "Iteration 174, loss = 0.36675724\n",
      "Iteration 175, loss = 0.36648839\n",
      "Iteration 176, loss = 0.36614816\n",
      "Iteration 177, loss = 0.36577603\n",
      "Iteration 178, loss = 0.36544076\n",
      "Iteration 179, loss = 0.36522792\n",
      "Iteration 180, loss = 0.36493377\n",
      "Iteration 181, loss = 0.36459348\n",
      "Iteration 182, loss = 0.36436823\n",
      "Iteration 183, loss = 0.36417791\n",
      "Iteration 184, loss = 0.36385056\n",
      "Iteration 185, loss = 0.36363014\n",
      "Iteration 186, loss = 0.36322235\n",
      "Iteration 187, loss = 0.36294116\n",
      "Iteration 188, loss = 0.36265881\n",
      "Iteration 189, loss = 0.36232833\n",
      "Iteration 190, loss = 0.36202789\n",
      "Iteration 191, loss = 0.36173900\n",
      "Iteration 192, loss = 0.36159155\n",
      "Iteration 193, loss = 0.36118096\n",
      "Iteration 194, loss = 0.36104987\n",
      "Iteration 195, loss = 0.36076226\n",
      "Iteration 196, loss = 0.36037796\n",
      "Iteration 197, loss = 0.36028668\n",
      "Iteration 198, loss = 0.35990570\n",
      "Iteration 199, loss = 0.35972914\n",
      "Iteration 200, loss = 0.35957963\n",
      "Iteration 1, loss = 0.79074236\n",
      "Iteration 2, loss = 0.74962373\n",
      "Iteration 3, loss = 0.70874083\n",
      "Iteration 4, loss = 0.67327908\n",
      "Iteration 5, loss = 0.64123459\n",
      "Iteration 6, loss = 0.61289303\n",
      "Iteration 7, loss = 0.58729564\n",
      "Iteration 8, loss = 0.56598432\n",
      "Iteration 9, loss = 0.54624001\n",
      "Iteration 10, loss = 0.52985608\n",
      "Iteration 11, loss = 0.51548203\n",
      "Iteration 12, loss = 0.50321008\n",
      "Iteration 13, loss = 0.49129952\n",
      "Iteration 14, loss = 0.48239402\n",
      "Iteration 15, loss = 0.47389774\n",
      "Iteration 16, loss = 0.46616043\n",
      "Iteration 17, loss = 0.45902712\n",
      "Iteration 18, loss = 0.45340769\n",
      "Iteration 19, loss = 0.44759464\n",
      "Iteration 20, loss = 0.44244426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.43769506\n",
      "Iteration 22, loss = 0.43335641\n",
      "Iteration 23, loss = 0.42966147\n",
      "Iteration 24, loss = 0.42603858\n",
      "Iteration 25, loss = 0.42276640\n",
      "Iteration 26, loss = 0.41982930\n",
      "Iteration 27, loss = 0.41700216\n",
      "Iteration 28, loss = 0.41467069\n",
      "Iteration 29, loss = 0.41226228\n",
      "Iteration 30, loss = 0.41009776\n",
      "Iteration 31, loss = 0.40819890\n",
      "Iteration 32, loss = 0.40655706\n",
      "Iteration 33, loss = 0.40480720\n",
      "Iteration 34, loss = 0.40334228\n",
      "Iteration 35, loss = 0.40191096\n",
      "Iteration 36, loss = 0.40069298\n",
      "Iteration 37, loss = 0.39940436\n",
      "Iteration 38, loss = 0.39832026\n",
      "Iteration 39, loss = 0.39726322\n",
      "Iteration 40, loss = 0.39625580\n",
      "Iteration 41, loss = 0.39529350\n",
      "Iteration 42, loss = 0.39438515\n",
      "Iteration 43, loss = 0.39352020\n",
      "Iteration 44, loss = 0.39279621\n",
      "Iteration 45, loss = 0.39189665\n",
      "Iteration 46, loss = 0.39122300\n",
      "Iteration 47, loss = 0.39047903\n",
      "Iteration 48, loss = 0.38971941\n",
      "Iteration 49, loss = 0.38903120\n",
      "Iteration 50, loss = 0.38837306\n",
      "Iteration 51, loss = 0.38780015\n",
      "Iteration 52, loss = 0.38710024\n",
      "Iteration 53, loss = 0.38644222\n",
      "Iteration 54, loss = 0.38591416\n",
      "Iteration 55, loss = 0.38528496\n",
      "Iteration 56, loss = 0.38472930\n",
      "Iteration 57, loss = 0.38421149\n",
      "Iteration 58, loss = 0.38363284\n",
      "Iteration 59, loss = 0.38301465\n",
      "Iteration 60, loss = 0.38249628\n",
      "Iteration 61, loss = 0.38212645\n",
      "Iteration 62, loss = 0.38168545\n",
      "Iteration 63, loss = 0.38114315\n",
      "Iteration 64, loss = 0.38057364\n",
      "Iteration 65, loss = 0.37999443\n",
      "Iteration 66, loss = 0.37960759\n",
      "Iteration 67, loss = 0.37905036\n",
      "Iteration 68, loss = 0.37875152\n",
      "Iteration 69, loss = 0.37813899\n",
      "Iteration 70, loss = 0.37776213\n",
      "Iteration 71, loss = 0.37732251\n",
      "Iteration 72, loss = 0.37682069\n",
      "Iteration 73, loss = 0.37644204\n",
      "Iteration 74, loss = 0.37598831\n",
      "Iteration 75, loss = 0.37560355\n",
      "Iteration 76, loss = 0.37521149\n",
      "Iteration 77, loss = 0.37474486\n",
      "Iteration 78, loss = 0.37438578\n",
      "Iteration 79, loss = 0.37392345\n",
      "Iteration 80, loss = 0.37355330\n",
      "Iteration 81, loss = 0.37326396\n",
      "Iteration 82, loss = 0.37274952\n",
      "Iteration 83, loss = 0.37247324\n",
      "Iteration 84, loss = 0.37202542\n",
      "Iteration 85, loss = 0.37159253\n",
      "Iteration 86, loss = 0.37134978\n",
      "Iteration 87, loss = 0.37089843\n",
      "Iteration 88, loss = 0.37068634\n",
      "Iteration 89, loss = 0.37019687\n",
      "Iteration 90, loss = 0.36986791\n",
      "Iteration 91, loss = 0.36962649\n",
      "Iteration 92, loss = 0.36926732\n",
      "Iteration 93, loss = 0.36885222\n",
      "Iteration 94, loss = 0.36859988\n",
      "Iteration 95, loss = 0.36819886\n",
      "Iteration 96, loss = 0.36808134\n",
      "Iteration 97, loss = 0.36759082\n",
      "Iteration 98, loss = 0.36732293\n",
      "Iteration 99, loss = 0.36696709\n",
      "Iteration 100, loss = 0.36673610\n",
      "Iteration 101, loss = 0.36634725\n",
      "Iteration 102, loss = 0.36610788\n",
      "Iteration 103, loss = 0.36574067\n",
      "Iteration 104, loss = 0.36553508\n",
      "Iteration 105, loss = 0.36509220\n",
      "Iteration 106, loss = 0.36486256\n",
      "Iteration 107, loss = 0.36457105\n",
      "Iteration 108, loss = 0.36434206\n",
      "Iteration 109, loss = 0.36397778\n",
      "Iteration 110, loss = 0.36371859\n",
      "Iteration 111, loss = 0.36337822\n",
      "Iteration 112, loss = 0.36301648\n",
      "Iteration 113, loss = 0.36280181\n",
      "Iteration 114, loss = 0.36248424\n",
      "Iteration 115, loss = 0.36227273\n",
      "Iteration 116, loss = 0.36198389\n",
      "Iteration 117, loss = 0.36164398\n",
      "Iteration 118, loss = 0.36140721\n",
      "Iteration 119, loss = 0.36108994\n",
      "Iteration 120, loss = 0.36078690\n",
      "Iteration 121, loss = 0.36058767\n",
      "Iteration 122, loss = 0.36026952\n",
      "Iteration 123, loss = 0.35999858\n",
      "Iteration 124, loss = 0.35979178\n",
      "Iteration 125, loss = 0.35950024\n",
      "Iteration 126, loss = 0.35916371\n",
      "Iteration 127, loss = 0.35897497\n",
      "Iteration 128, loss = 0.35863135\n",
      "Iteration 129, loss = 0.35836019\n",
      "Iteration 130, loss = 0.35809557\n",
      "Iteration 131, loss = 0.35791617\n",
      "Iteration 132, loss = 0.35760081\n",
      "Iteration 133, loss = 0.35729867\n",
      "Iteration 134, loss = 0.35707768\n",
      "Iteration 135, loss = 0.35676222\n",
      "Iteration 136, loss = 0.35654708\n",
      "Iteration 137, loss = 0.35640990\n",
      "Iteration 138, loss = 0.35600601\n",
      "Iteration 139, loss = 0.35585656\n",
      "Iteration 140, loss = 0.35557755\n",
      "Iteration 141, loss = 0.35530322\n",
      "Iteration 142, loss = 0.35504036\n",
      "Iteration 143, loss = 0.35483420\n",
      "Iteration 144, loss = 0.35452647\n",
      "Iteration 145, loss = 0.35448136\n",
      "Iteration 146, loss = 0.35418467\n",
      "Iteration 147, loss = 0.35382035\n",
      "Iteration 148, loss = 0.35374843\n",
      "Iteration 149, loss = 0.35339642\n",
      "Iteration 150, loss = 0.35310461\n",
      "Iteration 151, loss = 0.35300538\n",
      "Iteration 152, loss = 0.35266157\n",
      "Iteration 153, loss = 0.35243205\n",
      "Iteration 154, loss = 0.35236243\n",
      "Iteration 155, loss = 0.35219048\n",
      "Iteration 156, loss = 0.35171645\n",
      "Iteration 157, loss = 0.35149739\n",
      "Iteration 158, loss = 0.35132967\n",
      "Iteration 159, loss = 0.35106506\n",
      "Iteration 160, loss = 0.35090019\n",
      "Iteration 161, loss = 0.35061948\n",
      "Iteration 162, loss = 0.35047476\n",
      "Iteration 163, loss = 0.35017182\n",
      "Iteration 164, loss = 0.34999428\n",
      "Iteration 165, loss = 0.34983798\n",
      "Iteration 166, loss = 0.34962434\n",
      "Iteration 167, loss = 0.34940707\n",
      "Iteration 168, loss = 0.34910378\n",
      "Iteration 169, loss = 0.34894085\n",
      "Iteration 170, loss = 0.34869089\n",
      "Iteration 171, loss = 0.34843927\n",
      "Iteration 172, loss = 0.34840485\n",
      "Iteration 173, loss = 0.34824708\n",
      "Iteration 174, loss = 0.34783531\n",
      "Iteration 175, loss = 0.34767556\n",
      "Iteration 176, loss = 0.34776611\n",
      "Iteration 177, loss = 0.34745945\n",
      "Iteration 178, loss = 0.34712974\n",
      "Iteration 179, loss = 0.34703536\n",
      "Iteration 180, loss = 0.34665450\n",
      "Iteration 181, loss = 0.34675253\n",
      "Iteration 182, loss = 0.34632366\n",
      "Iteration 183, loss = 0.34619989\n",
      "Iteration 184, loss = 0.34600328\n",
      "Iteration 185, loss = 0.34583990\n",
      "Iteration 186, loss = 0.34556431\n",
      "Iteration 187, loss = 0.34550976\n",
      "Iteration 188, loss = 0.34510836\n",
      "Iteration 189, loss = 0.34492524\n",
      "Iteration 190, loss = 0.34484725\n",
      "Iteration 191, loss = 0.34448639\n",
      "Iteration 192, loss = 0.34440933\n",
      "Iteration 193, loss = 0.34422197\n",
      "Iteration 194, loss = 0.34406596\n",
      "Iteration 195, loss = 0.34386626\n",
      "Iteration 196, loss = 0.34364458\n",
      "Iteration 197, loss = 0.34337055\n",
      "Iteration 198, loss = 0.34328721\n",
      "Iteration 199, loss = 0.34306845\n",
      "Iteration 200, loss = 0.34284980\n",
      "Iteration 1, loss = 0.79435522\n",
      "Iteration 2, loss = 0.75380099\n",
      "Iteration 3, loss = 0.71410883\n",
      "Iteration 4, loss = 0.68082012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.65068711\n",
      "Iteration 6, loss = 0.62414139\n",
      "Iteration 7, loss = 0.60000052\n",
      "Iteration 8, loss = 0.58071140\n",
      "Iteration 9, loss = 0.56259724\n",
      "Iteration 10, loss = 0.54719305\n",
      "Iteration 11, loss = 0.53409425\n",
      "Iteration 12, loss = 0.52300381\n",
      "Iteration 13, loss = 0.51249975\n",
      "Iteration 14, loss = 0.50391915\n",
      "Iteration 15, loss = 0.49609908\n",
      "Iteration 16, loss = 0.48891110\n",
      "Iteration 17, loss = 0.48234066\n",
      "Iteration 18, loss = 0.47673789\n",
      "Iteration 19, loss = 0.47118115\n",
      "Iteration 20, loss = 0.46611987\n",
      "Iteration 21, loss = 0.46147209\n",
      "Iteration 22, loss = 0.45729097\n",
      "Iteration 23, loss = 0.45386500\n",
      "Iteration 24, loss = 0.45022248\n",
      "Iteration 25, loss = 0.44711452\n",
      "Iteration 26, loss = 0.44406290\n",
      "Iteration 27, loss = 0.44143962\n",
      "Iteration 28, loss = 0.43922841\n",
      "Iteration 29, loss = 0.43683241\n",
      "Iteration 30, loss = 0.43481540\n",
      "Iteration 31, loss = 0.43278305\n",
      "Iteration 32, loss = 0.43132470\n",
      "Iteration 33, loss = 0.42972816\n",
      "Iteration 34, loss = 0.42812431\n",
      "Iteration 35, loss = 0.42679249\n",
      "Iteration 36, loss = 0.42566523\n",
      "Iteration 37, loss = 0.42444277\n",
      "Iteration 38, loss = 0.42344643\n",
      "Iteration 39, loss = 0.42244872\n",
      "Iteration 40, loss = 0.42165404\n",
      "Iteration 41, loss = 0.42063543\n",
      "Iteration 42, loss = 0.41977256\n",
      "Iteration 43, loss = 0.41901797\n",
      "Iteration 44, loss = 0.41838458\n",
      "Iteration 45, loss = 0.41756123\n",
      "Iteration 46, loss = 0.41691941\n",
      "Iteration 47, loss = 0.41622834\n",
      "Iteration 48, loss = 0.41565021\n",
      "Iteration 49, loss = 0.41500293\n",
      "Iteration 50, loss = 0.41441721\n",
      "Iteration 51, loss = 0.41388371\n",
      "Iteration 52, loss = 0.41325253\n",
      "Iteration 53, loss = 0.41266521\n",
      "Iteration 54, loss = 0.41226738\n",
      "Iteration 55, loss = 0.41166623\n",
      "Iteration 56, loss = 0.41116664\n",
      "Iteration 57, loss = 0.41070477\n",
      "Iteration 58, loss = 0.41020953\n",
      "Iteration 59, loss = 0.40973792\n",
      "Iteration 60, loss = 0.40925698\n",
      "Iteration 61, loss = 0.40886022\n",
      "Iteration 62, loss = 0.40838478\n",
      "Iteration 63, loss = 0.40799090\n",
      "Iteration 64, loss = 0.40751027\n",
      "Iteration 65, loss = 0.40701854\n",
      "Iteration 66, loss = 0.40672150\n",
      "Iteration 67, loss = 0.40618143\n",
      "Iteration 68, loss = 0.40592643\n",
      "Iteration 69, loss = 0.40544716\n",
      "Iteration 70, loss = 0.40505233\n",
      "Iteration 71, loss = 0.40464222\n",
      "Iteration 72, loss = 0.40418652\n",
      "Iteration 73, loss = 0.40391751\n",
      "Iteration 74, loss = 0.40350589\n",
      "Iteration 75, loss = 0.40305653\n",
      "Iteration 76, loss = 0.40277177\n",
      "Iteration 77, loss = 0.40231253\n",
      "Iteration 78, loss = 0.40194103\n",
      "Iteration 79, loss = 0.40153730\n",
      "Iteration 80, loss = 0.40118602\n",
      "Iteration 81, loss = 0.40096533\n",
      "Iteration 82, loss = 0.40044358\n",
      "Iteration 83, loss = 0.40028709\n",
      "Iteration 84, loss = 0.39982449\n",
      "Iteration 85, loss = 0.39939737\n",
      "Iteration 86, loss = 0.39906374\n",
      "Iteration 87, loss = 0.39876330\n",
      "Iteration 88, loss = 0.39849308\n",
      "Iteration 89, loss = 0.39815405\n",
      "Iteration 90, loss = 0.39776696\n",
      "Iteration 91, loss = 0.39757386\n",
      "Iteration 92, loss = 0.39717403\n",
      "Iteration 93, loss = 0.39677598\n",
      "Iteration 94, loss = 0.39665428\n",
      "Iteration 95, loss = 0.39624328\n",
      "Iteration 96, loss = 0.39597948\n",
      "Iteration 97, loss = 0.39569837\n",
      "Iteration 98, loss = 0.39545419\n",
      "Iteration 99, loss = 0.39506034\n",
      "Iteration 100, loss = 0.39480309\n",
      "Iteration 101, loss = 0.39448822\n",
      "Iteration 102, loss = 0.39420942\n",
      "Iteration 103, loss = 0.39394927\n",
      "Iteration 104, loss = 0.39366897\n",
      "Iteration 105, loss = 0.39340940\n",
      "Iteration 106, loss = 0.39304512\n",
      "Iteration 107, loss = 0.39275132\n",
      "Iteration 108, loss = 0.39251488\n",
      "Iteration 109, loss = 0.39221448\n",
      "Iteration 110, loss = 0.39202499\n",
      "Iteration 111, loss = 0.39172568\n",
      "Iteration 112, loss = 0.39132492\n",
      "Iteration 113, loss = 0.39115083\n",
      "Iteration 114, loss = 0.39085913\n",
      "Iteration 115, loss = 0.39058085\n",
      "Iteration 116, loss = 0.39043157\n",
      "Iteration 117, loss = 0.39003202\n",
      "Iteration 118, loss = 0.38985208\n",
      "Iteration 119, loss = 0.38958614\n",
      "Iteration 120, loss = 0.38928259\n",
      "Iteration 121, loss = 0.38904763\n",
      "Iteration 122, loss = 0.38875774\n",
      "Iteration 123, loss = 0.38850139\n",
      "Iteration 124, loss = 0.38824661\n",
      "Iteration 125, loss = 0.38801318\n",
      "Iteration 126, loss = 0.38775012\n",
      "Iteration 127, loss = 0.38750906\n",
      "Iteration 128, loss = 0.38719841\n",
      "Iteration 129, loss = 0.38700038\n",
      "Iteration 130, loss = 0.38676103\n",
      "Iteration 131, loss = 0.38650596\n",
      "Iteration 132, loss = 0.38624985\n",
      "Iteration 133, loss = 0.38606972\n",
      "Iteration 134, loss = 0.38579653\n",
      "Iteration 135, loss = 0.38559684\n",
      "Iteration 136, loss = 0.38524321\n",
      "Iteration 137, loss = 0.38518608\n",
      "Iteration 138, loss = 0.38483353\n",
      "Iteration 139, loss = 0.38481930\n",
      "Iteration 140, loss = 0.38437723\n",
      "Iteration 141, loss = 0.38422710\n",
      "Iteration 142, loss = 0.38390950\n",
      "Iteration 143, loss = 0.38364070\n",
      "Iteration 144, loss = 0.38346930\n",
      "Iteration 145, loss = 0.38321769\n",
      "Iteration 146, loss = 0.38306452\n",
      "Iteration 147, loss = 0.38284013\n",
      "Iteration 148, loss = 0.38268813\n",
      "Iteration 149, loss = 0.38231022\n",
      "Iteration 150, loss = 0.38200370\n",
      "Iteration 151, loss = 0.38186649\n",
      "Iteration 152, loss = 0.38157614\n",
      "Iteration 153, loss = 0.38134425\n",
      "Iteration 154, loss = 0.38128667\n",
      "Iteration 155, loss = 0.38118029\n",
      "Iteration 156, loss = 0.38072636\n",
      "Iteration 157, loss = 0.38040935\n",
      "Iteration 158, loss = 0.38026539\n",
      "Iteration 159, loss = 0.38015174\n",
      "Iteration 160, loss = 0.37975453\n",
      "Iteration 161, loss = 0.37957872\n",
      "Iteration 162, loss = 0.37944101\n",
      "Iteration 163, loss = 0.37911668\n",
      "Iteration 164, loss = 0.37884499\n",
      "Iteration 165, loss = 0.37867324\n",
      "Iteration 166, loss = 0.37858559\n",
      "Iteration 167, loss = 0.37832251\n",
      "Iteration 168, loss = 0.37804871\n",
      "Iteration 169, loss = 0.37777035\n",
      "Iteration 170, loss = 0.37759973\n",
      "Iteration 171, loss = 0.37744318\n",
      "Iteration 172, loss = 0.37721376\n",
      "Iteration 173, loss = 0.37705158\n",
      "Iteration 174, loss = 0.37681147\n",
      "Iteration 175, loss = 0.37658160\n",
      "Iteration 176, loss = 0.37645749\n",
      "Iteration 177, loss = 0.37622178\n",
      "Iteration 178, loss = 0.37593157\n",
      "Iteration 179, loss = 0.37574843\n",
      "Iteration 180, loss = 0.37551644\n",
      "Iteration 181, loss = 0.37545527\n",
      "Iteration 182, loss = 0.37504913\n",
      "Iteration 183, loss = 0.37479413\n",
      "Iteration 184, loss = 0.37466540\n",
      "Iteration 185, loss = 0.37448868\n",
      "Iteration 186, loss = 0.37424042\n",
      "Iteration 187, loss = 0.37425630\n",
      "Iteration 188, loss = 0.37385190\n",
      "Iteration 189, loss = 0.37361728\n",
      "Iteration 190, loss = 0.37349120\n",
      "Iteration 191, loss = 0.37315312\n",
      "Iteration 192, loss = 0.37310417\n",
      "Iteration 193, loss = 0.37278877\n",
      "Iteration 194, loss = 0.37266941\n",
      "Iteration 195, loss = 0.37245593\n",
      "Iteration 196, loss = 0.37224481\n",
      "Iteration 197, loss = 0.37199495\n",
      "Iteration 198, loss = 0.37178954\n",
      "Iteration 199, loss = 0.37162877\n",
      "Iteration 200, loss = 0.37139011\n",
      "Iteration 1, loss = 0.79311956\n",
      "Iteration 2, loss = 0.75210508\n",
      "Iteration 3, loss = 0.71422592\n",
      "Iteration 4, loss = 0.68042522\n",
      "Iteration 5, loss = 0.65044156\n",
      "Iteration 6, loss = 0.62303803\n",
      "Iteration 7, loss = 0.60013513\n",
      "Iteration 8, loss = 0.57991114\n",
      "Iteration 9, loss = 0.56207407\n",
      "Iteration 10, loss = 0.54582624\n",
      "Iteration 11, loss = 0.53293066\n",
      "Iteration 12, loss = 0.52078737\n",
      "Iteration 13, loss = 0.51052325\n",
      "Iteration 14, loss = 0.50083071\n",
      "Iteration 15, loss = 0.49234182\n",
      "Iteration 16, loss = 0.48482341\n",
      "Iteration 17, loss = 0.47774952\n",
      "Iteration 18, loss = 0.47182881\n",
      "Iteration 19, loss = 0.46582481\n",
      "Iteration 20, loss = 0.46001074\n",
      "Iteration 21, loss = 0.45530341\n",
      "Iteration 22, loss = 0.45069057\n",
      "Iteration 23, loss = 0.44627517\n",
      "Iteration 24, loss = 0.44250378\n",
      "Iteration 25, loss = 0.43884739\n",
      "Iteration 26, loss = 0.43552066\n",
      "Iteration 27, loss = 0.43226422\n",
      "Iteration 28, loss = 0.42963645\n",
      "Iteration 29, loss = 0.42683689\n",
      "Iteration 30, loss = 0.42459832\n",
      "Iteration 31, loss = 0.42245423\n",
      "Iteration 32, loss = 0.42041323\n",
      "Iteration 33, loss = 0.41864753\n",
      "Iteration 34, loss = 0.41677479\n",
      "Iteration 35, loss = 0.41514373\n",
      "Iteration 36, loss = 0.41368632\n",
      "Iteration 37, loss = 0.41233262\n",
      "Iteration 38, loss = 0.41092257\n",
      "Iteration 39, loss = 0.40964293\n",
      "Iteration 40, loss = 0.40847645\n",
      "Iteration 41, loss = 0.40734286\n",
      "Iteration 42, loss = 0.40625932\n",
      "Iteration 43, loss = 0.40521636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.40431828\n",
      "Iteration 45, loss = 0.40327623\n",
      "Iteration 46, loss = 0.40238283\n",
      "Iteration 47, loss = 0.40144962\n",
      "Iteration 48, loss = 0.40061808\n",
      "Iteration 49, loss = 0.39983085\n",
      "Iteration 50, loss = 0.39904283\n",
      "Iteration 51, loss = 0.39822587\n",
      "Iteration 52, loss = 0.39749908\n",
      "Iteration 53, loss = 0.39677672\n",
      "Iteration 54, loss = 0.39602706\n",
      "Iteration 55, loss = 0.39532959\n",
      "Iteration 56, loss = 0.39471114\n",
      "Iteration 57, loss = 0.39401475\n",
      "Iteration 58, loss = 0.39332349\n",
      "Iteration 59, loss = 0.39266007\n",
      "Iteration 60, loss = 0.39208629\n",
      "Iteration 61, loss = 0.39143899\n",
      "Iteration 62, loss = 0.39088587\n",
      "Iteration 63, loss = 0.39022054\n",
      "Iteration 64, loss = 0.38974637\n",
      "Iteration 65, loss = 0.38912199\n",
      "Iteration 66, loss = 0.38864312\n",
      "Iteration 67, loss = 0.38819749\n",
      "Iteration 68, loss = 0.38756289\n",
      "Iteration 69, loss = 0.38694122\n",
      "Iteration 70, loss = 0.38650825\n",
      "Iteration 71, loss = 0.38589057\n",
      "Iteration 72, loss = 0.38538364\n",
      "Iteration 73, loss = 0.38488929\n",
      "Iteration 74, loss = 0.38439013\n",
      "Iteration 75, loss = 0.38392204\n",
      "Iteration 76, loss = 0.38350001\n",
      "Iteration 77, loss = 0.38290724\n",
      "Iteration 78, loss = 0.38252819\n",
      "Iteration 79, loss = 0.38206401\n",
      "Iteration 80, loss = 0.38163041\n",
      "Iteration 81, loss = 0.38120981\n",
      "Iteration 82, loss = 0.38081521\n",
      "Iteration 83, loss = 0.38033339\n",
      "Iteration 84, loss = 0.37991870\n",
      "Iteration 85, loss = 0.37952412\n",
      "Iteration 86, loss = 0.37907995\n",
      "Iteration 87, loss = 0.37867530\n",
      "Iteration 88, loss = 0.37821791\n",
      "Iteration 89, loss = 0.37777814\n",
      "Iteration 90, loss = 0.37745460\n",
      "Iteration 91, loss = 0.37696117\n",
      "Iteration 92, loss = 0.37684960\n",
      "Iteration 93, loss = 0.37623543\n",
      "Iteration 94, loss = 0.37613602\n",
      "Iteration 95, loss = 0.37546321\n",
      "Iteration 96, loss = 0.37504925\n",
      "Iteration 97, loss = 0.37470227\n",
      "Iteration 98, loss = 0.37429056\n",
      "Iteration 99, loss = 0.37398190\n",
      "Iteration 100, loss = 0.37358263\n",
      "Iteration 101, loss = 0.37341388\n",
      "Iteration 102, loss = 0.37299180\n",
      "Iteration 103, loss = 0.37260725\n",
      "Iteration 104, loss = 0.37241296\n",
      "Iteration 105, loss = 0.37195773\n",
      "Iteration 106, loss = 0.37167061\n",
      "Iteration 107, loss = 0.37134426\n",
      "Iteration 108, loss = 0.37106483\n",
      "Iteration 109, loss = 0.37068967\n",
      "Iteration 110, loss = 0.37036103\n",
      "Iteration 111, loss = 0.37003922\n",
      "Iteration 112, loss = 0.36979408\n",
      "Iteration 113, loss = 0.36939742\n",
      "Iteration 114, loss = 0.36914661\n",
      "Iteration 115, loss = 0.36883521\n",
      "Iteration 116, loss = 0.36843661\n",
      "Iteration 117, loss = 0.36814124\n",
      "Iteration 118, loss = 0.36786064\n",
      "Iteration 119, loss = 0.36764259\n",
      "Iteration 120, loss = 0.36734067\n",
      "Iteration 121, loss = 0.36702769\n",
      "Iteration 122, loss = 0.36674818\n",
      "Iteration 123, loss = 0.36639339\n",
      "Iteration 124, loss = 0.36610510\n",
      "Iteration 125, loss = 0.36589846\n",
      "Iteration 126, loss = 0.36552019\n",
      "Iteration 127, loss = 0.36518399\n",
      "Iteration 128, loss = 0.36500312\n",
      "Iteration 129, loss = 0.36481991\n",
      "Iteration 130, loss = 0.36437883\n",
      "Iteration 131, loss = 0.36414047\n",
      "Iteration 132, loss = 0.36386335\n",
      "Iteration 133, loss = 0.36358133\n",
      "Iteration 134, loss = 0.36333093\n",
      "Iteration 135, loss = 0.36295453\n",
      "Iteration 136, loss = 0.36291068\n",
      "Iteration 137, loss = 0.36242547\n",
      "Iteration 138, loss = 0.36223631\n",
      "Iteration 139, loss = 0.36205208\n",
      "Iteration 140, loss = 0.36176326\n",
      "Iteration 141, loss = 0.36144816\n",
      "Iteration 142, loss = 0.36127085\n",
      "Iteration 143, loss = 0.36099461\n",
      "Iteration 144, loss = 0.36077030\n",
      "Iteration 145, loss = 0.36055066\n",
      "Iteration 146, loss = 0.36024543\n",
      "Iteration 147, loss = 0.36005928\n",
      "Iteration 148, loss = 0.35984022\n",
      "Iteration 149, loss = 0.35950827\n",
      "Iteration 150, loss = 0.35920814\n",
      "Iteration 151, loss = 0.35901022\n",
      "Iteration 152, loss = 0.35892643\n",
      "Iteration 153, loss = 0.35883558\n",
      "Iteration 154, loss = 0.35844540\n",
      "Iteration 155, loss = 0.35796280\n",
      "Iteration 156, loss = 0.35784932\n",
      "Iteration 157, loss = 0.35760749\n",
      "Iteration 158, loss = 0.35733590\n",
      "Iteration 159, loss = 0.35716315\n",
      "Iteration 160, loss = 0.35695939\n",
      "Iteration 161, loss = 0.35673419\n",
      "Iteration 162, loss = 0.35650056\n",
      "Iteration 163, loss = 0.35623957\n",
      "Iteration 164, loss = 0.35610578\n",
      "Iteration 165, loss = 0.35593344\n",
      "Iteration 166, loss = 0.35566524\n",
      "Iteration 167, loss = 0.35535793\n",
      "Iteration 168, loss = 0.35517260\n",
      "Iteration 169, loss = 0.35501017\n",
      "Iteration 170, loss = 0.35490668\n",
      "Iteration 171, loss = 0.35455781\n",
      "Iteration 172, loss = 0.35451269\n",
      "Iteration 173, loss = 0.35428558\n",
      "Iteration 174, loss = 0.35403025\n",
      "Iteration 175, loss = 0.35379409\n",
      "Iteration 176, loss = 0.35357158\n",
      "Iteration 177, loss = 0.35327503\n",
      "Iteration 178, loss = 0.35331727\n",
      "Iteration 179, loss = 0.35287681\n",
      "Iteration 180, loss = 0.35269073\n",
      "Iteration 181, loss = 0.35246940\n",
      "Iteration 182, loss = 0.35226945\n",
      "Iteration 183, loss = 0.35196236\n",
      "Iteration 184, loss = 0.35187414\n",
      "Iteration 185, loss = 0.35169899\n",
      "Iteration 186, loss = 0.35146665\n",
      "Iteration 187, loss = 0.35125758\n",
      "Iteration 188, loss = 0.35098116\n",
      "Iteration 189, loss = 0.35079775\n",
      "Iteration 190, loss = 0.35054076\n",
      "Iteration 191, loss = 0.35035921\n",
      "Iteration 192, loss = 0.35010173\n",
      "Iteration 193, loss = 0.34989186\n",
      "Iteration 194, loss = 0.34983394\n",
      "Iteration 195, loss = 0.34959516\n",
      "Iteration 196, loss = 0.34924161\n",
      "Iteration 197, loss = 0.34923068\n",
      "Iteration 198, loss = 0.34883721\n",
      "Iteration 199, loss = 0.34871522\n",
      "Iteration 200, loss = 0.34834562\n",
      "Iteration 1, loss = 0.78837165\n",
      "Iteration 2, loss = 0.74867939\n",
      "Iteration 3, loss = 0.71169993\n",
      "Iteration 4, loss = 0.67865408\n",
      "Iteration 5, loss = 0.64838612\n",
      "Iteration 6, loss = 0.62160814\n",
      "Iteration 7, loss = 0.59814289\n",
      "Iteration 8, loss = 0.57707108\n",
      "Iteration 9, loss = 0.55870987\n",
      "Iteration 10, loss = 0.54198075\n",
      "Iteration 11, loss = 0.52834863\n",
      "Iteration 12, loss = 0.51575341\n",
      "Iteration 13, loss = 0.50511695\n",
      "Iteration 14, loss = 0.49480754\n",
      "Iteration 15, loss = 0.48610300\n",
      "Iteration 16, loss = 0.47815522\n",
      "Iteration 17, loss = 0.47096693\n",
      "Iteration 18, loss = 0.46476152\n",
      "Iteration 19, loss = 0.45826297\n",
      "Iteration 20, loss = 0.45272013\n",
      "Iteration 21, loss = 0.44750385\n",
      "Iteration 22, loss = 0.44279875\n",
      "Iteration 23, loss = 0.43821725\n",
      "Iteration 24, loss = 0.43428845\n",
      "Iteration 25, loss = 0.43055830\n",
      "Iteration 26, loss = 0.42710160\n",
      "Iteration 27, loss = 0.42394294\n",
      "Iteration 28, loss = 0.42120326\n",
      "Iteration 29, loss = 0.41843241\n",
      "Iteration 30, loss = 0.41604101\n",
      "Iteration 31, loss = 0.41396349\n",
      "Iteration 32, loss = 0.41187086\n",
      "Iteration 33, loss = 0.41007359\n",
      "Iteration 34, loss = 0.40821227\n",
      "Iteration 35, loss = 0.40650564\n",
      "Iteration 36, loss = 0.40522587\n",
      "Iteration 37, loss = 0.40379527\n",
      "Iteration 38, loss = 0.40236574\n",
      "Iteration 39, loss = 0.40119619\n",
      "Iteration 40, loss = 0.40004370\n",
      "Iteration 41, loss = 0.39888392\n",
      "Iteration 42, loss = 0.39785273\n",
      "Iteration 43, loss = 0.39691425\n",
      "Iteration 44, loss = 0.39598185\n",
      "Iteration 45, loss = 0.39502167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.39407096\n",
      "Iteration 47, loss = 0.39323025\n",
      "Iteration 48, loss = 0.39243729\n",
      "Iteration 49, loss = 0.39165294\n",
      "Iteration 50, loss = 0.39080382\n",
      "Iteration 51, loss = 0.38998963\n",
      "Iteration 52, loss = 0.38934680\n",
      "Iteration 53, loss = 0.38864371\n",
      "Iteration 54, loss = 0.38785742\n",
      "Iteration 55, loss = 0.38721530\n",
      "Iteration 56, loss = 0.38656292\n",
      "Iteration 57, loss = 0.38592761\n",
      "Iteration 58, loss = 0.38528801\n",
      "Iteration 59, loss = 0.38462819\n",
      "Iteration 60, loss = 0.38402089\n",
      "Iteration 61, loss = 0.38347118\n",
      "Iteration 62, loss = 0.38287909\n",
      "Iteration 63, loss = 0.38231626\n",
      "Iteration 64, loss = 0.38189447\n",
      "Iteration 65, loss = 0.38122568\n",
      "Iteration 66, loss = 0.38071206\n",
      "Iteration 67, loss = 0.38018542\n",
      "Iteration 68, loss = 0.37964287\n",
      "Iteration 69, loss = 0.37915958\n",
      "Iteration 70, loss = 0.37865219\n",
      "Iteration 71, loss = 0.37814313\n",
      "Iteration 72, loss = 0.37765730\n",
      "Iteration 73, loss = 0.37713658\n",
      "Iteration 74, loss = 0.37670817\n",
      "Iteration 75, loss = 0.37623390\n",
      "Iteration 76, loss = 0.37573483\n",
      "Iteration 77, loss = 0.37525811\n",
      "Iteration 78, loss = 0.37487742\n",
      "Iteration 79, loss = 0.37431644\n",
      "Iteration 80, loss = 0.37391754\n",
      "Iteration 81, loss = 0.37347067\n",
      "Iteration 82, loss = 0.37304261\n",
      "Iteration 83, loss = 0.37268864\n",
      "Iteration 84, loss = 0.37224020\n",
      "Iteration 85, loss = 0.37184632\n",
      "Iteration 86, loss = 0.37130213\n",
      "Iteration 87, loss = 0.37103370\n",
      "Iteration 88, loss = 0.37055028\n",
      "Iteration 89, loss = 0.37016888\n",
      "Iteration 90, loss = 0.36987233\n",
      "Iteration 91, loss = 0.36935273\n",
      "Iteration 92, loss = 0.36920144\n",
      "Iteration 93, loss = 0.36864690\n",
      "Iteration 94, loss = 0.36831001\n",
      "Iteration 95, loss = 0.36781408\n",
      "Iteration 96, loss = 0.36746559\n",
      "Iteration 97, loss = 0.36718531\n",
      "Iteration 98, loss = 0.36673465\n",
      "Iteration 99, loss = 0.36641985\n",
      "Iteration 100, loss = 0.36594115\n",
      "Iteration 101, loss = 0.36582096\n",
      "Iteration 102, loss = 0.36546273\n",
      "Iteration 103, loss = 0.36504874\n",
      "Iteration 104, loss = 0.36479608\n",
      "Iteration 105, loss = 0.36430955\n",
      "Iteration 106, loss = 0.36403381\n",
      "Iteration 107, loss = 0.36375790\n",
      "Iteration 108, loss = 0.36348549\n",
      "Iteration 109, loss = 0.36315121\n",
      "Iteration 110, loss = 0.36276818\n",
      "Iteration 111, loss = 0.36239547\n",
      "Iteration 112, loss = 0.36217292\n",
      "Iteration 113, loss = 0.36180340\n",
      "Iteration 114, loss = 0.36155663\n",
      "Iteration 115, loss = 0.36122314\n",
      "Iteration 116, loss = 0.36084181\n",
      "Iteration 117, loss = 0.36056706\n",
      "Iteration 118, loss = 0.36032855\n",
      "Iteration 119, loss = 0.36011078\n",
      "Iteration 120, loss = 0.35979274\n",
      "Iteration 121, loss = 0.35949784\n",
      "Iteration 122, loss = 0.35918568\n",
      "Iteration 123, loss = 0.35885446\n",
      "Iteration 124, loss = 0.35863131\n",
      "Iteration 125, loss = 0.35832742\n",
      "Iteration 126, loss = 0.35800174\n",
      "Iteration 127, loss = 0.35773371\n",
      "Iteration 128, loss = 0.35746530\n",
      "Iteration 129, loss = 0.35724335\n",
      "Iteration 130, loss = 0.35692651\n",
      "Iteration 131, loss = 0.35656539\n",
      "Iteration 132, loss = 0.35635519\n",
      "Iteration 133, loss = 0.35605730\n",
      "Iteration 134, loss = 0.35573019\n",
      "Iteration 135, loss = 0.35545227\n",
      "Iteration 136, loss = 0.35521916\n",
      "Iteration 137, loss = 0.35503161\n",
      "Iteration 138, loss = 0.35480477\n",
      "Iteration 139, loss = 0.35442270\n",
      "Iteration 140, loss = 0.35412204\n",
      "Iteration 141, loss = 0.35385684\n",
      "Iteration 142, loss = 0.35361569\n",
      "Iteration 143, loss = 0.35323473\n",
      "Iteration 144, loss = 0.35301043\n",
      "Iteration 145, loss = 0.35281500\n",
      "Iteration 146, loss = 0.35258267\n",
      "Iteration 147, loss = 0.35226052\n",
      "Iteration 148, loss = 0.35192424\n",
      "Iteration 149, loss = 0.35165146\n",
      "Iteration 150, loss = 0.35139178\n",
      "Iteration 151, loss = 0.35111949\n",
      "Iteration 152, loss = 0.35102107\n",
      "Iteration 153, loss = 0.35091135\n",
      "Iteration 154, loss = 0.35039865\n",
      "Iteration 155, loss = 0.35012888\n",
      "Iteration 156, loss = 0.34988603\n",
      "Iteration 157, loss = 0.34961346\n",
      "Iteration 158, loss = 0.34934281\n",
      "Iteration 159, loss = 0.34905578\n",
      "Iteration 160, loss = 0.34892233\n",
      "Iteration 161, loss = 0.34864851\n",
      "Iteration 162, loss = 0.34837143\n",
      "Iteration 163, loss = 0.34818102\n",
      "Iteration 164, loss = 0.34788075\n",
      "Iteration 165, loss = 0.34764684\n",
      "Iteration 166, loss = 0.34744296\n",
      "Iteration 167, loss = 0.34717388\n",
      "Iteration 168, loss = 0.34702519\n",
      "Iteration 169, loss = 0.34673002\n",
      "Iteration 170, loss = 0.34655721\n",
      "Iteration 171, loss = 0.34620119\n",
      "Iteration 172, loss = 0.34607622\n",
      "Iteration 173, loss = 0.34588857\n",
      "Iteration 174, loss = 0.34568306\n",
      "Iteration 175, loss = 0.34542550\n",
      "Iteration 176, loss = 0.34512730\n",
      "Iteration 177, loss = 0.34484865\n",
      "Iteration 178, loss = 0.34471836\n",
      "Iteration 179, loss = 0.34444051\n",
      "Iteration 180, loss = 0.34419265\n",
      "Iteration 181, loss = 0.34395814\n",
      "Iteration 182, loss = 0.34376313\n",
      "Iteration 183, loss = 0.34349947\n",
      "Iteration 184, loss = 0.34326324\n",
      "Iteration 185, loss = 0.34310236\n",
      "Iteration 186, loss = 0.34292093\n",
      "Iteration 187, loss = 0.34266100\n",
      "Iteration 188, loss = 0.34247505\n",
      "Iteration 189, loss = 0.34227692\n",
      "Iteration 190, loss = 0.34202861\n",
      "Iteration 191, loss = 0.34175613\n",
      "Iteration 192, loss = 0.34161571\n",
      "Iteration 193, loss = 0.34132640\n",
      "Iteration 194, loss = 0.34123623\n",
      "Iteration 195, loss = 0.34107189\n",
      "Iteration 196, loss = 0.34065581\n",
      "Iteration 197, loss = 0.34064428\n",
      "Iteration 198, loss = 0.34037882\n",
      "Iteration 199, loss = 0.34030058\n",
      "Iteration 200, loss = 0.33987765\n",
      "Iteration 1, loss = 0.78797516\n",
      "Iteration 2, loss = 0.74823744\n",
      "Iteration 3, loss = 0.71115542\n",
      "Iteration 4, loss = 0.67854971\n",
      "Iteration 5, loss = 0.64929313\n",
      "Iteration 6, loss = 0.62381169\n",
      "Iteration 7, loss = 0.60135616\n",
      "Iteration 8, loss = 0.58139630\n",
      "Iteration 9, loss = 0.56407385\n",
      "Iteration 10, loss = 0.54854467\n",
      "Iteration 11, loss = 0.53567726\n",
      "Iteration 12, loss = 0.52466624\n",
      "Iteration 13, loss = 0.51516622\n",
      "Iteration 14, loss = 0.50581061\n",
      "Iteration 15, loss = 0.49838221\n",
      "Iteration 16, loss = 0.49147993\n",
      "Iteration 17, loss = 0.48561972\n",
      "Iteration 18, loss = 0.47980273\n",
      "Iteration 19, loss = 0.47442615\n",
      "Iteration 20, loss = 0.46994698\n",
      "Iteration 21, loss = 0.46566950\n",
      "Iteration 22, loss = 0.46168480\n",
      "Iteration 23, loss = 0.45792427\n",
      "Iteration 24, loss = 0.45467686\n",
      "Iteration 25, loss = 0.45156717\n",
      "Iteration 26, loss = 0.44883707\n",
      "Iteration 27, loss = 0.44628596\n",
      "Iteration 28, loss = 0.44394595\n",
      "Iteration 29, loss = 0.44178104\n",
      "Iteration 30, loss = 0.43974875\n",
      "Iteration 31, loss = 0.43797969\n",
      "Iteration 32, loss = 0.43630131\n",
      "Iteration 33, loss = 0.43476181\n",
      "Iteration 34, loss = 0.43318162\n",
      "Iteration 35, loss = 0.43178688\n",
      "Iteration 36, loss = 0.43064863\n",
      "Iteration 37, loss = 0.42935194\n",
      "Iteration 38, loss = 0.42821928\n",
      "Iteration 39, loss = 0.42713742\n",
      "Iteration 40, loss = 0.42619703\n",
      "Iteration 41, loss = 0.42518075\n",
      "Iteration 42, loss = 0.42425699\n",
      "Iteration 43, loss = 0.42336305\n",
      "Iteration 44, loss = 0.42252714\n",
      "Iteration 45, loss = 0.42169998\n",
      "Iteration 46, loss = 0.42084389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.42009155\n",
      "Iteration 48, loss = 0.41922882\n",
      "Iteration 49, loss = 0.41853868\n",
      "Iteration 50, loss = 0.41780013\n",
      "Iteration 51, loss = 0.41700116\n",
      "Iteration 52, loss = 0.41634319\n",
      "Iteration 53, loss = 0.41565756\n",
      "Iteration 54, loss = 0.41492216\n",
      "Iteration 55, loss = 0.41422972\n",
      "Iteration 56, loss = 0.41357598\n",
      "Iteration 57, loss = 0.41291900\n",
      "Iteration 58, loss = 0.41236182\n",
      "Iteration 59, loss = 0.41160065\n",
      "Iteration 60, loss = 0.41097677\n",
      "Iteration 61, loss = 0.41037558\n",
      "Iteration 62, loss = 0.40976030\n",
      "Iteration 63, loss = 0.40920254\n",
      "Iteration 64, loss = 0.40863647\n",
      "Iteration 65, loss = 0.40810728\n",
      "Iteration 66, loss = 0.40753169\n",
      "Iteration 67, loss = 0.40692430\n",
      "Iteration 68, loss = 0.40638870\n",
      "Iteration 69, loss = 0.40590124\n",
      "Iteration 70, loss = 0.40530845\n",
      "Iteration 71, loss = 0.40468645\n",
      "Iteration 72, loss = 0.40414813\n",
      "Iteration 73, loss = 0.40354376\n",
      "Iteration 74, loss = 0.40308379\n",
      "Iteration 75, loss = 0.40245703\n",
      "Iteration 76, loss = 0.40199092\n",
      "Iteration 77, loss = 0.40148433\n",
      "Iteration 78, loss = 0.40108150\n",
      "Iteration 79, loss = 0.40038838\n",
      "Iteration 80, loss = 0.39994923\n",
      "Iteration 81, loss = 0.39946676\n",
      "Iteration 82, loss = 0.39892998\n",
      "Iteration 83, loss = 0.39855671\n",
      "Iteration 84, loss = 0.39800347\n",
      "Iteration 85, loss = 0.39756516\n",
      "Iteration 86, loss = 0.39704352\n",
      "Iteration 87, loss = 0.39679454\n",
      "Iteration 88, loss = 0.39625077\n",
      "Iteration 89, loss = 0.39568742\n",
      "Iteration 90, loss = 0.39531594\n",
      "Iteration 91, loss = 0.39485722\n",
      "Iteration 92, loss = 0.39453973\n",
      "Iteration 93, loss = 0.39395617\n",
      "Iteration 94, loss = 0.39358181\n",
      "Iteration 95, loss = 0.39310487\n",
      "Iteration 96, loss = 0.39271793\n",
      "Iteration 97, loss = 0.39236048\n",
      "Iteration 98, loss = 0.39186412\n",
      "Iteration 99, loss = 0.39146718\n",
      "Iteration 100, loss = 0.39100986\n",
      "Iteration 101, loss = 0.39088019\n",
      "Iteration 102, loss = 0.39033157\n",
      "Iteration 103, loss = 0.38999290\n",
      "Iteration 104, loss = 0.38959781\n",
      "Iteration 105, loss = 0.38916676\n",
      "Iteration 106, loss = 0.38873118\n",
      "Iteration 107, loss = 0.38839353\n",
      "Iteration 108, loss = 0.38806990\n",
      "Iteration 109, loss = 0.38776178\n",
      "Iteration 110, loss = 0.38726026\n",
      "Iteration 111, loss = 0.38683338\n",
      "Iteration 112, loss = 0.38656028\n",
      "Iteration 113, loss = 0.38613791\n",
      "Iteration 114, loss = 0.38578099\n",
      "Iteration 115, loss = 0.38549127\n",
      "Iteration 116, loss = 0.38508602\n",
      "Iteration 117, loss = 0.38475716\n",
      "Iteration 118, loss = 0.38442067\n",
      "Iteration 119, loss = 0.38414377\n",
      "Iteration 120, loss = 0.38393437\n",
      "Iteration 121, loss = 0.38337403\n",
      "Iteration 122, loss = 0.38309939\n",
      "Iteration 123, loss = 0.38271868\n",
      "Iteration 124, loss = 0.38250779\n",
      "Iteration 125, loss = 0.38205058\n",
      "Iteration 126, loss = 0.38186186\n",
      "Iteration 127, loss = 0.38139516\n",
      "Iteration 128, loss = 0.38107112\n",
      "Iteration 129, loss = 0.38076226\n",
      "Iteration 130, loss = 0.38047016\n",
      "Iteration 131, loss = 0.38007657\n",
      "Iteration 132, loss = 0.37976425\n",
      "Iteration 133, loss = 0.37945547\n",
      "Iteration 134, loss = 0.37909202\n",
      "Iteration 135, loss = 0.37882163\n",
      "Iteration 136, loss = 0.37848896\n",
      "Iteration 137, loss = 0.37819983\n",
      "Iteration 138, loss = 0.37775490\n",
      "Iteration 139, loss = 0.37765873\n",
      "Iteration 140, loss = 0.37728657\n",
      "Iteration 141, loss = 0.37687400\n",
      "Iteration 142, loss = 0.37658677\n",
      "Iteration 143, loss = 0.37623927\n",
      "Iteration 144, loss = 0.37595034\n",
      "Iteration 145, loss = 0.37565914\n",
      "Iteration 146, loss = 0.37528970\n",
      "Iteration 147, loss = 0.37508239\n",
      "Iteration 148, loss = 0.37462122\n",
      "Iteration 149, loss = 0.37431885\n",
      "Iteration 150, loss = 0.37397254\n",
      "Iteration 151, loss = 0.37369689\n",
      "Iteration 152, loss = 0.37345277\n",
      "Iteration 153, loss = 0.37334940\n",
      "Iteration 154, loss = 0.37278444\n",
      "Iteration 155, loss = 0.37255652\n",
      "Iteration 156, loss = 0.37216137\n",
      "Iteration 157, loss = 0.37190084\n",
      "Iteration 158, loss = 0.37155056\n",
      "Iteration 159, loss = 0.37120992\n",
      "Iteration 160, loss = 0.37101405\n",
      "Iteration 161, loss = 0.37056349\n",
      "Iteration 162, loss = 0.37022471\n",
      "Iteration 163, loss = 0.36998877\n",
      "Iteration 164, loss = 0.36970446\n",
      "Iteration 165, loss = 0.36935442\n",
      "Iteration 166, loss = 0.36925089\n",
      "Iteration 167, loss = 0.36886662\n",
      "Iteration 168, loss = 0.36848088\n",
      "Iteration 169, loss = 0.36826077\n",
      "Iteration 170, loss = 0.36794355\n",
      "Iteration 171, loss = 0.36756025\n",
      "Iteration 172, loss = 0.36729506\n",
      "Iteration 173, loss = 0.36702631\n",
      "Iteration 174, loss = 0.36676115\n",
      "Iteration 175, loss = 0.36649368\n",
      "Iteration 176, loss = 0.36615363\n",
      "Iteration 177, loss = 0.36577006\n",
      "Iteration 178, loss = 0.36542969\n",
      "Iteration 179, loss = 0.36521652\n",
      "Iteration 180, loss = 0.36492376\n",
      "Iteration 181, loss = 0.36458172\n",
      "Iteration 182, loss = 0.36435329\n",
      "Iteration 183, loss = 0.36415306\n",
      "Iteration 184, loss = 0.36382870\n",
      "Iteration 185, loss = 0.36360223\n",
      "Iteration 186, loss = 0.36320355\n",
      "Iteration 187, loss = 0.36291230\n",
      "Iteration 188, loss = 0.36263003\n",
      "Iteration 189, loss = 0.36230336\n",
      "Iteration 190, loss = 0.36199695\n",
      "Iteration 191, loss = 0.36170786\n",
      "Iteration 192, loss = 0.36156365\n",
      "Iteration 193, loss = 0.36115438\n",
      "Iteration 194, loss = 0.36102380\n",
      "Iteration 195, loss = 0.36074528\n",
      "Iteration 196, loss = 0.36036254\n",
      "Iteration 197, loss = 0.36026604\n",
      "Iteration 198, loss = 0.35989243\n",
      "Iteration 199, loss = 0.35971397\n",
      "Iteration 200, loss = 0.35955758\n",
      "Iteration 1, loss = 0.79074182\n",
      "Iteration 2, loss = 0.74962320\n",
      "Iteration 3, loss = 0.70874030\n",
      "Iteration 4, loss = 0.67327854\n",
      "Iteration 5, loss = 0.64123405\n",
      "Iteration 6, loss = 0.61289248\n",
      "Iteration 7, loss = 0.58729509\n",
      "Iteration 8, loss = 0.56598378\n",
      "Iteration 9, loss = 0.54623947\n",
      "Iteration 10, loss = 0.52985553\n",
      "Iteration 11, loss = 0.51548147\n",
      "Iteration 12, loss = 0.50320953\n",
      "Iteration 13, loss = 0.49129896\n",
      "Iteration 14, loss = 0.48239345\n",
      "Iteration 15, loss = 0.47389718\n",
      "Iteration 16, loss = 0.46615986\n",
      "Iteration 17, loss = 0.45902655\n",
      "Iteration 18, loss = 0.45340712\n",
      "Iteration 19, loss = 0.44759406\n",
      "Iteration 20, loss = 0.44244367\n",
      "Iteration 21, loss = 0.43769447\n",
      "Iteration 22, loss = 0.43335582\n",
      "Iteration 23, loss = 0.42966088\n",
      "Iteration 24, loss = 0.42603798\n",
      "Iteration 25, loss = 0.42276580\n",
      "Iteration 26, loss = 0.41982870\n",
      "Iteration 27, loss = 0.41700155\n",
      "Iteration 28, loss = 0.41467008\n",
      "Iteration 29, loss = 0.41226167\n",
      "Iteration 30, loss = 0.41009715\n",
      "Iteration 31, loss = 0.40819829\n",
      "Iteration 32, loss = 0.40655644\n",
      "Iteration 33, loss = 0.40480658\n",
      "Iteration 34, loss = 0.40334166\n",
      "Iteration 35, loss = 0.40191034\n",
      "Iteration 36, loss = 0.40069235\n",
      "Iteration 37, loss = 0.39940373\n",
      "Iteration 38, loss = 0.39831962\n",
      "Iteration 39, loss = 0.39726258\n",
      "Iteration 40, loss = 0.39625516\n",
      "Iteration 41, loss = 0.39529286\n",
      "Iteration 42, loss = 0.39438450\n",
      "Iteration 43, loss = 0.39351955\n",
      "Iteration 44, loss = 0.39279556\n",
      "Iteration 45, loss = 0.39189600\n",
      "Iteration 46, loss = 0.39122235\n",
      "Iteration 47, loss = 0.39047837\n",
      "Iteration 48, loss = 0.38971875\n",
      "Iteration 49, loss = 0.38903053\n",
      "Iteration 50, loss = 0.38837240\n",
      "Iteration 51, loss = 0.38779948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.38709957\n",
      "Iteration 53, loss = 0.38644154\n",
      "Iteration 54, loss = 0.38591348\n",
      "Iteration 55, loss = 0.38528428\n",
      "Iteration 56, loss = 0.38472862\n",
      "Iteration 57, loss = 0.38421081\n",
      "Iteration 58, loss = 0.38363215\n",
      "Iteration 59, loss = 0.38301397\n",
      "Iteration 60, loss = 0.38249559\n",
      "Iteration 61, loss = 0.38212576\n",
      "Iteration 62, loss = 0.38168475\n",
      "Iteration 63, loss = 0.38114245\n",
      "Iteration 64, loss = 0.38057294\n",
      "Iteration 65, loss = 0.37999373\n",
      "Iteration 66, loss = 0.37960688\n",
      "Iteration 67, loss = 0.37904965\n",
      "Iteration 68, loss = 0.37875081\n",
      "Iteration 69, loss = 0.37813828\n",
      "Iteration 70, loss = 0.37776141\n",
      "Iteration 71, loss = 0.37732179\n",
      "Iteration 72, loss = 0.37681996\n",
      "Iteration 73, loss = 0.37644131\n",
      "Iteration 74, loss = 0.37598759\n",
      "Iteration 75, loss = 0.37560282\n",
      "Iteration 76, loss = 0.37521075\n",
      "Iteration 77, loss = 0.37474412\n",
      "Iteration 78, loss = 0.37438504\n",
      "Iteration 79, loss = 0.37392270\n",
      "Iteration 80, loss = 0.37355255\n",
      "Iteration 81, loss = 0.37326321\n",
      "Iteration 82, loss = 0.37274876\n",
      "Iteration 83, loss = 0.37247248\n",
      "Iteration 84, loss = 0.37202466\n",
      "Iteration 85, loss = 0.37159176\n",
      "Iteration 86, loss = 0.37134900\n",
      "Iteration 87, loss = 0.37089765\n",
      "Iteration 88, loss = 0.37068555\n",
      "Iteration 89, loss = 0.37019608\n",
      "Iteration 90, loss = 0.36986711\n",
      "Iteration 91, loss = 0.36962569\n",
      "Iteration 92, loss = 0.36926652\n",
      "Iteration 93, loss = 0.36885142\n",
      "Iteration 94, loss = 0.36859908\n",
      "Iteration 95, loss = 0.36819804\n",
      "Iteration 96, loss = 0.36808051\n",
      "Iteration 97, loss = 0.36758998\n",
      "Iteration 98, loss = 0.36732211\n",
      "Iteration 99, loss = 0.36696627\n",
      "Iteration 100, loss = 0.36673528\n",
      "Iteration 101, loss = 0.36634641\n",
      "Iteration 102, loss = 0.36610700\n",
      "Iteration 103, loss = 0.36573982\n",
      "Iteration 104, loss = 0.36553430\n",
      "Iteration 105, loss = 0.36509146\n",
      "Iteration 106, loss = 0.36486177\n",
      "Iteration 107, loss = 0.36457011\n",
      "Iteration 108, loss = 0.36434136\n",
      "Iteration 109, loss = 0.36397697\n",
      "Iteration 110, loss = 0.36371772\n",
      "Iteration 111, loss = 0.36337740\n",
      "Iteration 112, loss = 0.36301570\n",
      "Iteration 113, loss = 0.36280091\n",
      "Iteration 114, loss = 0.36248311\n",
      "Iteration 115, loss = 0.36227178\n",
      "Iteration 116, loss = 0.36198280\n",
      "Iteration 117, loss = 0.36164319\n",
      "Iteration 118, loss = 0.36140650\n",
      "Iteration 119, loss = 0.36108919\n",
      "Iteration 120, loss = 0.36078563\n",
      "Iteration 121, loss = 0.36058629\n",
      "Iteration 122, loss = 0.36026839\n",
      "Iteration 123, loss = 0.35999665\n",
      "Iteration 124, loss = 0.35979028\n",
      "Iteration 125, loss = 0.35949887\n",
      "Iteration 126, loss = 0.35916212\n",
      "Iteration 127, loss = 0.35897493\n",
      "Iteration 128, loss = 0.35863002\n",
      "Iteration 129, loss = 0.35835906\n",
      "Iteration 130, loss = 0.35809351\n",
      "Iteration 131, loss = 0.35791335\n",
      "Iteration 132, loss = 0.35759788\n",
      "Iteration 133, loss = 0.35729729\n",
      "Iteration 134, loss = 0.35707808\n",
      "Iteration 135, loss = 0.35676169\n",
      "Iteration 136, loss = 0.35654662\n",
      "Iteration 137, loss = 0.35640813\n",
      "Iteration 138, loss = 0.35600427\n",
      "Iteration 139, loss = 0.35585277\n",
      "Iteration 140, loss = 0.35557770\n",
      "Iteration 141, loss = 0.35530402\n",
      "Iteration 142, loss = 0.35503913\n",
      "Iteration 143, loss = 0.35483390\n",
      "Iteration 144, loss = 0.35452559\n",
      "Iteration 145, loss = 0.35447727\n",
      "Iteration 146, loss = 0.35418538\n",
      "Iteration 147, loss = 0.35382006\n",
      "Iteration 148, loss = 0.35374534\n",
      "Iteration 149, loss = 0.35339241\n",
      "Iteration 150, loss = 0.35310299\n",
      "Iteration 151, loss = 0.35300247\n",
      "Iteration 152, loss = 0.35265985\n",
      "Iteration 153, loss = 0.35242932\n",
      "Iteration 154, loss = 0.35236258\n",
      "Iteration 155, loss = 0.35219108\n",
      "Iteration 156, loss = 0.35171634\n",
      "Iteration 157, loss = 0.35149894\n",
      "Iteration 158, loss = 0.35133255\n",
      "Iteration 159, loss = 0.35106459\n",
      "Iteration 160, loss = 0.35090088\n",
      "Iteration 161, loss = 0.35061904\n",
      "Iteration 162, loss = 0.35047081\n",
      "Iteration 163, loss = 0.35017397\n",
      "Iteration 164, loss = 0.34999843\n",
      "Iteration 165, loss = 0.34984682\n",
      "Iteration 166, loss = 0.34962542\n",
      "Iteration 167, loss = 0.34940522\n",
      "Iteration 168, loss = 0.34910553\n",
      "Iteration 169, loss = 0.34894569\n",
      "Iteration 170, loss = 0.34869553\n",
      "Iteration 171, loss = 0.34843833\n",
      "Iteration 172, loss = 0.34840813\n",
      "Iteration 173, loss = 0.34825221\n",
      "Iteration 174, loss = 0.34783649\n",
      "Iteration 175, loss = 0.34768110\n",
      "Iteration 176, loss = 0.34776106\n",
      "Iteration 177, loss = 0.34746054\n",
      "Iteration 178, loss = 0.34712839\n",
      "Iteration 179, loss = 0.34702906\n",
      "Iteration 180, loss = 0.34665163\n",
      "Iteration 181, loss = 0.34675641\n",
      "Iteration 182, loss = 0.34632307\n",
      "Iteration 183, loss = 0.34620260\n",
      "Iteration 184, loss = 0.34600649\n",
      "Iteration 185, loss = 0.34584076\n",
      "Iteration 186, loss = 0.34556566\n",
      "Iteration 187, loss = 0.34551224\n",
      "Iteration 188, loss = 0.34511326\n",
      "Iteration 189, loss = 0.34492542\n",
      "Iteration 190, loss = 0.34484117\n",
      "Iteration 191, loss = 0.34448625\n",
      "Iteration 192, loss = 0.34440773\n",
      "Iteration 193, loss = 0.34422181\n",
      "Iteration 194, loss = 0.34406522\n",
      "Iteration 195, loss = 0.34387136\n",
      "Iteration 196, loss = 0.34364982\n",
      "Iteration 197, loss = 0.34336950\n",
      "Iteration 198, loss = 0.34328470\n",
      "Iteration 199, loss = 0.34307070\n",
      "Iteration 200, loss = 0.34285191\n",
      "Iteration 1, loss = 0.79435468\n",
      "Iteration 2, loss = 0.75380045\n",
      "Iteration 3, loss = 0.71410829\n",
      "Iteration 4, loss = 0.68081958\n",
      "Iteration 5, loss = 0.65068657\n",
      "Iteration 6, loss = 0.62414085\n",
      "Iteration 7, loss = 0.59999998\n",
      "Iteration 8, loss = 0.58071085\n",
      "Iteration 9, loss = 0.56259669\n",
      "Iteration 10, loss = 0.54719250\n",
      "Iteration 11, loss = 0.53409369\n",
      "Iteration 12, loss = 0.52300326\n",
      "Iteration 13, loss = 0.51249919\n",
      "Iteration 14, loss = 0.50391859\n",
      "Iteration 15, loss = 0.49609851\n",
      "Iteration 16, loss = 0.48891052\n",
      "Iteration 17, loss = 0.48234008\n",
      "Iteration 18, loss = 0.47673732\n",
      "Iteration 19, loss = 0.47118056\n",
      "Iteration 20, loss = 0.46611928\n",
      "Iteration 21, loss = 0.46147150\n",
      "Iteration 22, loss = 0.45729038\n",
      "Iteration 23, loss = 0.45386440\n",
      "Iteration 24, loss = 0.45022188\n",
      "Iteration 25, loss = 0.44711392\n",
      "Iteration 26, loss = 0.44406230\n",
      "Iteration 27, loss = 0.44143902\n",
      "Iteration 28, loss = 0.43922781\n",
      "Iteration 29, loss = 0.43683180\n",
      "Iteration 30, loss = 0.43481479\n",
      "Iteration 31, loss = 0.43278244\n",
      "Iteration 32, loss = 0.43132408\n",
      "Iteration 33, loss = 0.42972754\n",
      "Iteration 34, loss = 0.42812369\n",
      "Iteration 35, loss = 0.42679187\n",
      "Iteration 36, loss = 0.42566460\n",
      "Iteration 37, loss = 0.42444214\n",
      "Iteration 38, loss = 0.42344580\n",
      "Iteration 39, loss = 0.42244809\n",
      "Iteration 40, loss = 0.42165341\n",
      "Iteration 41, loss = 0.42063479\n",
      "Iteration 42, loss = 0.41977192\n",
      "Iteration 43, loss = 0.41901732\n",
      "Iteration 44, loss = 0.41838394\n",
      "Iteration 45, loss = 0.41756059\n",
      "Iteration 46, loss = 0.41691876\n",
      "Iteration 47, loss = 0.41622769\n",
      "Iteration 48, loss = 0.41564955\n",
      "Iteration 49, loss = 0.41500228\n",
      "Iteration 50, loss = 0.41441655\n",
      "Iteration 51, loss = 0.41388305\n",
      "Iteration 52, loss = 0.41325186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.41266454\n",
      "Iteration 54, loss = 0.41226671\n",
      "Iteration 55, loss = 0.41166556\n",
      "Iteration 56, loss = 0.41116596\n",
      "Iteration 57, loss = 0.41070410\n",
      "Iteration 58, loss = 0.41020885\n",
      "Iteration 59, loss = 0.40973724\n",
      "Iteration 60, loss = 0.40925629\n",
      "Iteration 61, loss = 0.40885953\n",
      "Iteration 62, loss = 0.40838409\n",
      "Iteration 63, loss = 0.40799022\n",
      "Iteration 64, loss = 0.40750958\n",
      "Iteration 65, loss = 0.40701785\n",
      "Iteration 66, loss = 0.40672081\n",
      "Iteration 67, loss = 0.40618074\n",
      "Iteration 68, loss = 0.40592573\n",
      "Iteration 69, loss = 0.40544646\n",
      "Iteration 70, loss = 0.40505163\n",
      "Iteration 71, loss = 0.40464151\n",
      "Iteration 72, loss = 0.40418582\n",
      "Iteration 73, loss = 0.40391680\n",
      "Iteration 74, loss = 0.40350518\n",
      "Iteration 75, loss = 0.40305581\n",
      "Iteration 76, loss = 0.40277105\n",
      "Iteration 77, loss = 0.40231181\n",
      "Iteration 78, loss = 0.40194030\n",
      "Iteration 79, loss = 0.40153654\n",
      "Iteration 80, loss = 0.40118547\n",
      "Iteration 81, loss = 0.40096478\n",
      "Iteration 82, loss = 0.40044301\n",
      "Iteration 83, loss = 0.40028632\n",
      "Iteration 84, loss = 0.39982372\n",
      "Iteration 85, loss = 0.39939657\n",
      "Iteration 86, loss = 0.39906302\n",
      "Iteration 87, loss = 0.39876242\n",
      "Iteration 88, loss = 0.39849253\n",
      "Iteration 89, loss = 0.39815364\n",
      "Iteration 90, loss = 0.39776687\n",
      "Iteration 91, loss = 0.39757441\n",
      "Iteration 92, loss = 0.39717577\n",
      "Iteration 93, loss = 0.39677734\n",
      "Iteration 94, loss = 0.39665513\n",
      "Iteration 95, loss = 0.39624323\n",
      "Iteration 96, loss = 0.39597906\n",
      "Iteration 97, loss = 0.39569921\n",
      "Iteration 98, loss = 0.39545313\n",
      "Iteration 99, loss = 0.39505948\n",
      "Iteration 100, loss = 0.39480172\n",
      "Iteration 101, loss = 0.39448787\n",
      "Iteration 102, loss = 0.39420862\n",
      "Iteration 103, loss = 0.39394808\n",
      "Iteration 104, loss = 0.39366884\n",
      "Iteration 105, loss = 0.39340910\n",
      "Iteration 106, loss = 0.39304556\n",
      "Iteration 107, loss = 0.39275261\n",
      "Iteration 108, loss = 0.39251641\n",
      "Iteration 109, loss = 0.39221527\n",
      "Iteration 110, loss = 0.39202599\n",
      "Iteration 111, loss = 0.39172806\n",
      "Iteration 112, loss = 0.39132656\n",
      "Iteration 113, loss = 0.39115289\n",
      "Iteration 114, loss = 0.39086030\n",
      "Iteration 115, loss = 0.39058311\n",
      "Iteration 116, loss = 0.39043411\n",
      "Iteration 117, loss = 0.39003361\n",
      "Iteration 118, loss = 0.38985561\n",
      "Iteration 119, loss = 0.38958841\n",
      "Iteration 120, loss = 0.38928497\n",
      "Iteration 121, loss = 0.38905031\n",
      "Iteration 122, loss = 0.38875980\n",
      "Iteration 123, loss = 0.38850327\n",
      "Iteration 124, loss = 0.38824969\n",
      "Iteration 125, loss = 0.38801527\n",
      "Iteration 126, loss = 0.38775276\n",
      "Iteration 127, loss = 0.38750964\n",
      "Iteration 128, loss = 0.38720007\n",
      "Iteration 129, loss = 0.38700228\n",
      "Iteration 130, loss = 0.38676056\n",
      "Iteration 131, loss = 0.38650414\n",
      "Iteration 132, loss = 0.38625075\n",
      "Iteration 133, loss = 0.38607174\n",
      "Iteration 134, loss = 0.38580246\n",
      "Iteration 135, loss = 0.38560098\n",
      "Iteration 136, loss = 0.38524389\n",
      "Iteration 137, loss = 0.38518585\n",
      "Iteration 138, loss = 0.38483505\n",
      "Iteration 139, loss = 0.38482447\n",
      "Iteration 140, loss = 0.38437825\n",
      "Iteration 141, loss = 0.38422937\n",
      "Iteration 142, loss = 0.38391101\n",
      "Iteration 143, loss = 0.38364462\n",
      "Iteration 144, loss = 0.38347026\n",
      "Iteration 145, loss = 0.38321960\n",
      "Iteration 146, loss = 0.38306522\n",
      "Iteration 147, loss = 0.38283897\n",
      "Iteration 148, loss = 0.38269033\n",
      "Iteration 149, loss = 0.38231228\n",
      "Iteration 150, loss = 0.38200048\n",
      "Iteration 151, loss = 0.38186506\n",
      "Iteration 152, loss = 0.38157140\n",
      "Iteration 153, loss = 0.38133879\n",
      "Iteration 154, loss = 0.38128458\n",
      "Iteration 155, loss = 0.38117619\n",
      "Iteration 156, loss = 0.38071974\n",
      "Iteration 157, loss = 0.38040166\n",
      "Iteration 158, loss = 0.38025595\n",
      "Iteration 159, loss = 0.38014028\n",
      "Iteration 160, loss = 0.37974341\n",
      "Iteration 161, loss = 0.37957153\n",
      "Iteration 162, loss = 0.37943000\n",
      "Iteration 163, loss = 0.37910887\n",
      "Iteration 164, loss = 0.37883853\n",
      "Iteration 165, loss = 0.37866756\n",
      "Iteration 166, loss = 0.37857953\n",
      "Iteration 167, loss = 0.37831454\n",
      "Iteration 168, loss = 0.37804280\n",
      "Iteration 169, loss = 0.37776250\n",
      "Iteration 170, loss = 0.37759304\n",
      "Iteration 171, loss = 0.37744055\n",
      "Iteration 172, loss = 0.37721314\n",
      "Iteration 173, loss = 0.37705045\n",
      "Iteration 174, loss = 0.37680821\n",
      "Iteration 175, loss = 0.37657466\n",
      "Iteration 176, loss = 0.37645758\n",
      "Iteration 177, loss = 0.37622387\n",
      "Iteration 178, loss = 0.37593354\n",
      "Iteration 179, loss = 0.37574777\n",
      "Iteration 180, loss = 0.37550974\n",
      "Iteration 181, loss = 0.37545930\n",
      "Iteration 182, loss = 0.37504746\n",
      "Iteration 183, loss = 0.37479027\n",
      "Iteration 184, loss = 0.37465994\n",
      "Iteration 185, loss = 0.37447966\n",
      "Iteration 186, loss = 0.37423577\n",
      "Iteration 187, loss = 0.37426015\n",
      "Iteration 188, loss = 0.37384634\n",
      "Iteration 189, loss = 0.37361435\n",
      "Iteration 190, loss = 0.37348954\n",
      "Iteration 191, loss = 0.37315156\n",
      "Iteration 192, loss = 0.37309610\n",
      "Iteration 193, loss = 0.37278068\n",
      "Iteration 194, loss = 0.37266369\n",
      "Iteration 195, loss = 0.37245351\n",
      "Iteration 196, loss = 0.37223266\n",
      "Iteration 197, loss = 0.37198302\n",
      "Iteration 198, loss = 0.37178058\n",
      "Iteration 199, loss = 0.37161936\n",
      "Iteration 200, loss = 0.37137897\n",
      "Iteration 1, loss = 0.79311902\n",
      "Iteration 2, loss = 0.75210455\n",
      "Iteration 3, loss = 0.71422538\n",
      "Iteration 4, loss = 0.68042468\n",
      "Iteration 5, loss = 0.65044102\n",
      "Iteration 6, loss = 0.62303749\n",
      "Iteration 7, loss = 0.60013458\n",
      "Iteration 8, loss = 0.57991060\n",
      "Iteration 9, loss = 0.56207352\n",
      "Iteration 10, loss = 0.54582569\n",
      "Iteration 11, loss = 0.53293011\n",
      "Iteration 12, loss = 0.52078682\n",
      "Iteration 13, loss = 0.51052269\n",
      "Iteration 14, loss = 0.50083015\n",
      "Iteration 15, loss = 0.49234123\n",
      "Iteration 16, loss = 0.48482275\n",
      "Iteration 17, loss = 0.47774881\n",
      "Iteration 18, loss = 0.47182811\n",
      "Iteration 19, loss = 0.46582411\n",
      "Iteration 20, loss = 0.46001005\n",
      "Iteration 21, loss = 0.45530278\n",
      "Iteration 22, loss = 0.45068997\n",
      "Iteration 23, loss = 0.44627478\n",
      "Iteration 24, loss = 0.44250360\n",
      "Iteration 25, loss = 0.43884724\n",
      "Iteration 26, loss = 0.43552061\n",
      "Iteration 27, loss = 0.43226427\n",
      "Iteration 28, loss = 0.42963581\n",
      "Iteration 29, loss = 0.42683616\n",
      "Iteration 30, loss = 0.42459840\n",
      "Iteration 31, loss = 0.42245409\n",
      "Iteration 32, loss = 0.42041245\n",
      "Iteration 33, loss = 0.41864736\n",
      "Iteration 34, loss = 0.41677460\n",
      "Iteration 35, loss = 0.41514317\n",
      "Iteration 36, loss = 0.41368551\n",
      "Iteration 37, loss = 0.41233193\n",
      "Iteration 38, loss = 0.41092205\n",
      "Iteration 39, loss = 0.40964256\n",
      "Iteration 40, loss = 0.40847649\n",
      "Iteration 41, loss = 0.40734302\n",
      "Iteration 42, loss = 0.40625956\n",
      "Iteration 43, loss = 0.40521647\n",
      "Iteration 44, loss = 0.40431838\n",
      "Iteration 45, loss = 0.40327616\n",
      "Iteration 46, loss = 0.40238256\n",
      "Iteration 47, loss = 0.40144928\n",
      "Iteration 48, loss = 0.40061783\n",
      "Iteration 49, loss = 0.39983048\n",
      "Iteration 50, loss = 0.39904218\n",
      "Iteration 51, loss = 0.39822526\n",
      "Iteration 52, loss = 0.39749898\n",
      "Iteration 53, loss = 0.39677678\n",
      "Iteration 54, loss = 0.39602713\n",
      "Iteration 55, loss = 0.39532989\n",
      "Iteration 56, loss = 0.39471100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 0.39401510\n",
      "Iteration 58, loss = 0.39332421\n",
      "Iteration 59, loss = 0.39266023\n",
      "Iteration 60, loss = 0.39208784\n",
      "Iteration 61, loss = 0.39144104\n",
      "Iteration 62, loss = 0.39088789\n",
      "Iteration 63, loss = 0.39022204\n",
      "Iteration 64, loss = 0.38974790\n",
      "Iteration 65, loss = 0.38912438\n",
      "Iteration 66, loss = 0.38864378\n",
      "Iteration 67, loss = 0.38819799\n",
      "Iteration 68, loss = 0.38756405\n",
      "Iteration 69, loss = 0.38694235\n",
      "Iteration 70, loss = 0.38651032\n",
      "Iteration 71, loss = 0.38589304\n",
      "Iteration 72, loss = 0.38538458\n",
      "Iteration 73, loss = 0.38489075\n",
      "Iteration 74, loss = 0.38439144\n",
      "Iteration 75, loss = 0.38392234\n",
      "Iteration 76, loss = 0.38350118\n",
      "Iteration 77, loss = 0.38290821\n",
      "Iteration 78, loss = 0.38252874\n",
      "Iteration 79, loss = 0.38206481\n",
      "Iteration 80, loss = 0.38163032\n",
      "Iteration 81, loss = 0.38120748\n",
      "Iteration 82, loss = 0.38081150\n",
      "Iteration 83, loss = 0.38033188\n",
      "Iteration 84, loss = 0.37991584\n",
      "Iteration 85, loss = 0.37952216\n",
      "Iteration 86, loss = 0.37908039\n",
      "Iteration 87, loss = 0.37867428\n",
      "Iteration 88, loss = 0.37821780\n",
      "Iteration 89, loss = 0.37777633\n",
      "Iteration 90, loss = 0.37745453\n",
      "Iteration 91, loss = 0.37696179\n",
      "Iteration 92, loss = 0.37684894\n",
      "Iteration 93, loss = 0.37623912\n",
      "Iteration 94, loss = 0.37613910\n",
      "Iteration 95, loss = 0.37546597\n",
      "Iteration 96, loss = 0.37505045\n",
      "Iteration 97, loss = 0.37470308\n",
      "Iteration 98, loss = 0.37428947\n",
      "Iteration 99, loss = 0.37398115\n",
      "Iteration 100, loss = 0.37358419\n",
      "Iteration 101, loss = 0.37341750\n",
      "Iteration 102, loss = 0.37299696\n",
      "Iteration 103, loss = 0.37261324\n",
      "Iteration 104, loss = 0.37241581\n",
      "Iteration 105, loss = 0.37195889\n",
      "Iteration 106, loss = 0.37167471\n",
      "Iteration 107, loss = 0.37134688\n",
      "Iteration 108, loss = 0.37106744\n",
      "Iteration 109, loss = 0.37069217\n",
      "Iteration 110, loss = 0.37036367\n",
      "Iteration 111, loss = 0.37003964\n",
      "Iteration 112, loss = 0.36979814\n",
      "Iteration 113, loss = 0.36940303\n",
      "Iteration 114, loss = 0.36915147\n",
      "Iteration 115, loss = 0.36883753\n",
      "Iteration 116, loss = 0.36844244\n",
      "Iteration 117, loss = 0.36814272\n",
      "Iteration 118, loss = 0.36786107\n",
      "Iteration 119, loss = 0.36764695\n",
      "Iteration 120, loss = 0.36734262\n",
      "Iteration 121, loss = 0.36702420\n",
      "Iteration 122, loss = 0.36674153\n",
      "Iteration 123, loss = 0.36638749\n",
      "Iteration 124, loss = 0.36609662\n",
      "Iteration 125, loss = 0.36589244\n",
      "Iteration 126, loss = 0.36551442\n",
      "Iteration 127, loss = 0.36517533\n",
      "Iteration 128, loss = 0.36499768\n",
      "Iteration 129, loss = 0.36482059\n",
      "Iteration 130, loss = 0.36438007\n",
      "Iteration 131, loss = 0.36414199\n",
      "Iteration 132, loss = 0.36385690\n",
      "Iteration 133, loss = 0.36357598\n",
      "Iteration 134, loss = 0.36333097\n",
      "Iteration 135, loss = 0.36295814\n",
      "Iteration 136, loss = 0.36290359\n",
      "Iteration 137, loss = 0.36241572\n",
      "Iteration 138, loss = 0.36223017\n",
      "Iteration 139, loss = 0.36204547\n",
      "Iteration 140, loss = 0.36176123\n",
      "Iteration 141, loss = 0.36144697\n",
      "Iteration 142, loss = 0.36126809\n",
      "Iteration 143, loss = 0.36099070\n",
      "Iteration 144, loss = 0.36077316\n",
      "Iteration 145, loss = 0.36055210\n",
      "Iteration 146, loss = 0.36024514\n",
      "Iteration 147, loss = 0.36005857\n",
      "Iteration 148, loss = 0.35983786\n",
      "Iteration 149, loss = 0.35951018\n",
      "Iteration 150, loss = 0.35920844\n",
      "Iteration 151, loss = 0.35901322\n",
      "Iteration 152, loss = 0.35892337\n",
      "Iteration 153, loss = 0.35882328\n",
      "Iteration 154, loss = 0.35844164\n",
      "Iteration 155, loss = 0.35796134\n",
      "Iteration 156, loss = 0.35784889\n",
      "Iteration 157, loss = 0.35760878\n",
      "Iteration 158, loss = 0.35733954\n",
      "Iteration 159, loss = 0.35716494\n",
      "Iteration 160, loss = 0.35695973\n",
      "Iteration 161, loss = 0.35673504\n",
      "Iteration 162, loss = 0.35649763\n",
      "Iteration 163, loss = 0.35623556\n",
      "Iteration 164, loss = 0.35610403\n",
      "Iteration 165, loss = 0.35592563\n",
      "Iteration 166, loss = 0.35565706\n",
      "Iteration 167, loss = 0.35535052\n",
      "Iteration 168, loss = 0.35516651\n",
      "Iteration 169, loss = 0.35500856\n",
      "Iteration 170, loss = 0.35490200\n",
      "Iteration 171, loss = 0.35455203\n",
      "Iteration 172, loss = 0.35450776\n",
      "Iteration 173, loss = 0.35427953\n",
      "Iteration 174, loss = 0.35402082\n",
      "Iteration 175, loss = 0.35378767\n",
      "Iteration 176, loss = 0.35356563\n",
      "Iteration 177, loss = 0.35326449\n",
      "Iteration 178, loss = 0.35330019\n",
      "Iteration 179, loss = 0.35286563\n",
      "Iteration 180, loss = 0.35267809\n",
      "Iteration 181, loss = 0.35245363\n",
      "Iteration 182, loss = 0.35225438\n",
      "Iteration 183, loss = 0.35193993\n",
      "Iteration 184, loss = 0.35185200\n",
      "Iteration 185, loss = 0.35168022\n",
      "Iteration 186, loss = 0.35143995\n",
      "Iteration 187, loss = 0.35123146\n",
      "Iteration 188, loss = 0.35094623\n",
      "Iteration 189, loss = 0.35076875\n",
      "Iteration 190, loss = 0.35051211\n",
      "Iteration 191, loss = 0.35033122\n",
      "Iteration 192, loss = 0.35007460\n",
      "Iteration 193, loss = 0.34986257\n",
      "Iteration 194, loss = 0.34980591\n",
      "Iteration 195, loss = 0.34957404\n",
      "Iteration 196, loss = 0.34921523\n",
      "Iteration 197, loss = 0.34920282\n",
      "Iteration 198, loss = 0.34880331\n",
      "Iteration 199, loss = 0.34868186\n",
      "Iteration 200, loss = 0.34831353\n",
      "Iteration 1, loss = 0.78837112\n",
      "Iteration 2, loss = 0.74867885\n",
      "Iteration 3, loss = 0.71169939\n",
      "Iteration 4, loss = 0.67865354\n",
      "Iteration 5, loss = 0.64838558\n",
      "Iteration 6, loss = 0.62160760\n",
      "Iteration 7, loss = 0.59814235\n",
      "Iteration 8, loss = 0.57707054\n",
      "Iteration 9, loss = 0.55870932\n",
      "Iteration 10, loss = 0.54198020\n",
      "Iteration 11, loss = 0.52834807\n",
      "Iteration 12, loss = 0.51575285\n",
      "Iteration 13, loss = 0.50511639\n",
      "Iteration 14, loss = 0.49480697\n",
      "Iteration 15, loss = 0.48610243\n",
      "Iteration 16, loss = 0.47815465\n",
      "Iteration 17, loss = 0.47096636\n",
      "Iteration 18, loss = 0.46476094\n",
      "Iteration 19, loss = 0.45826239\n",
      "Iteration 20, loss = 0.45271955\n",
      "Iteration 21, loss = 0.44750326\n",
      "Iteration 22, loss = 0.44279816\n",
      "Iteration 23, loss = 0.43821665\n",
      "Iteration 24, loss = 0.43428785\n",
      "Iteration 25, loss = 0.43055770\n",
      "Iteration 26, loss = 0.42710099\n",
      "Iteration 27, loss = 0.42394233\n",
      "Iteration 28, loss = 0.42120265\n",
      "Iteration 29, loss = 0.41843179\n",
      "Iteration 30, loss = 0.41604039\n",
      "Iteration 31, loss = 0.41396287\n",
      "Iteration 32, loss = 0.41187024\n",
      "Iteration 33, loss = 0.41007297\n",
      "Iteration 34, loss = 0.40821165\n",
      "Iteration 35, loss = 0.40650501\n",
      "Iteration 36, loss = 0.40522524\n",
      "Iteration 37, loss = 0.40379463\n",
      "Iteration 38, loss = 0.40236510\n",
      "Iteration 39, loss = 0.40119555\n",
      "Iteration 40, loss = 0.40004305\n",
      "Iteration 41, loss = 0.39888328\n",
      "Iteration 42, loss = 0.39785209\n",
      "Iteration 43, loss = 0.39691360\n",
      "Iteration 44, loss = 0.39598120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.39502101\n",
      "Iteration 46, loss = 0.39407030\n",
      "Iteration 47, loss = 0.39322959\n",
      "Iteration 48, loss = 0.39243662\n",
      "Iteration 49, loss = 0.39165227\n",
      "Iteration 50, loss = 0.39080315\n",
      "Iteration 51, loss = 0.38998896\n",
      "Iteration 52, loss = 0.38934614\n",
      "Iteration 53, loss = 0.38864307\n",
      "Iteration 54, loss = 0.38785679\n",
      "Iteration 55, loss = 0.38721472\n",
      "Iteration 56, loss = 0.38656238\n",
      "Iteration 57, loss = 0.38592699\n",
      "Iteration 58, loss = 0.38528737\n",
      "Iteration 59, loss = 0.38462764\n",
      "Iteration 60, loss = 0.38402035\n",
      "Iteration 61, loss = 0.38347046\n",
      "Iteration 62, loss = 0.38287836\n",
      "Iteration 63, loss = 0.38231531\n",
      "Iteration 64, loss = 0.38189386\n",
      "Iteration 65, loss = 0.38122475\n",
      "Iteration 66, loss = 0.38071139\n",
      "Iteration 67, loss = 0.38018435\n",
      "Iteration 68, loss = 0.37964219\n",
      "Iteration 69, loss = 0.37915853\n",
      "Iteration 70, loss = 0.37865155\n",
      "Iteration 71, loss = 0.37814243\n",
      "Iteration 72, loss = 0.37765716\n",
      "Iteration 73, loss = 0.37713589\n",
      "Iteration 74, loss = 0.37670742\n",
      "Iteration 75, loss = 0.37623304\n",
      "Iteration 76, loss = 0.37573393\n",
      "Iteration 77, loss = 0.37525655\n",
      "Iteration 78, loss = 0.37487608\n",
      "Iteration 79, loss = 0.37431453\n",
      "Iteration 80, loss = 0.37391587\n",
      "Iteration 81, loss = 0.37346956\n",
      "Iteration 82, loss = 0.37304095\n",
      "Iteration 83, loss = 0.37268751\n",
      "Iteration 84, loss = 0.37223929\n",
      "Iteration 85, loss = 0.37184390\n",
      "Iteration 86, loss = 0.37130054\n",
      "Iteration 87, loss = 0.37103161\n",
      "Iteration 88, loss = 0.37054739\n",
      "Iteration 89, loss = 0.37016819\n",
      "Iteration 90, loss = 0.36987174\n",
      "Iteration 91, loss = 0.36935208\n",
      "Iteration 92, loss = 0.36920239\n",
      "Iteration 93, loss = 0.36864721\n",
      "Iteration 94, loss = 0.36830838\n",
      "Iteration 95, loss = 0.36781193\n",
      "Iteration 96, loss = 0.36746350\n",
      "Iteration 97, loss = 0.36718475\n",
      "Iteration 98, loss = 0.36673472\n",
      "Iteration 99, loss = 0.36641713\n",
      "Iteration 100, loss = 0.36593793\n",
      "Iteration 101, loss = 0.36581735\n",
      "Iteration 102, loss = 0.36545810\n",
      "Iteration 103, loss = 0.36504508\n",
      "Iteration 104, loss = 0.36479150\n",
      "Iteration 105, loss = 0.36430398\n",
      "Iteration 106, loss = 0.36402872\n",
      "Iteration 107, loss = 0.36374950\n",
      "Iteration 108, loss = 0.36347846\n",
      "Iteration 109, loss = 0.36314372\n",
      "Iteration 110, loss = 0.36276180\n",
      "Iteration 111, loss = 0.36238870\n",
      "Iteration 112, loss = 0.36216586\n",
      "Iteration 113, loss = 0.36179846\n",
      "Iteration 114, loss = 0.36154715\n",
      "Iteration 115, loss = 0.36121638\n",
      "Iteration 116, loss = 0.36083606\n",
      "Iteration 117, loss = 0.36056147\n",
      "Iteration 118, loss = 0.36032624\n",
      "Iteration 119, loss = 0.36010800\n",
      "Iteration 120, loss = 0.35978747\n",
      "Iteration 121, loss = 0.35949851\n",
      "Iteration 122, loss = 0.35918692\n",
      "Iteration 123, loss = 0.35885362\n",
      "Iteration 124, loss = 0.35863312\n",
      "Iteration 125, loss = 0.35832956\n",
      "Iteration 126, loss = 0.35800315\n",
      "Iteration 127, loss = 0.35773562\n",
      "Iteration 128, loss = 0.35746745\n",
      "Iteration 129, loss = 0.35724466\n",
      "Iteration 130, loss = 0.35692830\n",
      "Iteration 131, loss = 0.35656899\n",
      "Iteration 132, loss = 0.35635281\n",
      "Iteration 133, loss = 0.35605009\n",
      "Iteration 134, loss = 0.35572618\n",
      "Iteration 135, loss = 0.35544783\n",
      "Iteration 136, loss = 0.35521607\n",
      "Iteration 137, loss = 0.35502470\n",
      "Iteration 138, loss = 0.35480220\n",
      "Iteration 139, loss = 0.35442250\n",
      "Iteration 140, loss = 0.35411977\n",
      "Iteration 141, loss = 0.35385563\n",
      "Iteration 142, loss = 0.35361230\n",
      "Iteration 143, loss = 0.35322984\n",
      "Iteration 144, loss = 0.35300572\n",
      "Iteration 145, loss = 0.35280956\n",
      "Iteration 146, loss = 0.35257872\n",
      "Iteration 147, loss = 0.35225450\n",
      "Iteration 148, loss = 0.35192488\n",
      "Iteration 149, loss = 0.35165411\n",
      "Iteration 150, loss = 0.35139303\n",
      "Iteration 151, loss = 0.35111628\n",
      "Iteration 152, loss = 0.35101673\n",
      "Iteration 153, loss = 0.35091245\n",
      "Iteration 154, loss = 0.35039895\n",
      "Iteration 155, loss = 0.35012829\n",
      "Iteration 156, loss = 0.34988454\n",
      "Iteration 157, loss = 0.34961099\n",
      "Iteration 158, loss = 0.34934141\n",
      "Iteration 159, loss = 0.34905796\n",
      "Iteration 160, loss = 0.34891885\n",
      "Iteration 161, loss = 0.34864809\n",
      "Iteration 162, loss = 0.34837829\n",
      "Iteration 163, loss = 0.34818686\n",
      "Iteration 164, loss = 0.34788631\n",
      "Iteration 165, loss = 0.34765148\n",
      "Iteration 166, loss = 0.34745090\n",
      "Iteration 167, loss = 0.34717709\n",
      "Iteration 168, loss = 0.34702954\n",
      "Iteration 169, loss = 0.34673419\n",
      "Iteration 170, loss = 0.34656408\n",
      "Iteration 171, loss = 0.34621269\n",
      "Iteration 172, loss = 0.34608473\n",
      "Iteration 173, loss = 0.34589076\n",
      "Iteration 174, loss = 0.34569326\n",
      "Iteration 175, loss = 0.34543233\n",
      "Iteration 176, loss = 0.34513485\n",
      "Iteration 177, loss = 0.34485995\n",
      "Iteration 178, loss = 0.34472857\n",
      "Iteration 179, loss = 0.34444507\n",
      "Iteration 180, loss = 0.34419391\n",
      "Iteration 181, loss = 0.34396563\n",
      "Iteration 182, loss = 0.34377806\n",
      "Iteration 183, loss = 0.34350546\n",
      "Iteration 184, loss = 0.34326622\n",
      "Iteration 185, loss = 0.34310709\n",
      "Iteration 186, loss = 0.34292291\n",
      "Iteration 187, loss = 0.34266575\n",
      "Iteration 188, loss = 0.34248186\n",
      "Iteration 189, loss = 0.34227957\n",
      "Iteration 190, loss = 0.34203636\n",
      "Iteration 191, loss = 0.34175879\n",
      "Iteration 192, loss = 0.34162622\n",
      "Iteration 193, loss = 0.34133828\n",
      "Iteration 194, loss = 0.34124797\n",
      "Iteration 195, loss = 0.34108622\n",
      "Iteration 196, loss = 0.34067575\n",
      "Iteration 197, loss = 0.34065557\n",
      "Iteration 198, loss = 0.34039682\n",
      "Iteration 199, loss = 0.34031750\n",
      "Iteration 200, loss = 0.33988177\n",
      "Iteration 1, loss = 0.78797462\n",
      "Iteration 2, loss = 0.74823690\n",
      "Iteration 3, loss = 0.71115488\n",
      "Iteration 4, loss = 0.67854917\n",
      "Iteration 5, loss = 0.64929259\n",
      "Iteration 6, loss = 0.62381115\n",
      "Iteration 7, loss = 0.60135561\n",
      "Iteration 8, loss = 0.58139575\n",
      "Iteration 9, loss = 0.56407330\n",
      "Iteration 10, loss = 0.54854412\n",
      "Iteration 11, loss = 0.53567670\n",
      "Iteration 12, loss = 0.52466568\n",
      "Iteration 13, loss = 0.51516566\n",
      "Iteration 14, loss = 0.50581005\n",
      "Iteration 15, loss = 0.49838164\n",
      "Iteration 16, loss = 0.49147935\n",
      "Iteration 17, loss = 0.48561914\n",
      "Iteration 18, loss = 0.47980215\n",
      "Iteration 19, loss = 0.47442556\n",
      "Iteration 20, loss = 0.46994640\n",
      "Iteration 21, loss = 0.46566891\n",
      "Iteration 22, loss = 0.46168421\n",
      "Iteration 23, loss = 0.45792368\n",
      "Iteration 24, loss = 0.45467627\n",
      "Iteration 25, loss = 0.45156657\n",
      "Iteration 26, loss = 0.44883647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.44628536\n",
      "Iteration 28, loss = 0.44394534\n",
      "Iteration 29, loss = 0.44178042\n",
      "Iteration 30, loss = 0.43974813\n",
      "Iteration 31, loss = 0.43797908\n",
      "Iteration 32, loss = 0.43630070\n",
      "Iteration 33, loss = 0.43476119\n",
      "Iteration 34, loss = 0.43318100\n",
      "Iteration 35, loss = 0.43178625\n",
      "Iteration 36, loss = 0.43064800\n",
      "Iteration 37, loss = 0.42935131\n",
      "Iteration 38, loss = 0.42821865\n",
      "Iteration 39, loss = 0.42713679\n",
      "Iteration 40, loss = 0.42619639\n",
      "Iteration 41, loss = 0.42518010\n",
      "Iteration 42, loss = 0.42425635\n",
      "Iteration 43, loss = 0.42336240\n",
      "Iteration 44, loss = 0.42252649\n",
      "Iteration 45, loss = 0.42169933\n",
      "Iteration 46, loss = 0.42084323\n",
      "Iteration 47, loss = 0.42009089\n",
      "Iteration 48, loss = 0.41922816\n",
      "Iteration 49, loss = 0.41853802\n",
      "Iteration 50, loss = 0.41779947\n",
      "Iteration 51, loss = 0.41700049\n",
      "Iteration 52, loss = 0.41634252\n",
      "Iteration 53, loss = 0.41565689\n",
      "Iteration 54, loss = 0.41492149\n",
      "Iteration 55, loss = 0.41422904\n",
      "Iteration 56, loss = 0.41357530\n",
      "Iteration 57, loss = 0.41291832\n",
      "Iteration 58, loss = 0.41236114\n",
      "Iteration 59, loss = 0.41159996\n",
      "Iteration 60, loss = 0.41097608\n",
      "Iteration 61, loss = 0.41037488\n",
      "Iteration 62, loss = 0.40975962\n",
      "Iteration 63, loss = 0.40920186\n",
      "Iteration 64, loss = 0.40863579\n",
      "Iteration 65, loss = 0.40810660\n",
      "Iteration 66, loss = 0.40753101\n",
      "Iteration 67, loss = 0.40692362\n",
      "Iteration 68, loss = 0.40638802\n",
      "Iteration 69, loss = 0.40590055\n",
      "Iteration 70, loss = 0.40530776\n",
      "Iteration 71, loss = 0.40468577\n",
      "Iteration 72, loss = 0.40414743\n",
      "Iteration 73, loss = 0.40354306\n",
      "Iteration 74, loss = 0.40308309\n",
      "Iteration 75, loss = 0.40245633\n",
      "Iteration 76, loss = 0.40199021\n",
      "Iteration 77, loss = 0.40148361\n",
      "Iteration 78, loss = 0.40108078\n",
      "Iteration 79, loss = 0.40038764\n",
      "Iteration 80, loss = 0.39994848\n",
      "Iteration 81, loss = 0.39946602\n",
      "Iteration 82, loss = 0.39892922\n",
      "Iteration 83, loss = 0.39855595\n",
      "Iteration 84, loss = 0.39800271\n",
      "Iteration 85, loss = 0.39756435\n",
      "Iteration 86, loss = 0.39704280\n",
      "Iteration 87, loss = 0.39679394\n",
      "Iteration 88, loss = 0.39625015\n",
      "Iteration 89, loss = 0.39568679\n",
      "Iteration 90, loss = 0.39531526\n",
      "Iteration 91, loss = 0.39485651\n",
      "Iteration 92, loss = 0.39453901\n",
      "Iteration 93, loss = 0.39395551\n",
      "Iteration 94, loss = 0.39358116\n",
      "Iteration 95, loss = 0.39310421\n",
      "Iteration 96, loss = 0.39271728\n",
      "Iteration 97, loss = 0.39235987\n",
      "Iteration 98, loss = 0.39186353\n",
      "Iteration 99, loss = 0.39146665\n",
      "Iteration 100, loss = 0.39100935\n",
      "Iteration 101, loss = 0.39087938\n",
      "Iteration 102, loss = 0.39033080\n",
      "Iteration 103, loss = 0.38999222\n",
      "Iteration 104, loss = 0.38959707\n",
      "Iteration 105, loss = 0.38916601\n",
      "Iteration 106, loss = 0.38873047\n",
      "Iteration 107, loss = 0.38839280\n",
      "Iteration 108, loss = 0.38806910\n",
      "Iteration 109, loss = 0.38776082\n",
      "Iteration 110, loss = 0.38725948\n",
      "Iteration 111, loss = 0.38683264\n",
      "Iteration 112, loss = 0.38655942\n",
      "Iteration 113, loss = 0.38613693\n",
      "Iteration 114, loss = 0.38577999\n",
      "Iteration 115, loss = 0.38549073\n",
      "Iteration 116, loss = 0.38508527\n",
      "Iteration 117, loss = 0.38475600\n",
      "Iteration 118, loss = 0.38441932\n",
      "Iteration 119, loss = 0.38414288\n",
      "Iteration 120, loss = 0.38393419\n",
      "Iteration 121, loss = 0.38337357\n",
      "Iteration 122, loss = 0.38309895\n",
      "Iteration 123, loss = 0.38271895\n",
      "Iteration 124, loss = 0.38250573\n",
      "Iteration 125, loss = 0.38204754\n",
      "Iteration 126, loss = 0.38186068\n",
      "Iteration 127, loss = 0.38139443\n",
      "Iteration 128, loss = 0.38106941\n",
      "Iteration 129, loss = 0.38075947\n",
      "Iteration 130, loss = 0.38046702\n",
      "Iteration 131, loss = 0.38007576\n",
      "Iteration 132, loss = 0.37976355\n",
      "Iteration 133, loss = 0.37945648\n",
      "Iteration 134, loss = 0.37909449\n",
      "Iteration 135, loss = 0.37882300\n",
      "Iteration 136, loss = 0.37849112\n",
      "Iteration 137, loss = 0.37820061\n",
      "Iteration 138, loss = 0.37775546\n",
      "Iteration 139, loss = 0.37765867\n",
      "Iteration 140, loss = 0.37728795\n",
      "Iteration 141, loss = 0.37687302\n",
      "Iteration 142, loss = 0.37658568\n",
      "Iteration 143, loss = 0.37623839\n",
      "Iteration 144, loss = 0.37594798\n",
      "Iteration 145, loss = 0.37565690\n",
      "Iteration 146, loss = 0.37528568\n",
      "Iteration 147, loss = 0.37508188\n",
      "Iteration 148, loss = 0.37462264\n",
      "Iteration 149, loss = 0.37431640\n",
      "Iteration 150, loss = 0.37397131\n",
      "Iteration 151, loss = 0.37369105\n",
      "Iteration 152, loss = 0.37344393\n",
      "Iteration 153, loss = 0.37333728\n",
      "Iteration 154, loss = 0.37277768\n",
      "Iteration 155, loss = 0.37255180\n",
      "Iteration 156, loss = 0.37215573\n",
      "Iteration 157, loss = 0.37190050\n",
      "Iteration 158, loss = 0.37155420\n",
      "Iteration 159, loss = 0.37121658\n",
      "Iteration 160, loss = 0.37101514\n",
      "Iteration 161, loss = 0.37056838\n",
      "Iteration 162, loss = 0.37022689\n",
      "Iteration 163, loss = 0.36998660\n",
      "Iteration 164, loss = 0.36970345\n",
      "Iteration 165, loss = 0.36935820\n",
      "Iteration 166, loss = 0.36925282\n",
      "Iteration 167, loss = 0.36887608\n",
      "Iteration 168, loss = 0.36848279\n",
      "Iteration 169, loss = 0.36826548\n",
      "Iteration 170, loss = 0.36794663\n",
      "Iteration 171, loss = 0.36756402\n",
      "Iteration 172, loss = 0.36729547\n",
      "Iteration 173, loss = 0.36702676\n",
      "Iteration 174, loss = 0.36676417\n",
      "Iteration 175, loss = 0.36649593\n",
      "Iteration 176, loss = 0.36615436\n",
      "Iteration 177, loss = 0.36576733\n",
      "Iteration 178, loss = 0.36543079\n",
      "Iteration 179, loss = 0.36521518\n",
      "Iteration 180, loss = 0.36491064\n",
      "Iteration 181, loss = 0.36457652\n",
      "Iteration 182, loss = 0.36434823\n",
      "Iteration 183, loss = 0.36414819\n",
      "Iteration 184, loss = 0.36382089\n",
      "Iteration 185, loss = 0.36359212\n",
      "Iteration 186, loss = 0.36318547\n",
      "Iteration 187, loss = 0.36290353\n",
      "Iteration 188, loss = 0.36262241\n",
      "Iteration 189, loss = 0.36228156\n",
      "Iteration 190, loss = 0.36198313\n",
      "Iteration 191, loss = 0.36169999\n",
      "Iteration 192, loss = 0.36155511\n",
      "Iteration 193, loss = 0.36114098\n",
      "Iteration 194, loss = 0.36099882\n",
      "Iteration 195, loss = 0.36071996\n",
      "Iteration 196, loss = 0.36033903\n",
      "Iteration 197, loss = 0.36025437\n",
      "Iteration 198, loss = 0.35987011\n",
      "Iteration 199, loss = 0.35969968\n",
      "Iteration 200, loss = 0.35953576\n",
      "Iteration 1, loss = 0.79379501\n",
      "Iteration 2, loss = 0.73996204\n",
      "Iteration 3, loss = 0.69312931\n",
      "Iteration 4, loss = 0.65205138\n",
      "Iteration 5, loss = 0.61779916\n",
      "Iteration 6, loss = 0.58823356\n",
      "Iteration 7, loss = 0.56504490\n",
      "Iteration 8, loss = 0.54449832\n",
      "Iteration 9, loss = 0.52844334\n",
      "Iteration 10, loss = 0.51400582\n",
      "Iteration 11, loss = 0.50255395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.49250550\n",
      "Iteration 13, loss = 0.48383289\n",
      "Iteration 14, loss = 0.47608446\n",
      "Iteration 15, loss = 0.46901038\n",
      "Iteration 16, loss = 0.46310944\n",
      "Iteration 17, loss = 0.45730623\n",
      "Iteration 18, loss = 0.45291483\n",
      "Iteration 19, loss = 0.44859242\n",
      "Iteration 20, loss = 0.44502549\n",
      "Iteration 21, loss = 0.44145689\n",
      "Iteration 22, loss = 0.43859466\n",
      "Iteration 23, loss = 0.43584100\n",
      "Iteration 24, loss = 0.43360588\n",
      "Iteration 25, loss = 0.43128288\n",
      "Iteration 26, loss = 0.42953256\n",
      "Iteration 27, loss = 0.42752184\n",
      "Iteration 28, loss = 0.42597325\n",
      "Iteration 29, loss = 0.42475882\n",
      "Iteration 30, loss = 0.42343599\n",
      "Iteration 31, loss = 0.42214361\n",
      "Iteration 32, loss = 0.42106732\n",
      "Iteration 33, loss = 0.42009819\n",
      "Iteration 34, loss = 0.41906674\n",
      "Iteration 35, loss = 0.41808680\n",
      "Iteration 36, loss = 0.41720834\n",
      "Iteration 37, loss = 0.41625888\n",
      "Iteration 38, loss = 0.41563204\n",
      "Iteration 39, loss = 0.41480307\n",
      "Iteration 40, loss = 0.41399090\n",
      "Iteration 41, loss = 0.41334786\n",
      "Iteration 42, loss = 0.41268069\n",
      "Iteration 43, loss = 0.41209502\n",
      "Iteration 44, loss = 0.41131995\n",
      "Iteration 45, loss = 0.41060131\n",
      "Iteration 46, loss = 0.40995669\n",
      "Iteration 47, loss = 0.40944077\n",
      "Iteration 48, loss = 0.40877678\n",
      "Iteration 49, loss = 0.40833235\n",
      "Iteration 50, loss = 0.40767436\n",
      "Iteration 51, loss = 0.40726418\n",
      "Iteration 52, loss = 0.40666977\n",
      "Iteration 53, loss = 0.40608843\n",
      "Iteration 54, loss = 0.40560856\n",
      "Iteration 55, loss = 0.40500434\n",
      "Iteration 56, loss = 0.40460694\n",
      "Iteration 57, loss = 0.40399777\n",
      "Iteration 58, loss = 0.40351663\n",
      "Iteration 59, loss = 0.40317623\n",
      "Iteration 60, loss = 0.40264966\n",
      "Iteration 61, loss = 0.40229229\n",
      "Iteration 62, loss = 0.40174276\n",
      "Iteration 63, loss = 0.40148509\n",
      "Iteration 64, loss = 0.40102282\n",
      "Iteration 65, loss = 0.40057850\n",
      "Iteration 66, loss = 0.40016963\n",
      "Iteration 67, loss = 0.39964148\n",
      "Iteration 68, loss = 0.39946143\n",
      "Iteration 69, loss = 0.39889941\n",
      "Iteration 70, loss = 0.39848604\n",
      "Iteration 71, loss = 0.39809154\n",
      "Iteration 72, loss = 0.39777759\n",
      "Iteration 73, loss = 0.39755484\n",
      "Iteration 74, loss = 0.39714487\n",
      "Iteration 75, loss = 0.39674919\n",
      "Iteration 76, loss = 0.39636755\n",
      "Iteration 77, loss = 0.39605405\n",
      "Iteration 78, loss = 0.39584564\n",
      "Iteration 79, loss = 0.39539530\n",
      "Iteration 80, loss = 0.39502359\n",
      "Iteration 81, loss = 0.39479315\n",
      "Iteration 82, loss = 0.39448827\n",
      "Iteration 83, loss = 0.39406998\n",
      "Iteration 84, loss = 0.39378829\n",
      "Iteration 85, loss = 0.39345008\n",
      "Iteration 86, loss = 0.39322280\n",
      "Iteration 87, loss = 0.39296437\n",
      "Iteration 88, loss = 0.39250519\n",
      "Iteration 89, loss = 0.39236600\n",
      "Iteration 90, loss = 0.39213844\n",
      "Iteration 91, loss = 0.39177633\n",
      "Iteration 92, loss = 0.39135522\n",
      "Iteration 93, loss = 0.39108557\n",
      "Iteration 94, loss = 0.39096992\n",
      "Iteration 95, loss = 0.39071261\n",
      "Iteration 96, loss = 0.39039629\n",
      "Iteration 97, loss = 0.39007249\n",
      "Iteration 98, loss = 0.38996131\n",
      "Iteration 99, loss = 0.38958931\n",
      "Iteration 100, loss = 0.38945670\n",
      "Iteration 101, loss = 0.38905710\n",
      "Iteration 102, loss = 0.38887441\n",
      "Iteration 103, loss = 0.38858712\n",
      "Iteration 104, loss = 0.38828536\n",
      "Iteration 105, loss = 0.38816768\n",
      "Iteration 106, loss = 0.38787326\n",
      "Iteration 107, loss = 0.38779367\n",
      "Iteration 108, loss = 0.38761218\n",
      "Iteration 109, loss = 0.38714693\n",
      "Iteration 110, loss = 0.38694911\n",
      "Iteration 111, loss = 0.38649428\n",
      "Iteration 112, loss = 0.38653866\n",
      "Iteration 113, loss = 0.38613345\n",
      "Iteration 114, loss = 0.38610734\n",
      "Iteration 115, loss = 0.38567679\n",
      "Iteration 116, loss = 0.38554174\n",
      "Iteration 117, loss = 0.38525164\n",
      "Iteration 118, loss = 0.38508510\n",
      "Iteration 119, loss = 0.38479511\n",
      "Iteration 120, loss = 0.38452374\n",
      "Iteration 121, loss = 0.38419246\n",
      "Iteration 122, loss = 0.38398377\n",
      "Iteration 123, loss = 0.38388684\n",
      "Iteration 124, loss = 0.38377348\n",
      "Iteration 125, loss = 0.38367324\n",
      "Iteration 126, loss = 0.38318202\n",
      "Iteration 127, loss = 0.38294330\n",
      "Iteration 128, loss = 0.38273518\n",
      "Iteration 129, loss = 0.38246623\n",
      "Iteration 130, loss = 0.38221048\n",
      "Iteration 131, loss = 0.38207502\n",
      "Iteration 132, loss = 0.38183029\n",
      "Iteration 133, loss = 0.38177737\n",
      "Iteration 134, loss = 0.38167053\n",
      "Iteration 135, loss = 0.38138090\n",
      "Iteration 136, loss = 0.38109984\n",
      "Iteration 137, loss = 0.38079886\n",
      "Iteration 138, loss = 0.38075655\n",
      "Iteration 139, loss = 0.38037861\n",
      "Iteration 140, loss = 0.38029816\n",
      "Iteration 141, loss = 0.38003409\n",
      "Iteration 142, loss = 0.37968557\n",
      "Iteration 143, loss = 0.37947162\n",
      "Iteration 144, loss = 0.37922546\n",
      "Iteration 145, loss = 0.37905080\n",
      "Iteration 146, loss = 0.37882994\n",
      "Iteration 147, loss = 0.37870038\n",
      "Iteration 148, loss = 0.37845542\n",
      "Iteration 149, loss = 0.37823964\n",
      "Iteration 150, loss = 0.37796964\n",
      "Iteration 151, loss = 0.37776087\n",
      "Iteration 152, loss = 0.37771132\n",
      "Iteration 153, loss = 0.37747805\n",
      "Iteration 154, loss = 0.37743606\n",
      "Iteration 155, loss = 0.37726543\n",
      "Iteration 156, loss = 0.37697120\n",
      "Iteration 157, loss = 0.37657740\n",
      "Iteration 158, loss = 0.37657425\n",
      "Iteration 159, loss = 0.37614468\n",
      "Iteration 160, loss = 0.37678413\n",
      "Iteration 161, loss = 0.37601167\n",
      "Iteration 162, loss = 0.37586884\n",
      "Iteration 163, loss = 0.37546479\n",
      "Iteration 164, loss = 0.37522882\n",
      "Iteration 165, loss = 0.37513337\n",
      "Iteration 166, loss = 0.37499617\n",
      "Iteration 167, loss = 0.37478452\n",
      "Iteration 168, loss = 0.37451873\n",
      "Iteration 169, loss = 0.37439874\n",
      "Iteration 170, loss = 0.37408770\n",
      "Iteration 171, loss = 0.37399391\n",
      "Iteration 172, loss = 0.37391475\n",
      "Iteration 173, loss = 0.37363523\n",
      "Iteration 174, loss = 0.37355138\n",
      "Iteration 175, loss = 0.37339110\n",
      "Iteration 176, loss = 0.37302161\n",
      "Iteration 177, loss = 0.37290099\n",
      "Iteration 178, loss = 0.37275324\n",
      "Iteration 179, loss = 0.37251601\n",
      "Iteration 180, loss = 0.37267677\n",
      "Iteration 181, loss = 0.37224678\n",
      "Iteration 182, loss = 0.37194355\n",
      "Iteration 183, loss = 0.37189896\n",
      "Iteration 184, loss = 0.37164122\n",
      "Iteration 185, loss = 0.37153590\n",
      "Iteration 186, loss = 0.37152214\n",
      "Iteration 187, loss = 0.37125296\n",
      "Iteration 188, loss = 0.37099544\n",
      "Iteration 189, loss = 0.37091972\n",
      "Iteration 190, loss = 0.37079103\n",
      "Iteration 191, loss = 0.37060230\n",
      "Iteration 192, loss = 0.37035230\n",
      "Iteration 193, loss = 0.37018914\n",
      "Iteration 194, loss = 0.36998434\n",
      "Iteration 195, loss = 0.36988048\n",
      "Iteration 196, loss = 0.36956118\n",
      "Iteration 197, loss = 0.36954503\n",
      "Iteration 198, loss = 0.36925977\n",
      "Iteration 199, loss = 0.36912522\n",
      "Iteration 200, loss = 0.36897474\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'relu', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.809 (+/-0.054) for {'activation': 'identity', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'identity', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'identity', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'identity', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'identity', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'identity', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.811 (+/-0.060) for {'activation': 'logistic', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.814 (+/-0.057) for {'activation': 'logistic', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.814 (+/-0.057) for {'activation': 'logistic', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.814 (+/-0.057) for {'activation': 'logistic', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.814 (+/-0.057) for {'activation': 'logistic', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.814 (+/-0.057) for {'activation': 'logistic', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.816 (+/-0.046) for {'activation': 'tanh', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.818 (+/-0.044) for {'activation': 'tanh', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.818 (+/-0.044) for {'activation': 'tanh', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.818 (+/-0.044) for {'activation': 'tanh', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.818 (+/-0.044) for {'activation': 'tanh', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.818 (+/-0.044) for {'activation': 'tanh', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.823 (+/-0.069) for {'activation': 'relu', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.821 (+/-0.063) for {'activation': 'relu', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.821 (+/-0.063) for {'activation': 'relu', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.821 (+/-0.063) for {'activation': 'relu', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.821 (+/-0.063) for {'activation': 'relu', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.821 (+/-0.063) for {'activation': 'relu', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       110\n",
      "           1       0.82      0.71      0.76        69\n",
      "\n",
      "    accuracy                           0.83       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.83      0.83      0.82       179\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Iteration 1, loss = 0.77481206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.71851731\n",
      "Iteration 3, loss = 0.66705555\n",
      "Iteration 4, loss = 0.62726439\n",
      "Iteration 5, loss = 0.59526732\n",
      "Iteration 6, loss = 0.56771339\n",
      "Iteration 7, loss = 0.54434934\n",
      "Iteration 8, loss = 0.52595798\n",
      "Iteration 9, loss = 0.50867696\n",
      "Iteration 10, loss = 0.49471355\n",
      "Iteration 11, loss = 0.48181174\n",
      "Iteration 12, loss = 0.47126835\n",
      "Iteration 13, loss = 0.46111227\n",
      "Iteration 14, loss = 0.45320070\n",
      "Iteration 15, loss = 0.44627017\n",
      "Iteration 16, loss = 0.44053605\n",
      "Iteration 17, loss = 0.43560650\n",
      "Iteration 18, loss = 0.43229979\n",
      "Iteration 19, loss = 0.42867448\n",
      "Iteration 20, loss = 0.42607099\n",
      "Iteration 21, loss = 0.42403569\n",
      "Iteration 22, loss = 0.42198758\n",
      "Iteration 23, loss = 0.42090920\n",
      "Iteration 24, loss = 0.41969361\n",
      "Iteration 25, loss = 0.41905116\n",
      "Iteration 26, loss = 0.41846761\n",
      "Iteration 27, loss = 0.41787796\n",
      "Iteration 28, loss = 0.41749237\n",
      "Iteration 29, loss = 0.41720145\n",
      "Iteration 30, loss = 0.41696517\n",
      "Iteration 31, loss = 0.41671221\n",
      "Iteration 32, loss = 0.41676560\n",
      "Iteration 33, loss = 0.41657523\n",
      "Iteration 34, loss = 0.41639425\n",
      "Iteration 35, loss = 0.41636783\n",
      "Iteration 36, loss = 0.41632436\n",
      "Iteration 37, loss = 0.41623725\n",
      "Iteration 38, loss = 0.41618080\n",
      "Iteration 39, loss = 0.41611020\n",
      "Iteration 40, loss = 0.41615023\n",
      "Iteration 41, loss = 0.41614031\n",
      "Iteration 42, loss = 0.41615545\n",
      "Iteration 43, loss = 0.41620455\n",
      "Iteration 44, loss = 0.41616565\n",
      "Iteration 45, loss = 0.41607521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78431527\n",
      "Iteration 2, loss = 0.72956230\n",
      "Iteration 3, loss = 0.68030858\n",
      "Iteration 4, loss = 0.64281945\n",
      "Iteration 5, loss = 0.61244094\n",
      "Iteration 6, loss = 0.58599775\n",
      "Iteration 7, loss = 0.56310510\n",
      "Iteration 8, loss = 0.54577778\n",
      "Iteration 9, loss = 0.52907132\n",
      "Iteration 10, loss = 0.51565796\n",
      "Iteration 11, loss = 0.50315273\n",
      "Iteration 12, loss = 0.49290568\n",
      "Iteration 13, loss = 0.48348398\n",
      "Iteration 14, loss = 0.47605400\n",
      "Iteration 15, loss = 0.46934860\n",
      "Iteration 16, loss = 0.46407566\n",
      "Iteration 17, loss = 0.45953364\n",
      "Iteration 18, loss = 0.45624455\n",
      "Iteration 19, loss = 0.45287313\n",
      "Iteration 20, loss = 0.45035610\n",
      "Iteration 21, loss = 0.44829266\n",
      "Iteration 22, loss = 0.44652651\n",
      "Iteration 23, loss = 0.44558918\n",
      "Iteration 24, loss = 0.44442637\n",
      "Iteration 25, loss = 0.44382005\n",
      "Iteration 26, loss = 0.44307027\n",
      "Iteration 27, loss = 0.44273500\n",
      "Iteration 28, loss = 0.44233828\n",
      "Iteration 29, loss = 0.44187592\n",
      "Iteration 30, loss = 0.44182862\n",
      "Iteration 31, loss = 0.44144710\n",
      "Iteration 32, loss = 0.44149802\n",
      "Iteration 33, loss = 0.44128938\n",
      "Iteration 34, loss = 0.44107125\n",
      "Iteration 35, loss = 0.44097974\n",
      "Iteration 36, loss = 0.44094991\n",
      "Iteration 37, loss = 0.44083761\n",
      "Iteration 38, loss = 0.44080540\n",
      "Iteration 39, loss = 0.44072113\n",
      "Iteration 40, loss = 0.44080796\n",
      "Iteration 41, loss = 0.44067456\n",
      "Iteration 42, loss = 0.44071265\n",
      "Iteration 43, loss = 0.44069888\n",
      "Iteration 44, loss = 0.44070283\n",
      "Iteration 45, loss = 0.44058475\n",
      "Iteration 46, loss = 0.44060896\n",
      "Iteration 47, loss = 0.44056624\n",
      "Iteration 48, loss = 0.44059169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78408174\n",
      "Iteration 2, loss = 0.72922726\n",
      "Iteration 3, loss = 0.68446995\n",
      "Iteration 4, loss = 0.64709301\n",
      "Iteration 5, loss = 0.61601836\n",
      "Iteration 6, loss = 0.58869884\n",
      "Iteration 7, loss = 0.56661918\n",
      "Iteration 8, loss = 0.54627284\n",
      "Iteration 9, loss = 0.52804351\n",
      "Iteration 10, loss = 0.51229880\n",
      "Iteration 11, loss = 0.49880940\n",
      "Iteration 12, loss = 0.48687837\n",
      "Iteration 13, loss = 0.47679513\n",
      "Iteration 14, loss = 0.46803890\n",
      "Iteration 15, loss = 0.46090476\n",
      "Iteration 16, loss = 0.45510954\n",
      "Iteration 17, loss = 0.45005228\n",
      "Iteration 18, loss = 0.44645934\n",
      "Iteration 19, loss = 0.44291251\n",
      "Iteration 20, loss = 0.43951559\n",
      "Iteration 21, loss = 0.43750709\n",
      "Iteration 22, loss = 0.43557206\n",
      "Iteration 23, loss = 0.43388306\n",
      "Iteration 24, loss = 0.43282565\n",
      "Iteration 25, loss = 0.43189222\n",
      "Iteration 26, loss = 0.43105990\n",
      "Iteration 27, loss = 0.43025289\n",
      "Iteration 28, loss = 0.42992816\n",
      "Iteration 29, loss = 0.42928541\n",
      "Iteration 30, loss = 0.42900946\n",
      "Iteration 31, loss = 0.42880965\n",
      "Iteration 32, loss = 0.42846156\n",
      "Iteration 33, loss = 0.42858821\n",
      "Iteration 34, loss = 0.42807050\n",
      "Iteration 35, loss = 0.42810583\n",
      "Iteration 36, loss = 0.42780084\n",
      "Iteration 37, loss = 0.42782842\n",
      "Iteration 38, loss = 0.42769551\n",
      "Iteration 39, loss = 0.42747628\n",
      "Iteration 40, loss = 0.42756225\n",
      "Iteration 41, loss = 0.42739130\n",
      "Iteration 42, loss = 0.42745138\n",
      "Iteration 43, loss = 0.42719704\n",
      "Iteration 44, loss = 0.42742205\n",
      "Iteration 45, loss = 0.42719478\n",
      "Iteration 46, loss = 0.42708985\n",
      "Iteration 47, loss = 0.42701707\n",
      "Iteration 48, loss = 0.42714497\n",
      "Iteration 49, loss = 0.42700607\n",
      "Iteration 50, loss = 0.42700913\n",
      "Iteration 51, loss = 0.42702209\n",
      "Iteration 52, loss = 0.42708219\n",
      "Iteration 53, loss = 0.42693051\n",
      "Iteration 54, loss = 0.42684972\n",
      "Iteration 55, loss = 0.42685173\n",
      "Iteration 56, loss = 0.42695848\n",
      "Iteration 57, loss = 0.42700757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78717095\n",
      "Iteration 2, loss = 0.73263522\n",
      "Iteration 3, loss = 0.68786644\n",
      "Iteration 4, loss = 0.64886274\n",
      "Iteration 5, loss = 0.61589337\n",
      "Iteration 6, loss = 0.58830387\n",
      "Iteration 7, loss = 0.56537924\n",
      "Iteration 8, loss = 0.54491909\n",
      "Iteration 9, loss = 0.52663969\n",
      "Iteration 10, loss = 0.51072168\n",
      "Iteration 11, loss = 0.49744736\n",
      "Iteration 12, loss = 0.48544885\n",
      "Iteration 13, loss = 0.47517042\n",
      "Iteration 14, loss = 0.46617899\n",
      "Iteration 15, loss = 0.45902898\n",
      "Iteration 16, loss = 0.45267552\n",
      "Iteration 17, loss = 0.44758297\n",
      "Iteration 18, loss = 0.44343302\n",
      "Iteration 19, loss = 0.43935565\n",
      "Iteration 20, loss = 0.43630677\n",
      "Iteration 21, loss = 0.43367116\n",
      "Iteration 22, loss = 0.43152666\n",
      "Iteration 23, loss = 0.42959933\n",
      "Iteration 24, loss = 0.42819875\n",
      "Iteration 25, loss = 0.42723447\n",
      "Iteration 26, loss = 0.42618052\n",
      "Iteration 27, loss = 0.42541404\n",
      "Iteration 28, loss = 0.42494949\n",
      "Iteration 29, loss = 0.42437952\n",
      "Iteration 30, loss = 0.42389073\n",
      "Iteration 31, loss = 0.42374272\n",
      "Iteration 32, loss = 0.42351531\n",
      "Iteration 33, loss = 0.42356705\n",
      "Iteration 34, loss = 0.42307355\n",
      "Iteration 35, loss = 0.42286774\n",
      "Iteration 36, loss = 0.42292877\n",
      "Iteration 37, loss = 0.42279451\n",
      "Iteration 38, loss = 0.42261168\n",
      "Iteration 39, loss = 0.42258069\n",
      "Iteration 40, loss = 0.42268746\n",
      "Iteration 41, loss = 0.42251296\n",
      "Iteration 42, loss = 0.42253822\n",
      "Iteration 43, loss = 0.42242759\n",
      "Iteration 44, loss = 0.42249856\n",
      "Iteration 45, loss = 0.42235105\n",
      "Iteration 46, loss = 0.42233536\n",
      "Iteration 47, loss = 0.42226628\n",
      "Iteration 48, loss = 0.42232567\n",
      "Iteration 49, loss = 0.42231620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77853080\n",
      "Iteration 2, loss = 0.72544398\n",
      "Iteration 3, loss = 0.68030056\n",
      "Iteration 4, loss = 0.64357070\n",
      "Iteration 5, loss = 0.61289769\n",
      "Iteration 6, loss = 0.58735787\n",
      "Iteration 7, loss = 0.56682357\n",
      "Iteration 8, loss = 0.54795223\n",
      "Iteration 9, loss = 0.53208151\n",
      "Iteration 10, loss = 0.51762101\n",
      "Iteration 11, loss = 0.50572400\n",
      "Iteration 12, loss = 0.49572608\n",
      "Iteration 13, loss = 0.48698626\n",
      "Iteration 14, loss = 0.47916755\n",
      "Iteration 15, loss = 0.47364365\n",
      "Iteration 16, loss = 0.46848114\n",
      "Iteration 17, loss = 0.46509217\n",
      "Iteration 18, loss = 0.46155701\n",
      "Iteration 19, loss = 0.45861290\n",
      "Iteration 20, loss = 0.45679619\n",
      "Iteration 21, loss = 0.45528738\n",
      "Iteration 22, loss = 0.45361713\n",
      "Iteration 23, loss = 0.45264005\n",
      "Iteration 24, loss = 0.45183284\n",
      "Iteration 25, loss = 0.45122561\n",
      "Iteration 26, loss = 0.45064282\n",
      "Iteration 27, loss = 0.45028877\n",
      "Iteration 28, loss = 0.45020626\n",
      "Iteration 29, loss = 0.44983649\n",
      "Iteration 30, loss = 0.44958779\n",
      "Iteration 31, loss = 0.44951734\n",
      "Iteration 32, loss = 0.44941108\n",
      "Iteration 33, loss = 0.44943269\n",
      "Iteration 34, loss = 0.44923332\n",
      "Iteration 35, loss = 0.44903853\n",
      "Iteration 36, loss = 0.44912318\n",
      "Iteration 37, loss = 0.44905992\n",
      "Iteration 38, loss = 0.44892896\n",
      "Iteration 39, loss = 0.44886965\n",
      "Iteration 40, loss = 0.44896902\n",
      "Iteration 41, loss = 0.44885448\n",
      "Iteration 42, loss = 0.44885954\n",
      "Iteration 43, loss = 0.44877566\n",
      "Iteration 44, loss = 0.44877975\n",
      "Iteration 45, loss = 0.44874708\n",
      "Iteration 46, loss = 0.44873216\n",
      "Iteration 47, loss = 0.44864621\n",
      "Iteration 48, loss = 0.44868206\n",
      "Iteration 49, loss = 0.44872906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76912207\n",
      "Iteration 2, loss = 0.71291886\n",
      "Iteration 3, loss = 0.66154190\n",
      "Iteration 4, loss = 0.62181042\n",
      "Iteration 5, loss = 0.58985593\n",
      "Iteration 6, loss = 0.56232364\n",
      "Iteration 7, loss = 0.53896777\n",
      "Iteration 8, loss = 0.52057120\n",
      "Iteration 9, loss = 0.50327908\n",
      "Iteration 10, loss = 0.48929961\n",
      "Iteration 11, loss = 0.47637407\n",
      "Iteration 12, loss = 0.46581600\n",
      "Iteration 13, loss = 0.45564525\n",
      "Iteration 14, loss = 0.44772039\n",
      "Iteration 15, loss = 0.44077896\n",
      "Iteration 16, loss = 0.43503680\n",
      "Iteration 17, loss = 0.43009926\n",
      "Iteration 18, loss = 0.42678690\n",
      "Iteration 19, loss = 0.42315510\n",
      "Iteration 20, loss = 0.42054529\n",
      "Iteration 21, loss = 0.41850510\n",
      "Iteration 22, loss = 0.41645358\n",
      "Iteration 23, loss = 0.41537147\n",
      "Iteration 24, loss = 0.41415423\n",
      "Iteration 25, loss = 0.41351051\n",
      "Iteration 26, loss = 0.41292879\n",
      "Iteration 27, loss = 0.41233879\n",
      "Iteration 28, loss = 0.41195561\n",
      "Iteration 29, loss = 0.41166797\n",
      "Iteration 30, loss = 0.41143493\n",
      "Iteration 31, loss = 0.41118506\n",
      "Iteration 32, loss = 0.41124426\n",
      "Iteration 33, loss = 0.41105675\n",
      "Iteration 34, loss = 0.41088124\n",
      "Iteration 35, loss = 0.41086020\n",
      "Iteration 36, loss = 0.41082244\n",
      "Iteration 37, loss = 0.41074046\n",
      "Iteration 38, loss = 0.41068995\n",
      "Iteration 39, loss = 0.41062465\n",
      "Iteration 40, loss = 0.41067112\n",
      "Iteration 41, loss = 0.41066753\n",
      "Iteration 42, loss = 0.41068844\n",
      "Iteration 43, loss = 0.41074454\n",
      "Iteration 44, loss = 0.41071172\n",
      "Iteration 45, loss = 0.41062731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77895945\n",
      "Iteration 2, loss = 0.72419717\n",
      "Iteration 3, loss = 0.67492814\n",
      "Iteration 4, loss = 0.63742997\n",
      "Iteration 5, loss = 0.60704161\n",
      "Iteration 6, loss = 0.58059673\n",
      "Iteration 7, loss = 0.55770053\n",
      "Iteration 8, loss = 0.54037302\n",
      "Iteration 9, loss = 0.52366313\n",
      "Iteration 10, loss = 0.51024638\n",
      "Iteration 11, loss = 0.49773632\n",
      "Iteration 12, loss = 0.48748271\n",
      "Iteration 13, loss = 0.47805327\n",
      "Iteration 14, loss = 0.47061706\n",
      "Iteration 15, loss = 0.46390349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.45862325\n",
      "Iteration 17, loss = 0.45407358\n",
      "Iteration 18, loss = 0.45077836\n",
      "Iteration 19, loss = 0.44740142\n",
      "Iteration 20, loss = 0.44487804\n",
      "Iteration 21, loss = 0.44281006\n",
      "Iteration 22, loss = 0.44103996\n",
      "Iteration 23, loss = 0.44010134\n",
      "Iteration 24, loss = 0.43893616\n",
      "Iteration 25, loss = 0.43832996\n",
      "Iteration 26, loss = 0.43758014\n",
      "Iteration 27, loss = 0.43724675\n",
      "Iteration 28, loss = 0.43685227\n",
      "Iteration 29, loss = 0.43639130\n",
      "Iteration 30, loss = 0.43634936\n",
      "Iteration 31, loss = 0.43596923\n",
      "Iteration 32, loss = 0.43602614\n",
      "Iteration 33, loss = 0.43582167\n",
      "Iteration 34, loss = 0.43560795\n",
      "Iteration 35, loss = 0.43552155\n",
      "Iteration 36, loss = 0.43549766\n",
      "Iteration 37, loss = 0.43539098\n",
      "Iteration 38, loss = 0.43536448\n",
      "Iteration 39, loss = 0.43528596\n",
      "Iteration 40, loss = 0.43538001\n",
      "Iteration 41, loss = 0.43525176\n",
      "Iteration 42, loss = 0.43529673\n",
      "Iteration 43, loss = 0.43529016\n",
      "Iteration 44, loss = 0.43530075\n",
      "Iteration 45, loss = 0.43518846\n",
      "Iteration 46, loss = 0.43522002\n",
      "Iteration 47, loss = 0.43518348\n",
      "Iteration 48, loss = 0.43521595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77871646\n",
      "Iteration 2, loss = 0.72386373\n",
      "Iteration 3, loss = 0.67910199\n",
      "Iteration 4, loss = 0.64172258\n",
      "Iteration 5, loss = 0.61064493\n",
      "Iteration 6, loss = 0.58332174\n",
      "Iteration 7, loss = 0.56124111\n",
      "Iteration 8, loss = 0.54089009\n",
      "Iteration 9, loss = 0.52265478\n",
      "Iteration 10, loss = 0.50690347\n",
      "Iteration 11, loss = 0.49340407\n",
      "Iteration 12, loss = 0.48146126\n",
      "Iteration 13, loss = 0.47136660\n",
      "Iteration 14, loss = 0.46259850\n",
      "Iteration 15, loss = 0.45545247\n",
      "Iteration 16, loss = 0.44964625\n",
      "Iteration 17, loss = 0.44457927\n",
      "Iteration 18, loss = 0.44097660\n",
      "Iteration 19, loss = 0.43742002\n",
      "Iteration 20, loss = 0.43401320\n",
      "Iteration 21, loss = 0.43199786\n",
      "Iteration 22, loss = 0.43005689\n",
      "Iteration 23, loss = 0.42836188\n",
      "Iteration 24, loss = 0.42730069\n",
      "Iteration 25, loss = 0.42636367\n",
      "Iteration 26, loss = 0.42552977\n",
      "Iteration 27, loss = 0.42472049\n",
      "Iteration 28, loss = 0.42439639\n",
      "Iteration 29, loss = 0.42375230\n",
      "Iteration 30, loss = 0.42347837\n",
      "Iteration 31, loss = 0.42328012\n",
      "Iteration 32, loss = 0.42293380\n",
      "Iteration 33, loss = 0.42306500\n",
      "Iteration 34, loss = 0.42254779\n",
      "Iteration 35, loss = 0.42258857\n",
      "Iteration 36, loss = 0.42228638\n",
      "Iteration 37, loss = 0.42231826\n",
      "Iteration 38, loss = 0.42218981\n",
      "Iteration 39, loss = 0.42197413\n",
      "Iteration 40, loss = 0.42206569\n",
      "Iteration 41, loss = 0.42189906\n",
      "Iteration 42, loss = 0.42196548\n",
      "Iteration 43, loss = 0.42171509\n",
      "Iteration 44, loss = 0.42194771\n",
      "Iteration 45, loss = 0.42172500\n",
      "Iteration 46, loss = 0.42162505\n",
      "Iteration 47, loss = 0.42155762\n",
      "Iteration 48, loss = 0.42169330\n",
      "Iteration 49, loss = 0.42155879\n",
      "Iteration 50, loss = 0.42156820\n",
      "Iteration 51, loss = 0.42158853\n",
      "Iteration 52, loss = 0.42165502\n",
      "Iteration 53, loss = 0.42150825\n",
      "Iteration 54, loss = 0.42143350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78177752\n",
      "Iteration 2, loss = 0.72724221\n",
      "Iteration 3, loss = 0.68247318\n",
      "Iteration 4, loss = 0.64346930\n",
      "Iteration 5, loss = 0.61049707\n",
      "Iteration 6, loss = 0.58290550\n",
      "Iteration 7, loss = 0.55997948\n",
      "Iteration 8, loss = 0.53951593\n",
      "Iteration 9, loss = 0.52123286\n",
      "Iteration 10, loss = 0.50531175\n",
      "Iteration 11, loss = 0.49203208\n",
      "Iteration 12, loss = 0.48002647\n",
      "Iteration 13, loss = 0.46974083\n",
      "Iteration 14, loss = 0.46074113\n",
      "Iteration 15, loss = 0.45358246\n",
      "Iteration 16, loss = 0.44722160\n",
      "Iteration 17, loss = 0.44212288\n",
      "Iteration 18, loss = 0.43796583\n",
      "Iteration 19, loss = 0.43388033\n",
      "Iteration 20, loss = 0.43082596\n",
      "Iteration 21, loss = 0.42818384\n",
      "Iteration 22, loss = 0.42603532\n",
      "Iteration 23, loss = 0.42410304\n",
      "Iteration 24, loss = 0.42269933\n",
      "Iteration 25, loss = 0.42173334\n",
      "Iteration 26, loss = 0.42067814\n",
      "Iteration 27, loss = 0.41991117\n",
      "Iteration 28, loss = 0.41944790\n",
      "Iteration 29, loss = 0.41887821\n",
      "Iteration 30, loss = 0.41839123\n",
      "Iteration 31, loss = 0.41824733\n",
      "Iteration 32, loss = 0.41802285\n",
      "Iteration 33, loss = 0.41807980\n",
      "Iteration 34, loss = 0.41758802\n",
      "Iteration 35, loss = 0.41738659\n",
      "Iteration 36, loss = 0.41745414\n",
      "Iteration 37, loss = 0.41732482\n",
      "Iteration 38, loss = 0.41714680\n",
      "Iteration 39, loss = 0.41712160\n",
      "Iteration 40, loss = 0.41723512\n",
      "Iteration 41, loss = 0.41706589\n",
      "Iteration 42, loss = 0.41709764\n",
      "Iteration 43, loss = 0.41699277\n",
      "Iteration 44, loss = 0.41707154\n",
      "Iteration 45, loss = 0.41692978\n",
      "Iteration 46, loss = 0.41692003\n",
      "Iteration 47, loss = 0.41685722\n",
      "Iteration 48, loss = 0.41692553\n",
      "Iteration 49, loss = 0.41692234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77316474\n",
      "Iteration 2, loss = 0.72008036\n",
      "Iteration 3, loss = 0.67493590\n",
      "Iteration 4, loss = 0.63820327\n",
      "Iteration 5, loss = 0.60752610\n",
      "Iteration 6, loss = 0.58198303\n",
      "Iteration 7, loss = 0.56144638\n",
      "Iteration 8, loss = 0.54257132\n",
      "Iteration 9, loss = 0.52669676\n",
      "Iteration 10, loss = 0.51223255\n",
      "Iteration 11, loss = 0.50032897\n",
      "Iteration 12, loss = 0.49032244\n",
      "Iteration 13, loss = 0.48157366\n",
      "Iteration 14, loss = 0.47374463\n",
      "Iteration 15, loss = 0.46821008\n",
      "Iteration 16, loss = 0.46303878\n",
      "Iteration 17, loss = 0.45964359\n",
      "Iteration 18, loss = 0.45610033\n",
      "Iteration 19, loss = 0.45314820\n",
      "Iteration 20, loss = 0.45132719\n",
      "Iteration 21, loss = 0.44981408\n",
      "Iteration 22, loss = 0.44813925\n",
      "Iteration 23, loss = 0.44715925\n",
      "Iteration 24, loss = 0.44635059\n",
      "Iteration 25, loss = 0.44574349\n",
      "Iteration 26, loss = 0.44516091\n",
      "Iteration 27, loss = 0.44480852\n",
      "Iteration 28, loss = 0.44472938\n",
      "Iteration 29, loss = 0.44436141\n",
      "Iteration 30, loss = 0.44411593\n",
      "Iteration 31, loss = 0.44405054\n",
      "Iteration 32, loss = 0.44394818\n",
      "Iteration 33, loss = 0.44397496\n",
      "Iteration 34, loss = 0.44377936\n",
      "Iteration 35, loss = 0.44358909\n",
      "Iteration 36, loss = 0.44367987\n",
      "Iteration 37, loss = 0.44362255\n",
      "Iteration 38, loss = 0.44349649\n",
      "Iteration 39, loss = 0.44344296\n",
      "Iteration 40, loss = 0.44354906\n",
      "Iteration 41, loss = 0.44344001\n",
      "Iteration 42, loss = 0.44345145\n",
      "Iteration 43, loss = 0.44337330\n",
      "Iteration 44, loss = 0.44338396\n",
      "Iteration 45, loss = 0.44335743\n",
      "Iteration 46, loss = 0.44334843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76858129\n",
      "Iteration 2, loss = 0.71237891\n",
      "Iteration 3, loss = 0.66100215\n",
      "Iteration 4, loss = 0.62127072\n",
      "Iteration 5, loss = 0.58931603\n",
      "Iteration 6, loss = 0.56178348\n",
      "Iteration 7, loss = 0.53842701\n",
      "Iteration 8, loss = 0.52002982\n",
      "Iteration 9, loss = 0.50273686\n",
      "Iteration 10, loss = 0.48875633\n",
      "Iteration 11, loss = 0.47582956\n",
      "Iteration 12, loss = 0.46527008\n",
      "Iteration 13, loss = 0.45509777\n",
      "Iteration 14, loss = 0.44717130\n",
      "Iteration 15, loss = 0.44022817\n",
      "Iteration 16, loss = 0.43448442\n",
      "Iteration 17, loss = 0.42954521\n",
      "Iteration 18, loss = 0.42623146\n",
      "Iteration 19, loss = 0.42259829\n",
      "Iteration 20, loss = 0.41998709\n",
      "Iteration 21, loss = 0.41794576\n",
      "Iteration 22, loss = 0.41589309\n",
      "Iteration 23, loss = 0.41481014\n",
      "Iteration 24, loss = 0.41359197\n",
      "Iteration 25, loss = 0.41294757\n",
      "Iteration 26, loss = 0.41236527\n",
      "Iteration 27, loss = 0.41177465\n",
      "Iteration 28, loss = 0.41139104\n",
      "Iteration 29, loss = 0.41110305\n",
      "Iteration 30, loss = 0.41086971\n",
      "Iteration 31, loss = 0.41061956\n",
      "Iteration 32, loss = 0.41067870\n",
      "Iteration 33, loss = 0.41049090\n",
      "Iteration 34, loss = 0.41031534\n",
      "Iteration 35, loss = 0.41029423\n",
      "Iteration 36, loss = 0.41025645\n",
      "Iteration 37, loss = 0.41017441\n",
      "Iteration 38, loss = 0.41012390\n",
      "Iteration 39, loss = 0.41005856\n",
      "Iteration 40, loss = 0.41010507\n",
      "Iteration 41, loss = 0.41010153\n",
      "Iteration 42, loss = 0.41012243\n",
      "Iteration 43, loss = 0.41017863\n",
      "Iteration 44, loss = 0.41014584\n",
      "Iteration 45, loss = 0.41006146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77842207\n",
      "Iteration 2, loss = 0.72365989\n",
      "Iteration 3, loss = 0.67439026\n",
      "Iteration 4, loss = 0.63689134\n",
      "Iteration 5, loss = 0.60650210\n",
      "Iteration 6, loss = 0.58005654\n",
      "Iteration 7, loss = 0.55715953\n",
      "Iteration 8, loss = 0.53983144\n",
      "Iteration 9, loss = 0.52312085\n",
      "Iteration 10, loss = 0.50970330\n",
      "Iteration 11, loss = 0.49719242\n",
      "Iteration 12, loss = 0.48693775\n",
      "Iteration 13, loss = 0.47750703\n",
      "Iteration 14, loss = 0.47006969\n",
      "Iteration 15, loss = 0.46335475\n",
      "Iteration 16, loss = 0.45807318\n",
      "Iteration 17, loss = 0.45352215\n",
      "Iteration 18, loss = 0.45022572\n",
      "Iteration 19, loss = 0.44684763\n",
      "Iteration 20, loss = 0.44432299\n",
      "Iteration 21, loss = 0.44225395\n",
      "Iteration 22, loss = 0.44048284\n",
      "Iteration 23, loss = 0.43954351\n",
      "Iteration 24, loss = 0.43837747\n",
      "Iteration 25, loss = 0.43777067\n",
      "Iteration 26, loss = 0.43702022\n",
      "Iteration 27, loss = 0.43668641\n",
      "Iteration 28, loss = 0.43629153\n",
      "Iteration 29, loss = 0.43583011\n",
      "Iteration 30, loss = 0.43578808\n",
      "Iteration 31, loss = 0.43540750\n",
      "Iteration 32, loss = 0.43546439\n",
      "Iteration 33, loss = 0.43525973\n",
      "Iteration 34, loss = 0.43504586\n",
      "Iteration 35, loss = 0.43495936\n",
      "Iteration 36, loss = 0.43493547\n",
      "Iteration 37, loss = 0.43482875\n",
      "Iteration 38, loss = 0.43480221\n",
      "Iteration 39, loss = 0.43472366\n",
      "Iteration 40, loss = 0.43481783\n",
      "Iteration 41, loss = 0.43468948\n",
      "Iteration 42, loss = 0.43473452\n",
      "Iteration 43, loss = 0.43472806\n",
      "Iteration 44, loss = 0.43473871\n",
      "Iteration 45, loss = 0.43462637\n",
      "Iteration 46, loss = 0.43465806\n",
      "Iteration 47, loss = 0.43462152\n",
      "Iteration 48, loss = 0.43465409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77817994\n",
      "Iteration 2, loss = 0.72332743\n",
      "Iteration 3, loss = 0.67856532\n",
      "Iteration 4, loss = 0.64118556\n",
      "Iteration 5, loss = 0.61010744\n",
      "Iteration 6, loss = 0.58278364\n",
      "Iteration 7, loss = 0.56070256\n",
      "Iteration 8, loss = 0.54035078\n",
      "Iteration 9, loss = 0.52211452\n",
      "Iteration 10, loss = 0.50636216\n",
      "Iteration 11, loss = 0.49286137\n",
      "Iteration 12, loss = 0.48091695\n",
      "Iteration 13, loss = 0.47082068\n",
      "Iteration 14, loss = 0.46205086\n",
      "Iteration 15, loss = 0.45490312\n",
      "Iteration 16, loss = 0.44909525\n",
      "Iteration 17, loss = 0.44402674\n",
      "Iteration 18, loss = 0.44042255\n",
      "Iteration 19, loss = 0.43686442\n",
      "Iteration 20, loss = 0.43345606\n",
      "Iteration 21, loss = 0.43143945\n",
      "Iteration 22, loss = 0.42949730\n",
      "Iteration 23, loss = 0.42780110\n",
      "Iteration 24, loss = 0.42673893\n",
      "Iteration 25, loss = 0.42580094\n",
      "Iteration 26, loss = 0.42496629\n",
      "Iteration 27, loss = 0.42415620\n",
      "Iteration 28, loss = 0.42383155\n",
      "Iteration 29, loss = 0.42318674\n",
      "Iteration 30, loss = 0.42291242\n",
      "Iteration 31, loss = 0.42271373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.42236700\n",
      "Iteration 33, loss = 0.42249805\n",
      "Iteration 34, loss = 0.42198032\n",
      "Iteration 35, loss = 0.42202106\n",
      "Iteration 36, loss = 0.42171855\n",
      "Iteration 37, loss = 0.42175027\n",
      "Iteration 38, loss = 0.42162168\n",
      "Iteration 39, loss = 0.42140576\n",
      "Iteration 40, loss = 0.42149729\n",
      "Iteration 41, loss = 0.42133050\n",
      "Iteration 42, loss = 0.42139696\n",
      "Iteration 43, loss = 0.42114638\n",
      "Iteration 44, loss = 0.42137917\n",
      "Iteration 45, loss = 0.42115632\n",
      "Iteration 46, loss = 0.42105628\n",
      "Iteration 47, loss = 0.42098879\n",
      "Iteration 48, loss = 0.42112466\n",
      "Iteration 49, loss = 0.42098999\n",
      "Iteration 50, loss = 0.42099944\n",
      "Iteration 51, loss = 0.42101994\n",
      "Iteration 52, loss = 0.42108647\n",
      "Iteration 53, loss = 0.42093959\n",
      "Iteration 54, loss = 0.42086487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78124098\n",
      "Iteration 2, loss = 0.72670591\n",
      "Iteration 3, loss = 0.68193662\n",
      "Iteration 4, loss = 0.64293232\n",
      "Iteration 5, loss = 0.60995938\n",
      "Iteration 6, loss = 0.58236707\n",
      "Iteration 7, loss = 0.55944034\n",
      "Iteration 8, loss = 0.53897596\n",
      "Iteration 9, loss = 0.52069209\n",
      "Iteration 10, loss = 0.50477015\n",
      "Iteration 11, loss = 0.49148944\n",
      "Iteration 12, loss = 0.47948262\n",
      "Iteration 13, loss = 0.46919571\n",
      "Iteration 14, loss = 0.46019456\n",
      "Iteration 15, loss = 0.45303440\n",
      "Iteration 16, loss = 0.44667214\n",
      "Iteration 17, loss = 0.44157212\n",
      "Iteration 18, loss = 0.43741369\n",
      "Iteration 19, loss = 0.43332668\n",
      "Iteration 20, loss = 0.43027111\n",
      "Iteration 21, loss = 0.42762766\n",
      "Iteration 22, loss = 0.42547802\n",
      "Iteration 23, loss = 0.42354455\n",
      "Iteration 24, loss = 0.42213984\n",
      "Iteration 25, loss = 0.42117295\n",
      "Iteration 26, loss = 0.42011694\n",
      "Iteration 27, loss = 0.41934921\n",
      "Iteration 28, loss = 0.41888540\n",
      "Iteration 29, loss = 0.41831504\n",
      "Iteration 30, loss = 0.41782755\n",
      "Iteration 31, loss = 0.41768341\n",
      "Iteration 32, loss = 0.41745853\n",
      "Iteration 33, loss = 0.41751533\n",
      "Iteration 34, loss = 0.41702309\n",
      "Iteration 35, loss = 0.41682146\n",
      "Iteration 36, loss = 0.41688903\n",
      "Iteration 37, loss = 0.41675956\n",
      "Iteration 38, loss = 0.41658138\n",
      "Iteration 39, loss = 0.41655614\n",
      "Iteration 40, loss = 0.41666971\n",
      "Iteration 41, loss = 0.41650039\n",
      "Iteration 42, loss = 0.41653217\n",
      "Iteration 43, loss = 0.41642725\n",
      "Iteration 44, loss = 0.41650618\n",
      "Iteration 45, loss = 0.41636437\n",
      "Iteration 46, loss = 0.41635458\n",
      "Iteration 47, loss = 0.41629176\n",
      "Iteration 48, loss = 0.41636034\n",
      "Iteration 49, loss = 0.41635715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77262843\n",
      "Iteration 2, loss = 0.71954447\n",
      "Iteration 3, loss = 0.67440006\n",
      "Iteration 4, loss = 0.63766723\n",
      "Iteration 5, loss = 0.60698957\n",
      "Iteration 6, loss = 0.58144600\n",
      "Iteration 7, loss = 0.56090877\n",
      "Iteration 8, loss = 0.54203302\n",
      "Iteration 9, loss = 0.52615764\n",
      "Iteration 10, loss = 0.51169260\n",
      "Iteration 11, loss = 0.49978784\n",
      "Iteration 12, loss = 0.48977991\n",
      "Iteration 13, loss = 0.48102966\n",
      "Iteration 14, loss = 0.47319894\n",
      "Iteration 15, loss = 0.46766266\n",
      "Iteration 16, loss = 0.46248982\n",
      "Iteration 17, loss = 0.45909335\n",
      "Iteration 18, loss = 0.45554864\n",
      "Iteration 19, loss = 0.45259510\n",
      "Iteration 20, loss = 0.45077306\n",
      "Iteration 21, loss = 0.44925891\n",
      "Iteration 22, loss = 0.44758306\n",
      "Iteration 23, loss = 0.44660216\n",
      "Iteration 24, loss = 0.44579279\n",
      "Iteration 25, loss = 0.44518510\n",
      "Iteration 26, loss = 0.44460193\n",
      "Iteration 27, loss = 0.44424912\n",
      "Iteration 28, loss = 0.44416976\n",
      "Iteration 29, loss = 0.44380135\n",
      "Iteration 30, loss = 0.44355561\n",
      "Iteration 31, loss = 0.44349014\n",
      "Iteration 32, loss = 0.44338756\n",
      "Iteration 33, loss = 0.44341426\n",
      "Iteration 34, loss = 0.44321846\n",
      "Iteration 35, loss = 0.44302807\n",
      "Iteration 36, loss = 0.44311888\n",
      "Iteration 37, loss = 0.44306155\n",
      "Iteration 38, loss = 0.44293541\n",
      "Iteration 39, loss = 0.44288187\n",
      "Iteration 40, loss = 0.44298806\n",
      "Iteration 41, loss = 0.44287897\n",
      "Iteration 42, loss = 0.44289046\n",
      "Iteration 43, loss = 0.44281228\n",
      "Iteration 44, loss = 0.44282301\n",
      "Iteration 45, loss = 0.44279649\n",
      "Iteration 46, loss = 0.44278748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852758\n",
      "Iteration 2, loss = 0.71232522\n",
      "Iteration 3, loss = 0.66094842\n",
      "Iteration 4, loss = 0.62121691\n",
      "Iteration 5, loss = 0.58926212\n",
      "Iteration 6, loss = 0.56172948\n",
      "Iteration 7, loss = 0.53837289\n",
      "Iteration 8, loss = 0.51997561\n",
      "Iteration 9, loss = 0.50268255\n",
      "Iteration 10, loss = 0.48870191\n",
      "Iteration 11, loss = 0.47577503\n",
      "Iteration 12, loss = 0.46521541\n",
      "Iteration 13, loss = 0.45504295\n",
      "Iteration 14, loss = 0.44711631\n",
      "Iteration 15, loss = 0.44017301\n",
      "Iteration 16, loss = 0.43442910\n",
      "Iteration 17, loss = 0.42948971\n",
      "Iteration 18, loss = 0.42617583\n",
      "Iteration 19, loss = 0.42254252\n",
      "Iteration 20, loss = 0.41993118\n",
      "Iteration 21, loss = 0.41788974\n",
      "Iteration 22, loss = 0.41583695\n",
      "Iteration 23, loss = 0.41475391\n",
      "Iteration 24, loss = 0.41353564\n",
      "Iteration 25, loss = 0.41289117\n",
      "Iteration 26, loss = 0.41230881\n",
      "Iteration 27, loss = 0.41171812\n",
      "Iteration 28, loss = 0.41133447\n",
      "Iteration 29, loss = 0.41104644\n",
      "Iteration 30, loss = 0.41081306\n",
      "Iteration 31, loss = 0.41056287\n",
      "Iteration 32, loss = 0.41062200\n",
      "Iteration 33, loss = 0.41043417\n",
      "Iteration 34, loss = 0.41025859\n",
      "Iteration 35, loss = 0.41023748\n",
      "Iteration 36, loss = 0.41019969\n",
      "Iteration 37, loss = 0.41011763\n",
      "Iteration 38, loss = 0.41006712\n",
      "Iteration 39, loss = 0.41000177\n",
      "Iteration 40, loss = 0.41004828\n",
      "Iteration 41, loss = 0.41004474\n",
      "Iteration 42, loss = 0.41006563\n",
      "Iteration 43, loss = 0.41012183\n",
      "Iteration 44, loss = 0.41008904\n",
      "Iteration 45, loss = 0.41000465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836833\n",
      "Iteration 2, loss = 0.72360616\n",
      "Iteration 3, loss = 0.67433647\n",
      "Iteration 4, loss = 0.63683748\n",
      "Iteration 5, loss = 0.60644815\n",
      "Iteration 6, loss = 0.58000251\n",
      "Iteration 7, loss = 0.55710542\n",
      "Iteration 8, loss = 0.53977727\n",
      "Iteration 9, loss = 0.52306660\n",
      "Iteration 10, loss = 0.50964897\n",
      "Iteration 11, loss = 0.49713801\n",
      "Iteration 12, loss = 0.48688322\n",
      "Iteration 13, loss = 0.47745237\n",
      "Iteration 14, loss = 0.47001491\n",
      "Iteration 15, loss = 0.46329982\n",
      "Iteration 16, loss = 0.45801813\n",
      "Iteration 17, loss = 0.45346694\n",
      "Iteration 18, loss = 0.45017040\n",
      "Iteration 19, loss = 0.44679218\n",
      "Iteration 20, loss = 0.44426741\n",
      "Iteration 21, loss = 0.44219826\n",
      "Iteration 22, loss = 0.44042704\n",
      "Iteration 23, loss = 0.43948763\n",
      "Iteration 24, loss = 0.43832151\n",
      "Iteration 25, loss = 0.43771464\n",
      "Iteration 26, loss = 0.43696411\n",
      "Iteration 27, loss = 0.43663026\n",
      "Iteration 28, loss = 0.43623533\n",
      "Iteration 29, loss = 0.43577385\n",
      "Iteration 30, loss = 0.43573181\n",
      "Iteration 31, loss = 0.43535118\n",
      "Iteration 32, loss = 0.43540806\n",
      "Iteration 33, loss = 0.43520338\n",
      "Iteration 34, loss = 0.43498948\n",
      "Iteration 35, loss = 0.43490297\n",
      "Iteration 36, loss = 0.43487907\n",
      "Iteration 37, loss = 0.43477234\n",
      "Iteration 38, loss = 0.43474579\n",
      "Iteration 39, loss = 0.43466724\n",
      "Iteration 40, loss = 0.43476140\n",
      "Iteration 41, loss = 0.43463304\n",
      "Iteration 42, loss = 0.43467808\n",
      "Iteration 43, loss = 0.43467163\n",
      "Iteration 44, loss = 0.43468227\n",
      "Iteration 45, loss = 0.43456992\n",
      "Iteration 46, loss = 0.43460162\n",
      "Iteration 47, loss = 0.43456507\n",
      "Iteration 48, loss = 0.43459764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812629\n",
      "Iteration 2, loss = 0.72327380\n",
      "Iteration 3, loss = 0.67851166\n",
      "Iteration 4, loss = 0.64113186\n",
      "Iteration 5, loss = 0.61005369\n",
      "Iteration 6, loss = 0.58272983\n",
      "Iteration 7, loss = 0.56064870\n",
      "Iteration 8, loss = 0.54029684\n",
      "Iteration 9, loss = 0.52206048\n",
      "Iteration 10, loss = 0.50630801\n",
      "Iteration 11, loss = 0.49280708\n",
      "Iteration 12, loss = 0.48086250\n",
      "Iteration 13, loss = 0.47076606\n",
      "Iteration 14, loss = 0.46199606\n",
      "Iteration 15, loss = 0.45484814\n",
      "Iteration 16, loss = 0.44904010\n",
      "Iteration 17, loss = 0.44397144\n",
      "Iteration 18, loss = 0.44036708\n",
      "Iteration 19, loss = 0.43680880\n",
      "Iteration 20, loss = 0.43340028\n",
      "Iteration 21, loss = 0.43138353\n",
      "Iteration 22, loss = 0.42944126\n",
      "Iteration 23, loss = 0.42774493\n",
      "Iteration 24, loss = 0.42668266\n",
      "Iteration 25, loss = 0.42574457\n",
      "Iteration 26, loss = 0.42490984\n",
      "Iteration 27, loss = 0.42409965\n",
      "Iteration 28, loss = 0.42377495\n",
      "Iteration 29, loss = 0.42313006\n",
      "Iteration 30, loss = 0.42285569\n",
      "Iteration 31, loss = 0.42265695\n",
      "Iteration 32, loss = 0.42231018\n",
      "Iteration 33, loss = 0.42244120\n",
      "Iteration 34, loss = 0.42192341\n",
      "Iteration 35, loss = 0.42196414\n",
      "Iteration 36, loss = 0.42166160\n",
      "Iteration 37, loss = 0.42169329\n",
      "Iteration 38, loss = 0.42156468\n",
      "Iteration 39, loss = 0.42134873\n",
      "Iteration 40, loss = 0.42144026\n",
      "Iteration 41, loss = 0.42127344\n",
      "Iteration 42, loss = 0.42133990\n",
      "Iteration 43, loss = 0.42108929\n",
      "Iteration 44, loss = 0.42132209\n",
      "Iteration 45, loss = 0.42109922\n",
      "Iteration 46, loss = 0.42099917\n",
      "Iteration 47, loss = 0.42093167\n",
      "Iteration 48, loss = 0.42106754\n",
      "Iteration 49, loss = 0.42093285\n",
      "Iteration 50, loss = 0.42094230\n",
      "Iteration 51, loss = 0.42096280\n",
      "Iteration 52, loss = 0.42102933\n",
      "Iteration 53, loss = 0.42088244\n",
      "Iteration 54, loss = 0.42080771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118733\n",
      "Iteration 2, loss = 0.72665229\n",
      "Iteration 3, loss = 0.68188297\n",
      "Iteration 4, loss = 0.64287863\n",
      "Iteration 5, loss = 0.60990561\n",
      "Iteration 6, loss = 0.58231323\n",
      "Iteration 7, loss = 0.55938643\n",
      "Iteration 8, loss = 0.53892197\n",
      "Iteration 9, loss = 0.52063801\n",
      "Iteration 10, loss = 0.50471598\n",
      "Iteration 11, loss = 0.49143517\n",
      "Iteration 12, loss = 0.47942822\n",
      "Iteration 13, loss = 0.46914118\n",
      "Iteration 14, loss = 0.46013988\n",
      "Iteration 15, loss = 0.45297956\n",
      "Iteration 16, loss = 0.44661715\n",
      "Iteration 17, loss = 0.44151699\n",
      "Iteration 18, loss = 0.43735842\n",
      "Iteration 19, loss = 0.43327125\n",
      "Iteration 20, loss = 0.43021556\n",
      "Iteration 21, loss = 0.42757196\n",
      "Iteration 22, loss = 0.42542220\n",
      "Iteration 23, loss = 0.42348861\n",
      "Iteration 24, loss = 0.42208378\n",
      "Iteration 25, loss = 0.42111680\n",
      "Iteration 26, loss = 0.42006070\n",
      "Iteration 27, loss = 0.41929289\n",
      "Iteration 28, loss = 0.41882902\n",
      "Iteration 29, loss = 0.41825859\n",
      "Iteration 30, loss = 0.41777103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.41762686\n",
      "Iteration 32, loss = 0.41740193\n",
      "Iteration 33, loss = 0.41745871\n",
      "Iteration 34, loss = 0.41696642\n",
      "Iteration 35, loss = 0.41676476\n",
      "Iteration 36, loss = 0.41683233\n",
      "Iteration 37, loss = 0.41670283\n",
      "Iteration 38, loss = 0.41652463\n",
      "Iteration 39, loss = 0.41649938\n",
      "Iteration 40, loss = 0.41661295\n",
      "Iteration 41, loss = 0.41644362\n",
      "Iteration 42, loss = 0.41647539\n",
      "Iteration 43, loss = 0.41637046\n",
      "Iteration 44, loss = 0.41644939\n",
      "Iteration 45, loss = 0.41630757\n",
      "Iteration 46, loss = 0.41629777\n",
      "Iteration 47, loss = 0.41623495\n",
      "Iteration 48, loss = 0.41630355\n",
      "Iteration 49, loss = 0.41630035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77257477\n",
      "Iteration 2, loss = 0.71949083\n",
      "Iteration 3, loss = 0.67434640\n",
      "Iteration 4, loss = 0.63761353\n",
      "Iteration 5, loss = 0.60693579\n",
      "Iteration 6, loss = 0.58139214\n",
      "Iteration 7, loss = 0.56085485\n",
      "Iteration 8, loss = 0.54197902\n",
      "Iteration 9, loss = 0.52610355\n",
      "Iteration 10, loss = 0.51163843\n",
      "Iteration 11, loss = 0.49973355\n",
      "Iteration 12, loss = 0.48972548\n",
      "Iteration 13, loss = 0.48097510\n",
      "Iteration 14, loss = 0.47314422\n",
      "Iteration 15, loss = 0.46760778\n",
      "Iteration 16, loss = 0.46243478\n",
      "Iteration 17, loss = 0.45903819\n",
      "Iteration 18, loss = 0.45549335\n",
      "Iteration 19, loss = 0.45253967\n",
      "Iteration 20, loss = 0.45071753\n",
      "Iteration 21, loss = 0.44920328\n",
      "Iteration 22, loss = 0.44752732\n",
      "Iteration 23, loss = 0.44654633\n",
      "Iteration 24, loss = 0.44573689\n",
      "Iteration 25, loss = 0.44512914\n",
      "Iteration 26, loss = 0.44454590\n",
      "Iteration 27, loss = 0.44419305\n",
      "Iteration 28, loss = 0.44411366\n",
      "Iteration 29, loss = 0.44374520\n",
      "Iteration 30, loss = 0.44349943\n",
      "Iteration 31, loss = 0.44343395\n",
      "Iteration 32, loss = 0.44333135\n",
      "Iteration 33, loss = 0.44335803\n",
      "Iteration 34, loss = 0.44316220\n",
      "Iteration 35, loss = 0.44297179\n",
      "Iteration 36, loss = 0.44306260\n",
      "Iteration 37, loss = 0.44300527\n",
      "Iteration 38, loss = 0.44287910\n",
      "Iteration 39, loss = 0.44282556\n",
      "Iteration 40, loss = 0.44293176\n",
      "Iteration 41, loss = 0.44282265\n",
      "Iteration 42, loss = 0.44283414\n",
      "Iteration 43, loss = 0.44275595\n",
      "Iteration 44, loss = 0.44276668\n",
      "Iteration 45, loss = 0.44274015\n",
      "Iteration 46, loss = 0.44273114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852221\n",
      "Iteration 2, loss = 0.71231985\n",
      "Iteration 3, loss = 0.66094306\n",
      "Iteration 4, loss = 0.62121154\n",
      "Iteration 5, loss = 0.58925674\n",
      "Iteration 6, loss = 0.56172408\n",
      "Iteration 7, loss = 0.53836748\n",
      "Iteration 8, loss = 0.51997019\n",
      "Iteration 9, loss = 0.50267713\n",
      "Iteration 10, loss = 0.48869648\n",
      "Iteration 11, loss = 0.47576958\n",
      "Iteration 12, loss = 0.46520994\n",
      "Iteration 13, loss = 0.45503747\n",
      "Iteration 14, loss = 0.44711081\n",
      "Iteration 15, loss = 0.44016749\n",
      "Iteration 16, loss = 0.43442356\n",
      "Iteration 17, loss = 0.42948417\n",
      "Iteration 18, loss = 0.42617027\n",
      "Iteration 19, loss = 0.42253695\n",
      "Iteration 20, loss = 0.41992559\n",
      "Iteration 21, loss = 0.41788414\n",
      "Iteration 22, loss = 0.41583133\n",
      "Iteration 23, loss = 0.41474829\n",
      "Iteration 24, loss = 0.41353001\n",
      "Iteration 25, loss = 0.41288553\n",
      "Iteration 26, loss = 0.41230316\n",
      "Iteration 27, loss = 0.41171247\n",
      "Iteration 28, loss = 0.41132881\n",
      "Iteration 29, loss = 0.41104078\n",
      "Iteration 30, loss = 0.41080739\n",
      "Iteration 31, loss = 0.41055720\n",
      "Iteration 32, loss = 0.41061633\n",
      "Iteration 33, loss = 0.41042849\n",
      "Iteration 34, loss = 0.41025292\n",
      "Iteration 35, loss = 0.41023180\n",
      "Iteration 36, loss = 0.41019401\n",
      "Iteration 37, loss = 0.41011195\n",
      "Iteration 38, loss = 0.41006144\n",
      "Iteration 39, loss = 0.40999609\n",
      "Iteration 40, loss = 0.41004260\n",
      "Iteration 41, loss = 0.41003906\n",
      "Iteration 42, loss = 0.41005994\n",
      "Iteration 43, loss = 0.41011615\n",
      "Iteration 44, loss = 0.41008336\n",
      "Iteration 45, loss = 0.40999897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836295\n",
      "Iteration 2, loss = 0.72360079\n",
      "Iteration 3, loss = 0.67433109\n",
      "Iteration 4, loss = 0.63683209\n",
      "Iteration 5, loss = 0.60644275\n",
      "Iteration 6, loss = 0.57999711\n",
      "Iteration 7, loss = 0.55710001\n",
      "Iteration 8, loss = 0.53977185\n",
      "Iteration 9, loss = 0.52306118\n",
      "Iteration 10, loss = 0.50964353\n",
      "Iteration 11, loss = 0.49713257\n",
      "Iteration 12, loss = 0.48687777\n",
      "Iteration 13, loss = 0.47744691\n",
      "Iteration 14, loss = 0.47000943\n",
      "Iteration 15, loss = 0.46329433\n",
      "Iteration 16, loss = 0.45801262\n",
      "Iteration 17, loss = 0.45346142\n",
      "Iteration 18, loss = 0.45016486\n",
      "Iteration 19, loss = 0.44678664\n",
      "Iteration 20, loss = 0.44426185\n",
      "Iteration 21, loss = 0.44219269\n",
      "Iteration 22, loss = 0.44042146\n",
      "Iteration 23, loss = 0.43948204\n",
      "Iteration 24, loss = 0.43831591\n",
      "Iteration 25, loss = 0.43770903\n",
      "Iteration 26, loss = 0.43695850\n",
      "Iteration 27, loss = 0.43662464\n",
      "Iteration 28, loss = 0.43622971\n",
      "Iteration 29, loss = 0.43576823\n",
      "Iteration 30, loss = 0.43572619\n",
      "Iteration 31, loss = 0.43534555\n",
      "Iteration 32, loss = 0.43540243\n",
      "Iteration 33, loss = 0.43519774\n",
      "Iteration 34, loss = 0.43498384\n",
      "Iteration 35, loss = 0.43489733\n",
      "Iteration 36, loss = 0.43487343\n",
      "Iteration 37, loss = 0.43476670\n",
      "Iteration 38, loss = 0.43474015\n",
      "Iteration 39, loss = 0.43466159\n",
      "Iteration 40, loss = 0.43475576\n",
      "Iteration 41, loss = 0.43462739\n",
      "Iteration 42, loss = 0.43467244\n",
      "Iteration 43, loss = 0.43466598\n",
      "Iteration 44, loss = 0.43467662\n",
      "Iteration 45, loss = 0.43456428\n",
      "Iteration 46, loss = 0.43459597\n",
      "Iteration 47, loss = 0.43455942\n",
      "Iteration 48, loss = 0.43459199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812093\n",
      "Iteration 2, loss = 0.72326843\n",
      "Iteration 3, loss = 0.67850629\n",
      "Iteration 4, loss = 0.64112649\n",
      "Iteration 5, loss = 0.61004831\n",
      "Iteration 6, loss = 0.58272445\n",
      "Iteration 7, loss = 0.56064331\n",
      "Iteration 8, loss = 0.54029145\n",
      "Iteration 9, loss = 0.52205508\n",
      "Iteration 10, loss = 0.50630260\n",
      "Iteration 11, loss = 0.49280165\n",
      "Iteration 12, loss = 0.48085705\n",
      "Iteration 13, loss = 0.47076059\n",
      "Iteration 14, loss = 0.46199058\n",
      "Iteration 15, loss = 0.45484264\n",
      "Iteration 16, loss = 0.44903459\n",
      "Iteration 17, loss = 0.44396591\n",
      "Iteration 18, loss = 0.44036154\n",
      "Iteration 19, loss = 0.43680323\n",
      "Iteration 20, loss = 0.43339470\n",
      "Iteration 21, loss = 0.43137794\n",
      "Iteration 22, loss = 0.42943566\n",
      "Iteration 23, loss = 0.42773931\n",
      "Iteration 24, loss = 0.42667704\n",
      "Iteration 25, loss = 0.42573893\n",
      "Iteration 26, loss = 0.42490419\n",
      "Iteration 27, loss = 0.42409400\n",
      "Iteration 28, loss = 0.42376929\n",
      "Iteration 29, loss = 0.42312439\n",
      "Iteration 30, loss = 0.42285002\n",
      "Iteration 31, loss = 0.42265127\n",
      "Iteration 32, loss = 0.42230449\n",
      "Iteration 33, loss = 0.42243552\n",
      "Iteration 34, loss = 0.42191772\n",
      "Iteration 35, loss = 0.42195844\n",
      "Iteration 36, loss = 0.42165590\n",
      "Iteration 37, loss = 0.42168759\n",
      "Iteration 38, loss = 0.42155898\n",
      "Iteration 39, loss = 0.42134302\n",
      "Iteration 40, loss = 0.42143455\n",
      "Iteration 41, loss = 0.42126774\n",
      "Iteration 42, loss = 0.42133419\n",
      "Iteration 43, loss = 0.42108358\n",
      "Iteration 44, loss = 0.42131638\n",
      "Iteration 45, loss = 0.42109351\n",
      "Iteration 46, loss = 0.42099345\n",
      "Iteration 47, loss = 0.42092595\n",
      "Iteration 48, loss = 0.42106183\n",
      "Iteration 49, loss = 0.42092714\n",
      "Iteration 50, loss = 0.42093659\n",
      "Iteration 51, loss = 0.42095709\n",
      "Iteration 52, loss = 0.42102361\n",
      "Iteration 53, loss = 0.42087672\n",
      "Iteration 54, loss = 0.42080199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118196\n",
      "Iteration 2, loss = 0.72664692\n",
      "Iteration 3, loss = 0.68187760\n",
      "Iteration 4, loss = 0.64287326\n",
      "Iteration 5, loss = 0.60990024\n",
      "Iteration 6, loss = 0.58230784\n",
      "Iteration 7, loss = 0.55938104\n",
      "Iteration 8, loss = 0.53891657\n",
      "Iteration 9, loss = 0.52063260\n",
      "Iteration 10, loss = 0.50471056\n",
      "Iteration 11, loss = 0.49142974\n",
      "Iteration 12, loss = 0.47942278\n",
      "Iteration 13, loss = 0.46913573\n",
      "Iteration 14, loss = 0.46013441\n",
      "Iteration 15, loss = 0.45297407\n",
      "Iteration 16, loss = 0.44661165\n",
      "Iteration 17, loss = 0.44151148\n",
      "Iteration 18, loss = 0.43735289\n",
      "Iteration 19, loss = 0.43326571\n",
      "Iteration 20, loss = 0.43021000\n",
      "Iteration 21, loss = 0.42756638\n",
      "Iteration 22, loss = 0.42541662\n",
      "Iteration 23, loss = 0.42348301\n",
      "Iteration 24, loss = 0.42207818\n",
      "Iteration 25, loss = 0.42111118\n",
      "Iteration 26, loss = 0.42005507\n",
      "Iteration 27, loss = 0.41928726\n",
      "Iteration 28, loss = 0.41882338\n",
      "Iteration 29, loss = 0.41825294\n",
      "Iteration 30, loss = 0.41776538\n",
      "Iteration 31, loss = 0.41762121\n",
      "Iteration 32, loss = 0.41739627\n",
      "Iteration 33, loss = 0.41745305\n",
      "Iteration 34, loss = 0.41696075\n",
      "Iteration 35, loss = 0.41675909\n",
      "Iteration 36, loss = 0.41682666\n",
      "Iteration 37, loss = 0.41669716\n",
      "Iteration 38, loss = 0.41651896\n",
      "Iteration 39, loss = 0.41649370\n",
      "Iteration 40, loss = 0.41660727\n",
      "Iteration 41, loss = 0.41643794\n",
      "Iteration 42, loss = 0.41646971\n",
      "Iteration 43, loss = 0.41636478\n",
      "Iteration 44, loss = 0.41644371\n",
      "Iteration 45, loss = 0.41630189\n",
      "Iteration 46, loss = 0.41629209\n",
      "Iteration 47, loss = 0.41622926\n",
      "Iteration 48, loss = 0.41629786\n",
      "Iteration 49, loss = 0.41629466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77256941\n",
      "Iteration 2, loss = 0.71948547\n",
      "Iteration 3, loss = 0.67434103\n",
      "Iteration 4, loss = 0.63760816\n",
      "Iteration 5, loss = 0.60693042\n",
      "Iteration 6, loss = 0.58138676\n",
      "Iteration 7, loss = 0.56084946\n",
      "Iteration 8, loss = 0.54197361\n",
      "Iteration 9, loss = 0.52609814\n",
      "Iteration 10, loss = 0.51163301\n",
      "Iteration 11, loss = 0.49972812\n",
      "Iteration 12, loss = 0.48972004\n",
      "Iteration 13, loss = 0.48096964\n",
      "Iteration 14, loss = 0.47313874\n",
      "Iteration 15, loss = 0.46760229\n",
      "Iteration 16, loss = 0.46242928\n",
      "Iteration 17, loss = 0.45903268\n",
      "Iteration 18, loss = 0.45548782\n",
      "Iteration 19, loss = 0.45253413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.45071198\n",
      "Iteration 21, loss = 0.44919772\n",
      "Iteration 22, loss = 0.44752174\n",
      "Iteration 23, loss = 0.44654074\n",
      "Iteration 24, loss = 0.44573129\n",
      "Iteration 25, loss = 0.44512354\n",
      "Iteration 26, loss = 0.44454030\n",
      "Iteration 27, loss = 0.44418744\n",
      "Iteration 28, loss = 0.44410805\n",
      "Iteration 29, loss = 0.44373958\n",
      "Iteration 30, loss = 0.44349381\n",
      "Iteration 31, loss = 0.44342833\n",
      "Iteration 32, loss = 0.44332572\n",
      "Iteration 33, loss = 0.44335240\n",
      "Iteration 34, loss = 0.44315657\n",
      "Iteration 35, loss = 0.44296616\n",
      "Iteration 36, loss = 0.44305697\n",
      "Iteration 37, loss = 0.44299964\n",
      "Iteration 38, loss = 0.44287347\n",
      "Iteration 39, loss = 0.44281993\n",
      "Iteration 40, loss = 0.44292612\n",
      "Iteration 41, loss = 0.44281702\n",
      "Iteration 42, loss = 0.44282851\n",
      "Iteration 43, loss = 0.44275032\n",
      "Iteration 44, loss = 0.44276104\n",
      "Iteration 45, loss = 0.44273452\n",
      "Iteration 46, loss = 0.44272550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76852168\n",
      "Iteration 2, loss = 0.71231932\n",
      "Iteration 3, loss = 0.66094252\n",
      "Iteration 4, loss = 0.62121101\n",
      "Iteration 5, loss = 0.58925620\n",
      "Iteration 6, loss = 0.56172354\n",
      "Iteration 7, loss = 0.53836694\n",
      "Iteration 8, loss = 0.51996965\n",
      "Iteration 9, loss = 0.50267658\n",
      "Iteration 10, loss = 0.48869593\n",
      "Iteration 11, loss = 0.47576904\n",
      "Iteration 12, loss = 0.46520940\n",
      "Iteration 13, loss = 0.45503692\n",
      "Iteration 14, loss = 0.44711026\n",
      "Iteration 15, loss = 0.44016694\n",
      "Iteration 16, loss = 0.43442301\n",
      "Iteration 17, loss = 0.42948361\n",
      "Iteration 18, loss = 0.42616971\n",
      "Iteration 19, loss = 0.42253639\n",
      "Iteration 20, loss = 0.41992503\n",
      "Iteration 21, loss = 0.41788358\n",
      "Iteration 22, loss = 0.41583077\n",
      "Iteration 23, loss = 0.41474773\n",
      "Iteration 24, loss = 0.41352945\n",
      "Iteration 25, loss = 0.41288496\n",
      "Iteration 26, loss = 0.41230260\n",
      "Iteration 27, loss = 0.41171190\n",
      "Iteration 28, loss = 0.41132824\n",
      "Iteration 29, loss = 0.41104021\n",
      "Iteration 30, loss = 0.41080683\n",
      "Iteration 31, loss = 0.41055664\n",
      "Iteration 32, loss = 0.41061577\n",
      "Iteration 33, loss = 0.41042792\n",
      "Iteration 34, loss = 0.41025235\n",
      "Iteration 35, loss = 0.41023123\n",
      "Iteration 36, loss = 0.41019344\n",
      "Iteration 37, loss = 0.41011139\n",
      "Iteration 38, loss = 0.41006087\n",
      "Iteration 39, loss = 0.40999552\n",
      "Iteration 40, loss = 0.41004203\n",
      "Iteration 41, loss = 0.41003849\n",
      "Iteration 42, loss = 0.41005938\n",
      "Iteration 43, loss = 0.41011558\n",
      "Iteration 44, loss = 0.41008279\n",
      "Iteration 45, loss = 0.40999840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77836242\n",
      "Iteration 2, loss = 0.72360025\n",
      "Iteration 3, loss = 0.67433055\n",
      "Iteration 4, loss = 0.63683156\n",
      "Iteration 5, loss = 0.60644221\n",
      "Iteration 6, loss = 0.57999657\n",
      "Iteration 7, loss = 0.55709946\n",
      "Iteration 8, loss = 0.53977131\n",
      "Iteration 9, loss = 0.52306064\n",
      "Iteration 10, loss = 0.50964299\n",
      "Iteration 11, loss = 0.49713202\n",
      "Iteration 12, loss = 0.48687722\n",
      "Iteration 13, loss = 0.47744636\n",
      "Iteration 14, loss = 0.47000888\n",
      "Iteration 15, loss = 0.46329378\n",
      "Iteration 16, loss = 0.45801207\n",
      "Iteration 17, loss = 0.45346087\n",
      "Iteration 18, loss = 0.45016431\n",
      "Iteration 19, loss = 0.44678608\n",
      "Iteration 20, loss = 0.44426130\n",
      "Iteration 21, loss = 0.44219213\n",
      "Iteration 22, loss = 0.44042090\n",
      "Iteration 23, loss = 0.43948148\n",
      "Iteration 24, loss = 0.43831535\n",
      "Iteration 25, loss = 0.43770847\n",
      "Iteration 26, loss = 0.43695794\n",
      "Iteration 27, loss = 0.43662408\n",
      "Iteration 28, loss = 0.43622915\n",
      "Iteration 29, loss = 0.43576767\n",
      "Iteration 30, loss = 0.43572562\n",
      "Iteration 31, loss = 0.43534498\n",
      "Iteration 32, loss = 0.43540187\n",
      "Iteration 33, loss = 0.43519718\n",
      "Iteration 34, loss = 0.43498328\n",
      "Iteration 35, loss = 0.43489677\n",
      "Iteration 36, loss = 0.43487287\n",
      "Iteration 37, loss = 0.43476613\n",
      "Iteration 38, loss = 0.43473958\n",
      "Iteration 39, loss = 0.43466103\n",
      "Iteration 40, loss = 0.43475520\n",
      "Iteration 41, loss = 0.43462682\n",
      "Iteration 42, loss = 0.43467187\n",
      "Iteration 43, loss = 0.43466542\n",
      "Iteration 44, loss = 0.43467606\n",
      "Iteration 45, loss = 0.43456371\n",
      "Iteration 46, loss = 0.43459541\n",
      "Iteration 47, loss = 0.43455886\n",
      "Iteration 48, loss = 0.43459143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77812039\n",
      "Iteration 2, loss = 0.72326790\n",
      "Iteration 3, loss = 0.67850575\n",
      "Iteration 4, loss = 0.64112596\n",
      "Iteration 5, loss = 0.61004777\n",
      "Iteration 6, loss = 0.58272391\n",
      "Iteration 7, loss = 0.56064277\n",
      "Iteration 8, loss = 0.54029091\n",
      "Iteration 9, loss = 0.52205454\n",
      "Iteration 10, loss = 0.50630205\n",
      "Iteration 11, loss = 0.49280111\n",
      "Iteration 12, loss = 0.48085651\n",
      "Iteration 13, loss = 0.47076005\n",
      "Iteration 14, loss = 0.46199003\n",
      "Iteration 15, loss = 0.45484209\n",
      "Iteration 16, loss = 0.44903404\n",
      "Iteration 17, loss = 0.44396536\n",
      "Iteration 18, loss = 0.44036098\n",
      "Iteration 19, loss = 0.43680268\n",
      "Iteration 20, loss = 0.43339414\n",
      "Iteration 21, loss = 0.43137738\n",
      "Iteration 22, loss = 0.42943510\n",
      "Iteration 23, loss = 0.42773875\n",
      "Iteration 24, loss = 0.42667647\n",
      "Iteration 25, loss = 0.42573837\n",
      "Iteration 26, loss = 0.42490362\n",
      "Iteration 27, loss = 0.42409343\n",
      "Iteration 28, loss = 0.42376872\n",
      "Iteration 29, loss = 0.42312382\n",
      "Iteration 30, loss = 0.42284945\n",
      "Iteration 31, loss = 0.42265070\n",
      "Iteration 32, loss = 0.42230392\n",
      "Iteration 33, loss = 0.42243495\n",
      "Iteration 34, loss = 0.42191715\n",
      "Iteration 35, loss = 0.42195787\n",
      "Iteration 36, loss = 0.42165533\n",
      "Iteration 37, loss = 0.42168702\n",
      "Iteration 38, loss = 0.42155841\n",
      "Iteration 39, loss = 0.42134245\n",
      "Iteration 40, loss = 0.42143398\n",
      "Iteration 41, loss = 0.42126717\n",
      "Iteration 42, loss = 0.42133362\n",
      "Iteration 43, loss = 0.42108301\n",
      "Iteration 44, loss = 0.42131581\n",
      "Iteration 45, loss = 0.42109294\n",
      "Iteration 46, loss = 0.42099288\n",
      "Iteration 47, loss = 0.42092538\n",
      "Iteration 48, loss = 0.42106126\n",
      "Iteration 49, loss = 0.42092657\n",
      "Iteration 50, loss = 0.42093602\n",
      "Iteration 51, loss = 0.42095652\n",
      "Iteration 52, loss = 0.42102304\n",
      "Iteration 53, loss = 0.42087615\n",
      "Iteration 54, loss = 0.42080142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78118143\n",
      "Iteration 2, loss = 0.72664639\n",
      "Iteration 3, loss = 0.68187706\n",
      "Iteration 4, loss = 0.64287272\n",
      "Iteration 5, loss = 0.60989970\n",
      "Iteration 6, loss = 0.58230731\n",
      "Iteration 7, loss = 0.55938050\n",
      "Iteration 8, loss = 0.53891603\n",
      "Iteration 9, loss = 0.52063206\n",
      "Iteration 10, loss = 0.50471002\n",
      "Iteration 11, loss = 0.49142920\n",
      "Iteration 12, loss = 0.47942224\n",
      "Iteration 13, loss = 0.46913518\n",
      "Iteration 14, loss = 0.46013386\n",
      "Iteration 15, loss = 0.45297352\n",
      "Iteration 16, loss = 0.44661110\n",
      "Iteration 17, loss = 0.44151092\n",
      "Iteration 18, loss = 0.43735234\n",
      "Iteration 19, loss = 0.43326515\n",
      "Iteration 20, loss = 0.43020944\n",
      "Iteration 21, loss = 0.42756583\n",
      "Iteration 22, loss = 0.42541606\n",
      "Iteration 23, loss = 0.42348245\n",
      "Iteration 24, loss = 0.42207762\n",
      "Iteration 25, loss = 0.42111062\n",
      "Iteration 26, loss = 0.42005451\n",
      "Iteration 27, loss = 0.41928669\n",
      "Iteration 28, loss = 0.41882282\n",
      "Iteration 29, loss = 0.41825237\n",
      "Iteration 30, loss = 0.41776481\n",
      "Iteration 31, loss = 0.41762064\n",
      "Iteration 32, loss = 0.41739570\n",
      "Iteration 33, loss = 0.41745248\n",
      "Iteration 34, loss = 0.41696018\n",
      "Iteration 35, loss = 0.41675852\n",
      "Iteration 36, loss = 0.41682609\n",
      "Iteration 37, loss = 0.41669659\n",
      "Iteration 38, loss = 0.41651839\n",
      "Iteration 39, loss = 0.41649313\n",
      "Iteration 40, loss = 0.41660670\n",
      "Iteration 41, loss = 0.41643737\n",
      "Iteration 42, loss = 0.41646915\n",
      "Iteration 43, loss = 0.41636421\n",
      "Iteration 44, loss = 0.41644314\n",
      "Iteration 45, loss = 0.41630132\n",
      "Iteration 46, loss = 0.41629152\n",
      "Iteration 47, loss = 0.41622869\n",
      "Iteration 48, loss = 0.41629730\n",
      "Iteration 49, loss = 0.41629409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77256887\n",
      "Iteration 2, loss = 0.71948493\n",
      "Iteration 3, loss = 0.67434049\n",
      "Iteration 4, loss = 0.63760762\n",
      "Iteration 5, loss = 0.60692988\n",
      "Iteration 6, loss = 0.58138622\n",
      "Iteration 7, loss = 0.56084892\n",
      "Iteration 8, loss = 0.54197307\n",
      "Iteration 9, loss = 0.52609759\n",
      "Iteration 10, loss = 0.51163247\n",
      "Iteration 11, loss = 0.49972758\n",
      "Iteration 12, loss = 0.48971950\n",
      "Iteration 13, loss = 0.48096909\n",
      "Iteration 14, loss = 0.47313820\n",
      "Iteration 15, loss = 0.46760174\n",
      "Iteration 16, loss = 0.46242873\n",
      "Iteration 17, loss = 0.45903213\n",
      "Iteration 18, loss = 0.45548727\n",
      "Iteration 19, loss = 0.45253358\n",
      "Iteration 20, loss = 0.45071142\n",
      "Iteration 21, loss = 0.44919716\n",
      "Iteration 22, loss = 0.44752119\n",
      "Iteration 23, loss = 0.44654019\n",
      "Iteration 24, loss = 0.44573074\n",
      "Iteration 25, loss = 0.44512298\n",
      "Iteration 26, loss = 0.44453974\n",
      "Iteration 27, loss = 0.44418688\n",
      "Iteration 28, loss = 0.44410749\n",
      "Iteration 29, loss = 0.44373902\n",
      "Iteration 30, loss = 0.44349325\n",
      "Iteration 31, loss = 0.44342777\n",
      "Iteration 32, loss = 0.44332516\n",
      "Iteration 33, loss = 0.44335184\n",
      "Iteration 34, loss = 0.44315601\n",
      "Iteration 35, loss = 0.44296560\n",
      "Iteration 36, loss = 0.44305640\n",
      "Iteration 37, loss = 0.44299907\n",
      "Iteration 38, loss = 0.44287291\n",
      "Iteration 39, loss = 0.44281937\n",
      "Iteration 40, loss = 0.44292556\n",
      "Iteration 41, loss = 0.44281645\n",
      "Iteration 42, loss = 0.44282795\n",
      "Iteration 43, loss = 0.44274975\n",
      "Iteration 44, loss = 0.44276048\n",
      "Iteration 45, loss = 0.44273396\n",
      "Iteration 46, loss = 0.44272494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75172330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.71730302\n",
      "Iteration 3, loss = 0.68850250\n",
      "Iteration 4, loss = 0.66532281\n",
      "Iteration 5, loss = 0.64976537\n",
      "Iteration 6, loss = 0.63849846\n",
      "Iteration 7, loss = 0.63046567\n",
      "Iteration 8, loss = 0.62548138\n",
      "Iteration 9, loss = 0.62045553\n",
      "Iteration 10, loss = 0.61575445\n",
      "Iteration 11, loss = 0.61104546\n",
      "Iteration 12, loss = 0.60552488\n",
      "Iteration 13, loss = 0.59948762\n",
      "Iteration 14, loss = 0.59417297\n",
      "Iteration 15, loss = 0.58882313\n",
      "Iteration 16, loss = 0.58377637\n",
      "Iteration 17, loss = 0.57894930\n",
      "Iteration 18, loss = 0.57438554\n",
      "Iteration 19, loss = 0.57006792\n",
      "Iteration 20, loss = 0.56579470\n",
      "Iteration 21, loss = 0.56153468\n",
      "Iteration 22, loss = 0.55736253\n",
      "Iteration 23, loss = 0.55317038\n",
      "Iteration 24, loss = 0.54903845\n",
      "Iteration 25, loss = 0.54504278\n",
      "Iteration 26, loss = 0.54130154\n",
      "Iteration 27, loss = 0.53739037\n",
      "Iteration 28, loss = 0.53363942\n",
      "Iteration 29, loss = 0.53008810\n",
      "Iteration 30, loss = 0.52641073\n",
      "Iteration 31, loss = 0.52300871\n",
      "Iteration 32, loss = 0.51972559\n",
      "Iteration 33, loss = 0.51624293\n",
      "Iteration 34, loss = 0.51293562\n",
      "Iteration 35, loss = 0.50973895\n",
      "Iteration 36, loss = 0.50685714\n",
      "Iteration 37, loss = 0.50371075\n",
      "Iteration 38, loss = 0.50080842\n",
      "Iteration 39, loss = 0.49797175\n",
      "Iteration 40, loss = 0.49512392\n",
      "Iteration 41, loss = 0.49233520\n",
      "Iteration 42, loss = 0.48971014\n",
      "Iteration 43, loss = 0.48713829\n",
      "Iteration 44, loss = 0.48460892\n",
      "Iteration 45, loss = 0.48214614\n",
      "Iteration 46, loss = 0.47974340\n",
      "Iteration 47, loss = 0.47748346\n",
      "Iteration 48, loss = 0.47506600\n",
      "Iteration 49, loss = 0.47281226\n",
      "Iteration 50, loss = 0.47081977\n",
      "Iteration 51, loss = 0.46866236\n",
      "Iteration 52, loss = 0.46667278\n",
      "Iteration 53, loss = 0.46456898\n",
      "Iteration 54, loss = 0.46275398\n",
      "Iteration 55, loss = 0.46092176\n",
      "Iteration 56, loss = 0.45905868\n",
      "Iteration 57, loss = 0.45730393\n",
      "Iteration 58, loss = 0.45560347\n",
      "Iteration 59, loss = 0.45391989\n",
      "Iteration 60, loss = 0.45233327\n",
      "Iteration 61, loss = 0.45077804\n",
      "Iteration 62, loss = 0.44940399\n",
      "Iteration 63, loss = 0.44787200\n",
      "Iteration 64, loss = 0.44648110\n",
      "Iteration 65, loss = 0.44508367\n",
      "Iteration 66, loss = 0.44372645\n",
      "Iteration 67, loss = 0.44249018\n",
      "Iteration 68, loss = 0.44130092\n",
      "Iteration 69, loss = 0.44016545\n",
      "Iteration 70, loss = 0.43907240\n",
      "Iteration 71, loss = 0.43798036\n",
      "Iteration 72, loss = 0.43691928\n",
      "Iteration 73, loss = 0.43588553\n",
      "Iteration 74, loss = 0.43503492\n",
      "Iteration 75, loss = 0.43415715\n",
      "Iteration 76, loss = 0.43325043\n",
      "Iteration 77, loss = 0.43237963\n",
      "Iteration 78, loss = 0.43161453\n",
      "Iteration 79, loss = 0.43090887\n",
      "Iteration 80, loss = 0.43032781\n",
      "Iteration 81, loss = 0.42960982\n",
      "Iteration 82, loss = 0.42887629\n",
      "Iteration 83, loss = 0.42837581\n",
      "Iteration 84, loss = 0.42772950\n",
      "Iteration 85, loss = 0.42719685\n",
      "Iteration 86, loss = 0.42659590\n",
      "Iteration 87, loss = 0.42614888\n",
      "Iteration 88, loss = 0.42578914\n",
      "Iteration 89, loss = 0.42521831\n",
      "Iteration 90, loss = 0.42475191\n",
      "Iteration 91, loss = 0.42449390\n",
      "Iteration 92, loss = 0.42412998\n",
      "Iteration 93, loss = 0.42373526\n",
      "Iteration 94, loss = 0.42352070\n",
      "Iteration 95, loss = 0.42309295\n",
      "Iteration 96, loss = 0.42287068\n",
      "Iteration 97, loss = 0.42253365\n",
      "Iteration 98, loss = 0.42233259\n",
      "Iteration 99, loss = 0.42201018\n",
      "Iteration 100, loss = 0.42174803\n",
      "Iteration 101, loss = 0.42161239\n",
      "Iteration 102, loss = 0.42130843\n",
      "Iteration 103, loss = 0.42116351\n",
      "Iteration 104, loss = 0.42097994\n",
      "Iteration 105, loss = 0.42076198\n",
      "Iteration 106, loss = 0.42057860\n",
      "Iteration 107, loss = 0.42049844\n",
      "Iteration 108, loss = 0.42037498\n",
      "Iteration 109, loss = 0.42017154\n",
      "Iteration 110, loss = 0.42002497\n",
      "Iteration 111, loss = 0.41988851\n",
      "Iteration 112, loss = 0.41978472\n",
      "Iteration 113, loss = 0.41967922\n",
      "Iteration 114, loss = 0.41955694\n",
      "Iteration 115, loss = 0.41945841\n",
      "Iteration 116, loss = 0.41934694\n",
      "Iteration 117, loss = 0.41928473\n",
      "Iteration 118, loss = 0.41919667\n",
      "Iteration 119, loss = 0.41908128\n",
      "Iteration 120, loss = 0.41898595\n",
      "Iteration 121, loss = 0.41892122\n",
      "Iteration 122, loss = 0.41895436\n",
      "Iteration 123, loss = 0.41874332\n",
      "Iteration 124, loss = 0.41881933\n",
      "Iteration 125, loss = 0.41860211\n",
      "Iteration 126, loss = 0.41857773\n",
      "Iteration 127, loss = 0.41849502\n",
      "Iteration 128, loss = 0.41845283\n",
      "Iteration 129, loss = 0.41843056\n",
      "Iteration 130, loss = 0.41829482\n",
      "Iteration 131, loss = 0.41833045\n",
      "Iteration 132, loss = 0.41828709\n",
      "Iteration 133, loss = 0.41826024\n",
      "Iteration 134, loss = 0.41811749\n",
      "Iteration 135, loss = 0.41812091\n",
      "Iteration 136, loss = 0.41804064\n",
      "Iteration 137, loss = 0.41806104\n",
      "Iteration 138, loss = 0.41793315\n",
      "Iteration 139, loss = 0.41797471\n",
      "Iteration 140, loss = 0.41788613\n",
      "Iteration 141, loss = 0.41783309\n",
      "Iteration 142, loss = 0.41779391\n",
      "Iteration 143, loss = 0.41774654\n",
      "Iteration 144, loss = 0.41769053\n",
      "Iteration 145, loss = 0.41781311\n",
      "Iteration 146, loss = 0.41770831\n",
      "Iteration 147, loss = 0.41776122\n",
      "Iteration 148, loss = 0.41763572\n",
      "Iteration 149, loss = 0.41754199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75271921\n",
      "Iteration 2, loss = 0.71821707\n",
      "Iteration 3, loss = 0.68906612\n",
      "Iteration 4, loss = 0.66707242\n",
      "Iteration 5, loss = 0.65231432\n",
      "Iteration 6, loss = 0.64131112\n",
      "Iteration 7, loss = 0.63338020\n",
      "Iteration 8, loss = 0.62940206\n",
      "Iteration 9, loss = 0.62459600\n",
      "Iteration 10, loss = 0.62022429\n",
      "Iteration 11, loss = 0.61595107\n",
      "Iteration 12, loss = 0.61119525\n",
      "Iteration 13, loss = 0.60573763\n",
      "Iteration 14, loss = 0.60109478\n",
      "Iteration 15, loss = 0.59636614\n",
      "Iteration 16, loss = 0.59192906\n",
      "Iteration 17, loss = 0.58779230\n",
      "Iteration 18, loss = 0.58371259\n",
      "Iteration 19, loss = 0.58000732\n",
      "Iteration 20, loss = 0.57628212\n",
      "Iteration 21, loss = 0.57262536\n",
      "Iteration 22, loss = 0.56907829\n",
      "Iteration 23, loss = 0.56550361\n",
      "Iteration 24, loss = 0.56200561\n",
      "Iteration 25, loss = 0.55854844\n",
      "Iteration 26, loss = 0.55531147\n",
      "Iteration 27, loss = 0.55208567\n",
      "Iteration 28, loss = 0.54886129\n",
      "Iteration 29, loss = 0.54575138\n",
      "Iteration 30, loss = 0.54258576\n",
      "Iteration 31, loss = 0.53956885\n",
      "Iteration 32, loss = 0.53692262\n",
      "Iteration 33, loss = 0.53388525\n",
      "Iteration 34, loss = 0.53098956\n",
      "Iteration 35, loss = 0.52821678\n",
      "Iteration 36, loss = 0.52574999\n",
      "Iteration 37, loss = 0.52293509\n",
      "Iteration 38, loss = 0.52045392\n",
      "Iteration 39, loss = 0.51796875\n",
      "Iteration 40, loss = 0.51548986\n",
      "Iteration 41, loss = 0.51297591\n",
      "Iteration 42, loss = 0.51061848\n",
      "Iteration 43, loss = 0.50831726\n",
      "Iteration 44, loss = 0.50601434\n",
      "Iteration 45, loss = 0.50383022\n",
      "Iteration 46, loss = 0.50169593\n",
      "Iteration 47, loss = 0.49951165\n",
      "Iteration 48, loss = 0.49745224\n",
      "Iteration 49, loss = 0.49542329\n",
      "Iteration 50, loss = 0.49352404\n",
      "Iteration 51, loss = 0.49161022\n",
      "Iteration 52, loss = 0.48962691\n",
      "Iteration 53, loss = 0.48768165\n",
      "Iteration 54, loss = 0.48604623\n",
      "Iteration 55, loss = 0.48427080\n",
      "Iteration 56, loss = 0.48249684\n",
      "Iteration 57, loss = 0.48087422\n",
      "Iteration 58, loss = 0.47921664\n",
      "Iteration 59, loss = 0.47773709\n",
      "Iteration 60, loss = 0.47601949\n",
      "Iteration 61, loss = 0.47469841\n",
      "Iteration 62, loss = 0.47326943\n",
      "Iteration 63, loss = 0.47188034\n",
      "Iteration 64, loss = 0.47032352\n",
      "Iteration 65, loss = 0.46908469\n",
      "Iteration 66, loss = 0.46791272\n",
      "Iteration 67, loss = 0.46653158\n",
      "Iteration 68, loss = 0.46537357\n",
      "Iteration 69, loss = 0.46426879\n",
      "Iteration 70, loss = 0.46320669\n",
      "Iteration 71, loss = 0.46210964\n",
      "Iteration 72, loss = 0.46098074\n",
      "Iteration 73, loss = 0.45997013\n",
      "Iteration 74, loss = 0.45922505\n",
      "Iteration 75, loss = 0.45817893\n",
      "Iteration 76, loss = 0.45735841\n",
      "Iteration 77, loss = 0.45648275\n",
      "Iteration 78, loss = 0.45568195\n",
      "Iteration 79, loss = 0.45490239\n",
      "Iteration 80, loss = 0.45425118\n",
      "Iteration 81, loss = 0.45359398\n",
      "Iteration 82, loss = 0.45282267\n",
      "Iteration 83, loss = 0.45242161\n",
      "Iteration 84, loss = 0.45160187\n",
      "Iteration 85, loss = 0.45107666\n",
      "Iteration 86, loss = 0.45049118\n",
      "Iteration 87, loss = 0.44999685\n",
      "Iteration 88, loss = 0.44957309\n",
      "Iteration 89, loss = 0.44905628\n",
      "Iteration 90, loss = 0.44858045\n",
      "Iteration 91, loss = 0.44823742\n",
      "Iteration 92, loss = 0.44782011\n",
      "Iteration 93, loss = 0.44743271\n",
      "Iteration 94, loss = 0.44714606\n",
      "Iteration 95, loss = 0.44676941\n",
      "Iteration 96, loss = 0.44644527\n",
      "Iteration 97, loss = 0.44627703\n",
      "Iteration 98, loss = 0.44606452\n",
      "Iteration 99, loss = 0.44560423\n",
      "Iteration 100, loss = 0.44534227\n",
      "Iteration 101, loss = 0.44518509\n",
      "Iteration 102, loss = 0.44487673\n",
      "Iteration 103, loss = 0.44468494\n",
      "Iteration 104, loss = 0.44456776\n",
      "Iteration 105, loss = 0.44435352\n",
      "Iteration 106, loss = 0.44408991\n",
      "Iteration 107, loss = 0.44396082\n",
      "Iteration 108, loss = 0.44380584\n",
      "Iteration 109, loss = 0.44367538\n",
      "Iteration 110, loss = 0.44357390\n",
      "Iteration 111, loss = 0.44341511\n",
      "Iteration 112, loss = 0.44326447\n",
      "Iteration 113, loss = 0.44318574\n",
      "Iteration 114, loss = 0.44310812\n",
      "Iteration 115, loss = 0.44293504\n",
      "Iteration 116, loss = 0.44285602\n",
      "Iteration 117, loss = 0.44278495\n",
      "Iteration 118, loss = 0.44275318\n",
      "Iteration 119, loss = 0.44259253\n",
      "Iteration 120, loss = 0.44256380\n",
      "Iteration 121, loss = 0.44245025\n",
      "Iteration 122, loss = 0.44244556\n",
      "Iteration 123, loss = 0.44225928\n",
      "Iteration 124, loss = 0.44223913\n",
      "Iteration 125, loss = 0.44221523\n",
      "Iteration 126, loss = 0.44212902\n",
      "Iteration 127, loss = 0.44205374\n",
      "Iteration 128, loss = 0.44198175\n",
      "Iteration 129, loss = 0.44196878\n",
      "Iteration 130, loss = 0.44186242\n",
      "Iteration 131, loss = 0.44185734\n",
      "Iteration 132, loss = 0.44177011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 133, loss = 0.44180303\n",
      "Iteration 134, loss = 0.44171155\n",
      "Iteration 135, loss = 0.44172393\n",
      "Iteration 136, loss = 0.44162707\n",
      "Iteration 137, loss = 0.44161362\n",
      "Iteration 138, loss = 0.44160114\n",
      "Iteration 139, loss = 0.44160080\n",
      "Iteration 140, loss = 0.44148412\n",
      "Iteration 141, loss = 0.44152522\n",
      "Iteration 142, loss = 0.44145325\n",
      "Iteration 143, loss = 0.44137026\n",
      "Iteration 144, loss = 0.44143605\n",
      "Iteration 145, loss = 0.44135129\n",
      "Iteration 146, loss = 0.44139084\n",
      "Iteration 147, loss = 0.44157533\n",
      "Iteration 148, loss = 0.44140701\n",
      "Iteration 149, loss = 0.44138117\n",
      "Iteration 150, loss = 0.44122908\n",
      "Iteration 151, loss = 0.44119782\n",
      "Iteration 152, loss = 0.44121820\n",
      "Iteration 153, loss = 0.44111427\n",
      "Iteration 154, loss = 0.44114959\n",
      "Iteration 155, loss = 0.44123078\n",
      "Iteration 156, loss = 0.44112461\n",
      "Iteration 157, loss = 0.44103035\n",
      "Iteration 158, loss = 0.44108548\n",
      "Iteration 159, loss = 0.44105817\n",
      "Iteration 160, loss = 0.44095167\n",
      "Iteration 161, loss = 0.44096099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75444020\n",
      "Iteration 2, loss = 0.72079191\n",
      "Iteration 3, loss = 0.69132973\n",
      "Iteration 4, loss = 0.67066670\n",
      "Iteration 5, loss = 0.65464324\n",
      "Iteration 6, loss = 0.64375024\n",
      "Iteration 7, loss = 0.63604443\n",
      "Iteration 8, loss = 0.63108585\n",
      "Iteration 9, loss = 0.62670886\n",
      "Iteration 10, loss = 0.62240109\n",
      "Iteration 11, loss = 0.61829974\n",
      "Iteration 12, loss = 0.61355972\n",
      "Iteration 13, loss = 0.60888356\n",
      "Iteration 14, loss = 0.60484404\n",
      "Iteration 15, loss = 0.59943836\n",
      "Iteration 16, loss = 0.59505806\n",
      "Iteration 17, loss = 0.59111465\n",
      "Iteration 18, loss = 0.58704019\n",
      "Iteration 19, loss = 0.58342996\n",
      "Iteration 20, loss = 0.57936906\n",
      "Iteration 21, loss = 0.57587180\n",
      "Iteration 22, loss = 0.57233166\n",
      "Iteration 23, loss = 0.56856976\n",
      "Iteration 24, loss = 0.56511147\n",
      "Iteration 25, loss = 0.56178168\n",
      "Iteration 26, loss = 0.55826623\n",
      "Iteration 27, loss = 0.55489229\n",
      "Iteration 28, loss = 0.55176065\n",
      "Iteration 29, loss = 0.54843885\n",
      "Iteration 30, loss = 0.54537277\n",
      "Iteration 31, loss = 0.54219596\n",
      "Iteration 32, loss = 0.53918768\n",
      "Iteration 33, loss = 0.53621091\n",
      "Iteration 34, loss = 0.53320861\n",
      "Iteration 35, loss = 0.53040821\n",
      "Iteration 36, loss = 0.52753033\n",
      "Iteration 37, loss = 0.52470563\n",
      "Iteration 38, loss = 0.52186491\n",
      "Iteration 39, loss = 0.51915870\n",
      "Iteration 40, loss = 0.51654378\n",
      "Iteration 41, loss = 0.51383848\n",
      "Iteration 42, loss = 0.51135500\n",
      "Iteration 43, loss = 0.50872395\n",
      "Iteration 44, loss = 0.50639549\n",
      "Iteration 45, loss = 0.50380059\n",
      "Iteration 46, loss = 0.50144683\n",
      "Iteration 47, loss = 0.49898552\n",
      "Iteration 48, loss = 0.49663717\n",
      "Iteration 49, loss = 0.49435318\n",
      "Iteration 50, loss = 0.49226919\n",
      "Iteration 51, loss = 0.48992562\n",
      "Iteration 52, loss = 0.48769030\n",
      "Iteration 53, loss = 0.48571982\n",
      "Iteration 54, loss = 0.48367945\n",
      "Iteration 55, loss = 0.48155506\n",
      "Iteration 56, loss = 0.47956085\n",
      "Iteration 57, loss = 0.47769226\n",
      "Iteration 58, loss = 0.47586896\n",
      "Iteration 59, loss = 0.47391801\n",
      "Iteration 60, loss = 0.47209491\n",
      "Iteration 61, loss = 0.47034251\n",
      "Iteration 62, loss = 0.46878014\n",
      "Iteration 63, loss = 0.46702267\n",
      "Iteration 64, loss = 0.46542882\n",
      "Iteration 65, loss = 0.46386134\n",
      "Iteration 66, loss = 0.46239639\n",
      "Iteration 67, loss = 0.46105115\n",
      "Iteration 68, loss = 0.45953369\n",
      "Iteration 69, loss = 0.45817591\n",
      "Iteration 70, loss = 0.45707131\n",
      "Iteration 71, loss = 0.45563920\n",
      "Iteration 72, loss = 0.45446722\n",
      "Iteration 73, loss = 0.45330172\n",
      "Iteration 74, loss = 0.45214421\n",
      "Iteration 75, loss = 0.45107446\n",
      "Iteration 76, loss = 0.45013237\n",
      "Iteration 77, loss = 0.44907129\n",
      "Iteration 78, loss = 0.44825778\n",
      "Iteration 79, loss = 0.44727599\n",
      "Iteration 80, loss = 0.44633586\n",
      "Iteration 81, loss = 0.44557830\n",
      "Iteration 82, loss = 0.44489782\n",
      "Iteration 83, loss = 0.44411638\n",
      "Iteration 84, loss = 0.44332306\n",
      "Iteration 85, loss = 0.44276850\n",
      "Iteration 86, loss = 0.44201414\n",
      "Iteration 87, loss = 0.44142106\n",
      "Iteration 88, loss = 0.44100529\n",
      "Iteration 89, loss = 0.44029697\n",
      "Iteration 90, loss = 0.43976138\n",
      "Iteration 91, loss = 0.43928973\n",
      "Iteration 92, loss = 0.43892008\n",
      "Iteration 93, loss = 0.43831875\n",
      "Iteration 94, loss = 0.43799770\n",
      "Iteration 95, loss = 0.43743843\n",
      "Iteration 96, loss = 0.43708865\n",
      "Iteration 97, loss = 0.43672649\n",
      "Iteration 98, loss = 0.43636396\n",
      "Iteration 99, loss = 0.43599833\n",
      "Iteration 100, loss = 0.43568348\n",
      "Iteration 101, loss = 0.43550428\n",
      "Iteration 102, loss = 0.43512419\n",
      "Iteration 103, loss = 0.43485720\n",
      "Iteration 104, loss = 0.43460773\n",
      "Iteration 105, loss = 0.43432927\n",
      "Iteration 106, loss = 0.43409796\n",
      "Iteration 107, loss = 0.43386657\n",
      "Iteration 108, loss = 0.43367218\n",
      "Iteration 109, loss = 0.43342045\n",
      "Iteration 110, loss = 0.43322045\n",
      "Iteration 111, loss = 0.43310622\n",
      "Iteration 112, loss = 0.43286731\n",
      "Iteration 113, loss = 0.43268803\n",
      "Iteration 114, loss = 0.43253608\n",
      "Iteration 115, loss = 0.43243650\n",
      "Iteration 116, loss = 0.43220956\n",
      "Iteration 117, loss = 0.43206634\n",
      "Iteration 118, loss = 0.43191956\n",
      "Iteration 119, loss = 0.43178763\n",
      "Iteration 120, loss = 0.43164412\n",
      "Iteration 121, loss = 0.43151645\n",
      "Iteration 122, loss = 0.43142150\n",
      "Iteration 123, loss = 0.43127759\n",
      "Iteration 124, loss = 0.43115773\n",
      "Iteration 125, loss = 0.43114534\n",
      "Iteration 126, loss = 0.43093599\n",
      "Iteration 127, loss = 0.43082946\n",
      "Iteration 128, loss = 0.43078141\n",
      "Iteration 129, loss = 0.43066651\n",
      "Iteration 130, loss = 0.43061956\n",
      "Iteration 131, loss = 0.43046119\n",
      "Iteration 132, loss = 0.43046639\n",
      "Iteration 133, loss = 0.43030707\n",
      "Iteration 134, loss = 0.43021948\n",
      "Iteration 135, loss = 0.43014286\n",
      "Iteration 136, loss = 0.43010670\n",
      "Iteration 137, loss = 0.42997695\n",
      "Iteration 138, loss = 0.42991585\n",
      "Iteration 139, loss = 0.42986964\n",
      "Iteration 140, loss = 0.42983249\n",
      "Iteration 141, loss = 0.42970377\n",
      "Iteration 142, loss = 0.42980624\n",
      "Iteration 143, loss = 0.42973259\n",
      "Iteration 144, loss = 0.42955845\n",
      "Iteration 145, loss = 0.42948483\n",
      "Iteration 146, loss = 0.42946687\n",
      "Iteration 147, loss = 0.42941913\n",
      "Iteration 148, loss = 0.42932900\n",
      "Iteration 149, loss = 0.42926418\n",
      "Iteration 150, loss = 0.42915228\n",
      "Iteration 151, loss = 0.42909930\n",
      "Iteration 152, loss = 0.42907815\n",
      "Iteration 153, loss = 0.42918192\n",
      "Iteration 154, loss = 0.42896225\n",
      "Iteration 155, loss = 0.42888404\n",
      "Iteration 156, loss = 0.42887269\n",
      "Iteration 157, loss = 0.42882675\n",
      "Iteration 158, loss = 0.42882008\n",
      "Iteration 159, loss = 0.42886538\n",
      "Iteration 160, loss = 0.42873885\n",
      "Iteration 161, loss = 0.42866034\n",
      "Iteration 162, loss = 0.42863731\n",
      "Iteration 163, loss = 0.42858281\n",
      "Iteration 164, loss = 0.42865439\n",
      "Iteration 165, loss = 0.42855688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75362886\n",
      "Iteration 2, loss = 0.72026850\n",
      "Iteration 3, loss = 0.69057632\n",
      "Iteration 4, loss = 0.67002073\n",
      "Iteration 5, loss = 0.65396109\n",
      "Iteration 6, loss = 0.64326605\n",
      "Iteration 7, loss = 0.63562367\n",
      "Iteration 8, loss = 0.63044223\n",
      "Iteration 9, loss = 0.62606230\n",
      "Iteration 10, loss = 0.62163862\n",
      "Iteration 11, loss = 0.61732580\n",
      "Iteration 12, loss = 0.61260433\n",
      "Iteration 13, loss = 0.60763690\n",
      "Iteration 14, loss = 0.60359660\n",
      "Iteration 15, loss = 0.59812059\n",
      "Iteration 16, loss = 0.59360690\n",
      "Iteration 17, loss = 0.58940745\n",
      "Iteration 18, loss = 0.58541073\n",
      "Iteration 19, loss = 0.58165104\n",
      "Iteration 20, loss = 0.57751667\n",
      "Iteration 21, loss = 0.57362950\n",
      "Iteration 22, loss = 0.56994100\n",
      "Iteration 23, loss = 0.56614372\n",
      "Iteration 24, loss = 0.56254598\n",
      "Iteration 25, loss = 0.55895038\n",
      "Iteration 26, loss = 0.55537872\n",
      "Iteration 27, loss = 0.55174342\n",
      "Iteration 28, loss = 0.54847432\n",
      "Iteration 29, loss = 0.54500571\n",
      "Iteration 30, loss = 0.54173366\n",
      "Iteration 31, loss = 0.53849302\n",
      "Iteration 32, loss = 0.53521941\n",
      "Iteration 33, loss = 0.53204391\n",
      "Iteration 34, loss = 0.52882213\n",
      "Iteration 35, loss = 0.52583879\n",
      "Iteration 36, loss = 0.52282838\n",
      "Iteration 37, loss = 0.51983086\n",
      "Iteration 38, loss = 0.51687956\n",
      "Iteration 39, loss = 0.51397134\n",
      "Iteration 40, loss = 0.51113642\n",
      "Iteration 41, loss = 0.50839865\n",
      "Iteration 42, loss = 0.50567122\n",
      "Iteration 43, loss = 0.50297938\n",
      "Iteration 44, loss = 0.50030305\n",
      "Iteration 45, loss = 0.49782306\n",
      "Iteration 46, loss = 0.49516841\n",
      "Iteration 47, loss = 0.49269494\n",
      "Iteration 48, loss = 0.49021719\n",
      "Iteration 49, loss = 0.48788132\n",
      "Iteration 50, loss = 0.48546544\n",
      "Iteration 51, loss = 0.48313934\n",
      "Iteration 52, loss = 0.48089180\n",
      "Iteration 53, loss = 0.47891173\n",
      "Iteration 54, loss = 0.47657461\n",
      "Iteration 55, loss = 0.47442328\n",
      "Iteration 56, loss = 0.47229133\n",
      "Iteration 57, loss = 0.47040311\n",
      "Iteration 58, loss = 0.46858256\n",
      "Iteration 59, loss = 0.46652004\n",
      "Iteration 60, loss = 0.46459007\n",
      "Iteration 61, loss = 0.46291742\n",
      "Iteration 62, loss = 0.46126887\n",
      "Iteration 63, loss = 0.45956634\n",
      "Iteration 64, loss = 0.45787727\n",
      "Iteration 65, loss = 0.45621947\n",
      "Iteration 66, loss = 0.45474052\n",
      "Iteration 67, loss = 0.45329894\n",
      "Iteration 68, loss = 0.45188296\n",
      "Iteration 69, loss = 0.45041311\n",
      "Iteration 70, loss = 0.44927046\n",
      "Iteration 71, loss = 0.44786594\n",
      "Iteration 72, loss = 0.44667908\n",
      "Iteration 73, loss = 0.44550026\n",
      "Iteration 74, loss = 0.44428840\n",
      "Iteration 75, loss = 0.44322088\n",
      "Iteration 76, loss = 0.44224375\n",
      "Iteration 77, loss = 0.44126621\n",
      "Iteration 78, loss = 0.44037198\n",
      "Iteration 79, loss = 0.43938013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 0.43853742\n",
      "Iteration 81, loss = 0.43770411\n",
      "Iteration 82, loss = 0.43699362\n",
      "Iteration 83, loss = 0.43632604\n",
      "Iteration 84, loss = 0.43554872\n",
      "Iteration 85, loss = 0.43492868\n",
      "Iteration 86, loss = 0.43417518\n",
      "Iteration 87, loss = 0.43368377\n",
      "Iteration 88, loss = 0.43315179\n",
      "Iteration 89, loss = 0.43256075\n",
      "Iteration 90, loss = 0.43203849\n",
      "Iteration 91, loss = 0.43152416\n",
      "Iteration 92, loss = 0.43110186\n",
      "Iteration 93, loss = 0.43062331\n",
      "Iteration 94, loss = 0.43029511\n",
      "Iteration 95, loss = 0.42984887\n",
      "Iteration 96, loss = 0.42953369\n",
      "Iteration 97, loss = 0.42916627\n",
      "Iteration 98, loss = 0.42881081\n",
      "Iteration 99, loss = 0.42853085\n",
      "Iteration 100, loss = 0.42821861\n",
      "Iteration 101, loss = 0.42809270\n",
      "Iteration 102, loss = 0.42772642\n",
      "Iteration 103, loss = 0.42751061\n",
      "Iteration 104, loss = 0.42727213\n",
      "Iteration 105, loss = 0.42703912\n",
      "Iteration 106, loss = 0.42688077\n",
      "Iteration 107, loss = 0.42662429\n",
      "Iteration 108, loss = 0.42649499\n",
      "Iteration 109, loss = 0.42635068\n",
      "Iteration 110, loss = 0.42618304\n",
      "Iteration 111, loss = 0.42599325\n",
      "Iteration 112, loss = 0.42588711\n",
      "Iteration 113, loss = 0.42573917\n",
      "Iteration 114, loss = 0.42561599\n",
      "Iteration 115, loss = 0.42549609\n",
      "Iteration 116, loss = 0.42534833\n",
      "Iteration 117, loss = 0.42524966\n",
      "Iteration 118, loss = 0.42521313\n",
      "Iteration 119, loss = 0.42515206\n",
      "Iteration 120, loss = 0.42501976\n",
      "Iteration 121, loss = 0.42496601\n",
      "Iteration 122, loss = 0.42504411\n",
      "Iteration 123, loss = 0.42478144\n",
      "Iteration 124, loss = 0.42473776\n",
      "Iteration 125, loss = 0.42460430\n",
      "Iteration 126, loss = 0.42453726\n",
      "Iteration 127, loss = 0.42450580\n",
      "Iteration 128, loss = 0.42447580\n",
      "Iteration 129, loss = 0.42440982\n",
      "Iteration 130, loss = 0.42439379\n",
      "Iteration 131, loss = 0.42431873\n",
      "Iteration 132, loss = 0.42425364\n",
      "Iteration 133, loss = 0.42422432\n",
      "Iteration 134, loss = 0.42413699\n",
      "Iteration 135, loss = 0.42409407\n",
      "Iteration 136, loss = 0.42406124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75435554\n",
      "Iteration 2, loss = 0.71980844\n",
      "Iteration 3, loss = 0.69073607\n",
      "Iteration 4, loss = 0.66889749\n",
      "Iteration 5, loss = 0.65261334\n",
      "Iteration 6, loss = 0.64319081\n",
      "Iteration 7, loss = 0.63500209\n",
      "Iteration 8, loss = 0.63018267\n",
      "Iteration 9, loss = 0.62585370\n",
      "Iteration 10, loss = 0.62160591\n",
      "Iteration 11, loss = 0.61734006\n",
      "Iteration 12, loss = 0.61283159\n",
      "Iteration 13, loss = 0.60790511\n",
      "Iteration 14, loss = 0.60385519\n",
      "Iteration 15, loss = 0.59862172\n",
      "Iteration 16, loss = 0.59431808\n",
      "Iteration 17, loss = 0.59001876\n",
      "Iteration 18, loss = 0.58613508\n",
      "Iteration 19, loss = 0.58265626\n",
      "Iteration 20, loss = 0.57875558\n",
      "Iteration 21, loss = 0.57522188\n",
      "Iteration 22, loss = 0.57157280\n",
      "Iteration 23, loss = 0.56810250\n",
      "Iteration 24, loss = 0.56457174\n",
      "Iteration 25, loss = 0.56116713\n",
      "Iteration 26, loss = 0.55782356\n",
      "Iteration 27, loss = 0.55458169\n",
      "Iteration 28, loss = 0.55150923\n",
      "Iteration 29, loss = 0.54839592\n",
      "Iteration 30, loss = 0.54532762\n",
      "Iteration 31, loss = 0.54240125\n",
      "Iteration 32, loss = 0.53941125\n",
      "Iteration 33, loss = 0.53655181\n",
      "Iteration 34, loss = 0.53374337\n",
      "Iteration 35, loss = 0.53100679\n",
      "Iteration 36, loss = 0.52845906\n",
      "Iteration 37, loss = 0.52570156\n",
      "Iteration 38, loss = 0.52325583\n",
      "Iteration 39, loss = 0.52068731\n",
      "Iteration 40, loss = 0.51825183\n",
      "Iteration 41, loss = 0.51594010\n",
      "Iteration 42, loss = 0.51356627\n",
      "Iteration 43, loss = 0.51124326\n",
      "Iteration 44, loss = 0.50911821\n",
      "Iteration 45, loss = 0.50700852\n",
      "Iteration 46, loss = 0.50475720\n",
      "Iteration 47, loss = 0.50278563\n",
      "Iteration 48, loss = 0.50077988\n",
      "Iteration 49, loss = 0.49880306\n",
      "Iteration 50, loss = 0.49694648\n",
      "Iteration 51, loss = 0.49508936\n",
      "Iteration 52, loss = 0.49325617\n",
      "Iteration 53, loss = 0.49169169\n",
      "Iteration 54, loss = 0.48983417\n",
      "Iteration 55, loss = 0.48815946\n",
      "Iteration 56, loss = 0.48646298\n",
      "Iteration 57, loss = 0.48508048\n",
      "Iteration 58, loss = 0.48372644\n",
      "Iteration 59, loss = 0.48204651\n",
      "Iteration 60, loss = 0.48068479\n",
      "Iteration 61, loss = 0.47918528\n",
      "Iteration 62, loss = 0.47797860\n",
      "Iteration 63, loss = 0.47673054\n",
      "Iteration 64, loss = 0.47546050\n",
      "Iteration 65, loss = 0.47438448\n",
      "Iteration 66, loss = 0.47318661\n",
      "Iteration 67, loss = 0.47204700\n",
      "Iteration 68, loss = 0.47107030\n",
      "Iteration 69, loss = 0.46996996\n",
      "Iteration 70, loss = 0.46900151\n",
      "Iteration 71, loss = 0.46812955\n",
      "Iteration 72, loss = 0.46720457\n",
      "Iteration 73, loss = 0.46629510\n",
      "Iteration 74, loss = 0.46536800\n",
      "Iteration 75, loss = 0.46467115\n",
      "Iteration 76, loss = 0.46391913\n",
      "Iteration 77, loss = 0.46332549\n",
      "Iteration 78, loss = 0.46264309\n",
      "Iteration 79, loss = 0.46185566\n",
      "Iteration 80, loss = 0.46128658\n",
      "Iteration 81, loss = 0.46064479\n",
      "Iteration 82, loss = 0.46013041\n",
      "Iteration 83, loss = 0.45983866\n",
      "Iteration 84, loss = 0.45918918\n",
      "Iteration 85, loss = 0.45872000\n",
      "Iteration 86, loss = 0.45816153\n",
      "Iteration 87, loss = 0.45792890\n",
      "Iteration 88, loss = 0.45753980\n",
      "Iteration 89, loss = 0.45702871\n",
      "Iteration 90, loss = 0.45665949\n",
      "Iteration 91, loss = 0.45632853\n",
      "Iteration 92, loss = 0.45601331\n",
      "Iteration 93, loss = 0.45566340\n",
      "Iteration 94, loss = 0.45543855\n",
      "Iteration 95, loss = 0.45514239\n",
      "Iteration 96, loss = 0.45490944\n",
      "Iteration 97, loss = 0.45467700\n",
      "Iteration 98, loss = 0.45438451\n",
      "Iteration 99, loss = 0.45416088\n",
      "Iteration 100, loss = 0.45395368\n",
      "Iteration 101, loss = 0.45387355\n",
      "Iteration 102, loss = 0.45364006\n",
      "Iteration 103, loss = 0.45350753\n",
      "Iteration 104, loss = 0.45322527\n",
      "Iteration 105, loss = 0.45309944\n",
      "Iteration 106, loss = 0.45299565\n",
      "Iteration 107, loss = 0.45279014\n",
      "Iteration 108, loss = 0.45273746\n",
      "Iteration 109, loss = 0.45261824\n",
      "Iteration 110, loss = 0.45245495\n",
      "Iteration 111, loss = 0.45227056\n",
      "Iteration 112, loss = 0.45216901\n",
      "Iteration 113, loss = 0.45202041\n",
      "Iteration 114, loss = 0.45192837\n",
      "Iteration 115, loss = 0.45187223\n",
      "Iteration 116, loss = 0.45175469\n",
      "Iteration 117, loss = 0.45165710\n",
      "Iteration 118, loss = 0.45160038\n",
      "Iteration 119, loss = 0.45156001\n",
      "Iteration 120, loss = 0.45147986\n",
      "Iteration 121, loss = 0.45135781\n",
      "Iteration 122, loss = 0.45142805\n",
      "Iteration 123, loss = 0.45129306\n",
      "Iteration 124, loss = 0.45116098\n",
      "Iteration 125, loss = 0.45105255\n",
      "Iteration 126, loss = 0.45106230\n",
      "Iteration 127, loss = 0.45096272\n",
      "Iteration 128, loss = 0.45092239\n",
      "Iteration 129, loss = 0.45086107\n",
      "Iteration 130, loss = 0.45087784\n",
      "Iteration 131, loss = 0.45074691\n",
      "Iteration 132, loss = 0.45068362\n",
      "Iteration 133, loss = 0.45068702\n",
      "Iteration 134, loss = 0.45059044\n",
      "Iteration 135, loss = 0.45057672\n",
      "Iteration 136, loss = 0.45056235\n",
      "Iteration 137, loss = 0.45056909\n",
      "Iteration 138, loss = 0.45039819\n",
      "Iteration 139, loss = 0.45041531\n",
      "Iteration 140, loss = 0.45043752\n",
      "Iteration 141, loss = 0.45033969\n",
      "Iteration 142, loss = 0.45042231\n",
      "Iteration 143, loss = 0.45024831\n",
      "Iteration 144, loss = 0.45021885\n",
      "Iteration 145, loss = 0.45019585\n",
      "Iteration 146, loss = 0.45024287\n",
      "Iteration 147, loss = 0.45023227\n",
      "Iteration 148, loss = 0.45007078\n",
      "Iteration 149, loss = 0.45001976\n",
      "Iteration 150, loss = 0.45003126\n",
      "Iteration 151, loss = 0.45000600\n",
      "Iteration 152, loss = 0.45002232\n",
      "Iteration 153, loss = 0.45010018\n",
      "Iteration 154, loss = 0.44992841\n",
      "Iteration 155, loss = 0.44989144\n",
      "Iteration 156, loss = 0.44979414\n",
      "Iteration 157, loss = 0.44979617\n",
      "Iteration 158, loss = 0.44976495\n",
      "Iteration 159, loss = 0.44971680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74993101\n",
      "Iteration 2, loss = 0.71550939\n",
      "Iteration 3, loss = 0.68670283\n",
      "Iteration 4, loss = 0.66351435\n",
      "Iteration 5, loss = 0.64794291\n",
      "Iteration 6, loss = 0.63665932\n",
      "Iteration 7, loss = 0.62860531\n",
      "Iteration 8, loss = 0.62359627\n",
      "Iteration 9, loss = 0.61854122\n",
      "Iteration 10, loss = 0.61380692\n",
      "Iteration 11, loss = 0.60906157\n",
      "Iteration 12, loss = 0.60349963\n",
      "Iteration 13, loss = 0.59741594\n",
      "Iteration 14, loss = 0.59205438\n",
      "Iteration 15, loss = 0.58665197\n",
      "Iteration 16, loss = 0.58154973\n",
      "Iteration 17, loss = 0.57666187\n",
      "Iteration 18, loss = 0.57203756\n",
      "Iteration 19, loss = 0.56765495\n",
      "Iteration 20, loss = 0.56331483\n",
      "Iteration 21, loss = 0.55898559\n",
      "Iteration 22, loss = 0.55474342\n",
      "Iteration 23, loss = 0.55047653\n",
      "Iteration 24, loss = 0.54626871\n",
      "Iteration 25, loss = 0.54219546\n",
      "Iteration 26, loss = 0.53837935\n",
      "Iteration 27, loss = 0.53438676\n",
      "Iteration 28, loss = 0.53055504\n",
      "Iteration 29, loss = 0.52692651\n",
      "Iteration 30, loss = 0.52316827\n",
      "Iteration 31, loss = 0.51968682\n",
      "Iteration 32, loss = 0.51632622\n",
      "Iteration 33, loss = 0.51276269\n",
      "Iteration 34, loss = 0.50937721\n",
      "Iteration 35, loss = 0.50610568\n",
      "Iteration 36, loss = 0.50315064\n",
      "Iteration 37, loss = 0.49992921\n",
      "Iteration 38, loss = 0.49695672\n",
      "Iteration 39, loss = 0.49404883\n",
      "Iteration 40, loss = 0.49113210\n",
      "Iteration 41, loss = 0.48827787\n",
      "Iteration 42, loss = 0.48558860\n",
      "Iteration 43, loss = 0.48295642\n",
      "Iteration 44, loss = 0.48036372\n",
      "Iteration 45, loss = 0.47784815\n",
      "Iteration 46, loss = 0.47538638\n",
      "Iteration 47, loss = 0.47307573\n",
      "Iteration 48, loss = 0.47060542\n",
      "Iteration 49, loss = 0.46829767\n",
      "Iteration 50, loss = 0.46626253\n",
      "Iteration 51, loss = 0.46405635\n",
      "Iteration 52, loss = 0.46202013\n",
      "Iteration 53, loss = 0.45987386\n",
      "Iteration 54, loss = 0.45801768\n",
      "Iteration 55, loss = 0.45614290\n",
      "Iteration 56, loss = 0.45424208\n",
      "Iteration 57, loss = 0.45245082\n",
      "Iteration 58, loss = 0.45071445\n",
      "Iteration 59, loss = 0.44899655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.44737103\n",
      "Iteration 61, loss = 0.44578353\n",
      "Iteration 62, loss = 0.44437441\n",
      "Iteration 63, loss = 0.44280887\n",
      "Iteration 64, loss = 0.44139341\n",
      "Iteration 65, loss = 0.43996388\n",
      "Iteration 66, loss = 0.43857719\n",
      "Iteration 67, loss = 0.43730924\n",
      "Iteration 68, loss = 0.43609323\n",
      "Iteration 69, loss = 0.43493084\n",
      "Iteration 70, loss = 0.43380933\n",
      "Iteration 71, loss = 0.43269282\n",
      "Iteration 72, loss = 0.43160364\n",
      "Iteration 73, loss = 0.43054220\n",
      "Iteration 74, loss = 0.42967028\n",
      "Iteration 75, loss = 0.42876600\n",
      "Iteration 76, loss = 0.42783435\n",
      "Iteration 77, loss = 0.42694520\n",
      "Iteration 78, loss = 0.42615582\n",
      "Iteration 79, loss = 0.42542904\n",
      "Iteration 80, loss = 0.42482963\n",
      "Iteration 81, loss = 0.42409217\n",
      "Iteration 82, loss = 0.42333649\n",
      "Iteration 83, loss = 0.42281841\n",
      "Iteration 84, loss = 0.42215229\n",
      "Iteration 85, loss = 0.42160505\n",
      "Iteration 86, loss = 0.42098623\n",
      "Iteration 87, loss = 0.42052684\n",
      "Iteration 88, loss = 0.42015720\n",
      "Iteration 89, loss = 0.41956998\n",
      "Iteration 90, loss = 0.41908822\n",
      "Iteration 91, loss = 0.41882429\n",
      "Iteration 92, loss = 0.41845307\n",
      "Iteration 93, loss = 0.41804680\n",
      "Iteration 94, loss = 0.41782106\n",
      "Iteration 95, loss = 0.41738883\n",
      "Iteration 96, loss = 0.41715819\n",
      "Iteration 97, loss = 0.41681941\n",
      "Iteration 98, loss = 0.41660758\n",
      "Iteration 99, loss = 0.41628060\n",
      "Iteration 100, loss = 0.41601642\n",
      "Iteration 101, loss = 0.41588169\n",
      "Iteration 102, loss = 0.41557166\n",
      "Iteration 103, loss = 0.41543078\n",
      "Iteration 104, loss = 0.41524081\n",
      "Iteration 105, loss = 0.41502447\n",
      "Iteration 106, loss = 0.41483971\n",
      "Iteration 107, loss = 0.41476407\n",
      "Iteration 108, loss = 0.41464082\n",
      "Iteration 109, loss = 0.41443725\n",
      "Iteration 110, loss = 0.41429109\n",
      "Iteration 111, loss = 0.41415746\n",
      "Iteration 112, loss = 0.41405575\n",
      "Iteration 113, loss = 0.41395125\n",
      "Iteration 114, loss = 0.41383992\n",
      "Iteration 115, loss = 0.41374234\n",
      "Iteration 116, loss = 0.41363481\n",
      "Iteration 117, loss = 0.41358074\n",
      "Iteration 118, loss = 0.41349248\n",
      "Iteration 119, loss = 0.41338499\n",
      "Iteration 120, loss = 0.41329340\n",
      "Iteration 121, loss = 0.41323332\n",
      "Iteration 122, loss = 0.41327388\n",
      "Iteration 123, loss = 0.41306605\n",
      "Iteration 124, loss = 0.41314856\n",
      "Iteration 125, loss = 0.41293829\n",
      "Iteration 126, loss = 0.41291957\n",
      "Iteration 127, loss = 0.41284407\n",
      "Iteration 128, loss = 0.41280873\n",
      "Iteration 129, loss = 0.41279043\n",
      "Iteration 130, loss = 0.41266251\n",
      "Iteration 131, loss = 0.41270484\n",
      "Iteration 132, loss = 0.41266914\n",
      "Iteration 133, loss = 0.41264940\n",
      "Iteration 134, loss = 0.41251214\n",
      "Iteration 135, loss = 0.41252415\n",
      "Iteration 136, loss = 0.41244850\n",
      "Iteration 137, loss = 0.41247360\n",
      "Iteration 138, loss = 0.41235400\n",
      "Iteration 139, loss = 0.41239995\n",
      "Iteration 140, loss = 0.41231733\n",
      "Iteration 141, loss = 0.41227166\n",
      "Iteration 142, loss = 0.41223831\n",
      "Iteration 143, loss = 0.41219911\n",
      "Iteration 144, loss = 0.41214970\n",
      "Iteration 145, loss = 0.41227467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75092189\n",
      "Iteration 2, loss = 0.71641932\n",
      "Iteration 3, loss = 0.68726260\n",
      "Iteration 4, loss = 0.66525902\n",
      "Iteration 5, loss = 0.65048560\n",
      "Iteration 6, loss = 0.63946446\n",
      "Iteration 7, loss = 0.63151104\n",
      "Iteration 8, loss = 0.62750761\n",
      "Iteration 9, loss = 0.62267156\n",
      "Iteration 10, loss = 0.61826583\n",
      "Iteration 11, loss = 0.61395613\n",
      "Iteration 12, loss = 0.60915936\n",
      "Iteration 13, loss = 0.60365656\n",
      "Iteration 14, loss = 0.59896714\n",
      "Iteration 15, loss = 0.59418770\n",
      "Iteration 16, loss = 0.58969690\n",
      "Iteration 17, loss = 0.58550321\n",
      "Iteration 18, loss = 0.58136654\n",
      "Iteration 19, loss = 0.57760081\n",
      "Iteration 20, loss = 0.57381393\n",
      "Iteration 21, loss = 0.57009469\n",
      "Iteration 22, loss = 0.56648420\n",
      "Iteration 23, loss = 0.56284254\n",
      "Iteration 24, loss = 0.55927832\n",
      "Iteration 25, loss = 0.55575058\n",
      "Iteration 26, loss = 0.55244898\n",
      "Iteration 27, loss = 0.54915406\n",
      "Iteration 28, loss = 0.54585866\n",
      "Iteration 29, loss = 0.54268227\n",
      "Iteration 30, loss = 0.53944638\n",
      "Iteration 31, loss = 0.53636218\n",
      "Iteration 32, loss = 0.53365107\n",
      "Iteration 33, loss = 0.53054526\n",
      "Iteration 34, loss = 0.52758342\n",
      "Iteration 35, loss = 0.52474897\n",
      "Iteration 36, loss = 0.52222344\n",
      "Iteration 37, loss = 0.51934380\n",
      "Iteration 38, loss = 0.51680809\n",
      "Iteration 39, loss = 0.51426468\n",
      "Iteration 40, loss = 0.51172860\n",
      "Iteration 41, loss = 0.50915993\n",
      "Iteration 42, loss = 0.50675182\n",
      "Iteration 43, loss = 0.50440073\n",
      "Iteration 44, loss = 0.50204834\n",
      "Iteration 45, loss = 0.49981833\n",
      "Iteration 46, loss = 0.49763779\n",
      "Iteration 47, loss = 0.49540980\n",
      "Iteration 48, loss = 0.49331039\n",
      "Iteration 49, loss = 0.49123797\n",
      "Iteration 50, loss = 0.48930518\n",
      "Iteration 51, loss = 0.48734844\n",
      "Iteration 52, loss = 0.48532969\n",
      "Iteration 53, loss = 0.48334681\n",
      "Iteration 54, loss = 0.48168089\n",
      "Iteration 55, loss = 0.47986909\n",
      "Iteration 56, loss = 0.47806195\n",
      "Iteration 57, loss = 0.47641305\n",
      "Iteration 58, loss = 0.47472507\n",
      "Iteration 59, loss = 0.47321504\n",
      "Iteration 60, loss = 0.47146240\n",
      "Iteration 61, loss = 0.47011668\n",
      "Iteration 62, loss = 0.46866186\n",
      "Iteration 63, loss = 0.46724427\n",
      "Iteration 64, loss = 0.46566092\n",
      "Iteration 65, loss = 0.46439599\n",
      "Iteration 66, loss = 0.46319958\n",
      "Iteration 67, loss = 0.46179100\n",
      "Iteration 68, loss = 0.46060848\n",
      "Iteration 69, loss = 0.45948197\n",
      "Iteration 70, loss = 0.45839625\n",
      "Iteration 71, loss = 0.45727503\n",
      "Iteration 72, loss = 0.45612338\n",
      "Iteration 73, loss = 0.45508677\n",
      "Iteration 74, loss = 0.45432025\n",
      "Iteration 75, loss = 0.45325483\n",
      "Iteration 76, loss = 0.45241022\n",
      "Iteration 77, loss = 0.45151667\n",
      "Iteration 78, loss = 0.45069424\n",
      "Iteration 79, loss = 0.44989593\n",
      "Iteration 80, loss = 0.44922460\n",
      "Iteration 81, loss = 0.44854724\n",
      "Iteration 82, loss = 0.44775775\n",
      "Iteration 83, loss = 0.44733975\n",
      "Iteration 84, loss = 0.44649892\n",
      "Iteration 85, loss = 0.44595709\n",
      "Iteration 86, loss = 0.44535597\n",
      "Iteration 87, loss = 0.44484679\n",
      "Iteration 88, loss = 0.44440890\n",
      "Iteration 89, loss = 0.44387420\n",
      "Iteration 90, loss = 0.44338309\n",
      "Iteration 91, loss = 0.44302764\n",
      "Iteration 92, loss = 0.44259815\n",
      "Iteration 93, loss = 0.44219960\n",
      "Iteration 94, loss = 0.44189916\n",
      "Iteration 95, loss = 0.44150854\n",
      "Iteration 96, loss = 0.44117362\n",
      "Iteration 97, loss = 0.44099979\n",
      "Iteration 98, loss = 0.44077396\n",
      "Iteration 99, loss = 0.44030427\n",
      "Iteration 100, loss = 0.44003512\n",
      "Iteration 101, loss = 0.43987172\n",
      "Iteration 102, loss = 0.43955256\n",
      "Iteration 103, loss = 0.43935445\n",
      "Iteration 104, loss = 0.43922958\n",
      "Iteration 105, loss = 0.43900895\n",
      "Iteration 106, loss = 0.43874202\n",
      "Iteration 107, loss = 0.43860917\n",
      "Iteration 108, loss = 0.43844866\n",
      "Iteration 109, loss = 0.43831142\n",
      "Iteration 110, loss = 0.43820765\n",
      "Iteration 111, loss = 0.43804502\n",
      "Iteration 112, loss = 0.43789268\n",
      "Iteration 113, loss = 0.43780686\n",
      "Iteration 114, loss = 0.43773573\n",
      "Iteration 115, loss = 0.43755824\n",
      "Iteration 116, loss = 0.43747382\n",
      "Iteration 117, loss = 0.43740589\n",
      "Iteration 118, loss = 0.43736984\n",
      "Iteration 119, loss = 0.43720926\n",
      "Iteration 120, loss = 0.43718003\n",
      "Iteration 121, loss = 0.43706854\n",
      "Iteration 122, loss = 0.43706423\n",
      "Iteration 123, loss = 0.43687758\n",
      "Iteration 124, loss = 0.43685740\n",
      "Iteration 125, loss = 0.43683564\n",
      "Iteration 126, loss = 0.43675074\n",
      "Iteration 127, loss = 0.43667877\n",
      "Iteration 128, loss = 0.43660879\n",
      "Iteration 129, loss = 0.43659492\n",
      "Iteration 130, loss = 0.43649161\n",
      "Iteration 131, loss = 0.43649104\n",
      "Iteration 132, loss = 0.43640560\n",
      "Iteration 133, loss = 0.43644081\n",
      "Iteration 134, loss = 0.43635229\n",
      "Iteration 135, loss = 0.43636896\n",
      "Iteration 136, loss = 0.43627426\n",
      "Iteration 137, loss = 0.43626330\n",
      "Iteration 138, loss = 0.43625829\n",
      "Iteration 139, loss = 0.43625771\n",
      "Iteration 140, loss = 0.43614465\n",
      "Iteration 141, loss = 0.43618544\n",
      "Iteration 142, loss = 0.43612035\n",
      "Iteration 143, loss = 0.43604139\n",
      "Iteration 144, loss = 0.43610975\n",
      "Iteration 145, loss = 0.43602720\n",
      "Iteration 146, loss = 0.43607242\n",
      "Iteration 147, loss = 0.43626466\n",
      "Iteration 148, loss = 0.43609673\n",
      "Iteration 149, loss = 0.43607719\n",
      "Iteration 150, loss = 0.43592771\n",
      "Iteration 151, loss = 0.43590065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75265081\n",
      "Iteration 2, loss = 0.71900162\n",
      "Iteration 3, loss = 0.68953380\n",
      "Iteration 4, loss = 0.66886107\n",
      "Iteration 5, loss = 0.65282323\n",
      "Iteration 6, loss = 0.64191148\n",
      "Iteration 7, loss = 0.63418329\n",
      "Iteration 8, loss = 0.62919875\n",
      "Iteration 9, loss = 0.62479202\n",
      "Iteration 10, loss = 0.62045073\n",
      "Iteration 11, loss = 0.61631211\n",
      "Iteration 12, loss = 0.61153053\n",
      "Iteration 13, loss = 0.60681107\n",
      "Iteration 14, loss = 0.60272453\n",
      "Iteration 15, loss = 0.59726733\n",
      "Iteration 16, loss = 0.59283363\n",
      "Iteration 17, loss = 0.58883361\n",
      "Iteration 18, loss = 0.58469976\n",
      "Iteration 19, loss = 0.58102951\n",
      "Iteration 20, loss = 0.57690476\n",
      "Iteration 21, loss = 0.57334314\n",
      "Iteration 22, loss = 0.56973818\n",
      "Iteration 23, loss = 0.56590748\n",
      "Iteration 24, loss = 0.56238074\n",
      "Iteration 25, loss = 0.55898052\n",
      "Iteration 26, loss = 0.55539455\n",
      "Iteration 27, loss = 0.55195097\n",
      "Iteration 28, loss = 0.54875141\n",
      "Iteration 29, loss = 0.54535678\n",
      "Iteration 30, loss = 0.54222082\n",
      "Iteration 31, loss = 0.53897379\n",
      "Iteration 32, loss = 0.53589949\n",
      "Iteration 33, loss = 0.53285597\n",
      "Iteration 34, loss = 0.52978651\n",
      "Iteration 35, loss = 0.52692700\n",
      "Iteration 36, loss = 0.52398244\n",
      "Iteration 37, loss = 0.52109388\n",
      "Iteration 38, loss = 0.51819507\n",
      "Iteration 39, loss = 0.51543303\n",
      "Iteration 40, loss = 0.51276105\n",
      "Iteration 41, loss = 0.51000085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.50746661\n",
      "Iteration 43, loss = 0.50478391\n",
      "Iteration 44, loss = 0.50240909\n",
      "Iteration 45, loss = 0.49976346\n",
      "Iteration 46, loss = 0.49736283\n",
      "Iteration 47, loss = 0.49485917\n",
      "Iteration 48, loss = 0.49246141\n",
      "Iteration 49, loss = 0.49013831\n",
      "Iteration 50, loss = 0.48801366\n",
      "Iteration 51, loss = 0.48563155\n",
      "Iteration 52, loss = 0.48335358\n",
      "Iteration 53, loss = 0.48134793\n",
      "Iteration 54, loss = 0.47926628\n",
      "Iteration 55, loss = 0.47710430\n",
      "Iteration 56, loss = 0.47507383\n",
      "Iteration 57, loss = 0.47317135\n",
      "Iteration 58, loss = 0.47131082\n",
      "Iteration 59, loss = 0.46932769\n",
      "Iteration 60, loss = 0.46746850\n",
      "Iteration 61, loss = 0.46568196\n",
      "Iteration 62, loss = 0.46408918\n",
      "Iteration 63, loss = 0.46229773\n",
      "Iteration 64, loss = 0.46067106\n",
      "Iteration 65, loss = 0.45907436\n",
      "Iteration 66, loss = 0.45757924\n",
      "Iteration 67, loss = 0.45619843\n",
      "Iteration 68, loss = 0.45465705\n",
      "Iteration 69, loss = 0.45326815\n",
      "Iteration 70, loss = 0.45213438\n",
      "Iteration 71, loss = 0.45067791\n",
      "Iteration 72, loss = 0.44947668\n",
      "Iteration 73, loss = 0.44828315\n",
      "Iteration 74, loss = 0.44710112\n",
      "Iteration 75, loss = 0.44600719\n",
      "Iteration 76, loss = 0.44503754\n",
      "Iteration 77, loss = 0.44395837\n",
      "Iteration 78, loss = 0.44311880\n",
      "Iteration 79, loss = 0.44211679\n",
      "Iteration 80, loss = 0.44115536\n",
      "Iteration 81, loss = 0.44037551\n",
      "Iteration 82, loss = 0.43967627\n",
      "Iteration 83, loss = 0.43887541\n",
      "Iteration 84, loss = 0.43806068\n",
      "Iteration 85, loss = 0.43749531\n",
      "Iteration 86, loss = 0.43671965\n",
      "Iteration 87, loss = 0.43611109\n",
      "Iteration 88, loss = 0.43568314\n",
      "Iteration 89, loss = 0.43495711\n",
      "Iteration 90, loss = 0.43440464\n",
      "Iteration 91, loss = 0.43392625\n",
      "Iteration 92, loss = 0.43353261\n",
      "Iteration 93, loss = 0.43292822\n",
      "Iteration 94, loss = 0.43259682\n",
      "Iteration 95, loss = 0.43202721\n",
      "Iteration 96, loss = 0.43166735\n",
      "Iteration 97, loss = 0.43129525\n",
      "Iteration 98, loss = 0.43092399\n",
      "Iteration 99, loss = 0.43055008\n",
      "Iteration 100, loss = 0.43023021\n",
      "Iteration 101, loss = 0.43004350\n",
      "Iteration 102, loss = 0.42965439\n",
      "Iteration 103, loss = 0.42938527\n",
      "Iteration 104, loss = 0.42912704\n",
      "Iteration 105, loss = 0.42884365\n",
      "Iteration 106, loss = 0.42860325\n",
      "Iteration 107, loss = 0.42837105\n",
      "Iteration 108, loss = 0.42817372\n",
      "Iteration 109, loss = 0.42791714\n",
      "Iteration 110, loss = 0.42771390\n",
      "Iteration 111, loss = 0.42759508\n",
      "Iteration 112, loss = 0.42735510\n",
      "Iteration 113, loss = 0.42717692\n",
      "Iteration 114, loss = 0.42702185\n",
      "Iteration 115, loss = 0.42691631\n",
      "Iteration 116, loss = 0.42669167\n",
      "Iteration 117, loss = 0.42654711\n",
      "Iteration 118, loss = 0.42639846\n",
      "Iteration 119, loss = 0.42626641\n",
      "Iteration 120, loss = 0.42611844\n",
      "Iteration 121, loss = 0.42599204\n",
      "Iteration 122, loss = 0.42590106\n",
      "Iteration 123, loss = 0.42575259\n",
      "Iteration 124, loss = 0.42563081\n",
      "Iteration 125, loss = 0.42561935\n",
      "Iteration 126, loss = 0.42540991\n",
      "Iteration 127, loss = 0.42530597\n",
      "Iteration 128, loss = 0.42525287\n",
      "Iteration 129, loss = 0.42513815\n",
      "Iteration 130, loss = 0.42509477\n",
      "Iteration 131, loss = 0.42493604\n",
      "Iteration 132, loss = 0.42494320\n",
      "Iteration 133, loss = 0.42478063\n",
      "Iteration 134, loss = 0.42469343\n",
      "Iteration 135, loss = 0.42461857\n",
      "Iteration 136, loss = 0.42458001\n",
      "Iteration 137, loss = 0.42445235\n",
      "Iteration 138, loss = 0.42439149\n",
      "Iteration 139, loss = 0.42434397\n",
      "Iteration 140, loss = 0.42431029\n",
      "Iteration 141, loss = 0.42418087\n",
      "Iteration 142, loss = 0.42428380\n",
      "Iteration 143, loss = 0.42420921\n",
      "Iteration 144, loss = 0.42403566\n",
      "Iteration 145, loss = 0.42396076\n",
      "Iteration 146, loss = 0.42394518\n",
      "Iteration 147, loss = 0.42389747\n",
      "Iteration 148, loss = 0.42380743\n",
      "Iteration 149, loss = 0.42374315\n",
      "Iteration 150, loss = 0.42363490\n",
      "Iteration 151, loss = 0.42358227\n",
      "Iteration 152, loss = 0.42355925\n",
      "Iteration 153, loss = 0.42365907\n",
      "Iteration 154, loss = 0.42344331\n",
      "Iteration 155, loss = 0.42336873\n",
      "Iteration 156, loss = 0.42335730\n",
      "Iteration 157, loss = 0.42331283\n",
      "Iteration 158, loss = 0.42330618\n",
      "Iteration 159, loss = 0.42335110\n",
      "Iteration 160, loss = 0.42322439\n",
      "Iteration 161, loss = 0.42314946\n",
      "Iteration 162, loss = 0.42312637\n",
      "Iteration 163, loss = 0.42307406\n",
      "Iteration 164, loss = 0.42314486\n",
      "Iteration 165, loss = 0.42304859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75183985\n",
      "Iteration 2, loss = 0.71847798\n",
      "Iteration 3, loss = 0.68878111\n",
      "Iteration 4, loss = 0.66821631\n",
      "Iteration 5, loss = 0.65214328\n",
      "Iteration 6, loss = 0.64143199\n",
      "Iteration 7, loss = 0.63376959\n",
      "Iteration 8, loss = 0.62856575\n",
      "Iteration 9, loss = 0.62416079\n",
      "Iteration 10, loss = 0.61970702\n",
      "Iteration 11, loss = 0.61536278\n",
      "Iteration 12, loss = 0.61060582\n",
      "Iteration 13, loss = 0.60560015\n",
      "Iteration 14, loss = 0.60151998\n",
      "Iteration 15, loss = 0.59599842\n",
      "Iteration 16, loss = 0.59143795\n",
      "Iteration 17, loss = 0.58718816\n",
      "Iteration 18, loss = 0.58313961\n",
      "Iteration 19, loss = 0.57932588\n",
      "Iteration 20, loss = 0.57513540\n",
      "Iteration 21, loss = 0.57118944\n",
      "Iteration 22, loss = 0.56744184\n",
      "Iteration 23, loss = 0.56358456\n",
      "Iteration 24, loss = 0.55992355\n",
      "Iteration 25, loss = 0.55626273\n",
      "Iteration 26, loss = 0.55262883\n",
      "Iteration 27, loss = 0.54892453\n",
      "Iteration 28, loss = 0.54559427\n",
      "Iteration 29, loss = 0.54205761\n",
      "Iteration 30, loss = 0.53871929\n",
      "Iteration 31, loss = 0.53541395\n",
      "Iteration 32, loss = 0.53207380\n",
      "Iteration 33, loss = 0.52883576\n",
      "Iteration 34, loss = 0.52554792\n",
      "Iteration 35, loss = 0.52250505\n",
      "Iteration 36, loss = 0.51943095\n",
      "Iteration 37, loss = 0.51637035\n",
      "Iteration 38, loss = 0.51335945\n",
      "Iteration 39, loss = 0.51039323\n",
      "Iteration 40, loss = 0.50750023\n",
      "Iteration 41, loss = 0.50470879\n",
      "Iteration 42, loss = 0.50192634\n",
      "Iteration 43, loss = 0.49917913\n",
      "Iteration 44, loss = 0.49644973\n",
      "Iteration 45, loss = 0.49391888\n",
      "Iteration 46, loss = 0.49121460\n",
      "Iteration 47, loss = 0.48869123\n",
      "Iteration 48, loss = 0.48616565\n",
      "Iteration 49, loss = 0.48378332\n",
      "Iteration 50, loss = 0.48132227\n",
      "Iteration 51, loss = 0.47895389\n",
      "Iteration 52, loss = 0.47666123\n",
      "Iteration 53, loss = 0.47464064\n",
      "Iteration 54, loss = 0.47225891\n",
      "Iteration 55, loss = 0.47006548\n",
      "Iteration 56, loss = 0.46789435\n",
      "Iteration 57, loss = 0.46597021\n",
      "Iteration 58, loss = 0.46410915\n",
      "Iteration 59, loss = 0.46201334\n",
      "Iteration 60, loss = 0.46004146\n",
      "Iteration 61, loss = 0.45833441\n",
      "Iteration 62, loss = 0.45665349\n",
      "Iteration 63, loss = 0.45491870\n",
      "Iteration 64, loss = 0.45319230\n",
      "Iteration 65, loss = 0.45150504\n",
      "Iteration 66, loss = 0.44999447\n",
      "Iteration 67, loss = 0.44851390\n",
      "Iteration 68, loss = 0.44707434\n",
      "Iteration 69, loss = 0.44557485\n",
      "Iteration 70, loss = 0.44440473\n",
      "Iteration 71, loss = 0.44296848\n",
      "Iteration 72, loss = 0.44175505\n",
      "Iteration 73, loss = 0.44054915\n",
      "Iteration 74, loss = 0.43930894\n",
      "Iteration 75, loss = 0.43821825\n",
      "Iteration 76, loss = 0.43721526\n",
      "Iteration 77, loss = 0.43621614\n",
      "Iteration 78, loss = 0.43529446\n",
      "Iteration 79, loss = 0.43428090\n",
      "Iteration 80, loss = 0.43341259\n",
      "Iteration 81, loss = 0.43255874\n",
      "Iteration 82, loss = 0.43182730\n",
      "Iteration 83, loss = 0.43113877\n",
      "Iteration 84, loss = 0.43034076\n",
      "Iteration 85, loss = 0.42970318\n",
      "Iteration 86, loss = 0.42893076\n",
      "Iteration 87, loss = 0.42841791\n",
      "Iteration 88, loss = 0.42787142\n",
      "Iteration 89, loss = 0.42726165\n",
      "Iteration 90, loss = 0.42672126\n",
      "Iteration 91, loss = 0.42619658\n",
      "Iteration 92, loss = 0.42574901\n",
      "Iteration 93, loss = 0.42526422\n",
      "Iteration 94, loss = 0.42492133\n",
      "Iteration 95, loss = 0.42446351\n",
      "Iteration 96, loss = 0.42413532\n",
      "Iteration 97, loss = 0.42375317\n",
      "Iteration 98, loss = 0.42338776\n",
      "Iteration 99, loss = 0.42309650\n",
      "Iteration 100, loss = 0.42277630\n",
      "Iteration 101, loss = 0.42263989\n",
      "Iteration 102, loss = 0.42226083\n",
      "Iteration 103, loss = 0.42204112\n",
      "Iteration 104, loss = 0.42179156\n",
      "Iteration 105, loss = 0.42155496\n",
      "Iteration 106, loss = 0.42138577\n",
      "Iteration 107, loss = 0.42112459\n",
      "Iteration 108, loss = 0.42099101\n",
      "Iteration 109, loss = 0.42083671\n",
      "Iteration 110, loss = 0.42066997\n",
      "Iteration 111, loss = 0.42047363\n",
      "Iteration 112, loss = 0.42036394\n",
      "Iteration 113, loss = 0.42021509\n",
      "Iteration 114, loss = 0.42008737\n",
      "Iteration 115, loss = 0.41996146\n",
      "Iteration 116, loss = 0.41981574\n",
      "Iteration 117, loss = 0.41971418\n",
      "Iteration 118, loss = 0.41967677\n",
      "Iteration 119, loss = 0.41961509\n",
      "Iteration 120, loss = 0.41948139\n",
      "Iteration 121, loss = 0.41942926\n",
      "Iteration 122, loss = 0.41950872\n",
      "Iteration 123, loss = 0.41924530\n",
      "Iteration 124, loss = 0.41920268\n",
      "Iteration 125, loss = 0.41906932\n",
      "Iteration 126, loss = 0.41900238\n",
      "Iteration 127, loss = 0.41897401\n",
      "Iteration 128, loss = 0.41894395\n",
      "Iteration 129, loss = 0.41887768\n",
      "Iteration 130, loss = 0.41886734\n",
      "Iteration 131, loss = 0.41879328\n",
      "Iteration 132, loss = 0.41873211\n",
      "Iteration 133, loss = 0.41870254\n",
      "Iteration 134, loss = 0.41861995\n",
      "Iteration 135, loss = 0.41858002\n",
      "Iteration 136, loss = 0.41854900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75256678\n",
      "Iteration 2, loss = 0.71801819\n",
      "Iteration 3, loss = 0.68894080\n",
      "Iteration 4, loss = 0.66709225\n",
      "Iteration 5, loss = 0.65079413\n",
      "Iteration 6, loss = 0.64135371\n",
      "Iteration 7, loss = 0.63314362\n",
      "Iteration 8, loss = 0.62829873\n",
      "Iteration 9, loss = 0.62394173\n",
      "Iteration 10, loss = 0.61966015\n",
      "Iteration 11, loss = 0.61535917\n",
      "Iteration 12, loss = 0.61081066\n",
      "Iteration 13, loss = 0.60584062\n",
      "Iteration 14, loss = 0.60174401\n",
      "Iteration 15, loss = 0.59646031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.59210350\n",
      "Iteration 17, loss = 0.58774770\n",
      "Iteration 18, loss = 0.58380461\n",
      "Iteration 19, loss = 0.58026592\n",
      "Iteration 20, loss = 0.57630186\n",
      "Iteration 21, loss = 0.57270455\n",
      "Iteration 22, loss = 0.56898778\n",
      "Iteration 23, loss = 0.56544934\n",
      "Iteration 24, loss = 0.56184779\n",
      "Iteration 25, loss = 0.55837092\n",
      "Iteration 26, loss = 0.55495528\n",
      "Iteration 27, loss = 0.55164160\n",
      "Iteration 28, loss = 0.54849753\n",
      "Iteration 29, loss = 0.54531068\n",
      "Iteration 30, loss = 0.54216915\n",
      "Iteration 31, loss = 0.53917209\n",
      "Iteration 32, loss = 0.53610714\n",
      "Iteration 33, loss = 0.53317774\n",
      "Iteration 34, loss = 0.53029929\n",
      "Iteration 35, loss = 0.52749339\n",
      "Iteration 36, loss = 0.52487935\n",
      "Iteration 37, loss = 0.52205346\n",
      "Iteration 38, loss = 0.51954269\n",
      "Iteration 39, loss = 0.51691113\n",
      "Iteration 40, loss = 0.51441393\n",
      "Iteration 41, loss = 0.51204489\n",
      "Iteration 42, loss = 0.50961347\n",
      "Iteration 43, loss = 0.50723193\n",
      "Iteration 44, loss = 0.50505494\n",
      "Iteration 45, loss = 0.50289127\n",
      "Iteration 46, loss = 0.50059009\n",
      "Iteration 47, loss = 0.49856975\n",
      "Iteration 48, loss = 0.49651597\n",
      "Iteration 49, loss = 0.49449456\n",
      "Iteration 50, loss = 0.49259415\n",
      "Iteration 51, loss = 0.49069761\n",
      "Iteration 52, loss = 0.48882433\n",
      "Iteration 53, loss = 0.48722181\n",
      "Iteration 54, loss = 0.48532211\n",
      "Iteration 55, loss = 0.48360855\n",
      "Iteration 56, loss = 0.48187742\n",
      "Iteration 57, loss = 0.48046591\n",
      "Iteration 58, loss = 0.47907390\n",
      "Iteration 59, loss = 0.47736614\n",
      "Iteration 60, loss = 0.47596982\n",
      "Iteration 61, loss = 0.47443793\n",
      "Iteration 62, loss = 0.47320356\n",
      "Iteration 63, loss = 0.47193187\n",
      "Iteration 64, loss = 0.47062711\n",
      "Iteration 65, loss = 0.46952937\n",
      "Iteration 66, loss = 0.46830515\n",
      "Iteration 67, loss = 0.46713399\n",
      "Iteration 68, loss = 0.46614276\n",
      "Iteration 69, loss = 0.46501569\n",
      "Iteration 70, loss = 0.46402355\n",
      "Iteration 71, loss = 0.46313124\n",
      "Iteration 72, loss = 0.46218698\n",
      "Iteration 73, loss = 0.46125599\n",
      "Iteration 74, loss = 0.46030546\n",
      "Iteration 75, loss = 0.45959472\n",
      "Iteration 76, loss = 0.45882400\n",
      "Iteration 77, loss = 0.45821571\n",
      "Iteration 78, loss = 0.45751240\n",
      "Iteration 79, loss = 0.45671217\n",
      "Iteration 80, loss = 0.45612556\n",
      "Iteration 81, loss = 0.45547064\n",
      "Iteration 82, loss = 0.45494389\n",
      "Iteration 83, loss = 0.45464056\n",
      "Iteration 84, loss = 0.45397899\n",
      "Iteration 85, loss = 0.45350084\n",
      "Iteration 86, loss = 0.45292993\n",
      "Iteration 87, loss = 0.45268650\n",
      "Iteration 88, loss = 0.45229199\n",
      "Iteration 89, loss = 0.45177355\n",
      "Iteration 90, loss = 0.45139634\n",
      "Iteration 91, loss = 0.45106153\n",
      "Iteration 92, loss = 0.45073271\n",
      "Iteration 93, loss = 0.45038178\n",
      "Iteration 94, loss = 0.45015286\n",
      "Iteration 95, loss = 0.44985332\n",
      "Iteration 96, loss = 0.44961838\n",
      "Iteration 97, loss = 0.44938001\n",
      "Iteration 98, loss = 0.44908650\n",
      "Iteration 99, loss = 0.44885823\n",
      "Iteration 100, loss = 0.44865192\n",
      "Iteration 101, loss = 0.44856945\n",
      "Iteration 102, loss = 0.44833462\n",
      "Iteration 103, loss = 0.44820333\n",
      "Iteration 104, loss = 0.44791739\n",
      "Iteration 105, loss = 0.44779364\n",
      "Iteration 106, loss = 0.44768981\n",
      "Iteration 107, loss = 0.44748553\n",
      "Iteration 108, loss = 0.44743307\n",
      "Iteration 109, loss = 0.44731084\n",
      "Iteration 110, loss = 0.44715348\n",
      "Iteration 111, loss = 0.44697175\n",
      "Iteration 112, loss = 0.44686901\n",
      "Iteration 113, loss = 0.44672620\n",
      "Iteration 114, loss = 0.44663888\n",
      "Iteration 115, loss = 0.44658231\n",
      "Iteration 116, loss = 0.44646866\n",
      "Iteration 117, loss = 0.44637434\n",
      "Iteration 118, loss = 0.44631948\n",
      "Iteration 119, loss = 0.44628444\n",
      "Iteration 120, loss = 0.44620496\n",
      "Iteration 121, loss = 0.44608911\n",
      "Iteration 122, loss = 0.44616763\n",
      "Iteration 123, loss = 0.44603097\n",
      "Iteration 124, loss = 0.44590442\n",
      "Iteration 125, loss = 0.44579968\n",
      "Iteration 126, loss = 0.44581168\n",
      "Iteration 127, loss = 0.44571994\n",
      "Iteration 128, loss = 0.44568170\n",
      "Iteration 129, loss = 0.44562495\n",
      "Iteration 130, loss = 0.44564636\n",
      "Iteration 131, loss = 0.44552048\n",
      "Iteration 132, loss = 0.44546124\n",
      "Iteration 133, loss = 0.44546682\n",
      "Iteration 134, loss = 0.44537718\n",
      "Iteration 135, loss = 0.44536650\n",
      "Iteration 136, loss = 0.44535667\n",
      "Iteration 137, loss = 0.44536716\n",
      "Iteration 138, loss = 0.44520460\n",
      "Iteration 139, loss = 0.44522296\n",
      "Iteration 140, loss = 0.44524949\n",
      "Iteration 141, loss = 0.44515753\n",
      "Iteration 142, loss = 0.44524532\n",
      "Iteration 143, loss = 0.44507528\n",
      "Iteration 144, loss = 0.44504946\n",
      "Iteration 145, loss = 0.44503123\n",
      "Iteration 146, loss = 0.44508005\n",
      "Iteration 147, loss = 0.44507702\n",
      "Iteration 148, loss = 0.44491894\n",
      "Iteration 149, loss = 0.44487249\n",
      "Iteration 150, loss = 0.44489042\n",
      "Iteration 151, loss = 0.44487254\n",
      "Iteration 152, loss = 0.44488865\n",
      "Iteration 153, loss = 0.44496846\n",
      "Iteration 154, loss = 0.44480295\n",
      "Iteration 155, loss = 0.44477177\n",
      "Iteration 156, loss = 0.44467947\n",
      "Iteration 157, loss = 0.44468853\n",
      "Iteration 158, loss = 0.44465859\n",
      "Iteration 159, loss = 0.44461437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74975171\n",
      "Iteration 2, loss = 0.71532951\n",
      "Iteration 3, loss = 0.68652202\n",
      "Iteration 4, loss = 0.66333237\n",
      "Iteration 5, loss = 0.64775934\n",
      "Iteration 6, loss = 0.63647382\n",
      "Iteration 7, loss = 0.62841742\n",
      "Iteration 8, loss = 0.62340559\n",
      "Iteration 9, loss = 0.61834730\n",
      "Iteration 10, loss = 0.61360934\n",
      "Iteration 11, loss = 0.60886003\n",
      "Iteration 12, loss = 0.60329364\n",
      "Iteration 13, loss = 0.59720499\n",
      "Iteration 14, loss = 0.59183842\n",
      "Iteration 15, loss = 0.58643043\n",
      "Iteration 16, loss = 0.58132232\n",
      "Iteration 17, loss = 0.57642806\n",
      "Iteration 18, loss = 0.57179739\n",
      "Iteration 19, loss = 0.56740795\n",
      "Iteration 20, loss = 0.56306085\n",
      "Iteration 21, loss = 0.55872438\n",
      "Iteration 22, loss = 0.55447487\n",
      "Iteration 23, loss = 0.55020021\n",
      "Iteration 24, loss = 0.54598447\n",
      "Iteration 25, loss = 0.54190314\n",
      "Iteration 26, loss = 0.53807919\n",
      "Iteration 27, loss = 0.53407811\n",
      "Iteration 28, loss = 0.53023795\n",
      "Iteration 29, loss = 0.52660131\n",
      "Iteration 30, loss = 0.52283458\n",
      "Iteration 31, loss = 0.51934476\n",
      "Iteration 32, loss = 0.51597595\n",
      "Iteration 33, loss = 0.51240387\n",
      "Iteration 34, loss = 0.50901008\n",
      "Iteration 35, loss = 0.50573052\n",
      "Iteration 36, loss = 0.50276760\n",
      "Iteration 37, loss = 0.49953807\n",
      "Iteration 38, loss = 0.49655795\n",
      "Iteration 39, loss = 0.49364228\n",
      "Iteration 40, loss = 0.49071800\n",
      "Iteration 41, loss = 0.48785653\n",
      "Iteration 42, loss = 0.48516011\n",
      "Iteration 43, loss = 0.48252119\n",
      "Iteration 44, loss = 0.47992139\n",
      "Iteration 45, loss = 0.47739978\n",
      "Iteration 46, loss = 0.47493132\n",
      "Iteration 47, loss = 0.47261481\n",
      "Iteration 48, loss = 0.47013841\n",
      "Iteration 49, loss = 0.46782446\n",
      "Iteration 50, loss = 0.46578424\n",
      "Iteration 51, loss = 0.46357233\n",
      "Iteration 52, loss = 0.46153061\n",
      "Iteration 53, loss = 0.45937931\n",
      "Iteration 54, loss = 0.45751815\n",
      "Iteration 55, loss = 0.45563828\n",
      "Iteration 56, loss = 0.45373285\n",
      "Iteration 57, loss = 0.45193713\n",
      "Iteration 58, loss = 0.45019637\n",
      "Iteration 59, loss = 0.44847423\n",
      "Iteration 60, loss = 0.44684394\n",
      "Iteration 61, loss = 0.44525236\n",
      "Iteration 62, loss = 0.44383893\n",
      "Iteration 63, loss = 0.44226921\n",
      "Iteration 64, loss = 0.44085052\n",
      "Iteration 65, loss = 0.43941702\n",
      "Iteration 66, loss = 0.43802657\n",
      "Iteration 67, loss = 0.43675461\n",
      "Iteration 68, loss = 0.43553517\n",
      "Iteration 69, loss = 0.43436927\n",
      "Iteration 70, loss = 0.43324403\n",
      "Iteration 71, loss = 0.43212430\n",
      "Iteration 72, loss = 0.43103146\n",
      "Iteration 73, loss = 0.42996643\n",
      "Iteration 74, loss = 0.42909155\n",
      "Iteration 75, loss = 0.42818370\n",
      "Iteration 76, loss = 0.42724873\n",
      "Iteration 77, loss = 0.42635694\n",
      "Iteration 78, loss = 0.42556430\n",
      "Iteration 79, loss = 0.42483451\n",
      "Iteration 80, loss = 0.42423237\n",
      "Iteration 81, loss = 0.42349207\n",
      "Iteration 82, loss = 0.42273328\n",
      "Iteration 83, loss = 0.42221257\n",
      "Iteration 84, loss = 0.42154353\n",
      "Iteration 85, loss = 0.42099392\n",
      "Iteration 86, loss = 0.42037247\n",
      "Iteration 87, loss = 0.41991092\n",
      "Iteration 88, loss = 0.41953942\n",
      "Iteration 89, loss = 0.41894965\n",
      "Iteration 90, loss = 0.41846539\n",
      "Iteration 91, loss = 0.41820009\n",
      "Iteration 92, loss = 0.41782720\n",
      "Iteration 93, loss = 0.41741887\n",
      "Iteration 94, loss = 0.41719116\n",
      "Iteration 95, loss = 0.41675759\n",
      "Iteration 96, loss = 0.41652522\n",
      "Iteration 97, loss = 0.41618545\n",
      "Iteration 98, loss = 0.41597167\n",
      "Iteration 99, loss = 0.41564332\n",
      "Iteration 100, loss = 0.41537813\n",
      "Iteration 101, loss = 0.41524267\n",
      "Iteration 102, loss = 0.41493115\n",
      "Iteration 103, loss = 0.41478995\n",
      "Iteration 104, loss = 0.41459840\n",
      "Iteration 105, loss = 0.41438148\n",
      "Iteration 106, loss = 0.41419574\n",
      "Iteration 107, loss = 0.41411979\n",
      "Iteration 108, loss = 0.41399581\n",
      "Iteration 109, loss = 0.41379140\n",
      "Iteration 110, loss = 0.41364449\n",
      "Iteration 111, loss = 0.41351035\n",
      "Iteration 112, loss = 0.41340813\n",
      "Iteration 113, loss = 0.41330297\n",
      "Iteration 114, loss = 0.41319203\n",
      "Iteration 115, loss = 0.41309377\n",
      "Iteration 116, loss = 0.41298589\n",
      "Iteration 117, loss = 0.41293201\n",
      "Iteration 118, loss = 0.41284290\n",
      "Iteration 119, loss = 0.41273554\n",
      "Iteration 120, loss = 0.41264361\n",
      "Iteration 121, loss = 0.41258327\n",
      "Iteration 122, loss = 0.41262397\n",
      "Iteration 123, loss = 0.41241572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 124, loss = 0.41249814\n",
      "Iteration 125, loss = 0.41228798\n",
      "Iteration 126, loss = 0.41226921\n",
      "Iteration 127, loss = 0.41219374\n",
      "Iteration 128, loss = 0.41215840\n",
      "Iteration 129, loss = 0.41213984\n",
      "Iteration 130, loss = 0.41201212\n",
      "Iteration 131, loss = 0.41205449\n",
      "Iteration 132, loss = 0.41201893\n",
      "Iteration 133, loss = 0.41199927\n",
      "Iteration 134, loss = 0.41186194\n",
      "Iteration 135, loss = 0.41187429\n",
      "Iteration 136, loss = 0.41179839\n",
      "Iteration 137, loss = 0.41182331\n",
      "Iteration 138, loss = 0.41170396\n",
      "Iteration 139, loss = 0.41174967\n",
      "Iteration 140, loss = 0.41166713\n",
      "Iteration 141, loss = 0.41162163\n",
      "Iteration 142, loss = 0.41158820\n",
      "Iteration 143, loss = 0.41154925\n",
      "Iteration 144, loss = 0.41149992\n",
      "Iteration 145, loss = 0.41162448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75074257\n",
      "Iteration 2, loss = 0.71623946\n",
      "Iteration 3, loss = 0.68708186\n",
      "Iteration 4, loss = 0.66507706\n",
      "Iteration 5, loss = 0.65030209\n",
      "Iteration 6, loss = 0.63927895\n",
      "Iteration 7, loss = 0.63132306\n",
      "Iteration 8, loss = 0.62731676\n",
      "Iteration 9, loss = 0.62247735\n",
      "Iteration 10, loss = 0.61806781\n",
      "Iteration 11, loss = 0.61375406\n",
      "Iteration 12, loss = 0.60895280\n",
      "Iteration 13, loss = 0.60344511\n",
      "Iteration 14, loss = 0.59875066\n",
      "Iteration 15, loss = 0.59396578\n",
      "Iteration 16, loss = 0.58946925\n",
      "Iteration 17, loss = 0.58526949\n",
      "Iteration 18, loss = 0.58112679\n",
      "Iteration 19, loss = 0.57735460\n",
      "Iteration 20, loss = 0.57356119\n",
      "Iteration 21, loss = 0.56983530\n",
      "Iteration 22, loss = 0.56621805\n",
      "Iteration 23, loss = 0.56256928\n",
      "Iteration 24, loss = 0.55899802\n",
      "Iteration 25, loss = 0.55546283\n",
      "Iteration 26, loss = 0.55215430\n",
      "Iteration 27, loss = 0.54885201\n",
      "Iteration 28, loss = 0.54554906\n",
      "Iteration 29, loss = 0.54236551\n",
      "Iteration 30, loss = 0.53912212\n",
      "Iteration 31, loss = 0.53603066\n",
      "Iteration 32, loss = 0.53331251\n",
      "Iteration 33, loss = 0.53019928\n",
      "Iteration 34, loss = 0.52723026\n",
      "Iteration 35, loss = 0.52438904\n",
      "Iteration 36, loss = 0.52185698\n",
      "Iteration 37, loss = 0.51897023\n",
      "Iteration 38, loss = 0.51642837\n",
      "Iteration 39, loss = 0.51387846\n",
      "Iteration 40, loss = 0.51133597\n",
      "Iteration 41, loss = 0.50876112\n",
      "Iteration 42, loss = 0.50634720\n",
      "Iteration 43, loss = 0.50399039\n",
      "Iteration 44, loss = 0.50163231\n",
      "Iteration 45, loss = 0.49939695\n",
      "Iteration 46, loss = 0.49721101\n",
      "Iteration 47, loss = 0.49497790\n",
      "Iteration 48, loss = 0.49287374\n",
      "Iteration 49, loss = 0.49079624\n",
      "Iteration 50, loss = 0.48885935\n",
      "Iteration 51, loss = 0.48689749\n",
      "Iteration 52, loss = 0.48487450\n",
      "Iteration 53, loss = 0.48288711\n",
      "Iteration 54, loss = 0.48121743\n",
      "Iteration 55, loss = 0.47940125\n",
      "Iteration 56, loss = 0.47759003\n",
      "Iteration 57, loss = 0.47593784\n",
      "Iteration 58, loss = 0.47424610\n",
      "Iteration 59, loss = 0.47273234\n",
      "Iteration 60, loss = 0.47097542\n",
      "Iteration 61, loss = 0.46962652\n",
      "Iteration 62, loss = 0.46816851\n",
      "Iteration 63, loss = 0.46674735\n",
      "Iteration 64, loss = 0.46516068\n",
      "Iteration 65, loss = 0.46389251\n",
      "Iteration 66, loss = 0.46269301\n",
      "Iteration 67, loss = 0.46128099\n",
      "Iteration 68, loss = 0.46009539\n",
      "Iteration 69, loss = 0.45896608\n",
      "Iteration 70, loss = 0.45787732\n",
      "Iteration 71, loss = 0.45675303\n",
      "Iteration 72, loss = 0.45559851\n",
      "Iteration 73, loss = 0.45455864\n",
      "Iteration 74, loss = 0.45378935\n",
      "Iteration 75, loss = 0.45272140\n",
      "Iteration 76, loss = 0.45187367\n",
      "Iteration 77, loss = 0.45097778\n",
      "Iteration 78, loss = 0.45015257\n",
      "Iteration 79, loss = 0.44935173\n",
      "Iteration 80, loss = 0.44867772\n",
      "Iteration 81, loss = 0.44799757\n",
      "Iteration 82, loss = 0.44720575\n",
      "Iteration 83, loss = 0.44678540\n",
      "Iteration 84, loss = 0.44594169\n",
      "Iteration 85, loss = 0.44539755\n",
      "Iteration 86, loss = 0.44479431\n",
      "Iteration 87, loss = 0.44428291\n",
      "Iteration 88, loss = 0.44384293\n",
      "Iteration 89, loss = 0.44330573\n",
      "Iteration 90, loss = 0.44281237\n",
      "Iteration 91, loss = 0.44245504\n",
      "Iteration 92, loss = 0.44202357\n",
      "Iteration 93, loss = 0.44162327\n",
      "Iteration 94, loss = 0.44132072\n",
      "Iteration 95, loss = 0.44092791\n",
      "Iteration 96, loss = 0.44059125\n",
      "Iteration 97, loss = 0.44041616\n",
      "Iteration 98, loss = 0.44018830\n",
      "Iteration 99, loss = 0.43971684\n",
      "Iteration 100, loss = 0.43944628\n",
      "Iteration 101, loss = 0.43928157\n",
      "Iteration 102, loss = 0.43896057\n",
      "Iteration 103, loss = 0.43876102\n",
      "Iteration 104, loss = 0.43863464\n",
      "Iteration 105, loss = 0.43841262\n",
      "Iteration 106, loss = 0.43814467\n",
      "Iteration 107, loss = 0.43801068\n",
      "Iteration 108, loss = 0.43784888\n",
      "Iteration 109, loss = 0.43771018\n",
      "Iteration 110, loss = 0.43760542\n",
      "Iteration 111, loss = 0.43744159\n",
      "Iteration 112, loss = 0.43728844\n",
      "Iteration 113, loss = 0.43720111\n",
      "Iteration 114, loss = 0.43712992\n",
      "Iteration 115, loss = 0.43695118\n",
      "Iteration 116, loss = 0.43686540\n",
      "Iteration 117, loss = 0.43679714\n",
      "Iteration 118, loss = 0.43675980\n",
      "Iteration 119, loss = 0.43659845\n",
      "Iteration 120, loss = 0.43656844\n",
      "Iteration 121, loss = 0.43645643\n",
      "Iteration 122, loss = 0.43645141\n",
      "Iteration 123, loss = 0.43626394\n",
      "Iteration 124, loss = 0.43624296\n",
      "Iteration 125, loss = 0.43622071\n",
      "Iteration 126, loss = 0.43613519\n",
      "Iteration 127, loss = 0.43606276\n",
      "Iteration 128, loss = 0.43599222\n",
      "Iteration 129, loss = 0.43597741\n",
      "Iteration 130, loss = 0.43587373\n",
      "Iteration 131, loss = 0.43587286\n",
      "Iteration 132, loss = 0.43578687\n",
      "Iteration 133, loss = 0.43582152\n",
      "Iteration 134, loss = 0.43573253\n",
      "Iteration 135, loss = 0.43574896\n",
      "Iteration 136, loss = 0.43565366\n",
      "Iteration 137, loss = 0.43564216\n",
      "Iteration 138, loss = 0.43563717\n",
      "Iteration 139, loss = 0.43563578\n",
      "Iteration 140, loss = 0.43552241\n",
      "Iteration 141, loss = 0.43556242\n",
      "Iteration 142, loss = 0.43549724\n",
      "Iteration 143, loss = 0.43541796\n",
      "Iteration 144, loss = 0.43548575\n",
      "Iteration 145, loss = 0.43540274\n",
      "Iteration 146, loss = 0.43544772\n",
      "Iteration 147, loss = 0.43564014\n",
      "Iteration 148, loss = 0.43547142\n",
      "Iteration 149, loss = 0.43545182\n",
      "Iteration 150, loss = 0.43530183\n",
      "Iteration 151, loss = 0.43527446\n",
      "Iteration 152, loss = 0.43530037\n",
      "Iteration 153, loss = 0.43519901\n",
      "Iteration 154, loss = 0.43523553\n",
      "Iteration 155, loss = 0.43531521\n",
      "Iteration 156, loss = 0.43521791\n",
      "Iteration 157, loss = 0.43513172\n",
      "Iteration 158, loss = 0.43519005\n",
      "Iteration 159, loss = 0.43516299\n",
      "Iteration 160, loss = 0.43506530\n",
      "Iteration 161, loss = 0.43507731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75247095\n",
      "Iteration 2, loss = 0.71882122\n",
      "Iteration 3, loss = 0.68935235\n",
      "Iteration 4, loss = 0.66867844\n",
      "Iteration 5, loss = 0.65263897\n",
      "Iteration 6, loss = 0.64172520\n",
      "Iteration 7, loss = 0.63399452\n",
      "Iteration 8, loss = 0.62900715\n",
      "Iteration 9, loss = 0.62459716\n",
      "Iteration 10, loss = 0.62025221\n",
      "Iteration 11, loss = 0.61610958\n",
      "Iteration 12, loss = 0.61132355\n",
      "Iteration 13, loss = 0.60659944\n",
      "Iteration 14, loss = 0.60250786\n",
      "Iteration 15, loss = 0.59704522\n",
      "Iteration 16, loss = 0.59260583\n",
      "Iteration 17, loss = 0.58859981\n",
      "Iteration 18, loss = 0.58445969\n",
      "Iteration 19, loss = 0.58078310\n",
      "Iteration 20, loss = 0.57665163\n",
      "Iteration 21, loss = 0.57308322\n",
      "Iteration 22, loss = 0.56947140\n",
      "Iteration 23, loss = 0.56563345\n",
      "Iteration 24, loss = 0.56209948\n",
      "Iteration 25, loss = 0.55869180\n",
      "Iteration 26, loss = 0.55509835\n",
      "Iteration 27, loss = 0.55164736\n",
      "Iteration 28, loss = 0.54844052\n",
      "Iteration 29, loss = 0.54503813\n",
      "Iteration 30, loss = 0.54189469\n",
      "Iteration 31, loss = 0.53864010\n",
      "Iteration 32, loss = 0.53555862\n",
      "Iteration 33, loss = 0.53250784\n",
      "Iteration 34, loss = 0.52943108\n",
      "Iteration 35, loss = 0.52656502\n",
      "Iteration 36, loss = 0.52361315\n",
      "Iteration 37, loss = 0.52071758\n",
      "Iteration 38, loss = 0.51781229\n",
      "Iteration 39, loss = 0.51504398\n",
      "Iteration 40, loss = 0.51236558\n",
      "Iteration 41, loss = 0.50959920\n",
      "Iteration 42, loss = 0.50705919\n",
      "Iteration 43, loss = 0.50437064\n",
      "Iteration 44, loss = 0.50199048\n",
      "Iteration 45, loss = 0.49933908\n",
      "Iteration 46, loss = 0.49693307\n",
      "Iteration 47, loss = 0.49442453\n",
      "Iteration 48, loss = 0.49202111\n",
      "Iteration 49, loss = 0.48969344\n",
      "Iteration 50, loss = 0.48756403\n",
      "Iteration 51, loss = 0.48517746\n",
      "Iteration 52, loss = 0.48289454\n",
      "Iteration 53, loss = 0.48088470\n",
      "Iteration 54, loss = 0.47879827\n",
      "Iteration 55, loss = 0.47663187\n",
      "Iteration 56, loss = 0.47459711\n",
      "Iteration 57, loss = 0.47269062\n",
      "Iteration 58, loss = 0.47082570\n",
      "Iteration 59, loss = 0.46883873\n",
      "Iteration 60, loss = 0.46697526\n",
      "Iteration 61, loss = 0.46518469\n",
      "Iteration 62, loss = 0.46358818\n",
      "Iteration 63, loss = 0.46179276\n",
      "Iteration 64, loss = 0.46016213\n",
      "Iteration 65, loss = 0.45856190\n",
      "Iteration 66, loss = 0.45706310\n",
      "Iteration 67, loss = 0.45567799\n",
      "Iteration 68, loss = 0.45413374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69, loss = 0.45274094\n",
      "Iteration 70, loss = 0.45160355\n",
      "Iteration 71, loss = 0.45014410\n",
      "Iteration 72, loss = 0.44893916\n",
      "Iteration 73, loss = 0.44774224\n",
      "Iteration 74, loss = 0.44655705\n",
      "Iteration 75, loss = 0.44546011\n",
      "Iteration 76, loss = 0.44448693\n",
      "Iteration 77, loss = 0.44340536\n",
      "Iteration 78, loss = 0.44256246\n",
      "Iteration 79, loss = 0.44155773\n",
      "Iteration 80, loss = 0.44059349\n",
      "Iteration 81, loss = 0.43981067\n",
      "Iteration 82, loss = 0.43910887\n",
      "Iteration 83, loss = 0.43830542\n",
      "Iteration 84, loss = 0.43748782\n",
      "Iteration 85, loss = 0.43692068\n",
      "Iteration 86, loss = 0.43614222\n",
      "Iteration 87, loss = 0.43553137\n",
      "Iteration 88, loss = 0.43510163\n",
      "Iteration 89, loss = 0.43437300\n",
      "Iteration 90, loss = 0.43381812\n",
      "Iteration 91, loss = 0.43333841\n",
      "Iteration 92, loss = 0.43294149\n",
      "Iteration 93, loss = 0.43233630\n",
      "Iteration 94, loss = 0.43200307\n",
      "Iteration 95, loss = 0.43143187\n",
      "Iteration 96, loss = 0.43107028\n",
      "Iteration 97, loss = 0.43069647\n",
      "Iteration 98, loss = 0.43032365\n",
      "Iteration 99, loss = 0.42994819\n",
      "Iteration 100, loss = 0.42962719\n",
      "Iteration 101, loss = 0.42943897\n",
      "Iteration 102, loss = 0.42904825\n",
      "Iteration 103, loss = 0.42877834\n",
      "Iteration 104, loss = 0.42851835\n",
      "Iteration 105, loss = 0.42823387\n",
      "Iteration 106, loss = 0.42799181\n",
      "Iteration 107, loss = 0.42775891\n",
      "Iteration 108, loss = 0.42756068\n",
      "Iteration 109, loss = 0.42730283\n",
      "Iteration 110, loss = 0.42709862\n",
      "Iteration 111, loss = 0.42697860\n",
      "Iteration 112, loss = 0.42673788\n",
      "Iteration 113, loss = 0.42655911\n",
      "Iteration 114, loss = 0.42640308\n",
      "Iteration 115, loss = 0.42629626\n",
      "Iteration 116, loss = 0.42607122\n",
      "Iteration 117, loss = 0.42592586\n",
      "Iteration 118, loss = 0.42577631\n",
      "Iteration 119, loss = 0.42564360\n",
      "Iteration 120, loss = 0.42549449\n",
      "Iteration 121, loss = 0.42536753\n",
      "Iteration 122, loss = 0.42527637\n",
      "Iteration 123, loss = 0.42512674\n",
      "Iteration 124, loss = 0.42500402\n",
      "Iteration 125, loss = 0.42499203\n",
      "Iteration 126, loss = 0.42478192\n",
      "Iteration 127, loss = 0.42467764\n",
      "Iteration 128, loss = 0.42462326\n",
      "Iteration 129, loss = 0.42450788\n",
      "Iteration 130, loss = 0.42446427\n",
      "Iteration 131, loss = 0.42430489\n",
      "Iteration 132, loss = 0.42431154\n",
      "Iteration 133, loss = 0.42414800\n",
      "Iteration 134, loss = 0.42406015\n",
      "Iteration 135, loss = 0.42398479\n",
      "Iteration 136, loss = 0.42394528\n",
      "Iteration 137, loss = 0.42381718\n",
      "Iteration 138, loss = 0.42375574\n",
      "Iteration 139, loss = 0.42370743\n",
      "Iteration 140, loss = 0.42367338\n",
      "Iteration 141, loss = 0.42354324\n",
      "Iteration 142, loss = 0.42364559\n",
      "Iteration 143, loss = 0.42357011\n",
      "Iteration 144, loss = 0.42339603\n",
      "Iteration 145, loss = 0.42332031\n",
      "Iteration 146, loss = 0.42330420\n",
      "Iteration 147, loss = 0.42325588\n",
      "Iteration 148, loss = 0.42316517\n",
      "Iteration 149, loss = 0.42310028\n",
      "Iteration 150, loss = 0.42299176\n",
      "Iteration 151, loss = 0.42293848\n",
      "Iteration 152, loss = 0.42291453\n",
      "Iteration 153, loss = 0.42301320\n",
      "Iteration 154, loss = 0.42279724\n",
      "Iteration 155, loss = 0.42272239\n",
      "Iteration 156, loss = 0.42271028\n",
      "Iteration 157, loss = 0.42266523\n",
      "Iteration 158, loss = 0.42265792\n",
      "Iteration 159, loss = 0.42270206\n",
      "Iteration 160, loss = 0.42257461\n",
      "Iteration 161, loss = 0.42249948\n",
      "Iteration 162, loss = 0.42247564\n",
      "Iteration 163, loss = 0.42242287\n",
      "Iteration 164, loss = 0.42249278\n",
      "Iteration 165, loss = 0.42239603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75166090\n",
      "Iteration 2, loss = 0.71829844\n",
      "Iteration 3, loss = 0.68860063\n",
      "Iteration 4, loss = 0.66803455\n",
      "Iteration 5, loss = 0.65195989\n",
      "Iteration 6, loss = 0.64124667\n",
      "Iteration 7, loss = 0.63358190\n",
      "Iteration 8, loss = 0.62837544\n",
      "Iteration 9, loss = 0.62396756\n",
      "Iteration 10, loss = 0.61951036\n",
      "Iteration 11, loss = 0.61516253\n",
      "Iteration 12, loss = 0.61040162\n",
      "Iteration 13, loss = 0.60539166\n",
      "Iteration 14, loss = 0.60130709\n",
      "Iteration 15, loss = 0.59578056\n",
      "Iteration 16, loss = 0.59121500\n",
      "Iteration 17, loss = 0.58695978\n",
      "Iteration 18, loss = 0.58290565\n",
      "Iteration 19, loss = 0.57908611\n",
      "Iteration 20, loss = 0.57488967\n",
      "Iteration 21, loss = 0.57093749\n",
      "Iteration 22, loss = 0.56718359\n",
      "Iteration 23, loss = 0.56331994\n",
      "Iteration 24, loss = 0.55965225\n",
      "Iteration 25, loss = 0.55598456\n",
      "Iteration 26, loss = 0.55234403\n",
      "Iteration 27, loss = 0.54863250\n",
      "Iteration 28, loss = 0.54529570\n",
      "Iteration 29, loss = 0.54175185\n",
      "Iteration 30, loss = 0.53840651\n",
      "Iteration 31, loss = 0.53509427\n",
      "Iteration 32, loss = 0.53174701\n",
      "Iteration 33, loss = 0.52850225\n",
      "Iteration 34, loss = 0.52520736\n",
      "Iteration 35, loss = 0.52215803\n",
      "Iteration 36, loss = 0.51907706\n",
      "Iteration 37, loss = 0.51600965\n",
      "Iteration 38, loss = 0.51299228\n",
      "Iteration 39, loss = 0.51001968\n",
      "Iteration 40, loss = 0.50712029\n",
      "Iteration 41, loss = 0.50432290\n",
      "Iteration 42, loss = 0.50153436\n",
      "Iteration 43, loss = 0.49878100\n",
      "Iteration 44, loss = 0.49604565\n",
      "Iteration 45, loss = 0.49350906\n",
      "Iteration 46, loss = 0.49079919\n",
      "Iteration 47, loss = 0.48827016\n",
      "Iteration 48, loss = 0.48573911\n",
      "Iteration 49, loss = 0.48335145\n",
      "Iteration 50, loss = 0.48088521\n",
      "Iteration 51, loss = 0.47851193\n",
      "Iteration 52, loss = 0.47621403\n",
      "Iteration 53, loss = 0.47418869\n",
      "Iteration 54, loss = 0.47180182\n",
      "Iteration 55, loss = 0.46960346\n",
      "Iteration 56, loss = 0.46742771\n",
      "Iteration 57, loss = 0.46549930\n",
      "Iteration 58, loss = 0.46363346\n",
      "Iteration 59, loss = 0.46153365\n",
      "Iteration 60, loss = 0.45955686\n",
      "Iteration 61, loss = 0.45784567\n",
      "Iteration 62, loss = 0.45616084\n",
      "Iteration 63, loss = 0.45442214\n",
      "Iteration 64, loss = 0.45269132\n",
      "Iteration 65, loss = 0.45100051\n",
      "Iteration 66, loss = 0.44948603\n",
      "Iteration 67, loss = 0.44800086\n",
      "Iteration 68, loss = 0.44655839\n",
      "Iteration 69, loss = 0.44505525\n",
      "Iteration 70, loss = 0.44388170\n",
      "Iteration 71, loss = 0.44244163\n",
      "Iteration 72, loss = 0.44122486\n",
      "Iteration 73, loss = 0.44001571\n",
      "Iteration 74, loss = 0.43877197\n",
      "Iteration 75, loss = 0.43767835\n",
      "Iteration 76, loss = 0.43667213\n",
      "Iteration 77, loss = 0.43567028\n",
      "Iteration 78, loss = 0.43474515\n",
      "Iteration 79, loss = 0.43372886\n",
      "Iteration 80, loss = 0.43285721\n",
      "Iteration 81, loss = 0.43200075\n",
      "Iteration 82, loss = 0.43126664\n",
      "Iteration 83, loss = 0.43057541\n",
      "Iteration 84, loss = 0.42977464\n",
      "Iteration 85, loss = 0.42913467\n",
      "Iteration 86, loss = 0.42835980\n",
      "Iteration 87, loss = 0.42784402\n",
      "Iteration 88, loss = 0.42729560\n",
      "Iteration 89, loss = 0.42668324\n",
      "Iteration 90, loss = 0.42614034\n",
      "Iteration 91, loss = 0.42561410\n",
      "Iteration 92, loss = 0.42516312\n",
      "Iteration 93, loss = 0.42467729\n",
      "Iteration 94, loss = 0.42433215\n",
      "Iteration 95, loss = 0.42387262\n",
      "Iteration 96, loss = 0.42354244\n",
      "Iteration 97, loss = 0.42315816\n",
      "Iteration 98, loss = 0.42279103\n",
      "Iteration 99, loss = 0.42249795\n",
      "Iteration 100, loss = 0.42217631\n",
      "Iteration 101, loss = 0.42203807\n",
      "Iteration 102, loss = 0.42165707\n",
      "Iteration 103, loss = 0.42143631\n",
      "Iteration 104, loss = 0.42118479\n",
      "Iteration 105, loss = 0.42094723\n",
      "Iteration 106, loss = 0.42077622\n",
      "Iteration 107, loss = 0.42051390\n",
      "Iteration 108, loss = 0.42037916\n",
      "Iteration 109, loss = 0.42022303\n",
      "Iteration 110, loss = 0.42005577\n",
      "Iteration 111, loss = 0.41985800\n",
      "Iteration 112, loss = 0.41974724\n",
      "Iteration 113, loss = 0.41959756\n",
      "Iteration 114, loss = 0.41946866\n",
      "Iteration 115, loss = 0.41934139\n",
      "Iteration 116, loss = 0.41919519\n",
      "Iteration 117, loss = 0.41909260\n",
      "Iteration 118, loss = 0.41905434\n",
      "Iteration 119, loss = 0.41899186\n",
      "Iteration 120, loss = 0.41885729\n",
      "Iteration 121, loss = 0.41880463\n",
      "Iteration 122, loss = 0.41888350\n",
      "Iteration 123, loss = 0.41861925\n",
      "Iteration 124, loss = 0.41857595\n",
      "Iteration 125, loss = 0.41844190\n",
      "Iteration 126, loss = 0.41837425\n",
      "Iteration 127, loss = 0.41834546\n",
      "Iteration 128, loss = 0.41831462\n",
      "Iteration 129, loss = 0.41824754\n",
      "Iteration 130, loss = 0.41823707\n",
      "Iteration 131, loss = 0.41816242\n",
      "Iteration 132, loss = 0.41810089\n",
      "Iteration 133, loss = 0.41807060\n",
      "Iteration 134, loss = 0.41798774\n",
      "Iteration 135, loss = 0.41794734\n",
      "Iteration 136, loss = 0.41791577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75238777\n",
      "Iteration 2, loss = 0.71783857\n",
      "Iteration 3, loss = 0.68876032\n",
      "Iteration 4, loss = 0.66691054\n",
      "Iteration 5, loss = 0.65061089\n",
      "Iteration 6, loss = 0.64116853\n",
      "Iteration 7, loss = 0.63295609\n",
      "Iteration 8, loss = 0.62810841\n",
      "Iteration 9, loss = 0.62374831\n",
      "Iteration 10, loss = 0.61946306\n",
      "Iteration 11, loss = 0.61515822\n",
      "Iteration 12, loss = 0.61060539\n",
      "Iteration 13, loss = 0.60563065\n",
      "Iteration 14, loss = 0.60152902\n",
      "Iteration 15, loss = 0.59623998\n",
      "Iteration 16, loss = 0.59187749\n",
      "Iteration 17, loss = 0.58751569\n",
      "Iteration 18, loss = 0.58356633\n",
      "Iteration 19, loss = 0.58002127\n",
      "Iteration 20, loss = 0.57605051\n",
      "Iteration 21, loss = 0.57244648\n",
      "Iteration 22, loss = 0.56872258\n",
      "Iteration 23, loss = 0.56517693\n",
      "Iteration 24, loss = 0.56156791\n",
      "Iteration 25, loss = 0.55808344\n",
      "Iteration 26, loss = 0.55466017\n",
      "Iteration 27, loss = 0.55133888\n",
      "Iteration 28, loss = 0.54818720\n",
      "Iteration 29, loss = 0.54499252\n",
      "Iteration 30, loss = 0.54184320\n",
      "Iteration 31, loss = 0.53883855\n",
      "Iteration 32, loss = 0.53576559\n",
      "Iteration 33, loss = 0.53282863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.52994260\n",
      "Iteration 35, loss = 0.52712918\n",
      "Iteration 36, loss = 0.52450786\n",
      "Iteration 37, loss = 0.52167451\n",
      "Iteration 38, loss = 0.51915657\n",
      "Iteration 39, loss = 0.51651801\n",
      "Iteration 40, loss = 0.51401390\n",
      "Iteration 41, loss = 0.51163839\n",
      "Iteration 42, loss = 0.50920047\n",
      "Iteration 43, loss = 0.50681232\n",
      "Iteration 44, loss = 0.50462937\n",
      "Iteration 45, loss = 0.50245949\n",
      "Iteration 46, loss = 0.50015253\n",
      "Iteration 47, loss = 0.49812650\n",
      "Iteration 48, loss = 0.49606712\n",
      "Iteration 49, loss = 0.49404044\n",
      "Iteration 50, loss = 0.49213480\n",
      "Iteration 51, loss = 0.49023354\n",
      "Iteration 52, loss = 0.48835543\n",
      "Iteration 53, loss = 0.48674827\n",
      "Iteration 54, loss = 0.48484354\n",
      "Iteration 55, loss = 0.48312528\n",
      "Iteration 56, loss = 0.48138984\n",
      "Iteration 57, loss = 0.47997462\n",
      "Iteration 58, loss = 0.47857792\n",
      "Iteration 59, loss = 0.47686662\n",
      "Iteration 60, loss = 0.47546599\n",
      "Iteration 61, loss = 0.47393004\n",
      "Iteration 62, loss = 0.47269207\n",
      "Iteration 63, loss = 0.47141719\n",
      "Iteration 64, loss = 0.47010807\n",
      "Iteration 65, loss = 0.46900735\n",
      "Iteration 66, loss = 0.46777966\n",
      "Iteration 67, loss = 0.46660445\n",
      "Iteration 68, loss = 0.46561105\n",
      "Iteration 69, loss = 0.46448036\n",
      "Iteration 70, loss = 0.46348500\n",
      "Iteration 71, loss = 0.46258984\n",
      "Iteration 72, loss = 0.46164276\n",
      "Iteration 73, loss = 0.46070885\n",
      "Iteration 74, loss = 0.45975511\n",
      "Iteration 75, loss = 0.45904215\n",
      "Iteration 76, loss = 0.45826874\n",
      "Iteration 77, loss = 0.45765807\n",
      "Iteration 78, loss = 0.45695186\n",
      "Iteration 79, loss = 0.45614956\n",
      "Iteration 80, loss = 0.45556026\n",
      "Iteration 81, loss = 0.45490327\n",
      "Iteration 82, loss = 0.45437451\n",
      "Iteration 83, loss = 0.45406919\n",
      "Iteration 84, loss = 0.45340556\n",
      "Iteration 85, loss = 0.45292569\n",
      "Iteration 86, loss = 0.45235282\n",
      "Iteration 87, loss = 0.45210734\n",
      "Iteration 88, loss = 0.45171165\n",
      "Iteration 89, loss = 0.45119165\n",
      "Iteration 90, loss = 0.45081286\n",
      "Iteration 91, loss = 0.45047694\n",
      "Iteration 92, loss = 0.45014584\n",
      "Iteration 93, loss = 0.44979416\n",
      "Iteration 94, loss = 0.44956407\n",
      "Iteration 95, loss = 0.44926350\n",
      "Iteration 96, loss = 0.44902762\n",
      "Iteration 97, loss = 0.44878790\n",
      "Iteration 98, loss = 0.44849348\n",
      "Iteration 99, loss = 0.44826408\n",
      "Iteration 100, loss = 0.44805709\n",
      "Iteration 101, loss = 0.44797363\n",
      "Iteration 102, loss = 0.44773794\n",
      "Iteration 103, loss = 0.44760606\n",
      "Iteration 104, loss = 0.44731898\n",
      "Iteration 105, loss = 0.44719480\n",
      "Iteration 106, loss = 0.44709024\n",
      "Iteration 107, loss = 0.44688535\n",
      "Iteration 108, loss = 0.44683216\n",
      "Iteration 109, loss = 0.44670878\n",
      "Iteration 110, loss = 0.44655148\n",
      "Iteration 111, loss = 0.44636929\n",
      "Iteration 112, loss = 0.44626571\n",
      "Iteration 113, loss = 0.44612274\n",
      "Iteration 114, loss = 0.44603529\n",
      "Iteration 115, loss = 0.44597783\n",
      "Iteration 116, loss = 0.44586394\n",
      "Iteration 117, loss = 0.44576929\n",
      "Iteration 118, loss = 0.44571384\n",
      "Iteration 119, loss = 0.44567870\n",
      "Iteration 120, loss = 0.44559851\n",
      "Iteration 121, loss = 0.44548264\n",
      "Iteration 122, loss = 0.44556133\n",
      "Iteration 123, loss = 0.44542374\n",
      "Iteration 124, loss = 0.44529714\n",
      "Iteration 125, loss = 0.44519204\n",
      "Iteration 126, loss = 0.44520361\n",
      "Iteration 127, loss = 0.44511200\n",
      "Iteration 128, loss = 0.44507326\n",
      "Iteration 129, loss = 0.44501627\n",
      "Iteration 130, loss = 0.44503739\n",
      "Iteration 131, loss = 0.44491141\n",
      "Iteration 132, loss = 0.44485190\n",
      "Iteration 133, loss = 0.44485706\n",
      "Iteration 134, loss = 0.44476738\n",
      "Iteration 135, loss = 0.44475630\n",
      "Iteration 136, loss = 0.44474630\n",
      "Iteration 137, loss = 0.44475641\n",
      "Iteration 138, loss = 0.44459408\n",
      "Iteration 139, loss = 0.44461189\n",
      "Iteration 140, loss = 0.44463814\n",
      "Iteration 141, loss = 0.44454612\n",
      "Iteration 142, loss = 0.44463381\n",
      "Iteration 143, loss = 0.44446345\n",
      "Iteration 144, loss = 0.44443731\n",
      "Iteration 145, loss = 0.44441895\n",
      "Iteration 146, loss = 0.44446716\n",
      "Iteration 147, loss = 0.44446432\n",
      "Iteration 148, loss = 0.44430587\n",
      "Iteration 149, loss = 0.44425918\n",
      "Iteration 150, loss = 0.44427713\n",
      "Iteration 151, loss = 0.44425941\n",
      "Iteration 152, loss = 0.44427467\n",
      "Iteration 153, loss = 0.44435401\n",
      "Iteration 154, loss = 0.44418867\n",
      "Iteration 155, loss = 0.44415731\n",
      "Iteration 156, loss = 0.44406490\n",
      "Iteration 157, loss = 0.44407393\n",
      "Iteration 158, loss = 0.44404343\n",
      "Iteration 159, loss = 0.44399899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973378\n",
      "Iteration 2, loss = 0.71531152\n",
      "Iteration 3, loss = 0.68650393\n",
      "Iteration 4, loss = 0.66331417\n",
      "Iteration 5, loss = 0.64774098\n",
      "Iteration 6, loss = 0.63645526\n",
      "Iteration 7, loss = 0.62839862\n",
      "Iteration 8, loss = 0.62338652\n",
      "Iteration 9, loss = 0.61832790\n",
      "Iteration 10, loss = 0.61358958\n",
      "Iteration 11, loss = 0.60883986\n",
      "Iteration 12, loss = 0.60327302\n",
      "Iteration 13, loss = 0.59718387\n",
      "Iteration 14, loss = 0.59181679\n",
      "Iteration 15, loss = 0.58640824\n",
      "Iteration 16, loss = 0.58129955\n",
      "Iteration 17, loss = 0.57640464\n",
      "Iteration 18, loss = 0.57177334\n",
      "Iteration 19, loss = 0.56738321\n",
      "Iteration 20, loss = 0.56303541\n",
      "Iteration 21, loss = 0.55869821\n",
      "Iteration 22, loss = 0.55444797\n",
      "Iteration 23, loss = 0.55017252\n",
      "Iteration 24, loss = 0.54595599\n",
      "Iteration 25, loss = 0.54187386\n",
      "Iteration 26, loss = 0.53804911\n",
      "Iteration 27, loss = 0.53404718\n",
      "Iteration 28, loss = 0.53020618\n",
      "Iteration 29, loss = 0.52656872\n",
      "Iteration 30, loss = 0.52280114\n",
      "Iteration 31, loss = 0.51931049\n",
      "Iteration 32, loss = 0.51594085\n",
      "Iteration 33, loss = 0.51236792\n",
      "Iteration 34, loss = 0.50897328\n",
      "Iteration 35, loss = 0.50569292\n",
      "Iteration 36, loss = 0.50272920\n",
      "Iteration 37, loss = 0.49949887\n",
      "Iteration 38, loss = 0.49651797\n",
      "Iteration 39, loss = 0.49360152\n",
      "Iteration 40, loss = 0.49067648\n",
      "Iteration 41, loss = 0.48781428\n",
      "Iteration 42, loss = 0.48511713\n",
      "Iteration 43, loss = 0.48247753\n",
      "Iteration 44, loss = 0.47987701\n",
      "Iteration 45, loss = 0.47735479\n",
      "Iteration 46, loss = 0.47488566\n",
      "Iteration 47, loss = 0.47256855\n",
      "Iteration 48, loss = 0.47009154\n",
      "Iteration 49, loss = 0.46777695\n",
      "Iteration 50, loss = 0.46573622\n",
      "Iteration 51, loss = 0.46352372\n",
      "Iteration 52, loss = 0.46148144\n",
      "Iteration 53, loss = 0.45932963\n",
      "Iteration 54, loss = 0.45746796\n",
      "Iteration 55, loss = 0.45558758\n",
      "Iteration 56, loss = 0.45368167\n",
      "Iteration 57, loss = 0.45188549\n",
      "Iteration 58, loss = 0.45014428\n",
      "Iteration 59, loss = 0.44842171\n",
      "Iteration 60, loss = 0.44679092\n",
      "Iteration 61, loss = 0.44519893\n",
      "Iteration 62, loss = 0.44378505\n",
      "Iteration 63, loss = 0.44221490\n",
      "Iteration 64, loss = 0.44079588\n",
      "Iteration 65, loss = 0.43936197\n",
      "Iteration 66, loss = 0.43797113\n",
      "Iteration 67, loss = 0.43669876\n",
      "Iteration 68, loss = 0.43547896\n",
      "Iteration 69, loss = 0.43431270\n",
      "Iteration 70, loss = 0.43318707\n",
      "Iteration 71, loss = 0.43206701\n",
      "Iteration 72, loss = 0.43097379\n",
      "Iteration 73, loss = 0.42990838\n",
      "Iteration 74, loss = 0.42903320\n",
      "Iteration 75, loss = 0.42812498\n",
      "Iteration 76, loss = 0.42718966\n",
      "Iteration 77, loss = 0.42629759\n",
      "Iteration 78, loss = 0.42550461\n",
      "Iteration 79, loss = 0.42477451\n",
      "Iteration 80, loss = 0.42417208\n",
      "Iteration 81, loss = 0.42343148\n",
      "Iteration 82, loss = 0.42267237\n",
      "Iteration 83, loss = 0.42215138\n",
      "Iteration 84, loss = 0.42148203\n",
      "Iteration 85, loss = 0.42093217\n",
      "Iteration 86, loss = 0.42031044\n",
      "Iteration 87, loss = 0.41984866\n",
      "Iteration 88, loss = 0.41947695\n",
      "Iteration 89, loss = 0.41888691\n",
      "Iteration 90, loss = 0.41840239\n",
      "Iteration 91, loss = 0.41813694\n",
      "Iteration 92, loss = 0.41776387\n",
      "Iteration 93, loss = 0.41735532\n",
      "Iteration 94, loss = 0.41712740\n",
      "Iteration 95, loss = 0.41669368\n",
      "Iteration 96, loss = 0.41646112\n",
      "Iteration 97, loss = 0.41612124\n",
      "Iteration 98, loss = 0.41590725\n",
      "Iteration 99, loss = 0.41557875\n",
      "Iteration 100, loss = 0.41531344\n",
      "Iteration 101, loss = 0.41517789\n",
      "Iteration 102, loss = 0.41486621\n",
      "Iteration 103, loss = 0.41472497\n",
      "Iteration 104, loss = 0.41453324\n",
      "Iteration 105, loss = 0.41431625\n",
      "Iteration 106, loss = 0.41413040\n",
      "Iteration 107, loss = 0.41405440\n",
      "Iteration 108, loss = 0.41393034\n",
      "Iteration 109, loss = 0.41372583\n",
      "Iteration 110, loss = 0.41357883\n",
      "Iteration 111, loss = 0.41344462\n",
      "Iteration 112, loss = 0.41334234\n",
      "Iteration 113, loss = 0.41323710\n",
      "Iteration 114, loss = 0.41312619\n",
      "Iteration 115, loss = 0.41302784\n",
      "Iteration 116, loss = 0.41291992\n",
      "Iteration 117, loss = 0.41286605\n",
      "Iteration 118, loss = 0.41277684\n",
      "Iteration 119, loss = 0.41266948\n",
      "Iteration 120, loss = 0.41257750\n",
      "Iteration 121, loss = 0.41251713\n",
      "Iteration 122, loss = 0.41255783\n",
      "Iteration 123, loss = 0.41234952\n",
      "Iteration 124, loss = 0.41243192\n",
      "Iteration 125, loss = 0.41222176\n",
      "Iteration 126, loss = 0.41220297\n",
      "Iteration 127, loss = 0.41212749\n",
      "Iteration 128, loss = 0.41209214\n",
      "Iteration 129, loss = 0.41207354\n",
      "Iteration 130, loss = 0.41194583\n",
      "Iteration 131, loss = 0.41198819\n",
      "Iteration 132, loss = 0.41195264\n",
      "Iteration 133, loss = 0.41193297\n",
      "Iteration 134, loss = 0.41179562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 135, loss = 0.41180800\n",
      "Iteration 136, loss = 0.41173206\n",
      "Iteration 137, loss = 0.41175696\n",
      "Iteration 138, loss = 0.41163762\n",
      "Iteration 139, loss = 0.41168329\n",
      "Iteration 140, loss = 0.41160074\n",
      "Iteration 141, loss = 0.41155525\n",
      "Iteration 142, loss = 0.41152180\n",
      "Iteration 143, loss = 0.41148287\n",
      "Iteration 144, loss = 0.41143353\n",
      "Iteration 145, loss = 0.41155804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072464\n",
      "Iteration 2, loss = 0.71622147\n",
      "Iteration 3, loss = 0.68706378\n",
      "Iteration 4, loss = 0.66505886\n",
      "Iteration 5, loss = 0.65028373\n",
      "Iteration 6, loss = 0.63926039\n",
      "Iteration 7, loss = 0.63130425\n",
      "Iteration 8, loss = 0.62729766\n",
      "Iteration 9, loss = 0.62245792\n",
      "Iteration 10, loss = 0.61804800\n",
      "Iteration 11, loss = 0.61373384\n",
      "Iteration 12, loss = 0.60893213\n",
      "Iteration 13, loss = 0.60342394\n",
      "Iteration 14, loss = 0.59872899\n",
      "Iteration 15, loss = 0.59394356\n",
      "Iteration 16, loss = 0.58944645\n",
      "Iteration 17, loss = 0.58524608\n",
      "Iteration 18, loss = 0.58110277\n",
      "Iteration 19, loss = 0.57732994\n",
      "Iteration 20, loss = 0.57353587\n",
      "Iteration 21, loss = 0.56980930\n",
      "Iteration 22, loss = 0.56619137\n",
      "Iteration 23, loss = 0.56254190\n",
      "Iteration 24, loss = 0.55896992\n",
      "Iteration 25, loss = 0.55543399\n",
      "Iteration 26, loss = 0.55212476\n",
      "Iteration 27, loss = 0.54882173\n",
      "Iteration 28, loss = 0.54551802\n",
      "Iteration 29, loss = 0.54233375\n",
      "Iteration 30, loss = 0.53908961\n",
      "Iteration 31, loss = 0.53599742\n",
      "Iteration 32, loss = 0.53327856\n",
      "Iteration 33, loss = 0.53016458\n",
      "Iteration 34, loss = 0.52719484\n",
      "Iteration 35, loss = 0.52435293\n",
      "Iteration 36, loss = 0.52182022\n",
      "Iteration 37, loss = 0.51893275\n",
      "Iteration 38, loss = 0.51639027\n",
      "Iteration 39, loss = 0.51383970\n",
      "Iteration 40, loss = 0.51129656\n",
      "Iteration 41, loss = 0.50872109\n",
      "Iteration 42, loss = 0.50630658\n",
      "Iteration 43, loss = 0.50394919\n",
      "Iteration 44, loss = 0.50159053\n",
      "Iteration 45, loss = 0.49935462\n",
      "Iteration 46, loss = 0.49716814\n",
      "Iteration 47, loss = 0.49493450\n",
      "Iteration 48, loss = 0.49282986\n",
      "Iteration 49, loss = 0.49075185\n",
      "Iteration 50, loss = 0.48881453\n",
      "Iteration 51, loss = 0.48685215\n",
      "Iteration 52, loss = 0.48482872\n",
      "Iteration 53, loss = 0.48284087\n",
      "Iteration 54, loss = 0.48117080\n",
      "Iteration 55, loss = 0.47935418\n",
      "Iteration 56, loss = 0.47754254\n",
      "Iteration 57, loss = 0.47589000\n",
      "Iteration 58, loss = 0.47419789\n",
      "Iteration 59, loss = 0.47268374\n",
      "Iteration 60, loss = 0.47092638\n",
      "Iteration 61, loss = 0.46957715\n",
      "Iteration 62, loss = 0.46811881\n",
      "Iteration 63, loss = 0.46669728\n",
      "Iteration 64, loss = 0.46511027\n",
      "Iteration 65, loss = 0.46384177\n",
      "Iteration 66, loss = 0.46264194\n",
      "Iteration 67, loss = 0.46122956\n",
      "Iteration 68, loss = 0.46004365\n",
      "Iteration 69, loss = 0.45891405\n",
      "Iteration 70, loss = 0.45782497\n",
      "Iteration 71, loss = 0.45670036\n",
      "Iteration 72, loss = 0.45554554\n",
      "Iteration 73, loss = 0.45450533\n",
      "Iteration 74, loss = 0.45373575\n",
      "Iteration 75, loss = 0.45266754\n",
      "Iteration 76, loss = 0.45181949\n",
      "Iteration 77, loss = 0.45092335\n",
      "Iteration 78, loss = 0.45009785\n",
      "Iteration 79, loss = 0.44929675\n",
      "Iteration 80, loss = 0.44862246\n",
      "Iteration 81, loss = 0.44794201\n",
      "Iteration 82, loss = 0.44714995\n",
      "Iteration 83, loss = 0.44672935\n",
      "Iteration 84, loss = 0.44588534\n",
      "Iteration 85, loss = 0.44534096\n",
      "Iteration 86, loss = 0.44473749\n",
      "Iteration 87, loss = 0.44422586\n",
      "Iteration 88, loss = 0.44378566\n",
      "Iteration 89, loss = 0.44324820\n",
      "Iteration 90, loss = 0.44275460\n",
      "Iteration 91, loss = 0.44239707\n",
      "Iteration 92, loss = 0.44196539\n",
      "Iteration 93, loss = 0.44156490\n",
      "Iteration 94, loss = 0.44126214\n",
      "Iteration 95, loss = 0.44086909\n",
      "Iteration 96, loss = 0.44053224\n",
      "Iteration 97, loss = 0.44035701\n",
      "Iteration 98, loss = 0.44012894\n",
      "Iteration 99, loss = 0.43965729\n",
      "Iteration 100, loss = 0.43938658\n",
      "Iteration 101, loss = 0.43922173\n",
      "Iteration 102, loss = 0.43890053\n",
      "Iteration 103, loss = 0.43870082\n",
      "Iteration 104, loss = 0.43857428\n",
      "Iteration 105, loss = 0.43835211\n",
      "Iteration 106, loss = 0.43808405\n",
      "Iteration 107, loss = 0.43794993\n",
      "Iteration 108, loss = 0.43778799\n",
      "Iteration 109, loss = 0.43764913\n",
      "Iteration 110, loss = 0.43754426\n",
      "Iteration 111, loss = 0.43738030\n",
      "Iteration 112, loss = 0.43722706\n",
      "Iteration 113, loss = 0.43713956\n",
      "Iteration 114, loss = 0.43706836\n",
      "Iteration 115, loss = 0.43688948\n",
      "Iteration 116, loss = 0.43680355\n",
      "Iteration 117, loss = 0.43673524\n",
      "Iteration 118, loss = 0.43669776\n",
      "Iteration 119, loss = 0.43653632\n",
      "Iteration 120, loss = 0.43650622\n",
      "Iteration 121, loss = 0.43639414\n",
      "Iteration 122, loss = 0.43638905\n",
      "Iteration 123, loss = 0.43620149\n",
      "Iteration 124, loss = 0.43618041\n",
      "Iteration 125, loss = 0.43615809\n",
      "Iteration 126, loss = 0.43607250\n",
      "Iteration 127, loss = 0.43600001\n",
      "Iteration 128, loss = 0.43592941\n",
      "Iteration 129, loss = 0.43591449\n",
      "Iteration 130, loss = 0.43581076\n",
      "Iteration 131, loss = 0.43580986\n",
      "Iteration 132, loss = 0.43572380\n",
      "Iteration 133, loss = 0.43575837\n",
      "Iteration 134, loss = 0.43566933\n",
      "Iteration 135, loss = 0.43568572\n",
      "Iteration 136, loss = 0.43559035\n",
      "Iteration 137, loss = 0.43557878\n",
      "Iteration 138, loss = 0.43557378\n",
      "Iteration 139, loss = 0.43557230\n",
      "Iteration 140, loss = 0.43545888\n",
      "Iteration 141, loss = 0.43549881\n",
      "Iteration 142, loss = 0.43543360\n",
      "Iteration 143, loss = 0.43535428\n",
      "Iteration 144, loss = 0.43542199\n",
      "Iteration 145, loss = 0.43533893\n",
      "Iteration 146, loss = 0.43538387\n",
      "Iteration 147, loss = 0.43557630\n",
      "Iteration 148, loss = 0.43540749\n",
      "Iteration 149, loss = 0.43538787\n",
      "Iteration 150, loss = 0.43523781\n",
      "Iteration 151, loss = 0.43521040\n",
      "Iteration 152, loss = 0.43523629\n",
      "Iteration 153, loss = 0.43513487\n",
      "Iteration 154, loss = 0.43517132\n",
      "Iteration 155, loss = 0.43525089\n",
      "Iteration 156, loss = 0.43515360\n",
      "Iteration 157, loss = 0.43506742\n",
      "Iteration 158, loss = 0.43512570\n",
      "Iteration 159, loss = 0.43509856\n",
      "Iteration 160, loss = 0.43500087\n",
      "Iteration 161, loss = 0.43501283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245305\n",
      "Iteration 2, loss = 0.71880328\n",
      "Iteration 3, loss = 0.68933433\n",
      "Iteration 4, loss = 0.66866033\n",
      "Iteration 5, loss = 0.65262072\n",
      "Iteration 6, loss = 0.64170675\n",
      "Iteration 7, loss = 0.63397583\n",
      "Iteration 8, loss = 0.62898817\n",
      "Iteration 9, loss = 0.62457784\n",
      "Iteration 10, loss = 0.62023252\n",
      "Iteration 11, loss = 0.61608946\n",
      "Iteration 12, loss = 0.61130297\n",
      "Iteration 13, loss = 0.60657838\n",
      "Iteration 14, loss = 0.60248630\n",
      "Iteration 15, loss = 0.59702309\n",
      "Iteration 16, loss = 0.59258314\n",
      "Iteration 17, loss = 0.58857651\n",
      "Iteration 18, loss = 0.58443576\n",
      "Iteration 19, loss = 0.58075852\n",
      "Iteration 20, loss = 0.57662637\n",
      "Iteration 21, loss = 0.57305727\n",
      "Iteration 22, loss = 0.56944476\n",
      "Iteration 23, loss = 0.56560607\n",
      "Iteration 24, loss = 0.56207137\n",
      "Iteration 25, loss = 0.55866294\n",
      "Iteration 26, loss = 0.55506874\n",
      "Iteration 27, loss = 0.55161699\n",
      "Iteration 28, loss = 0.54840941\n",
      "Iteration 29, loss = 0.54500624\n",
      "Iteration 30, loss = 0.54186204\n",
      "Iteration 31, loss = 0.53860668\n",
      "Iteration 32, loss = 0.53552448\n",
      "Iteration 33, loss = 0.53247296\n",
      "Iteration 34, loss = 0.52939545\n",
      "Iteration 35, loss = 0.52652872\n",
      "Iteration 36, loss = 0.52357612\n",
      "Iteration 37, loss = 0.52067983\n",
      "Iteration 38, loss = 0.51777389\n",
      "Iteration 39, loss = 0.51500494\n",
      "Iteration 40, loss = 0.51232588\n",
      "Iteration 41, loss = 0.50955887\n",
      "Iteration 42, loss = 0.50701827\n",
      "Iteration 43, loss = 0.50432913\n",
      "Iteration 44, loss = 0.50194842\n",
      "Iteration 45, loss = 0.49929643\n",
      "Iteration 46, loss = 0.49688987\n",
      "Iteration 47, loss = 0.49438083\n",
      "Iteration 48, loss = 0.49197684\n",
      "Iteration 49, loss = 0.48964870\n",
      "Iteration 50, loss = 0.48751879\n",
      "Iteration 51, loss = 0.48513177\n",
      "Iteration 52, loss = 0.48284835\n",
      "Iteration 53, loss = 0.48083807\n",
      "Iteration 54, loss = 0.47875116\n",
      "Iteration 55, loss = 0.47658430\n",
      "Iteration 56, loss = 0.47454910\n",
      "Iteration 57, loss = 0.47264221\n",
      "Iteration 58, loss = 0.47077684\n",
      "Iteration 59, loss = 0.46878947\n",
      "Iteration 60, loss = 0.46692557\n",
      "Iteration 61, loss = 0.46513459\n",
      "Iteration 62, loss = 0.46353769\n",
      "Iteration 63, loss = 0.46174185\n",
      "Iteration 64, loss = 0.46011082\n",
      "Iteration 65, loss = 0.45851023\n",
      "Iteration 66, loss = 0.45701104\n",
      "Iteration 67, loss = 0.45562550\n",
      "Iteration 68, loss = 0.45408095\n",
      "Iteration 69, loss = 0.45268775\n",
      "Iteration 70, loss = 0.45154998\n",
      "Iteration 71, loss = 0.45009022\n",
      "Iteration 72, loss = 0.44888490\n",
      "Iteration 73, loss = 0.44768763\n",
      "Iteration 74, loss = 0.44650211\n",
      "Iteration 75, loss = 0.44540486\n",
      "Iteration 76, loss = 0.44443132\n",
      "Iteration 77, loss = 0.44334950\n",
      "Iteration 78, loss = 0.44250625\n",
      "Iteration 79, loss = 0.44150125\n",
      "Iteration 80, loss = 0.44053670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 0.43975358\n",
      "Iteration 82, loss = 0.43905151\n",
      "Iteration 83, loss = 0.43824780\n",
      "Iteration 84, loss = 0.43742990\n",
      "Iteration 85, loss = 0.43686257\n",
      "Iteration 86, loss = 0.43608382\n",
      "Iteration 87, loss = 0.43547272\n",
      "Iteration 88, loss = 0.43504279\n",
      "Iteration 89, loss = 0.43431389\n",
      "Iteration 90, loss = 0.43375876\n",
      "Iteration 91, loss = 0.43327890\n",
      "Iteration 92, loss = 0.43288165\n",
      "Iteration 93, loss = 0.43227636\n",
      "Iteration 94, loss = 0.43194294\n",
      "Iteration 95, loss = 0.43137157\n",
      "Iteration 96, loss = 0.43100980\n",
      "Iteration 97, loss = 0.43063581\n",
      "Iteration 98, loss = 0.43026282\n",
      "Iteration 99, loss = 0.42988719\n",
      "Iteration 100, loss = 0.42956606\n",
      "Iteration 101, loss = 0.42937768\n",
      "Iteration 102, loss = 0.42898680\n",
      "Iteration 103, loss = 0.42871680\n",
      "Iteration 104, loss = 0.42845662\n",
      "Iteration 105, loss = 0.42817201\n",
      "Iteration 106, loss = 0.42792978\n",
      "Iteration 107, loss = 0.42769680\n",
      "Iteration 108, loss = 0.42749847\n",
      "Iteration 109, loss = 0.42724048\n",
      "Iteration 110, loss = 0.42703617\n",
      "Iteration 111, loss = 0.42691601\n",
      "Iteration 112, loss = 0.42667521\n",
      "Iteration 113, loss = 0.42649637\n",
      "Iteration 114, loss = 0.42634023\n",
      "Iteration 115, loss = 0.42623328\n",
      "Iteration 116, loss = 0.42600818\n",
      "Iteration 117, loss = 0.42586273\n",
      "Iteration 118, loss = 0.42571308\n",
      "Iteration 119, loss = 0.42558030\n",
      "Iteration 120, loss = 0.42543106\n",
      "Iteration 121, loss = 0.42530404\n",
      "Iteration 122, loss = 0.42521285\n",
      "Iteration 123, loss = 0.42506309\n",
      "Iteration 124, loss = 0.42494026\n",
      "Iteration 125, loss = 0.42492821\n",
      "Iteration 126, loss = 0.42471802\n",
      "Iteration 127, loss = 0.42461370\n",
      "Iteration 128, loss = 0.42455917\n",
      "Iteration 129, loss = 0.42444372\n",
      "Iteration 130, loss = 0.42440008\n",
      "Iteration 131, loss = 0.42424062\n",
      "Iteration 132, loss = 0.42424722\n",
      "Iteration 133, loss = 0.42408357\n",
      "Iteration 134, loss = 0.42399564\n",
      "Iteration 135, loss = 0.42392022\n",
      "Iteration 136, loss = 0.42388060\n",
      "Iteration 137, loss = 0.42375245\n",
      "Iteration 138, loss = 0.42369093\n",
      "Iteration 139, loss = 0.42364254\n",
      "Iteration 140, loss = 0.42360844\n",
      "Iteration 141, loss = 0.42347822\n",
      "Iteration 142, loss = 0.42358050\n",
      "Iteration 143, loss = 0.42350492\n",
      "Iteration 144, loss = 0.42333077\n",
      "Iteration 145, loss = 0.42325496\n",
      "Iteration 146, loss = 0.42323878\n",
      "Iteration 147, loss = 0.42319039\n",
      "Iteration 148, loss = 0.42309960\n",
      "Iteration 149, loss = 0.42303464\n",
      "Iteration 150, loss = 0.42292609\n",
      "Iteration 151, loss = 0.42287273\n",
      "Iteration 152, loss = 0.42284867\n",
      "Iteration 153, loss = 0.42294721\n",
      "Iteration 154, loss = 0.42273122\n",
      "Iteration 155, loss = 0.42265633\n",
      "Iteration 156, loss = 0.42264415\n",
      "Iteration 157, loss = 0.42259903\n",
      "Iteration 158, loss = 0.42259164\n",
      "Iteration 159, loss = 0.42263569\n",
      "Iteration 160, loss = 0.42250815\n",
      "Iteration 161, loss = 0.42243299\n",
      "Iteration 162, loss = 0.42240906\n",
      "Iteration 163, loss = 0.42235624\n",
      "Iteration 164, loss = 0.42242605\n",
      "Iteration 165, loss = 0.42232924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164301\n",
      "Iteration 2, loss = 0.71828047\n",
      "Iteration 3, loss = 0.68858255\n",
      "Iteration 4, loss = 0.66801634\n",
      "Iteration 5, loss = 0.65194152\n",
      "Iteration 6, loss = 0.64122810\n",
      "Iteration 7, loss = 0.63356309\n",
      "Iteration 8, loss = 0.62835637\n",
      "Iteration 9, loss = 0.62394819\n",
      "Iteration 10, loss = 0.61949065\n",
      "Iteration 11, loss = 0.61514245\n",
      "Iteration 12, loss = 0.61038115\n",
      "Iteration 13, loss = 0.60537075\n",
      "Iteration 14, loss = 0.60128573\n",
      "Iteration 15, loss = 0.59575871\n",
      "Iteration 16, loss = 0.59119262\n",
      "Iteration 17, loss = 0.58693686\n",
      "Iteration 18, loss = 0.58288217\n",
      "Iteration 19, loss = 0.57906204\n",
      "Iteration 20, loss = 0.57486500\n",
      "Iteration 21, loss = 0.57091219\n",
      "Iteration 22, loss = 0.56715766\n",
      "Iteration 23, loss = 0.56329337\n",
      "Iteration 24, loss = 0.55962501\n",
      "Iteration 25, loss = 0.55595662\n",
      "Iteration 26, loss = 0.55231543\n",
      "Iteration 27, loss = 0.54860317\n",
      "Iteration 28, loss = 0.54526571\n",
      "Iteration 29, loss = 0.54172114\n",
      "Iteration 30, loss = 0.53837509\n",
      "Iteration 31, loss = 0.53506217\n",
      "Iteration 32, loss = 0.53171418\n",
      "Iteration 33, loss = 0.52846875\n",
      "Iteration 34, loss = 0.52517316\n",
      "Iteration 35, loss = 0.52212317\n",
      "Iteration 36, loss = 0.51904151\n",
      "Iteration 37, loss = 0.51597341\n",
      "Iteration 38, loss = 0.51295539\n",
      "Iteration 39, loss = 0.50998215\n",
      "Iteration 40, loss = 0.50708211\n",
      "Iteration 41, loss = 0.50428412\n",
      "Iteration 42, loss = 0.50149496\n",
      "Iteration 43, loss = 0.49874098\n",
      "Iteration 44, loss = 0.49600503\n",
      "Iteration 45, loss = 0.49346786\n",
      "Iteration 46, loss = 0.49075742\n",
      "Iteration 47, loss = 0.48822782\n",
      "Iteration 48, loss = 0.48569622\n",
      "Iteration 49, loss = 0.48330801\n",
      "Iteration 50, loss = 0.48084124\n",
      "Iteration 51, loss = 0.47846747\n",
      "Iteration 52, loss = 0.47616903\n",
      "Iteration 53, loss = 0.47414321\n",
      "Iteration 54, loss = 0.47175582\n",
      "Iteration 55, loss = 0.46955696\n",
      "Iteration 56, loss = 0.46738073\n",
      "Iteration 57, loss = 0.46545188\n",
      "Iteration 58, loss = 0.46358555\n",
      "Iteration 59, loss = 0.46148534\n",
      "Iteration 60, loss = 0.45950805\n",
      "Iteration 61, loss = 0.45779643\n",
      "Iteration 62, loss = 0.45611120\n",
      "Iteration 63, loss = 0.45437211\n",
      "Iteration 64, loss = 0.45264083\n",
      "Iteration 65, loss = 0.45094965\n",
      "Iteration 66, loss = 0.44943477\n",
      "Iteration 67, loss = 0.44794913\n",
      "Iteration 68, loss = 0.44650636\n",
      "Iteration 69, loss = 0.44500284\n",
      "Iteration 70, loss = 0.44382894\n",
      "Iteration 71, loss = 0.44238848\n",
      "Iteration 72, loss = 0.44117136\n",
      "Iteration 73, loss = 0.43996187\n",
      "Iteration 74, loss = 0.43871777\n",
      "Iteration 75, loss = 0.43762385\n",
      "Iteration 76, loss = 0.43661730\n",
      "Iteration 77, loss = 0.43561516\n",
      "Iteration 78, loss = 0.43468968\n",
      "Iteration 79, loss = 0.43367310\n",
      "Iteration 80, loss = 0.43280111\n",
      "Iteration 81, loss = 0.43194437\n",
      "Iteration 82, loss = 0.43120998\n",
      "Iteration 83, loss = 0.43051848\n",
      "Iteration 84, loss = 0.42971742\n",
      "Iteration 85, loss = 0.42907720\n",
      "Iteration 86, loss = 0.42830208\n",
      "Iteration 87, loss = 0.42778600\n",
      "Iteration 88, loss = 0.42723737\n",
      "Iteration 89, loss = 0.42662474\n",
      "Iteration 90, loss = 0.42608158\n",
      "Iteration 91, loss = 0.42555517\n",
      "Iteration 92, loss = 0.42510383\n",
      "Iteration 93, loss = 0.42461789\n",
      "Iteration 94, loss = 0.42427252\n",
      "Iteration 95, loss = 0.42381280\n",
      "Iteration 96, loss = 0.42348242\n",
      "Iteration 97, loss = 0.42309791\n",
      "Iteration 98, loss = 0.42273060\n",
      "Iteration 99, loss = 0.42243733\n",
      "Iteration 100, loss = 0.42211553\n",
      "Iteration 101, loss = 0.42197710\n",
      "Iteration 102, loss = 0.42159589\n",
      "Iteration 103, loss = 0.42137502\n",
      "Iteration 104, loss = 0.42112329\n",
      "Iteration 105, loss = 0.42088562\n",
      "Iteration 106, loss = 0.42071442\n",
      "Iteration 107, loss = 0.42045198\n",
      "Iteration 108, loss = 0.42031710\n",
      "Iteration 109, loss = 0.42016078\n",
      "Iteration 110, loss = 0.41999347\n",
      "Iteration 111, loss = 0.41979554\n",
      "Iteration 112, loss = 0.41968466\n",
      "Iteration 113, loss = 0.41953489\n",
      "Iteration 114, loss = 0.41940585\n",
      "Iteration 115, loss = 0.41927844\n",
      "Iteration 116, loss = 0.41913218\n",
      "Iteration 117, loss = 0.41902948\n",
      "Iteration 118, loss = 0.41899112\n",
      "Iteration 119, loss = 0.41892855\n",
      "Iteration 120, loss = 0.41879388\n",
      "Iteration 121, loss = 0.41874115\n",
      "Iteration 122, loss = 0.41881995\n",
      "Iteration 123, loss = 0.41855561\n",
      "Iteration 124, loss = 0.41851223\n",
      "Iteration 125, loss = 0.41837810\n",
      "Iteration 126, loss = 0.41831037\n",
      "Iteration 127, loss = 0.41828153\n",
      "Iteration 128, loss = 0.41825059\n",
      "Iteration 129, loss = 0.41818343\n",
      "Iteration 130, loss = 0.41817293\n",
      "Iteration 131, loss = 0.41809821\n",
      "Iteration 132, loss = 0.41803663\n",
      "Iteration 133, loss = 0.41800626\n",
      "Iteration 134, loss = 0.41792336\n",
      "Iteration 135, loss = 0.41788290\n",
      "Iteration 136, loss = 0.41785127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236987\n",
      "Iteration 2, loss = 0.71782060\n",
      "Iteration 3, loss = 0.68874227\n",
      "Iteration 4, loss = 0.66689237\n",
      "Iteration 5, loss = 0.65059256\n",
      "Iteration 6, loss = 0.64115001\n",
      "Iteration 7, loss = 0.63293734\n",
      "Iteration 8, loss = 0.62808938\n",
      "Iteration 9, loss = 0.62372897\n",
      "Iteration 10, loss = 0.61944335\n",
      "Iteration 11, loss = 0.61513811\n",
      "Iteration 12, loss = 0.61058485\n",
      "Iteration 13, loss = 0.60560964\n",
      "Iteration 14, loss = 0.60150750\n",
      "Iteration 15, loss = 0.59621793\n",
      "Iteration 16, loss = 0.59185487\n",
      "Iteration 17, loss = 0.58749247\n",
      "Iteration 18, loss = 0.58354247\n",
      "Iteration 19, loss = 0.57999677\n",
      "Iteration 20, loss = 0.57602534\n",
      "Iteration 21, loss = 0.57242063\n",
      "Iteration 22, loss = 0.56869602\n",
      "Iteration 23, loss = 0.56514964\n",
      "Iteration 24, loss = 0.56153987\n",
      "Iteration 25, loss = 0.55805464\n",
      "Iteration 26, loss = 0.55463061\n",
      "Iteration 27, loss = 0.55130855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.54815610\n",
      "Iteration 29, loss = 0.54496064\n",
      "Iteration 30, loss = 0.54181054\n",
      "Iteration 31, loss = 0.53880512\n",
      "Iteration 32, loss = 0.53573135\n",
      "Iteration 33, loss = 0.53279364\n",
      "Iteration 34, loss = 0.52990684\n",
      "Iteration 35, loss = 0.52709267\n",
      "Iteration 36, loss = 0.52447061\n",
      "Iteration 37, loss = 0.52163651\n",
      "Iteration 38, loss = 0.51911784\n",
      "Iteration 39, loss = 0.51647858\n",
      "Iteration 40, loss = 0.51397377\n",
      "Iteration 41, loss = 0.51159760\n",
      "Iteration 42, loss = 0.50915903\n",
      "Iteration 43, loss = 0.50677020\n",
      "Iteration 44, loss = 0.50458665\n",
      "Iteration 45, loss = 0.50241613\n",
      "Iteration 46, loss = 0.50010859\n",
      "Iteration 47, loss = 0.49808198\n",
      "Iteration 48, loss = 0.49602202\n",
      "Iteration 49, loss = 0.49399480\n",
      "Iteration 50, loss = 0.49208864\n",
      "Iteration 51, loss = 0.49018689\n",
      "Iteration 52, loss = 0.48830828\n",
      "Iteration 53, loss = 0.48670065\n",
      "Iteration 54, loss = 0.48479540\n",
      "Iteration 55, loss = 0.48307666\n",
      "Iteration 56, loss = 0.48134078\n",
      "Iteration 57, loss = 0.47992518\n",
      "Iteration 58, loss = 0.47852799\n",
      "Iteration 59, loss = 0.47681633\n",
      "Iteration 60, loss = 0.47541526\n",
      "Iteration 61, loss = 0.47387889\n",
      "Iteration 62, loss = 0.47264054\n",
      "Iteration 63, loss = 0.47136533\n",
      "Iteration 64, loss = 0.47005576\n",
      "Iteration 65, loss = 0.46895473\n",
      "Iteration 66, loss = 0.46772668\n",
      "Iteration 67, loss = 0.46655104\n",
      "Iteration 68, loss = 0.46555741\n",
      "Iteration 69, loss = 0.46442635\n",
      "Iteration 70, loss = 0.46343066\n",
      "Iteration 71, loss = 0.46253520\n",
      "Iteration 72, loss = 0.46158781\n",
      "Iteration 73, loss = 0.46065360\n",
      "Iteration 74, loss = 0.45969953\n",
      "Iteration 75, loss = 0.45898632\n",
      "Iteration 76, loss = 0.45821264\n",
      "Iteration 77, loss = 0.45760171\n",
      "Iteration 78, loss = 0.45689520\n",
      "Iteration 79, loss = 0.45609268\n",
      "Iteration 80, loss = 0.45550310\n",
      "Iteration 81, loss = 0.45484588\n",
      "Iteration 82, loss = 0.45431691\n",
      "Iteration 83, loss = 0.45401138\n",
      "Iteration 84, loss = 0.45334752\n",
      "Iteration 85, loss = 0.45286746\n",
      "Iteration 86, loss = 0.45229438\n",
      "Iteration 87, loss = 0.45204869\n",
      "Iteration 88, loss = 0.45165287\n",
      "Iteration 89, loss = 0.45113269\n",
      "Iteration 90, loss = 0.45075374\n",
      "Iteration 91, loss = 0.45041769\n",
      "Iteration 92, loss = 0.45008635\n",
      "Iteration 93, loss = 0.44973458\n",
      "Iteration 94, loss = 0.44950436\n",
      "Iteration 95, loss = 0.44920368\n",
      "Iteration 96, loss = 0.44896769\n",
      "Iteration 97, loss = 0.44872782\n",
      "Iteration 98, loss = 0.44843330\n",
      "Iteration 99, loss = 0.44820377\n",
      "Iteration 100, loss = 0.44799669\n",
      "Iteration 101, loss = 0.44791313\n",
      "Iteration 102, loss = 0.44767734\n",
      "Iteration 103, loss = 0.44754539\n",
      "Iteration 104, loss = 0.44725818\n",
      "Iteration 105, loss = 0.44713394\n",
      "Iteration 106, loss = 0.44702930\n",
      "Iteration 107, loss = 0.44682434\n",
      "Iteration 108, loss = 0.44677106\n",
      "Iteration 109, loss = 0.44664755\n",
      "Iteration 110, loss = 0.44649025\n",
      "Iteration 111, loss = 0.44630800\n",
      "Iteration 112, loss = 0.44620432\n",
      "Iteration 113, loss = 0.44606133\n",
      "Iteration 114, loss = 0.44597385\n",
      "Iteration 115, loss = 0.44591629\n",
      "Iteration 116, loss = 0.44580236\n",
      "Iteration 117, loss = 0.44570766\n",
      "Iteration 118, loss = 0.44565214\n",
      "Iteration 119, loss = 0.44561698\n",
      "Iteration 120, loss = 0.44553671\n",
      "Iteration 121, loss = 0.44542083\n",
      "Iteration 122, loss = 0.44549953\n",
      "Iteration 123, loss = 0.44536183\n",
      "Iteration 124, loss = 0.44523522\n",
      "Iteration 125, loss = 0.44513007\n",
      "Iteration 126, loss = 0.44514158\n",
      "Iteration 127, loss = 0.44504997\n",
      "Iteration 128, loss = 0.44501116\n",
      "Iteration 129, loss = 0.44495415\n",
      "Iteration 130, loss = 0.44497523\n",
      "Iteration 131, loss = 0.44484922\n",
      "Iteration 132, loss = 0.44478967\n",
      "Iteration 133, loss = 0.44479478\n",
      "Iteration 134, loss = 0.44470508\n",
      "Iteration 135, loss = 0.44469395\n",
      "Iteration 136, loss = 0.44468391\n",
      "Iteration 137, loss = 0.44469398\n",
      "Iteration 138, loss = 0.44453166\n",
      "Iteration 139, loss = 0.44454940\n",
      "Iteration 140, loss = 0.44457561\n",
      "Iteration 141, loss = 0.44448358\n",
      "Iteration 142, loss = 0.44457125\n",
      "Iteration 143, loss = 0.44440084\n",
      "Iteration 144, loss = 0.44437465\n",
      "Iteration 145, loss = 0.44435627\n",
      "Iteration 146, loss = 0.44440441\n",
      "Iteration 147, loss = 0.44440157\n",
      "Iteration 148, loss = 0.44424307\n",
      "Iteration 149, loss = 0.44419634\n",
      "Iteration 150, loss = 0.44421429\n",
      "Iteration 151, loss = 0.44419657\n",
      "Iteration 152, loss = 0.44421173\n",
      "Iteration 153, loss = 0.44429101\n",
      "Iteration 154, loss = 0.44412568\n",
      "Iteration 155, loss = 0.44409428\n",
      "Iteration 156, loss = 0.44400186\n",
      "Iteration 157, loss = 0.44401086\n",
      "Iteration 158, loss = 0.44398030\n",
      "Iteration 159, loss = 0.44393583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973198\n",
      "Iteration 2, loss = 0.71530972\n",
      "Iteration 3, loss = 0.68650212\n",
      "Iteration 4, loss = 0.66331235\n",
      "Iteration 5, loss = 0.64773914\n",
      "Iteration 6, loss = 0.63645341\n",
      "Iteration 7, loss = 0.62839674\n",
      "Iteration 8, loss = 0.62338461\n",
      "Iteration 9, loss = 0.61832596\n",
      "Iteration 10, loss = 0.61358760\n",
      "Iteration 11, loss = 0.60883785\n",
      "Iteration 12, loss = 0.60327096\n",
      "Iteration 13, loss = 0.59718176\n",
      "Iteration 14, loss = 0.59181463\n",
      "Iteration 15, loss = 0.58640602\n",
      "Iteration 16, loss = 0.58129727\n",
      "Iteration 17, loss = 0.57640230\n",
      "Iteration 18, loss = 0.57177093\n",
      "Iteration 19, loss = 0.56738073\n",
      "Iteration 20, loss = 0.56303287\n",
      "Iteration 21, loss = 0.55869559\n",
      "Iteration 22, loss = 0.55444528\n",
      "Iteration 23, loss = 0.55016975\n",
      "Iteration 24, loss = 0.54595314\n",
      "Iteration 25, loss = 0.54187093\n",
      "Iteration 26, loss = 0.53804610\n",
      "Iteration 27, loss = 0.53404409\n",
      "Iteration 28, loss = 0.53020300\n",
      "Iteration 29, loss = 0.52656546\n",
      "Iteration 30, loss = 0.52279780\n",
      "Iteration 31, loss = 0.51930706\n",
      "Iteration 32, loss = 0.51593733\n",
      "Iteration 33, loss = 0.51236432\n",
      "Iteration 34, loss = 0.50896960\n",
      "Iteration 35, loss = 0.50568916\n",
      "Iteration 36, loss = 0.50272536\n",
      "Iteration 37, loss = 0.49949495\n",
      "Iteration 38, loss = 0.49651397\n",
      "Iteration 39, loss = 0.49359745\n",
      "Iteration 40, loss = 0.49067233\n",
      "Iteration 41, loss = 0.48781005\n",
      "Iteration 42, loss = 0.48511284\n",
      "Iteration 43, loss = 0.48247317\n",
      "Iteration 44, loss = 0.47987258\n",
      "Iteration 45, loss = 0.47735029\n",
      "Iteration 46, loss = 0.47488109\n",
      "Iteration 47, loss = 0.47256392\n",
      "Iteration 48, loss = 0.47008685\n",
      "Iteration 49, loss = 0.46777220\n",
      "Iteration 50, loss = 0.46573142\n",
      "Iteration 51, loss = 0.46351886\n",
      "Iteration 52, loss = 0.46147652\n",
      "Iteration 53, loss = 0.45932466\n",
      "Iteration 54, loss = 0.45746294\n",
      "Iteration 55, loss = 0.45558250\n",
      "Iteration 56, loss = 0.45367655\n",
      "Iteration 57, loss = 0.45188032\n",
      "Iteration 58, loss = 0.45013907\n",
      "Iteration 59, loss = 0.44841645\n",
      "Iteration 60, loss = 0.44678562\n",
      "Iteration 61, loss = 0.44519358\n",
      "Iteration 62, loss = 0.44377966\n",
      "Iteration 63, loss = 0.44220947\n",
      "Iteration 64, loss = 0.44079041\n",
      "Iteration 65, loss = 0.43935646\n",
      "Iteration 66, loss = 0.43796558\n",
      "Iteration 67, loss = 0.43669317\n",
      "Iteration 68, loss = 0.43547334\n",
      "Iteration 69, loss = 0.43430704\n",
      "Iteration 70, loss = 0.43318137\n",
      "Iteration 71, loss = 0.43206127\n",
      "Iteration 72, loss = 0.43096802\n",
      "Iteration 73, loss = 0.42990257\n",
      "Iteration 74, loss = 0.42902736\n",
      "Iteration 75, loss = 0.42811910\n",
      "Iteration 76, loss = 0.42718375\n",
      "Iteration 77, loss = 0.42629165\n",
      "Iteration 78, loss = 0.42549863\n",
      "Iteration 79, loss = 0.42476850\n",
      "Iteration 80, loss = 0.42416604\n",
      "Iteration 81, loss = 0.42342541\n",
      "Iteration 82, loss = 0.42266627\n",
      "Iteration 83, loss = 0.42214526\n",
      "Iteration 84, loss = 0.42147587\n",
      "Iteration 85, loss = 0.42092599\n",
      "Iteration 86, loss = 0.42030423\n",
      "Iteration 87, loss = 0.41984243\n",
      "Iteration 88, loss = 0.41947070\n",
      "Iteration 89, loss = 0.41888063\n",
      "Iteration 90, loss = 0.41839608\n",
      "Iteration 91, loss = 0.41813062\n",
      "Iteration 92, loss = 0.41775753\n",
      "Iteration 93, loss = 0.41734896\n",
      "Iteration 94, loss = 0.41712101\n",
      "Iteration 95, loss = 0.41668728\n",
      "Iteration 96, loss = 0.41645470\n",
      "Iteration 97, loss = 0.41611481\n",
      "Iteration 98, loss = 0.41590080\n",
      "Iteration 99, loss = 0.41557228\n",
      "Iteration 100, loss = 0.41530696\n",
      "Iteration 101, loss = 0.41517140\n",
      "Iteration 102, loss = 0.41485970\n",
      "Iteration 103, loss = 0.41471846\n",
      "Iteration 104, loss = 0.41452672\n",
      "Iteration 105, loss = 0.41430972\n",
      "Iteration 106, loss = 0.41412385\n",
      "Iteration 107, loss = 0.41404786\n",
      "Iteration 108, loss = 0.41392378\n",
      "Iteration 109, loss = 0.41371926\n",
      "Iteration 110, loss = 0.41357226\n",
      "Iteration 111, loss = 0.41343804\n",
      "Iteration 112, loss = 0.41333576\n",
      "Iteration 113, loss = 0.41323051\n",
      "Iteration 114, loss = 0.41311959\n",
      "Iteration 115, loss = 0.41302124\n",
      "Iteration 116, loss = 0.41291331\n",
      "Iteration 117, loss = 0.41285944\n",
      "Iteration 118, loss = 0.41277022\n",
      "Iteration 119, loss = 0.41266286\n",
      "Iteration 120, loss = 0.41257088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 121, loss = 0.41251050\n",
      "Iteration 122, loss = 0.41255120\n",
      "Iteration 123, loss = 0.41234289\n",
      "Iteration 124, loss = 0.41242528\n",
      "Iteration 125, loss = 0.41221512\n",
      "Iteration 126, loss = 0.41219634\n",
      "Iteration 127, loss = 0.41212086\n",
      "Iteration 128, loss = 0.41208551\n",
      "Iteration 129, loss = 0.41206690\n",
      "Iteration 130, loss = 0.41193919\n",
      "Iteration 131, loss = 0.41198155\n",
      "Iteration 132, loss = 0.41194600\n",
      "Iteration 133, loss = 0.41192632\n",
      "Iteration 134, loss = 0.41178898\n",
      "Iteration 135, loss = 0.41180135\n",
      "Iteration 136, loss = 0.41172541\n",
      "Iteration 137, loss = 0.41175031\n",
      "Iteration 138, loss = 0.41163097\n",
      "Iteration 139, loss = 0.41167664\n",
      "Iteration 140, loss = 0.41159409\n",
      "Iteration 141, loss = 0.41154859\n",
      "Iteration 142, loss = 0.41151515\n",
      "Iteration 143, loss = 0.41147621\n",
      "Iteration 144, loss = 0.41142688\n",
      "Iteration 145, loss = 0.41155138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072285\n",
      "Iteration 2, loss = 0.71621967\n",
      "Iteration 3, loss = 0.68706197\n",
      "Iteration 4, loss = 0.66505704\n",
      "Iteration 5, loss = 0.65028189\n",
      "Iteration 6, loss = 0.63925854\n",
      "Iteration 7, loss = 0.63130237\n",
      "Iteration 8, loss = 0.62729575\n",
      "Iteration 9, loss = 0.62245598\n",
      "Iteration 10, loss = 0.61804602\n",
      "Iteration 11, loss = 0.61373182\n",
      "Iteration 12, loss = 0.60893006\n",
      "Iteration 13, loss = 0.60342183\n",
      "Iteration 14, loss = 0.59872682\n",
      "Iteration 15, loss = 0.59394134\n",
      "Iteration 16, loss = 0.58944417\n",
      "Iteration 17, loss = 0.58524374\n",
      "Iteration 18, loss = 0.58110037\n",
      "Iteration 19, loss = 0.57732747\n",
      "Iteration 20, loss = 0.57353333\n",
      "Iteration 21, loss = 0.56980670\n",
      "Iteration 22, loss = 0.56618871\n",
      "Iteration 23, loss = 0.56253916\n",
      "Iteration 24, loss = 0.55896711\n",
      "Iteration 25, loss = 0.55543110\n",
      "Iteration 26, loss = 0.55212181\n",
      "Iteration 27, loss = 0.54881870\n",
      "Iteration 28, loss = 0.54551491\n",
      "Iteration 29, loss = 0.54233057\n",
      "Iteration 30, loss = 0.53908636\n",
      "Iteration 31, loss = 0.53599410\n",
      "Iteration 32, loss = 0.53327516\n",
      "Iteration 33, loss = 0.53016111\n",
      "Iteration 34, loss = 0.52719130\n",
      "Iteration 35, loss = 0.52434932\n",
      "Iteration 36, loss = 0.52181654\n",
      "Iteration 37, loss = 0.51892900\n",
      "Iteration 38, loss = 0.51638646\n",
      "Iteration 39, loss = 0.51383582\n",
      "Iteration 40, loss = 0.51129262\n",
      "Iteration 41, loss = 0.50871709\n",
      "Iteration 42, loss = 0.50630251\n",
      "Iteration 43, loss = 0.50394506\n",
      "Iteration 44, loss = 0.50158635\n",
      "Iteration 45, loss = 0.49935039\n",
      "Iteration 46, loss = 0.49716385\n",
      "Iteration 47, loss = 0.49493016\n",
      "Iteration 48, loss = 0.49282547\n",
      "Iteration 49, loss = 0.49074741\n",
      "Iteration 50, loss = 0.48881004\n",
      "Iteration 51, loss = 0.48684761\n",
      "Iteration 52, loss = 0.48482414\n",
      "Iteration 53, loss = 0.48283625\n",
      "Iteration 54, loss = 0.48116614\n",
      "Iteration 55, loss = 0.47934947\n",
      "Iteration 56, loss = 0.47753779\n",
      "Iteration 57, loss = 0.47588522\n",
      "Iteration 58, loss = 0.47419306\n",
      "Iteration 59, loss = 0.47267887\n",
      "Iteration 60, loss = 0.47092147\n",
      "Iteration 61, loss = 0.46957220\n",
      "Iteration 62, loss = 0.46811384\n",
      "Iteration 63, loss = 0.46669226\n",
      "Iteration 64, loss = 0.46510522\n",
      "Iteration 65, loss = 0.46383669\n",
      "Iteration 66, loss = 0.46263683\n",
      "Iteration 67, loss = 0.46122442\n",
      "Iteration 68, loss = 0.46003847\n",
      "Iteration 69, loss = 0.45890884\n",
      "Iteration 70, loss = 0.45781973\n",
      "Iteration 71, loss = 0.45669509\n",
      "Iteration 72, loss = 0.45554024\n",
      "Iteration 73, loss = 0.45449999\n",
      "Iteration 74, loss = 0.45373039\n",
      "Iteration 75, loss = 0.45266215\n",
      "Iteration 76, loss = 0.45181407\n",
      "Iteration 77, loss = 0.45091790\n",
      "Iteration 78, loss = 0.45009237\n",
      "Iteration 79, loss = 0.44929124\n",
      "Iteration 80, loss = 0.44861693\n",
      "Iteration 81, loss = 0.44793645\n",
      "Iteration 82, loss = 0.44714437\n",
      "Iteration 83, loss = 0.44672374\n",
      "Iteration 84, loss = 0.44587970\n",
      "Iteration 85, loss = 0.44533529\n",
      "Iteration 86, loss = 0.44473180\n",
      "Iteration 87, loss = 0.44422015\n",
      "Iteration 88, loss = 0.44377993\n",
      "Iteration 89, loss = 0.44324244\n",
      "Iteration 90, loss = 0.44274882\n",
      "Iteration 91, loss = 0.44239126\n",
      "Iteration 92, loss = 0.44195957\n",
      "Iteration 93, loss = 0.44155906\n",
      "Iteration 94, loss = 0.44125627\n",
      "Iteration 95, loss = 0.44086320\n",
      "Iteration 96, loss = 0.44052634\n",
      "Iteration 97, loss = 0.44035109\n",
      "Iteration 98, loss = 0.44012300\n",
      "Iteration 99, loss = 0.43965133\n",
      "Iteration 100, loss = 0.43938060\n",
      "Iteration 101, loss = 0.43921573\n",
      "Iteration 102, loss = 0.43889451\n",
      "Iteration 103, loss = 0.43869479\n",
      "Iteration 104, loss = 0.43856823\n",
      "Iteration 105, loss = 0.43834605\n",
      "Iteration 106, loss = 0.43807798\n",
      "Iteration 107, loss = 0.43794385\n",
      "Iteration 108, loss = 0.43778189\n",
      "Iteration 109, loss = 0.43764301\n",
      "Iteration 110, loss = 0.43753813\n",
      "Iteration 111, loss = 0.43737416\n",
      "Iteration 112, loss = 0.43722091\n",
      "Iteration 113, loss = 0.43713340\n",
      "Iteration 114, loss = 0.43706219\n",
      "Iteration 115, loss = 0.43688330\n",
      "Iteration 116, loss = 0.43679735\n",
      "Iteration 117, loss = 0.43672904\n",
      "Iteration 118, loss = 0.43669154\n",
      "Iteration 119, loss = 0.43653010\n",
      "Iteration 120, loss = 0.43649998\n",
      "Iteration 121, loss = 0.43638790\n",
      "Iteration 122, loss = 0.43638280\n",
      "Iteration 123, loss = 0.43619523\n",
      "Iteration 124, loss = 0.43617414\n",
      "Iteration 125, loss = 0.43615182\n",
      "Iteration 126, loss = 0.43606622\n",
      "Iteration 127, loss = 0.43599373\n",
      "Iteration 128, loss = 0.43592311\n",
      "Iteration 129, loss = 0.43590818\n",
      "Iteration 130, loss = 0.43580445\n",
      "Iteration 131, loss = 0.43580354\n",
      "Iteration 132, loss = 0.43571748\n",
      "Iteration 133, loss = 0.43575205\n",
      "Iteration 134, loss = 0.43566300\n",
      "Iteration 135, loss = 0.43567938\n",
      "Iteration 136, loss = 0.43558401\n",
      "Iteration 137, loss = 0.43557242\n",
      "Iteration 138, loss = 0.43556743\n",
      "Iteration 139, loss = 0.43556593\n",
      "Iteration 140, loss = 0.43545252\n",
      "Iteration 141, loss = 0.43549243\n",
      "Iteration 142, loss = 0.43542722\n",
      "Iteration 143, loss = 0.43534789\n",
      "Iteration 144, loss = 0.43541560\n",
      "Iteration 145, loss = 0.43533253\n",
      "Iteration 146, loss = 0.43537747\n",
      "Iteration 147, loss = 0.43556990\n",
      "Iteration 148, loss = 0.43540108\n",
      "Iteration 149, loss = 0.43538146\n",
      "Iteration 150, loss = 0.43523140\n",
      "Iteration 151, loss = 0.43520398\n",
      "Iteration 152, loss = 0.43522986\n",
      "Iteration 153, loss = 0.43512844\n",
      "Iteration 154, loss = 0.43516488\n",
      "Iteration 155, loss = 0.43524445\n",
      "Iteration 156, loss = 0.43514715\n",
      "Iteration 157, loss = 0.43506097\n",
      "Iteration 158, loss = 0.43511925\n",
      "Iteration 159, loss = 0.43509210\n",
      "Iteration 160, loss = 0.43499441\n",
      "Iteration 161, loss = 0.43500636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245126\n",
      "Iteration 2, loss = 0.71880148\n",
      "Iteration 3, loss = 0.68933253\n",
      "Iteration 4, loss = 0.66865852\n",
      "Iteration 5, loss = 0.65261890\n",
      "Iteration 6, loss = 0.64170491\n",
      "Iteration 7, loss = 0.63397397\n",
      "Iteration 8, loss = 0.62898627\n",
      "Iteration 9, loss = 0.62457592\n",
      "Iteration 10, loss = 0.62023055\n",
      "Iteration 11, loss = 0.61608745\n",
      "Iteration 12, loss = 0.61130092\n",
      "Iteration 13, loss = 0.60657628\n",
      "Iteration 14, loss = 0.60248415\n",
      "Iteration 15, loss = 0.59702089\n",
      "Iteration 16, loss = 0.59258087\n",
      "Iteration 17, loss = 0.58857418\n",
      "Iteration 18, loss = 0.58443337\n",
      "Iteration 19, loss = 0.58075607\n",
      "Iteration 20, loss = 0.57662385\n",
      "Iteration 21, loss = 0.57305468\n",
      "Iteration 22, loss = 0.56944210\n",
      "Iteration 23, loss = 0.56560334\n",
      "Iteration 24, loss = 0.56206857\n",
      "Iteration 25, loss = 0.55866006\n",
      "Iteration 26, loss = 0.55506578\n",
      "Iteration 27, loss = 0.55161395\n",
      "Iteration 28, loss = 0.54840630\n",
      "Iteration 29, loss = 0.54500306\n",
      "Iteration 30, loss = 0.54185878\n",
      "Iteration 31, loss = 0.53860334\n",
      "Iteration 32, loss = 0.53552106\n",
      "Iteration 33, loss = 0.53246947\n",
      "Iteration 34, loss = 0.52939189\n",
      "Iteration 35, loss = 0.52652510\n",
      "Iteration 36, loss = 0.52357242\n",
      "Iteration 37, loss = 0.52067606\n",
      "Iteration 38, loss = 0.51777005\n",
      "Iteration 39, loss = 0.51500104\n",
      "Iteration 40, loss = 0.51232191\n",
      "Iteration 41, loss = 0.50955484\n",
      "Iteration 42, loss = 0.50701418\n",
      "Iteration 43, loss = 0.50432497\n",
      "Iteration 44, loss = 0.50194421\n",
      "Iteration 45, loss = 0.49929216\n",
      "Iteration 46, loss = 0.49688555\n",
      "Iteration 47, loss = 0.49437646\n",
      "Iteration 48, loss = 0.49197241\n",
      "Iteration 49, loss = 0.48964422\n",
      "Iteration 50, loss = 0.48751427\n",
      "Iteration 51, loss = 0.48512720\n",
      "Iteration 52, loss = 0.48284373\n",
      "Iteration 53, loss = 0.48083341\n",
      "Iteration 54, loss = 0.47874644\n",
      "Iteration 55, loss = 0.47657954\n",
      "Iteration 56, loss = 0.47454430\n",
      "Iteration 57, loss = 0.47263736\n",
      "Iteration 58, loss = 0.47077194\n",
      "Iteration 59, loss = 0.46878454\n",
      "Iteration 60, loss = 0.46692059\n",
      "Iteration 61, loss = 0.46512957\n",
      "Iteration 62, loss = 0.46353264\n",
      "Iteration 63, loss = 0.46173676\n",
      "Iteration 64, loss = 0.46010568\n",
      "Iteration 65, loss = 0.45850506\n",
      "Iteration 66, loss = 0.45700583\n",
      "Iteration 67, loss = 0.45562025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.45407567\n",
      "Iteration 69, loss = 0.45268243\n",
      "Iteration 70, loss = 0.45154462\n",
      "Iteration 71, loss = 0.45008483\n",
      "Iteration 72, loss = 0.44887947\n",
      "Iteration 73, loss = 0.44768217\n",
      "Iteration 74, loss = 0.44649661\n",
      "Iteration 75, loss = 0.44539933\n",
      "Iteration 76, loss = 0.44442576\n",
      "Iteration 77, loss = 0.44334391\n",
      "Iteration 78, loss = 0.44250063\n",
      "Iteration 79, loss = 0.44149559\n",
      "Iteration 80, loss = 0.44053102\n",
      "Iteration 81, loss = 0.43974787\n",
      "Iteration 82, loss = 0.43904577\n",
      "Iteration 83, loss = 0.43824203\n",
      "Iteration 84, loss = 0.43742410\n",
      "Iteration 85, loss = 0.43685675\n",
      "Iteration 86, loss = 0.43607797\n",
      "Iteration 87, loss = 0.43546685\n",
      "Iteration 88, loss = 0.43503690\n",
      "Iteration 89, loss = 0.43430798\n",
      "Iteration 90, loss = 0.43375281\n",
      "Iteration 91, loss = 0.43327295\n",
      "Iteration 92, loss = 0.43287566\n",
      "Iteration 93, loss = 0.43227036\n",
      "Iteration 94, loss = 0.43193692\n",
      "Iteration 95, loss = 0.43136554\n",
      "Iteration 96, loss = 0.43100374\n",
      "Iteration 97, loss = 0.43062974\n",
      "Iteration 98, loss = 0.43025673\n",
      "Iteration 99, loss = 0.42988108\n",
      "Iteration 100, loss = 0.42955994\n",
      "Iteration 101, loss = 0.42937155\n",
      "Iteration 102, loss = 0.42898064\n",
      "Iteration 103, loss = 0.42871063\n",
      "Iteration 104, loss = 0.42845044\n",
      "Iteration 105, loss = 0.42816582\n",
      "Iteration 106, loss = 0.42792357\n",
      "Iteration 107, loss = 0.42769058\n",
      "Iteration 108, loss = 0.42749224\n",
      "Iteration 109, loss = 0.42723424\n",
      "Iteration 110, loss = 0.42702991\n",
      "Iteration 111, loss = 0.42690974\n",
      "Iteration 112, loss = 0.42666893\n",
      "Iteration 113, loss = 0.42649009\n",
      "Iteration 114, loss = 0.42633393\n",
      "Iteration 115, loss = 0.42622697\n",
      "Iteration 116, loss = 0.42600187\n",
      "Iteration 117, loss = 0.42585641\n",
      "Iteration 118, loss = 0.42570675\n",
      "Iteration 119, loss = 0.42557396\n",
      "Iteration 120, loss = 0.42542471\n",
      "Iteration 121, loss = 0.42529768\n",
      "Iteration 122, loss = 0.42520649\n",
      "Iteration 123, loss = 0.42505672\n",
      "Iteration 124, loss = 0.42493387\n",
      "Iteration 125, loss = 0.42492181\n",
      "Iteration 126, loss = 0.42471162\n",
      "Iteration 127, loss = 0.42460730\n",
      "Iteration 128, loss = 0.42455275\n",
      "Iteration 129, loss = 0.42443729\n",
      "Iteration 130, loss = 0.42439365\n",
      "Iteration 131, loss = 0.42423419\n",
      "Iteration 132, loss = 0.42424077\n",
      "Iteration 133, loss = 0.42407711\n",
      "Iteration 134, loss = 0.42398918\n",
      "Iteration 135, loss = 0.42391375\n",
      "Iteration 136, loss = 0.42387412\n",
      "Iteration 137, loss = 0.42374596\n",
      "Iteration 138, loss = 0.42368444\n",
      "Iteration 139, loss = 0.42363603\n",
      "Iteration 140, loss = 0.42360193\n",
      "Iteration 141, loss = 0.42347171\n",
      "Iteration 142, loss = 0.42357398\n",
      "Iteration 143, loss = 0.42349839\n",
      "Iteration 144, loss = 0.42332423\n",
      "Iteration 145, loss = 0.42324841\n",
      "Iteration 146, loss = 0.42323223\n",
      "Iteration 147, loss = 0.42318383\n",
      "Iteration 148, loss = 0.42309303\n",
      "Iteration 149, loss = 0.42302806\n",
      "Iteration 150, loss = 0.42291951\n",
      "Iteration 151, loss = 0.42286614\n",
      "Iteration 152, loss = 0.42284207\n",
      "Iteration 153, loss = 0.42294060\n",
      "Iteration 154, loss = 0.42272461\n",
      "Iteration 155, loss = 0.42264971\n",
      "Iteration 156, loss = 0.42263752\n",
      "Iteration 157, loss = 0.42259239\n",
      "Iteration 158, loss = 0.42258499\n",
      "Iteration 159, loss = 0.42262903\n",
      "Iteration 160, loss = 0.42250149\n",
      "Iteration 161, loss = 0.42242633\n",
      "Iteration 162, loss = 0.42240239\n",
      "Iteration 163, loss = 0.42234956\n",
      "Iteration 164, loss = 0.42241936\n",
      "Iteration 165, loss = 0.42232254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164122\n",
      "Iteration 2, loss = 0.71827868\n",
      "Iteration 3, loss = 0.68858075\n",
      "Iteration 4, loss = 0.66801452\n",
      "Iteration 5, loss = 0.65193968\n",
      "Iteration 6, loss = 0.64122624\n",
      "Iteration 7, loss = 0.63356121\n",
      "Iteration 8, loss = 0.62835446\n",
      "Iteration 9, loss = 0.62394626\n",
      "Iteration 10, loss = 0.61948867\n",
      "Iteration 11, loss = 0.61514044\n",
      "Iteration 12, loss = 0.61037910\n",
      "Iteration 13, loss = 0.60536866\n",
      "Iteration 14, loss = 0.60128359\n",
      "Iteration 15, loss = 0.59575652\n",
      "Iteration 16, loss = 0.59119039\n",
      "Iteration 17, loss = 0.58693456\n",
      "Iteration 18, loss = 0.58287982\n",
      "Iteration 19, loss = 0.57905963\n",
      "Iteration 20, loss = 0.57486253\n",
      "Iteration 21, loss = 0.57090966\n",
      "Iteration 22, loss = 0.56715506\n",
      "Iteration 23, loss = 0.56329071\n",
      "Iteration 24, loss = 0.55962228\n",
      "Iteration 25, loss = 0.55595383\n",
      "Iteration 26, loss = 0.55231257\n",
      "Iteration 27, loss = 0.54860023\n",
      "Iteration 28, loss = 0.54526271\n",
      "Iteration 29, loss = 0.54171806\n",
      "Iteration 30, loss = 0.53837195\n",
      "Iteration 31, loss = 0.53505895\n",
      "Iteration 32, loss = 0.53171090\n",
      "Iteration 33, loss = 0.52846540\n",
      "Iteration 34, loss = 0.52516973\n",
      "Iteration 35, loss = 0.52211969\n",
      "Iteration 36, loss = 0.51903796\n",
      "Iteration 37, loss = 0.51596979\n",
      "Iteration 38, loss = 0.51295170\n",
      "Iteration 39, loss = 0.50997839\n",
      "Iteration 40, loss = 0.50707829\n",
      "Iteration 41, loss = 0.50428024\n",
      "Iteration 42, loss = 0.50149102\n",
      "Iteration 43, loss = 0.49873698\n",
      "Iteration 44, loss = 0.49600097\n",
      "Iteration 45, loss = 0.49346373\n",
      "Iteration 46, loss = 0.49075325\n",
      "Iteration 47, loss = 0.48822359\n",
      "Iteration 48, loss = 0.48569193\n",
      "Iteration 49, loss = 0.48330367\n",
      "Iteration 50, loss = 0.48083684\n",
      "Iteration 51, loss = 0.47846302\n",
      "Iteration 52, loss = 0.47616453\n",
      "Iteration 53, loss = 0.47413866\n",
      "Iteration 54, loss = 0.47175122\n",
      "Iteration 55, loss = 0.46955230\n",
      "Iteration 56, loss = 0.46737603\n",
      "Iteration 57, loss = 0.46544714\n",
      "Iteration 58, loss = 0.46358076\n",
      "Iteration 59, loss = 0.46148051\n",
      "Iteration 60, loss = 0.45950316\n",
      "Iteration 61, loss = 0.45779150\n",
      "Iteration 62, loss = 0.45610624\n",
      "Iteration 63, loss = 0.45436710\n",
      "Iteration 64, loss = 0.45263578\n",
      "Iteration 65, loss = 0.45094456\n",
      "Iteration 66, loss = 0.44942964\n",
      "Iteration 67, loss = 0.44794395\n",
      "Iteration 68, loss = 0.44650115\n",
      "Iteration 69, loss = 0.44499759\n",
      "Iteration 70, loss = 0.44382366\n",
      "Iteration 71, loss = 0.44238316\n",
      "Iteration 72, loss = 0.44116600\n",
      "Iteration 73, loss = 0.43995648\n",
      "Iteration 74, loss = 0.43871234\n",
      "Iteration 75, loss = 0.43761840\n",
      "Iteration 76, loss = 0.43661181\n",
      "Iteration 77, loss = 0.43560964\n",
      "Iteration 78, loss = 0.43468412\n",
      "Iteration 79, loss = 0.43366752\n",
      "Iteration 80, loss = 0.43279549\n",
      "Iteration 81, loss = 0.43193873\n",
      "Iteration 82, loss = 0.43120431\n",
      "Iteration 83, loss = 0.43051278\n",
      "Iteration 84, loss = 0.42971169\n",
      "Iteration 85, loss = 0.42907145\n",
      "Iteration 86, loss = 0.42829630\n",
      "Iteration 87, loss = 0.42778019\n",
      "Iteration 88, loss = 0.42723154\n",
      "Iteration 89, loss = 0.42661888\n",
      "Iteration 90, loss = 0.42607569\n",
      "Iteration 91, loss = 0.42554927\n",
      "Iteration 92, loss = 0.42509790\n",
      "Iteration 93, loss = 0.42461194\n",
      "Iteration 94, loss = 0.42426655\n",
      "Iteration 95, loss = 0.42380681\n",
      "Iteration 96, loss = 0.42347641\n",
      "Iteration 97, loss = 0.42309188\n",
      "Iteration 98, loss = 0.42272455\n",
      "Iteration 99, loss = 0.42243126\n",
      "Iteration 100, loss = 0.42210944\n",
      "Iteration 101, loss = 0.42197099\n",
      "Iteration 102, loss = 0.42158976\n",
      "Iteration 103, loss = 0.42136888\n",
      "Iteration 104, loss = 0.42111713\n",
      "Iteration 105, loss = 0.42087945\n",
      "Iteration 106, loss = 0.42070823\n",
      "Iteration 107, loss = 0.42044578\n",
      "Iteration 108, loss = 0.42031089\n",
      "Iteration 109, loss = 0.42015455\n",
      "Iteration 110, loss = 0.41998723\n",
      "Iteration 111, loss = 0.41978929\n",
      "Iteration 112, loss = 0.41967839\n",
      "Iteration 113, loss = 0.41952861\n",
      "Iteration 114, loss = 0.41939956\n",
      "Iteration 115, loss = 0.41927214\n",
      "Iteration 116, loss = 0.41912587\n",
      "Iteration 117, loss = 0.41902315\n",
      "Iteration 118, loss = 0.41898478\n",
      "Iteration 119, loss = 0.41892221\n",
      "Iteration 120, loss = 0.41878753\n",
      "Iteration 121, loss = 0.41873479\n",
      "Iteration 122, loss = 0.41881358\n",
      "Iteration 123, loss = 0.41854924\n",
      "Iteration 124, loss = 0.41850585\n",
      "Iteration 125, loss = 0.41837171\n",
      "Iteration 126, loss = 0.41830397\n",
      "Iteration 127, loss = 0.41827513\n",
      "Iteration 128, loss = 0.41824418\n",
      "Iteration 129, loss = 0.41817700\n",
      "Iteration 130, loss = 0.41816650\n",
      "Iteration 131, loss = 0.41809178\n",
      "Iteration 132, loss = 0.41803020\n",
      "Iteration 133, loss = 0.41799981\n",
      "Iteration 134, loss = 0.41791691\n",
      "Iteration 135, loss = 0.41787644\n",
      "Iteration 136, loss = 0.41784480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236808\n",
      "Iteration 2, loss = 0.71781880\n",
      "Iteration 3, loss = 0.68874046\n",
      "Iteration 4, loss = 0.66689055\n",
      "Iteration 5, loss = 0.65059073\n",
      "Iteration 6, loss = 0.64114816\n",
      "Iteration 7, loss = 0.63293547\n",
      "Iteration 8, loss = 0.62808748\n",
      "Iteration 9, loss = 0.62372704\n",
      "Iteration 10, loss = 0.61944137\n",
      "Iteration 11, loss = 0.61513610\n",
      "Iteration 12, loss = 0.61058280\n",
      "Iteration 13, loss = 0.60560754\n",
      "Iteration 14, loss = 0.60150535\n",
      "Iteration 15, loss = 0.59621572\n",
      "Iteration 16, loss = 0.59185260\n",
      "Iteration 17, loss = 0.58749014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.58354009\n",
      "Iteration 19, loss = 0.57999432\n",
      "Iteration 20, loss = 0.57602282\n",
      "Iteration 21, loss = 0.57241805\n",
      "Iteration 22, loss = 0.56869336\n",
      "Iteration 23, loss = 0.56514691\n",
      "Iteration 24, loss = 0.56153707\n",
      "Iteration 25, loss = 0.55805176\n",
      "Iteration 26, loss = 0.55462765\n",
      "Iteration 27, loss = 0.55130552\n",
      "Iteration 28, loss = 0.54815299\n",
      "Iteration 29, loss = 0.54495745\n",
      "Iteration 30, loss = 0.54180727\n",
      "Iteration 31, loss = 0.53880178\n",
      "Iteration 32, loss = 0.53572793\n",
      "Iteration 33, loss = 0.53279014\n",
      "Iteration 34, loss = 0.52990326\n",
      "Iteration 35, loss = 0.52708901\n",
      "Iteration 36, loss = 0.52446689\n",
      "Iteration 37, loss = 0.52163271\n",
      "Iteration 38, loss = 0.51911396\n",
      "Iteration 39, loss = 0.51647463\n",
      "Iteration 40, loss = 0.51396975\n",
      "Iteration 41, loss = 0.51159352\n",
      "Iteration 42, loss = 0.50915488\n",
      "Iteration 43, loss = 0.50676599\n",
      "Iteration 44, loss = 0.50458237\n",
      "Iteration 45, loss = 0.50241180\n",
      "Iteration 46, loss = 0.50010419\n",
      "Iteration 47, loss = 0.49807753\n",
      "Iteration 48, loss = 0.49601751\n",
      "Iteration 49, loss = 0.49399024\n",
      "Iteration 50, loss = 0.49208402\n",
      "Iteration 51, loss = 0.49018222\n",
      "Iteration 52, loss = 0.48830357\n",
      "Iteration 53, loss = 0.48669588\n",
      "Iteration 54, loss = 0.48479059\n",
      "Iteration 55, loss = 0.48307179\n",
      "Iteration 56, loss = 0.48133587\n",
      "Iteration 57, loss = 0.47992023\n",
      "Iteration 58, loss = 0.47852299\n",
      "Iteration 59, loss = 0.47681130\n",
      "Iteration 60, loss = 0.47541018\n",
      "Iteration 61, loss = 0.47387377\n",
      "Iteration 62, loss = 0.47263538\n",
      "Iteration 63, loss = 0.47136014\n",
      "Iteration 64, loss = 0.47005052\n",
      "Iteration 65, loss = 0.46894946\n",
      "Iteration 66, loss = 0.46772137\n",
      "Iteration 67, loss = 0.46654570\n",
      "Iteration 68, loss = 0.46555204\n",
      "Iteration 69, loss = 0.46442095\n",
      "Iteration 70, loss = 0.46342522\n",
      "Iteration 71, loss = 0.46252973\n",
      "Iteration 72, loss = 0.46158231\n",
      "Iteration 73, loss = 0.46064807\n",
      "Iteration 74, loss = 0.45969396\n",
      "Iteration 75, loss = 0.45898074\n",
      "Iteration 76, loss = 0.45820702\n",
      "Iteration 77, loss = 0.45759607\n",
      "Iteration 78, loss = 0.45688952\n",
      "Iteration 79, loss = 0.45608698\n",
      "Iteration 80, loss = 0.45549737\n",
      "Iteration 81, loss = 0.45484013\n",
      "Iteration 82, loss = 0.45431114\n",
      "Iteration 83, loss = 0.45400559\n",
      "Iteration 84, loss = 0.45334171\n",
      "Iteration 85, loss = 0.45286163\n",
      "Iteration 86, loss = 0.45228853\n",
      "Iteration 87, loss = 0.45204281\n",
      "Iteration 88, loss = 0.45164698\n",
      "Iteration 89, loss = 0.45112679\n",
      "Iteration 90, loss = 0.45074782\n",
      "Iteration 91, loss = 0.45041176\n",
      "Iteration 92, loss = 0.45008039\n",
      "Iteration 93, loss = 0.44972861\n",
      "Iteration 94, loss = 0.44949838\n",
      "Iteration 95, loss = 0.44919769\n",
      "Iteration 96, loss = 0.44896168\n",
      "Iteration 97, loss = 0.44872180\n",
      "Iteration 98, loss = 0.44842727\n",
      "Iteration 99, loss = 0.44819773\n",
      "Iteration 100, loss = 0.44799064\n",
      "Iteration 101, loss = 0.44790707\n",
      "Iteration 102, loss = 0.44767127\n",
      "Iteration 103, loss = 0.44753931\n",
      "Iteration 104, loss = 0.44725209\n",
      "Iteration 105, loss = 0.44712785\n",
      "Iteration 106, loss = 0.44702319\n",
      "Iteration 107, loss = 0.44681822\n",
      "Iteration 108, loss = 0.44676494\n",
      "Iteration 109, loss = 0.44664142\n",
      "Iteration 110, loss = 0.44648411\n",
      "Iteration 111, loss = 0.44630186\n",
      "Iteration 112, loss = 0.44619817\n",
      "Iteration 113, loss = 0.44605517\n",
      "Iteration 114, loss = 0.44596769\n",
      "Iteration 115, loss = 0.44591012\n",
      "Iteration 116, loss = 0.44579619\n",
      "Iteration 117, loss = 0.44570149\n",
      "Iteration 118, loss = 0.44564596\n",
      "Iteration 119, loss = 0.44561080\n",
      "Iteration 120, loss = 0.44553052\n",
      "Iteration 121, loss = 0.44541463\n",
      "Iteration 122, loss = 0.44549334\n",
      "Iteration 123, loss = 0.44535563\n",
      "Iteration 124, loss = 0.44522901\n",
      "Iteration 125, loss = 0.44512386\n",
      "Iteration 126, loss = 0.44513537\n",
      "Iteration 127, loss = 0.44504376\n",
      "Iteration 128, loss = 0.44500494\n",
      "Iteration 129, loss = 0.44494792\n",
      "Iteration 130, loss = 0.44496899\n",
      "Iteration 131, loss = 0.44484299\n",
      "Iteration 132, loss = 0.44478343\n",
      "Iteration 133, loss = 0.44478854\n",
      "Iteration 134, loss = 0.44469884\n",
      "Iteration 135, loss = 0.44468770\n",
      "Iteration 136, loss = 0.44467766\n",
      "Iteration 137, loss = 0.44468772\n",
      "Iteration 138, loss = 0.44452541\n",
      "Iteration 139, loss = 0.44454314\n",
      "Iteration 140, loss = 0.44456935\n",
      "Iteration 141, loss = 0.44447731\n",
      "Iteration 142, loss = 0.44456497\n",
      "Iteration 143, loss = 0.44439456\n",
      "Iteration 144, loss = 0.44436837\n",
      "Iteration 145, loss = 0.44434998\n",
      "Iteration 146, loss = 0.44439812\n",
      "Iteration 147, loss = 0.44439528\n",
      "Iteration 148, loss = 0.44423677\n",
      "Iteration 149, loss = 0.44419004\n",
      "Iteration 150, loss = 0.44420799\n",
      "Iteration 151, loss = 0.44419027\n",
      "Iteration 152, loss = 0.44420542\n",
      "Iteration 153, loss = 0.44428470\n",
      "Iteration 154, loss = 0.44411936\n",
      "Iteration 155, loss = 0.44408796\n",
      "Iteration 156, loss = 0.44399554\n",
      "Iteration 157, loss = 0.44400454\n",
      "Iteration 158, loss = 0.44397397\n",
      "Iteration 159, loss = 0.44392950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74973181\n",
      "Iteration 2, loss = 0.71530954\n",
      "Iteration 3, loss = 0.68650194\n",
      "Iteration 4, loss = 0.66331217\n",
      "Iteration 5, loss = 0.64773896\n",
      "Iteration 6, loss = 0.63645322\n",
      "Iteration 7, loss = 0.62839655\n",
      "Iteration 8, loss = 0.62338442\n",
      "Iteration 9, loss = 0.61832576\n",
      "Iteration 10, loss = 0.61358740\n",
      "Iteration 11, loss = 0.60883765\n",
      "Iteration 12, loss = 0.60327075\n",
      "Iteration 13, loss = 0.59718155\n",
      "Iteration 14, loss = 0.59181441\n",
      "Iteration 15, loss = 0.58640580\n",
      "Iteration 16, loss = 0.58129704\n",
      "Iteration 17, loss = 0.57640207\n",
      "Iteration 18, loss = 0.57177069\n",
      "Iteration 19, loss = 0.56738049\n",
      "Iteration 20, loss = 0.56303261\n",
      "Iteration 21, loss = 0.55869533\n",
      "Iteration 22, loss = 0.55444501\n",
      "Iteration 23, loss = 0.55016948\n",
      "Iteration 24, loss = 0.54595286\n",
      "Iteration 25, loss = 0.54187063\n",
      "Iteration 26, loss = 0.53804580\n",
      "Iteration 27, loss = 0.53404378\n",
      "Iteration 28, loss = 0.53020269\n",
      "Iteration 29, loss = 0.52656514\n",
      "Iteration 30, loss = 0.52279747\n",
      "Iteration 31, loss = 0.51930671\n",
      "Iteration 32, loss = 0.51593698\n",
      "Iteration 33, loss = 0.51236396\n",
      "Iteration 34, loss = 0.50896924\n",
      "Iteration 35, loss = 0.50568878\n",
      "Iteration 36, loss = 0.50272498\n",
      "Iteration 37, loss = 0.49949455\n",
      "Iteration 38, loss = 0.49651357\n",
      "Iteration 39, loss = 0.49359704\n",
      "Iteration 40, loss = 0.49067191\n",
      "Iteration 41, loss = 0.48780963\n",
      "Iteration 42, loss = 0.48511241\n",
      "Iteration 43, loss = 0.48247273\n",
      "Iteration 44, loss = 0.47987213\n",
      "Iteration 45, loss = 0.47734984\n",
      "Iteration 46, loss = 0.47488063\n",
      "Iteration 47, loss = 0.47256346\n",
      "Iteration 48, loss = 0.47008638\n",
      "Iteration 49, loss = 0.46777172\n",
      "Iteration 50, loss = 0.46573094\n",
      "Iteration 51, loss = 0.46351837\n",
      "Iteration 52, loss = 0.46147603\n",
      "Iteration 53, loss = 0.45932416\n",
      "Iteration 54, loss = 0.45746244\n",
      "Iteration 55, loss = 0.45558199\n",
      "Iteration 56, loss = 0.45367604\n",
      "Iteration 57, loss = 0.45187981\n",
      "Iteration 58, loss = 0.45013855\n",
      "Iteration 59, loss = 0.44841592\n",
      "Iteration 60, loss = 0.44678509\n",
      "Iteration 61, loss = 0.44519305\n",
      "Iteration 62, loss = 0.44377912\n",
      "Iteration 63, loss = 0.44220893\n",
      "Iteration 64, loss = 0.44078986\n",
      "Iteration 65, loss = 0.43935591\n",
      "Iteration 66, loss = 0.43796503\n",
      "Iteration 67, loss = 0.43669261\n",
      "Iteration 68, loss = 0.43547277\n",
      "Iteration 69, loss = 0.43430647\n",
      "Iteration 70, loss = 0.43318080\n",
      "Iteration 71, loss = 0.43206070\n",
      "Iteration 72, loss = 0.43096744\n",
      "Iteration 73, loss = 0.42990199\n",
      "Iteration 74, loss = 0.42902677\n",
      "Iteration 75, loss = 0.42811851\n",
      "Iteration 76, loss = 0.42718316\n",
      "Iteration 77, loss = 0.42629106\n",
      "Iteration 78, loss = 0.42549804\n",
      "Iteration 79, loss = 0.42476790\n",
      "Iteration 80, loss = 0.42416544\n",
      "Iteration 81, loss = 0.42342481\n",
      "Iteration 82, loss = 0.42266566\n",
      "Iteration 83, loss = 0.42214464\n",
      "Iteration 84, loss = 0.42147526\n",
      "Iteration 85, loss = 0.42092537\n",
      "Iteration 86, loss = 0.42030361\n",
      "Iteration 87, loss = 0.41984181\n",
      "Iteration 88, loss = 0.41947008\n",
      "Iteration 89, loss = 0.41888000\n",
      "Iteration 90, loss = 0.41839545\n",
      "Iteration 91, loss = 0.41812999\n",
      "Iteration 92, loss = 0.41775689\n",
      "Iteration 93, loss = 0.41734832\n",
      "Iteration 94, loss = 0.41712038\n",
      "Iteration 95, loss = 0.41668664\n",
      "Iteration 96, loss = 0.41645406\n",
      "Iteration 97, loss = 0.41611417\n",
      "Iteration 98, loss = 0.41590015\n",
      "Iteration 99, loss = 0.41557164\n",
      "Iteration 100, loss = 0.41530631\n",
      "Iteration 101, loss = 0.41517075\n",
      "Iteration 102, loss = 0.41485905\n",
      "Iteration 103, loss = 0.41471781\n",
      "Iteration 104, loss = 0.41452607\n",
      "Iteration 105, loss = 0.41430907\n",
      "Iteration 106, loss = 0.41412320\n",
      "Iteration 107, loss = 0.41404720\n",
      "Iteration 108, loss = 0.41392312\n",
      "Iteration 109, loss = 0.41371860\n",
      "Iteration 110, loss = 0.41357160\n",
      "Iteration 111, loss = 0.41343738\n",
      "Iteration 112, loss = 0.41333510\n",
      "Iteration 113, loss = 0.41322985\n",
      "Iteration 114, loss = 0.41311893\n",
      "Iteration 115, loss = 0.41302058\n",
      "Iteration 116, loss = 0.41291265\n",
      "Iteration 117, loss = 0.41285878\n",
      "Iteration 118, loss = 0.41276956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119, loss = 0.41266220\n",
      "Iteration 120, loss = 0.41257022\n",
      "Iteration 121, loss = 0.41250984\n",
      "Iteration 122, loss = 0.41255054\n",
      "Iteration 123, loss = 0.41234222\n",
      "Iteration 124, loss = 0.41242462\n",
      "Iteration 125, loss = 0.41221446\n",
      "Iteration 126, loss = 0.41219567\n",
      "Iteration 127, loss = 0.41212019\n",
      "Iteration 128, loss = 0.41208484\n",
      "Iteration 129, loss = 0.41206624\n",
      "Iteration 130, loss = 0.41193852\n",
      "Iteration 131, loss = 0.41198089\n",
      "Iteration 132, loss = 0.41194533\n",
      "Iteration 133, loss = 0.41192566\n",
      "Iteration 134, loss = 0.41178831\n",
      "Iteration 135, loss = 0.41180069\n",
      "Iteration 136, loss = 0.41172475\n",
      "Iteration 137, loss = 0.41174964\n",
      "Iteration 138, loss = 0.41163030\n",
      "Iteration 139, loss = 0.41167597\n",
      "Iteration 140, loss = 0.41159343\n",
      "Iteration 141, loss = 0.41154793\n",
      "Iteration 142, loss = 0.41151448\n",
      "Iteration 143, loss = 0.41147555\n",
      "Iteration 144, loss = 0.41142621\n",
      "Iteration 145, loss = 0.41155071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75072267\n",
      "Iteration 2, loss = 0.71621949\n",
      "Iteration 3, loss = 0.68706179\n",
      "Iteration 4, loss = 0.66505685\n",
      "Iteration 5, loss = 0.65028171\n",
      "Iteration 6, loss = 0.63925835\n",
      "Iteration 7, loss = 0.63130218\n",
      "Iteration 8, loss = 0.62729556\n",
      "Iteration 9, loss = 0.62245579\n",
      "Iteration 10, loss = 0.61804582\n",
      "Iteration 11, loss = 0.61373162\n",
      "Iteration 12, loss = 0.60892985\n",
      "Iteration 13, loss = 0.60342161\n",
      "Iteration 14, loss = 0.59872660\n",
      "Iteration 15, loss = 0.59394112\n",
      "Iteration 16, loss = 0.58944394\n",
      "Iteration 17, loss = 0.58524351\n",
      "Iteration 18, loss = 0.58110013\n",
      "Iteration 19, loss = 0.57732722\n",
      "Iteration 20, loss = 0.57353308\n",
      "Iteration 21, loss = 0.56980644\n",
      "Iteration 22, loss = 0.56618844\n",
      "Iteration 23, loss = 0.56253888\n",
      "Iteration 24, loss = 0.55896683\n",
      "Iteration 25, loss = 0.55543081\n",
      "Iteration 26, loss = 0.55212151\n",
      "Iteration 27, loss = 0.54881840\n",
      "Iteration 28, loss = 0.54551460\n",
      "Iteration 29, loss = 0.54233026\n",
      "Iteration 30, loss = 0.53908603\n",
      "Iteration 31, loss = 0.53599376\n",
      "Iteration 32, loss = 0.53327482\n",
      "Iteration 33, loss = 0.53016076\n",
      "Iteration 34, loss = 0.52719094\n",
      "Iteration 35, loss = 0.52434896\n",
      "Iteration 36, loss = 0.52181617\n",
      "Iteration 37, loss = 0.51892862\n",
      "Iteration 38, loss = 0.51638608\n",
      "Iteration 39, loss = 0.51383543\n",
      "Iteration 40, loss = 0.51129223\n",
      "Iteration 41, loss = 0.50871669\n",
      "Iteration 42, loss = 0.50630211\n",
      "Iteration 43, loss = 0.50394465\n",
      "Iteration 44, loss = 0.50158593\n",
      "Iteration 45, loss = 0.49934996\n",
      "Iteration 46, loss = 0.49716342\n",
      "Iteration 47, loss = 0.49492972\n",
      "Iteration 48, loss = 0.49282503\n",
      "Iteration 49, loss = 0.49074696\n",
      "Iteration 50, loss = 0.48880960\n",
      "Iteration 51, loss = 0.48684716\n",
      "Iteration 52, loss = 0.48482368\n",
      "Iteration 53, loss = 0.48283578\n",
      "Iteration 54, loss = 0.48116567\n",
      "Iteration 55, loss = 0.47934900\n",
      "Iteration 56, loss = 0.47753731\n",
      "Iteration 57, loss = 0.47588474\n",
      "Iteration 58, loss = 0.47419258\n",
      "Iteration 59, loss = 0.47267839\n",
      "Iteration 60, loss = 0.47092098\n",
      "Iteration 61, loss = 0.46957171\n",
      "Iteration 62, loss = 0.46811334\n",
      "Iteration 63, loss = 0.46669176\n",
      "Iteration 64, loss = 0.46510472\n",
      "Iteration 65, loss = 0.46383618\n",
      "Iteration 66, loss = 0.46263632\n",
      "Iteration 67, loss = 0.46122390\n",
      "Iteration 68, loss = 0.46003795\n",
      "Iteration 69, loss = 0.45890832\n",
      "Iteration 70, loss = 0.45781921\n",
      "Iteration 71, loss = 0.45669456\n",
      "Iteration 72, loss = 0.45553970\n",
      "Iteration 73, loss = 0.45449946\n",
      "Iteration 74, loss = 0.45372985\n",
      "Iteration 75, loss = 0.45266161\n",
      "Iteration 76, loss = 0.45181352\n",
      "Iteration 77, loss = 0.45091736\n",
      "Iteration 78, loss = 0.45009182\n",
      "Iteration 79, loss = 0.44929069\n",
      "Iteration 80, loss = 0.44861638\n",
      "Iteration 81, loss = 0.44793589\n",
      "Iteration 82, loss = 0.44714381\n",
      "Iteration 83, loss = 0.44672318\n",
      "Iteration 84, loss = 0.44587914\n",
      "Iteration 85, loss = 0.44533472\n",
      "Iteration 86, loss = 0.44473123\n",
      "Iteration 87, loss = 0.44421958\n",
      "Iteration 88, loss = 0.44377935\n",
      "Iteration 89, loss = 0.44324187\n",
      "Iteration 90, loss = 0.44274824\n",
      "Iteration 91, loss = 0.44239068\n",
      "Iteration 92, loss = 0.44195899\n",
      "Iteration 93, loss = 0.44155848\n",
      "Iteration 94, loss = 0.44125568\n",
      "Iteration 95, loss = 0.44086261\n",
      "Iteration 96, loss = 0.44052574\n",
      "Iteration 97, loss = 0.44035050\n",
      "Iteration 98, loss = 0.44012240\n",
      "Iteration 99, loss = 0.43965073\n",
      "Iteration 100, loss = 0.43938001\n",
      "Iteration 101, loss = 0.43921513\n",
      "Iteration 102, loss = 0.43889391\n",
      "Iteration 103, loss = 0.43869419\n",
      "Iteration 104, loss = 0.43856763\n",
      "Iteration 105, loss = 0.43834544\n",
      "Iteration 106, loss = 0.43807737\n",
      "Iteration 107, loss = 0.43794324\n",
      "Iteration 108, loss = 0.43778128\n",
      "Iteration 109, loss = 0.43764240\n",
      "Iteration 110, loss = 0.43753752\n",
      "Iteration 111, loss = 0.43737355\n",
      "Iteration 112, loss = 0.43722029\n",
      "Iteration 113, loss = 0.43713278\n",
      "Iteration 114, loss = 0.43706157\n",
      "Iteration 115, loss = 0.43688268\n",
      "Iteration 116, loss = 0.43679673\n",
      "Iteration 117, loss = 0.43672842\n",
      "Iteration 118, loss = 0.43669092\n",
      "Iteration 119, loss = 0.43652947\n",
      "Iteration 120, loss = 0.43649936\n",
      "Iteration 121, loss = 0.43638728\n",
      "Iteration 122, loss = 0.43638218\n",
      "Iteration 123, loss = 0.43619460\n",
      "Iteration 124, loss = 0.43617351\n",
      "Iteration 125, loss = 0.43615119\n",
      "Iteration 126, loss = 0.43606559\n",
      "Iteration 127, loss = 0.43599310\n",
      "Iteration 128, loss = 0.43592249\n",
      "Iteration 129, loss = 0.43590755\n",
      "Iteration 130, loss = 0.43580382\n",
      "Iteration 131, loss = 0.43580291\n",
      "Iteration 132, loss = 0.43571684\n",
      "Iteration 133, loss = 0.43575141\n",
      "Iteration 134, loss = 0.43566236\n",
      "Iteration 135, loss = 0.43567875\n",
      "Iteration 136, loss = 0.43558337\n",
      "Iteration 137, loss = 0.43557179\n",
      "Iteration 138, loss = 0.43556680\n",
      "Iteration 139, loss = 0.43556530\n",
      "Iteration 140, loss = 0.43545188\n",
      "Iteration 141, loss = 0.43549179\n",
      "Iteration 142, loss = 0.43542658\n",
      "Iteration 143, loss = 0.43534725\n",
      "Iteration 144, loss = 0.43541497\n",
      "Iteration 145, loss = 0.43533189\n",
      "Iteration 146, loss = 0.43537683\n",
      "Iteration 147, loss = 0.43556926\n",
      "Iteration 148, loss = 0.43540044\n",
      "Iteration 149, loss = 0.43538082\n",
      "Iteration 150, loss = 0.43523076\n",
      "Iteration 151, loss = 0.43520333\n",
      "Iteration 152, loss = 0.43522922\n",
      "Iteration 153, loss = 0.43512779\n",
      "Iteration 154, loss = 0.43516424\n",
      "Iteration 155, loss = 0.43524380\n",
      "Iteration 156, loss = 0.43514650\n",
      "Iteration 157, loss = 0.43506033\n",
      "Iteration 158, loss = 0.43511860\n",
      "Iteration 159, loss = 0.43509145\n",
      "Iteration 160, loss = 0.43499376\n",
      "Iteration 161, loss = 0.43500572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75245108\n",
      "Iteration 2, loss = 0.71880130\n",
      "Iteration 3, loss = 0.68933235\n",
      "Iteration 4, loss = 0.66865834\n",
      "Iteration 5, loss = 0.65261871\n",
      "Iteration 6, loss = 0.64170473\n",
      "Iteration 7, loss = 0.63397378\n",
      "Iteration 8, loss = 0.62898609\n",
      "Iteration 9, loss = 0.62457573\n",
      "Iteration 10, loss = 0.62023036\n",
      "Iteration 11, loss = 0.61608725\n",
      "Iteration 12, loss = 0.61130071\n",
      "Iteration 13, loss = 0.60657607\n",
      "Iteration 14, loss = 0.60248393\n",
      "Iteration 15, loss = 0.59702067\n",
      "Iteration 16, loss = 0.59258065\n",
      "Iteration 17, loss = 0.58857395\n",
      "Iteration 18, loss = 0.58443313\n",
      "Iteration 19, loss = 0.58075582\n",
      "Iteration 20, loss = 0.57662359\n",
      "Iteration 21, loss = 0.57305442\n",
      "Iteration 22, loss = 0.56944183\n",
      "Iteration 23, loss = 0.56560306\n",
      "Iteration 24, loss = 0.56206829\n",
      "Iteration 25, loss = 0.55865977\n",
      "Iteration 26, loss = 0.55506548\n",
      "Iteration 27, loss = 0.55161365\n",
      "Iteration 28, loss = 0.54840599\n",
      "Iteration 29, loss = 0.54500274\n",
      "Iteration 30, loss = 0.54185845\n",
      "Iteration 31, loss = 0.53860301\n",
      "Iteration 32, loss = 0.53552072\n",
      "Iteration 33, loss = 0.53246912\n",
      "Iteration 34, loss = 0.52939153\n",
      "Iteration 35, loss = 0.52652473\n",
      "Iteration 36, loss = 0.52357205\n",
      "Iteration 37, loss = 0.52067568\n",
      "Iteration 38, loss = 0.51776966\n",
      "Iteration 39, loss = 0.51500064\n",
      "Iteration 40, loss = 0.51232151\n",
      "Iteration 41, loss = 0.50955444\n",
      "Iteration 42, loss = 0.50701377\n",
      "Iteration 43, loss = 0.50432456\n",
      "Iteration 44, loss = 0.50194379\n",
      "Iteration 45, loss = 0.49929173\n",
      "Iteration 46, loss = 0.49688512\n",
      "Iteration 47, loss = 0.49437602\n",
      "Iteration 48, loss = 0.49197196\n",
      "Iteration 49, loss = 0.48964377\n",
      "Iteration 50, loss = 0.48751382\n",
      "Iteration 51, loss = 0.48512674\n",
      "Iteration 52, loss = 0.48284327\n",
      "Iteration 53, loss = 0.48083294\n",
      "Iteration 54, loss = 0.47874597\n",
      "Iteration 55, loss = 0.47657907\n",
      "Iteration 56, loss = 0.47454382\n",
      "Iteration 57, loss = 0.47263688\n",
      "Iteration 58, loss = 0.47077146\n",
      "Iteration 59, loss = 0.46878405\n",
      "Iteration 60, loss = 0.46692010\n",
      "Iteration 61, loss = 0.46512907\n",
      "Iteration 62, loss = 0.46353213\n",
      "Iteration 63, loss = 0.46173625\n",
      "Iteration 64, loss = 0.46010517\n",
      "Iteration 65, loss = 0.45850454\n",
      "Iteration 66, loss = 0.45700531\n",
      "Iteration 67, loss = 0.45561972\n",
      "Iteration 68, loss = 0.45407514\n",
      "Iteration 69, loss = 0.45268190\n",
      "Iteration 70, loss = 0.45154409\n",
      "Iteration 71, loss = 0.45008429\n",
      "Iteration 72, loss = 0.44887893\n",
      "Iteration 73, loss = 0.44768162\n",
      "Iteration 74, loss = 0.44649606\n",
      "Iteration 75, loss = 0.44539878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.44442520\n",
      "Iteration 77, loss = 0.44334335\n",
      "Iteration 78, loss = 0.44250006\n",
      "Iteration 79, loss = 0.44149503\n",
      "Iteration 80, loss = 0.44053045\n",
      "Iteration 81, loss = 0.43974730\n",
      "Iteration 82, loss = 0.43904520\n",
      "Iteration 83, loss = 0.43824145\n",
      "Iteration 84, loss = 0.43742352\n",
      "Iteration 85, loss = 0.43685617\n",
      "Iteration 86, loss = 0.43607738\n",
      "Iteration 87, loss = 0.43546626\n",
      "Iteration 88, loss = 0.43503631\n",
      "Iteration 89, loss = 0.43430738\n",
      "Iteration 90, loss = 0.43375222\n",
      "Iteration 91, loss = 0.43327235\n",
      "Iteration 92, loss = 0.43287506\n",
      "Iteration 93, loss = 0.43226976\n",
      "Iteration 94, loss = 0.43193632\n",
      "Iteration 95, loss = 0.43136493\n",
      "Iteration 96, loss = 0.43100314\n",
      "Iteration 97, loss = 0.43062913\n",
      "Iteration 98, loss = 0.43025612\n",
      "Iteration 99, loss = 0.42988047\n",
      "Iteration 100, loss = 0.42955933\n",
      "Iteration 101, loss = 0.42937093\n",
      "Iteration 102, loss = 0.42898003\n",
      "Iteration 103, loss = 0.42871002\n",
      "Iteration 104, loss = 0.42844982\n",
      "Iteration 105, loss = 0.42816520\n",
      "Iteration 106, loss = 0.42792295\n",
      "Iteration 107, loss = 0.42768996\n",
      "Iteration 108, loss = 0.42749162\n",
      "Iteration 109, loss = 0.42723361\n",
      "Iteration 110, loss = 0.42702929\n",
      "Iteration 111, loss = 0.42690911\n",
      "Iteration 112, loss = 0.42666830\n",
      "Iteration 113, loss = 0.42648946\n",
      "Iteration 114, loss = 0.42633330\n",
      "Iteration 115, loss = 0.42622634\n",
      "Iteration 116, loss = 0.42600124\n",
      "Iteration 117, loss = 0.42585577\n",
      "Iteration 118, loss = 0.42570612\n",
      "Iteration 119, loss = 0.42557332\n",
      "Iteration 120, loss = 0.42542407\n",
      "Iteration 121, loss = 0.42529704\n",
      "Iteration 122, loss = 0.42520585\n",
      "Iteration 123, loss = 0.42505608\n",
      "Iteration 124, loss = 0.42493324\n",
      "Iteration 125, loss = 0.42492117\n",
      "Iteration 126, loss = 0.42471098\n",
      "Iteration 127, loss = 0.42460666\n",
      "Iteration 128, loss = 0.42455211\n",
      "Iteration 129, loss = 0.42443665\n",
      "Iteration 130, loss = 0.42439300\n",
      "Iteration 131, loss = 0.42423354\n",
      "Iteration 132, loss = 0.42424013\n",
      "Iteration 133, loss = 0.42407647\n",
      "Iteration 134, loss = 0.42398853\n",
      "Iteration 135, loss = 0.42391310\n",
      "Iteration 136, loss = 0.42387348\n",
      "Iteration 137, loss = 0.42374531\n",
      "Iteration 138, loss = 0.42368379\n",
      "Iteration 139, loss = 0.42363538\n",
      "Iteration 140, loss = 0.42360128\n",
      "Iteration 141, loss = 0.42347105\n",
      "Iteration 142, loss = 0.42357333\n",
      "Iteration 143, loss = 0.42349773\n",
      "Iteration 144, loss = 0.42332358\n",
      "Iteration 145, loss = 0.42324776\n",
      "Iteration 146, loss = 0.42323157\n",
      "Iteration 147, loss = 0.42318317\n",
      "Iteration 148, loss = 0.42309237\n",
      "Iteration 149, loss = 0.42302741\n",
      "Iteration 150, loss = 0.42291885\n",
      "Iteration 151, loss = 0.42286548\n",
      "Iteration 152, loss = 0.42284141\n",
      "Iteration 153, loss = 0.42293994\n",
      "Iteration 154, loss = 0.42272394\n",
      "Iteration 155, loss = 0.42264905\n",
      "Iteration 156, loss = 0.42263686\n",
      "Iteration 157, loss = 0.42259173\n",
      "Iteration 158, loss = 0.42258433\n",
      "Iteration 159, loss = 0.42262837\n",
      "Iteration 160, loss = 0.42250082\n",
      "Iteration 161, loss = 0.42242566\n",
      "Iteration 162, loss = 0.42240172\n",
      "Iteration 163, loss = 0.42234889\n",
      "Iteration 164, loss = 0.42241869\n",
      "Iteration 165, loss = 0.42232187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75164104\n",
      "Iteration 2, loss = 0.71827850\n",
      "Iteration 3, loss = 0.68858057\n",
      "Iteration 4, loss = 0.66801434\n",
      "Iteration 5, loss = 0.65193950\n",
      "Iteration 6, loss = 0.64122605\n",
      "Iteration 7, loss = 0.63356102\n",
      "Iteration 8, loss = 0.62835427\n",
      "Iteration 9, loss = 0.62394606\n",
      "Iteration 10, loss = 0.61948848\n",
      "Iteration 11, loss = 0.61514024\n",
      "Iteration 12, loss = 0.61037890\n",
      "Iteration 13, loss = 0.60536845\n",
      "Iteration 14, loss = 0.60128338\n",
      "Iteration 15, loss = 0.59575630\n",
      "Iteration 16, loss = 0.59119016\n",
      "Iteration 17, loss = 0.58693434\n",
      "Iteration 18, loss = 0.58287958\n",
      "Iteration 19, loss = 0.57905939\n",
      "Iteration 20, loss = 0.57486229\n",
      "Iteration 21, loss = 0.57090941\n",
      "Iteration 22, loss = 0.56715480\n",
      "Iteration 23, loss = 0.56329045\n",
      "Iteration 24, loss = 0.55962201\n",
      "Iteration 25, loss = 0.55595355\n",
      "Iteration 26, loss = 0.55231228\n",
      "Iteration 27, loss = 0.54859994\n",
      "Iteration 28, loss = 0.54526241\n",
      "Iteration 29, loss = 0.54171776\n",
      "Iteration 30, loss = 0.53837163\n",
      "Iteration 31, loss = 0.53505863\n",
      "Iteration 32, loss = 0.53171057\n",
      "Iteration 33, loss = 0.52846507\n",
      "Iteration 34, loss = 0.52516939\n",
      "Iteration 35, loss = 0.52211934\n",
      "Iteration 36, loss = 0.51903760\n",
      "Iteration 37, loss = 0.51596943\n",
      "Iteration 38, loss = 0.51295133\n",
      "Iteration 39, loss = 0.50997802\n",
      "Iteration 40, loss = 0.50707791\n",
      "Iteration 41, loss = 0.50427985\n",
      "Iteration 42, loss = 0.50149063\n",
      "Iteration 43, loss = 0.49873658\n",
      "Iteration 44, loss = 0.49600056\n",
      "Iteration 45, loss = 0.49346332\n",
      "Iteration 46, loss = 0.49075283\n",
      "Iteration 47, loss = 0.48822316\n",
      "Iteration 48, loss = 0.48569150\n",
      "Iteration 49, loss = 0.48330323\n",
      "Iteration 50, loss = 0.48083640\n",
      "Iteration 51, loss = 0.47846257\n",
      "Iteration 52, loss = 0.47616408\n",
      "Iteration 53, loss = 0.47413820\n",
      "Iteration 54, loss = 0.47175076\n",
      "Iteration 55, loss = 0.46955184\n",
      "Iteration 56, loss = 0.46737556\n",
      "Iteration 57, loss = 0.46544666\n",
      "Iteration 58, loss = 0.46358028\n",
      "Iteration 59, loss = 0.46148003\n",
      "Iteration 60, loss = 0.45950267\n",
      "Iteration 61, loss = 0.45779101\n",
      "Iteration 62, loss = 0.45610574\n",
      "Iteration 63, loss = 0.45436660\n",
      "Iteration 64, loss = 0.45263528\n",
      "Iteration 65, loss = 0.45094405\n",
      "Iteration 66, loss = 0.44942913\n",
      "Iteration 67, loss = 0.44794344\n",
      "Iteration 68, loss = 0.44650063\n",
      "Iteration 69, loss = 0.44499707\n",
      "Iteration 70, loss = 0.44382313\n",
      "Iteration 71, loss = 0.44238263\n",
      "Iteration 72, loss = 0.44116547\n",
      "Iteration 73, loss = 0.43995594\n",
      "Iteration 74, loss = 0.43871180\n",
      "Iteration 75, loss = 0.43761785\n",
      "Iteration 76, loss = 0.43661126\n",
      "Iteration 77, loss = 0.43560909\n",
      "Iteration 78, loss = 0.43468357\n",
      "Iteration 79, loss = 0.43366696\n",
      "Iteration 80, loss = 0.43279493\n",
      "Iteration 81, loss = 0.43193817\n",
      "Iteration 82, loss = 0.43120375\n",
      "Iteration 83, loss = 0.43051221\n",
      "Iteration 84, loss = 0.42971112\n",
      "Iteration 85, loss = 0.42907087\n",
      "Iteration 86, loss = 0.42829572\n",
      "Iteration 87, loss = 0.42777960\n",
      "Iteration 88, loss = 0.42723096\n",
      "Iteration 89, loss = 0.42661829\n",
      "Iteration 90, loss = 0.42607511\n",
      "Iteration 91, loss = 0.42554868\n",
      "Iteration 92, loss = 0.42509730\n",
      "Iteration 93, loss = 0.42461135\n",
      "Iteration 94, loss = 0.42426595\n",
      "Iteration 95, loss = 0.42380621\n",
      "Iteration 96, loss = 0.42347581\n",
      "Iteration 97, loss = 0.42309128\n",
      "Iteration 98, loss = 0.42272394\n",
      "Iteration 99, loss = 0.42243065\n",
      "Iteration 100, loss = 0.42210883\n",
      "Iteration 101, loss = 0.42197038\n",
      "Iteration 102, loss = 0.42158915\n",
      "Iteration 103, loss = 0.42136827\n",
      "Iteration 104, loss = 0.42111651\n",
      "Iteration 105, loss = 0.42087883\n",
      "Iteration 106, loss = 0.42070761\n",
      "Iteration 107, loss = 0.42044516\n",
      "Iteration 108, loss = 0.42031027\n",
      "Iteration 109, loss = 0.42015392\n",
      "Iteration 110, loss = 0.41998660\n",
      "Iteration 111, loss = 0.41978866\n",
      "Iteration 112, loss = 0.41967777\n",
      "Iteration 113, loss = 0.41952798\n",
      "Iteration 114, loss = 0.41939893\n",
      "Iteration 115, loss = 0.41927151\n",
      "Iteration 116, loss = 0.41912524\n",
      "Iteration 117, loss = 0.41902252\n",
      "Iteration 118, loss = 0.41898415\n",
      "Iteration 119, loss = 0.41892157\n",
      "Iteration 120, loss = 0.41878689\n",
      "Iteration 121, loss = 0.41873415\n",
      "Iteration 122, loss = 0.41881295\n",
      "Iteration 123, loss = 0.41854860\n",
      "Iteration 124, loss = 0.41850521\n",
      "Iteration 125, loss = 0.41837107\n",
      "Iteration 126, loss = 0.41830333\n",
      "Iteration 127, loss = 0.41827448\n",
      "Iteration 128, loss = 0.41824354\n",
      "Iteration 129, loss = 0.41817636\n",
      "Iteration 130, loss = 0.41816586\n",
      "Iteration 131, loss = 0.41809114\n",
      "Iteration 132, loss = 0.41802955\n",
      "Iteration 133, loss = 0.41799917\n",
      "Iteration 134, loss = 0.41791627\n",
      "Iteration 135, loss = 0.41787579\n",
      "Iteration 136, loss = 0.41784416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75236790\n",
      "Iteration 2, loss = 0.71781862\n",
      "Iteration 3, loss = 0.68874028\n",
      "Iteration 4, loss = 0.66689037\n",
      "Iteration 5, loss = 0.65059055\n",
      "Iteration 6, loss = 0.64114797\n",
      "Iteration 7, loss = 0.63293528\n",
      "Iteration 8, loss = 0.62808729\n",
      "Iteration 9, loss = 0.62372684\n",
      "Iteration 10, loss = 0.61944118\n",
      "Iteration 11, loss = 0.61513590\n",
      "Iteration 12, loss = 0.61058260\n",
      "Iteration 13, loss = 0.60560733\n",
      "Iteration 14, loss = 0.60150513\n",
      "Iteration 15, loss = 0.59621550\n",
      "Iteration 16, loss = 0.59185238\n",
      "Iteration 17, loss = 0.58748991\n",
      "Iteration 18, loss = 0.58353985\n",
      "Iteration 19, loss = 0.57999408\n",
      "Iteration 20, loss = 0.57602257\n",
      "Iteration 21, loss = 0.57241779\n",
      "Iteration 22, loss = 0.56869310\n",
      "Iteration 23, loss = 0.56514664\n",
      "Iteration 24, loss = 0.56153679\n",
      "Iteration 25, loss = 0.55805147\n",
      "Iteration 26, loss = 0.55462736\n",
      "Iteration 27, loss = 0.55130521\n",
      "Iteration 28, loss = 0.54815268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.54495713\n",
      "Iteration 30, loss = 0.54180694\n",
      "Iteration 31, loss = 0.53880144\n",
      "Iteration 32, loss = 0.53572758\n",
      "Iteration 33, loss = 0.53278979\n",
      "Iteration 34, loss = 0.52990290\n",
      "Iteration 35, loss = 0.52708865\n",
      "Iteration 36, loss = 0.52446652\n",
      "Iteration 37, loss = 0.52163233\n",
      "Iteration 38, loss = 0.51911358\n",
      "Iteration 39, loss = 0.51647424\n",
      "Iteration 40, loss = 0.51396935\n",
      "Iteration 41, loss = 0.51159312\n",
      "Iteration 42, loss = 0.50915447\n",
      "Iteration 43, loss = 0.50676557\n",
      "Iteration 44, loss = 0.50458194\n",
      "Iteration 45, loss = 0.50241136\n",
      "Iteration 46, loss = 0.50010375\n",
      "Iteration 47, loss = 0.49807708\n",
      "Iteration 48, loss = 0.49601706\n",
      "Iteration 49, loss = 0.49398978\n",
      "Iteration 50, loss = 0.49208356\n",
      "Iteration 51, loss = 0.49018175\n",
      "Iteration 52, loss = 0.48830309\n",
      "Iteration 53, loss = 0.48669541\n",
      "Iteration 54, loss = 0.48479011\n",
      "Iteration 55, loss = 0.48307131\n",
      "Iteration 56, loss = 0.48133538\n",
      "Iteration 57, loss = 0.47991974\n",
      "Iteration 58, loss = 0.47852249\n",
      "Iteration 59, loss = 0.47681080\n",
      "Iteration 60, loss = 0.47540967\n",
      "Iteration 61, loss = 0.47387326\n",
      "Iteration 62, loss = 0.47263487\n",
      "Iteration 63, loss = 0.47135962\n",
      "Iteration 64, loss = 0.47005000\n",
      "Iteration 65, loss = 0.46894893\n",
      "Iteration 66, loss = 0.46772084\n",
      "Iteration 67, loss = 0.46654516\n",
      "Iteration 68, loss = 0.46555151\n",
      "Iteration 69, loss = 0.46442041\n",
      "Iteration 70, loss = 0.46342467\n",
      "Iteration 71, loss = 0.46252918\n",
      "Iteration 72, loss = 0.46158176\n",
      "Iteration 73, loss = 0.46064752\n",
      "Iteration 74, loss = 0.45969341\n",
      "Iteration 75, loss = 0.45898018\n",
      "Iteration 76, loss = 0.45820646\n",
      "Iteration 77, loss = 0.45759551\n",
      "Iteration 78, loss = 0.45688896\n",
      "Iteration 79, loss = 0.45608641\n",
      "Iteration 80, loss = 0.45549680\n",
      "Iteration 81, loss = 0.45483956\n",
      "Iteration 82, loss = 0.45431056\n",
      "Iteration 83, loss = 0.45400501\n",
      "Iteration 84, loss = 0.45334113\n",
      "Iteration 85, loss = 0.45286105\n",
      "Iteration 86, loss = 0.45228795\n",
      "Iteration 87, loss = 0.45204223\n",
      "Iteration 88, loss = 0.45164639\n",
      "Iteration 89, loss = 0.45112620\n",
      "Iteration 90, loss = 0.45074722\n",
      "Iteration 91, loss = 0.45041116\n",
      "Iteration 92, loss = 0.45007979\n",
      "Iteration 93, loss = 0.44972802\n",
      "Iteration 94, loss = 0.44949778\n",
      "Iteration 95, loss = 0.44919709\n",
      "Iteration 96, loss = 0.44896108\n",
      "Iteration 97, loss = 0.44872120\n",
      "Iteration 98, loss = 0.44842666\n",
      "Iteration 99, loss = 0.44819713\n",
      "Iteration 100, loss = 0.44799004\n",
      "Iteration 101, loss = 0.44790647\n",
      "Iteration 102, loss = 0.44767066\n",
      "Iteration 103, loss = 0.44753870\n",
      "Iteration 104, loss = 0.44725148\n",
      "Iteration 105, loss = 0.44712724\n",
      "Iteration 106, loss = 0.44702258\n",
      "Iteration 107, loss = 0.44681761\n",
      "Iteration 108, loss = 0.44676433\n",
      "Iteration 109, loss = 0.44664081\n",
      "Iteration 110, loss = 0.44648350\n",
      "Iteration 111, loss = 0.44630124\n",
      "Iteration 112, loss = 0.44619756\n",
      "Iteration 113, loss = 0.44605456\n",
      "Iteration 114, loss = 0.44596708\n",
      "Iteration 115, loss = 0.44590950\n",
      "Iteration 116, loss = 0.44579558\n",
      "Iteration 117, loss = 0.44570087\n",
      "Iteration 118, loss = 0.44564534\n",
      "Iteration 119, loss = 0.44561018\n",
      "Iteration 120, loss = 0.44552990\n",
      "Iteration 121, loss = 0.44541401\n",
      "Iteration 122, loss = 0.44549272\n",
      "Iteration 123, loss = 0.44535501\n",
      "Iteration 124, loss = 0.44522839\n",
      "Iteration 125, loss = 0.44512324\n",
      "Iteration 126, loss = 0.44513474\n",
      "Iteration 127, loss = 0.44504313\n",
      "Iteration 128, loss = 0.44500432\n",
      "Iteration 129, loss = 0.44494730\n",
      "Iteration 130, loss = 0.44496837\n",
      "Iteration 131, loss = 0.44484237\n",
      "Iteration 132, loss = 0.44478281\n",
      "Iteration 133, loss = 0.44478792\n",
      "Iteration 134, loss = 0.44469822\n",
      "Iteration 135, loss = 0.44468708\n",
      "Iteration 136, loss = 0.44467704\n",
      "Iteration 137, loss = 0.44468709\n",
      "Iteration 138, loss = 0.44452478\n",
      "Iteration 139, loss = 0.44454251\n",
      "Iteration 140, loss = 0.44456872\n",
      "Iteration 141, loss = 0.44447668\n",
      "Iteration 142, loss = 0.44456435\n",
      "Iteration 143, loss = 0.44439393\n",
      "Iteration 144, loss = 0.44436774\n",
      "Iteration 145, loss = 0.44434935\n",
      "Iteration 146, loss = 0.44439749\n",
      "Iteration 147, loss = 0.44439465\n",
      "Iteration 148, loss = 0.44423614\n",
      "Iteration 149, loss = 0.44418941\n",
      "Iteration 150, loss = 0.44420736\n",
      "Iteration 151, loss = 0.44418964\n",
      "Iteration 152, loss = 0.44420478\n",
      "Iteration 153, loss = 0.44428406\n",
      "Iteration 154, loss = 0.44411873\n",
      "Iteration 155, loss = 0.44408733\n",
      "Iteration 156, loss = 0.44399490\n",
      "Iteration 157, loss = 0.44400391\n",
      "Iteration 158, loss = 0.44397333\n",
      "Iteration 159, loss = 0.44392886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75655102\n",
      "Iteration 2, loss = 0.70781657\n",
      "Iteration 3, loss = 0.66314489\n",
      "Iteration 4, loss = 0.62732861\n",
      "Iteration 5, loss = 0.59753880\n",
      "Iteration 6, loss = 0.57181014\n",
      "Iteration 7, loss = 0.54953529\n",
      "Iteration 8, loss = 0.53175424\n",
      "Iteration 9, loss = 0.51508273\n",
      "Iteration 10, loss = 0.50150213\n",
      "Iteration 11, loss = 0.48937772\n",
      "Iteration 12, loss = 0.47916104\n",
      "Iteration 13, loss = 0.46905657\n",
      "Iteration 14, loss = 0.46119326\n",
      "Iteration 15, loss = 0.45410954\n",
      "Iteration 16, loss = 0.44785418\n",
      "Iteration 17, loss = 0.44234217\n",
      "Iteration 18, loss = 0.43850239\n",
      "Iteration 19, loss = 0.43444529\n",
      "Iteration 20, loss = 0.43116446\n",
      "Iteration 21, loss = 0.42859814\n",
      "Iteration 22, loss = 0.42605802\n",
      "Iteration 23, loss = 0.42449832\n",
      "Iteration 24, loss = 0.42284434\n",
      "Iteration 25, loss = 0.42174250\n",
      "Iteration 26, loss = 0.42068692\n",
      "Iteration 27, loss = 0.41972525\n",
      "Iteration 28, loss = 0.41904601\n",
      "Iteration 29, loss = 0.41842408\n",
      "Iteration 30, loss = 0.41788128\n",
      "Iteration 31, loss = 0.41743592\n",
      "Iteration 32, loss = 0.41721547\n",
      "Iteration 33, loss = 0.41685562\n",
      "Iteration 34, loss = 0.41651371\n",
      "Iteration 35, loss = 0.41630088\n",
      "Iteration 36, loss = 0.41610896\n",
      "Iteration 37, loss = 0.41586169\n",
      "Iteration 38, loss = 0.41568035\n",
      "Iteration 39, loss = 0.41550236\n",
      "Iteration 40, loss = 0.41538168\n",
      "Iteration 41, loss = 0.41523203\n",
      "Iteration 42, loss = 0.41514256\n",
      "Iteration 43, loss = 0.41503626\n",
      "Iteration 44, loss = 0.41486384\n",
      "Iteration 45, loss = 0.41470775\n",
      "Iteration 46, loss = 0.41462271\n",
      "Iteration 47, loss = 0.41440965\n",
      "Iteration 48, loss = 0.41427056\n",
      "Iteration 49, loss = 0.41413269\n",
      "Iteration 50, loss = 0.41400061\n",
      "Iteration 51, loss = 0.41394125\n",
      "Iteration 52, loss = 0.41380842\n",
      "Iteration 53, loss = 0.41359244\n",
      "Iteration 54, loss = 0.41355827\n",
      "Iteration 55, loss = 0.41339878\n",
      "Iteration 56, loss = 0.41328062\n",
      "Iteration 57, loss = 0.41324160\n",
      "Iteration 58, loss = 0.41316098\n",
      "Iteration 59, loss = 0.41285654\n",
      "Iteration 60, loss = 0.41273941\n",
      "Iteration 61, loss = 0.41283072\n",
      "Iteration 62, loss = 0.41287205\n",
      "Iteration 63, loss = 0.41257751\n",
      "Iteration 64, loss = 0.41238770\n",
      "Iteration 65, loss = 0.41211520\n",
      "Iteration 66, loss = 0.41209146\n",
      "Iteration 67, loss = 0.41188086\n",
      "Iteration 68, loss = 0.41193537\n",
      "Iteration 69, loss = 0.41166278\n",
      "Iteration 70, loss = 0.41160812\n",
      "Iteration 71, loss = 0.41144096\n",
      "Iteration 72, loss = 0.41136005\n",
      "Iteration 73, loss = 0.41119236\n",
      "Iteration 74, loss = 0.41109465\n",
      "Iteration 75, loss = 0.41098529\n",
      "Iteration 76, loss = 0.41093000\n",
      "Iteration 77, loss = 0.41072549\n",
      "Iteration 78, loss = 0.41071467\n",
      "Iteration 79, loss = 0.41045613\n",
      "Iteration 80, loss = 0.41040753\n",
      "Iteration 81, loss = 0.41031437\n",
      "Iteration 82, loss = 0.41014274\n",
      "Iteration 83, loss = 0.41017575\n",
      "Iteration 84, loss = 0.40996937\n",
      "Iteration 85, loss = 0.40979589\n",
      "Iteration 86, loss = 0.40984899\n",
      "Iteration 87, loss = 0.40957526\n",
      "Iteration 88, loss = 0.40959407\n",
      "Iteration 89, loss = 0.40931871\n",
      "Iteration 90, loss = 0.40924805\n",
      "Iteration 91, loss = 0.40921498\n",
      "Iteration 92, loss = 0.40905426\n",
      "Iteration 93, loss = 0.40886965\n",
      "Iteration 94, loss = 0.40883283\n",
      "Iteration 95, loss = 0.40859470\n",
      "Iteration 96, loss = 0.40880839\n",
      "Iteration 97, loss = 0.40839403\n",
      "Iteration 98, loss = 0.40831894\n",
      "Iteration 99, loss = 0.40816818\n",
      "Iteration 100, loss = 0.40819205\n",
      "Iteration 101, loss = 0.40794648\n",
      "Iteration 102, loss = 0.40785835\n",
      "Iteration 103, loss = 0.40765693\n",
      "Iteration 104, loss = 0.40764303\n",
      "Iteration 105, loss = 0.40744174\n",
      "Iteration 106, loss = 0.40735286\n",
      "Iteration 107, loss = 0.40726093\n",
      "Iteration 108, loss = 0.40729413\n",
      "Iteration 109, loss = 0.40699658\n",
      "Iteration 110, loss = 0.40697029\n",
      "Iteration 111, loss = 0.40678945\n",
      "Iteration 112, loss = 0.40658646\n",
      "Iteration 113, loss = 0.40658342\n",
      "Iteration 114, loss = 0.40634611\n",
      "Iteration 115, loss = 0.40628379\n",
      "Iteration 116, loss = 0.40618372\n",
      "Iteration 117, loss = 0.40598909\n",
      "Iteration 118, loss = 0.40594864\n",
      "Iteration 119, loss = 0.40581182\n",
      "Iteration 120, loss = 0.40566023\n",
      "Iteration 121, loss = 0.40566171\n",
      "Iteration 122, loss = 0.40541536\n",
      "Iteration 123, loss = 0.40540424\n",
      "Iteration 124, loss = 0.40538503\n",
      "Iteration 125, loss = 0.40519908\n",
      "Iteration 126, loss = 0.40496485\n",
      "Iteration 127, loss = 0.40484357\n",
      "Iteration 128, loss = 0.40471636\n",
      "Iteration 129, loss = 0.40458365\n",
      "Iteration 130, loss = 0.40445386\n",
      "Iteration 131, loss = 0.40448260\n",
      "Iteration 132, loss = 0.40419868\n",
      "Iteration 133, loss = 0.40411372\n",
      "Iteration 134, loss = 0.40399646\n",
      "Iteration 135, loss = 0.40382597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 136, loss = 0.40368231\n",
      "Iteration 137, loss = 0.40370798\n",
      "Iteration 138, loss = 0.40344603\n",
      "Iteration 139, loss = 0.40351338\n",
      "Iteration 140, loss = 0.40338249\n",
      "Iteration 141, loss = 0.40314555\n",
      "Iteration 142, loss = 0.40298126\n",
      "Iteration 143, loss = 0.40291376\n",
      "Iteration 144, loss = 0.40273134\n",
      "Iteration 145, loss = 0.40288478\n",
      "Iteration 146, loss = 0.40263410\n",
      "Iteration 147, loss = 0.40238755\n",
      "Iteration 148, loss = 0.40241347\n",
      "Iteration 149, loss = 0.40219673\n",
      "Iteration 150, loss = 0.40201163\n",
      "Iteration 151, loss = 0.40202240\n",
      "Iteration 152, loss = 0.40176151\n",
      "Iteration 153, loss = 0.40166051\n",
      "Iteration 154, loss = 0.40163510\n",
      "Iteration 155, loss = 0.40183553\n",
      "Iteration 156, loss = 0.40121195\n",
      "Iteration 157, loss = 0.40109979\n",
      "Iteration 158, loss = 0.40104201\n",
      "Iteration 159, loss = 0.40087374\n",
      "Iteration 160, loss = 0.40078370\n",
      "Iteration 161, loss = 0.40057832\n",
      "Iteration 162, loss = 0.40061751\n",
      "Iteration 163, loss = 0.40039459\n",
      "Iteration 164, loss = 0.40023920\n",
      "Iteration 165, loss = 0.40023453\n",
      "Iteration 166, loss = 0.40004613\n",
      "Iteration 167, loss = 0.39993002\n",
      "Iteration 168, loss = 0.39972695\n",
      "Iteration 169, loss = 0.39972597\n",
      "Iteration 170, loss = 0.39944778\n",
      "Iteration 171, loss = 0.39928003\n",
      "Iteration 172, loss = 0.39929091\n",
      "Iteration 173, loss = 0.39920402\n",
      "Iteration 174, loss = 0.39894228\n",
      "Iteration 175, loss = 0.39874180\n",
      "Iteration 176, loss = 0.39906631\n",
      "Iteration 177, loss = 0.39868722\n",
      "Iteration 178, loss = 0.39849674\n",
      "Iteration 179, loss = 0.39840216\n",
      "Iteration 180, loss = 0.39816242\n",
      "Iteration 181, loss = 0.39819884\n",
      "Iteration 182, loss = 0.39792698\n",
      "Iteration 183, loss = 0.39790758\n",
      "Iteration 184, loss = 0.39770691\n",
      "Iteration 185, loss = 0.39765468\n",
      "Iteration 186, loss = 0.39742633\n",
      "Iteration 187, loss = 0.39745225\n",
      "Iteration 188, loss = 0.39714101\n",
      "Iteration 189, loss = 0.39701021\n",
      "Iteration 190, loss = 0.39701887\n",
      "Iteration 191, loss = 0.39666710\n",
      "Iteration 192, loss = 0.39667852\n",
      "Iteration 193, loss = 0.39650846\n",
      "Iteration 194, loss = 0.39651310\n",
      "Iteration 195, loss = 0.39642314\n",
      "Iteration 196, loss = 0.39613975\n",
      "Iteration 197, loss = 0.39586352\n",
      "Iteration 198, loss = 0.39588017\n",
      "Iteration 199, loss = 0.39571294\n",
      "Iteration 200, loss = 0.39561707\n",
      "Iteration 1, loss = 0.76370564\n",
      "Iteration 2, loss = 0.71709415\n",
      "Iteration 3, loss = 0.67447676\n",
      "Iteration 4, loss = 0.64084281\n",
      "Iteration 5, loss = 0.61220263\n",
      "Iteration 6, loss = 0.58765287\n",
      "Iteration 7, loss = 0.56580839\n",
      "Iteration 8, loss = 0.54906245\n",
      "Iteration 9, loss = 0.53316446\n",
      "Iteration 10, loss = 0.52021648\n",
      "Iteration 11, loss = 0.50845066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.49865077\n",
      "Iteration 13, loss = 0.48924856\n",
      "Iteration 14, loss = 0.48189432\n",
      "Iteration 15, loss = 0.47508564\n",
      "Iteration 16, loss = 0.46934614\n",
      "Iteration 17, loss = 0.46432262\n",
      "Iteration 18, loss = 0.46063720\n",
      "Iteration 19, loss = 0.45693961\n",
      "Iteration 20, loss = 0.45378686\n",
      "Iteration 21, loss = 0.45131613\n",
      "Iteration 22, loss = 0.44909990\n",
      "Iteration 23, loss = 0.44778259\n",
      "Iteration 24, loss = 0.44618625\n",
      "Iteration 25, loss = 0.44522025\n",
      "Iteration 26, loss = 0.44407019\n",
      "Iteration 27, loss = 0.44337731\n",
      "Iteration 28, loss = 0.44272978\n",
      "Iteration 29, loss = 0.44196250\n",
      "Iteration 30, loss = 0.44160958\n",
      "Iteration 31, loss = 0.44103110\n",
      "Iteration 32, loss = 0.44086890\n",
      "Iteration 33, loss = 0.44046749\n",
      "Iteration 34, loss = 0.44009735\n",
      "Iteration 35, loss = 0.43982880\n",
      "Iteration 36, loss = 0.43968061\n",
      "Iteration 37, loss = 0.43937505\n",
      "Iteration 38, loss = 0.43919366\n",
      "Iteration 39, loss = 0.43900726\n",
      "Iteration 40, loss = 0.43893935\n",
      "Iteration 41, loss = 0.43864620\n",
      "Iteration 42, loss = 0.43856740\n",
      "Iteration 43, loss = 0.43843183\n",
      "Iteration 44, loss = 0.43830775\n",
      "Iteration 45, loss = 0.43808021\n",
      "Iteration 46, loss = 0.43796351\n",
      "Iteration 47, loss = 0.43780104\n",
      "Iteration 48, loss = 0.43771534\n",
      "Iteration 49, loss = 0.43761025\n",
      "Iteration 50, loss = 0.43749093\n",
      "Iteration 51, loss = 0.43735533\n",
      "Iteration 52, loss = 0.43718061\n",
      "Iteration 53, loss = 0.43708045\n",
      "Iteration 54, loss = 0.43698937\n",
      "Iteration 55, loss = 0.43682848\n",
      "Iteration 56, loss = 0.43674106\n",
      "Iteration 57, loss = 0.43660843\n",
      "Iteration 58, loss = 0.43650490\n",
      "Iteration 59, loss = 0.43635708\n",
      "Iteration 60, loss = 0.43629161\n",
      "Iteration 61, loss = 0.43624781\n",
      "Iteration 62, loss = 0.43607239\n",
      "Iteration 63, loss = 0.43590168\n",
      "Iteration 64, loss = 0.43586939\n",
      "Iteration 65, loss = 0.43565957\n",
      "Iteration 66, loss = 0.43563562\n",
      "Iteration 67, loss = 0.43539828\n",
      "Iteration 68, loss = 0.43547204\n",
      "Iteration 69, loss = 0.43532453\n",
      "Iteration 70, loss = 0.43514931\n",
      "Iteration 71, loss = 0.43500393\n",
      "Iteration 72, loss = 0.43483209\n",
      "Iteration 73, loss = 0.43485119\n",
      "Iteration 74, loss = 0.43473042\n",
      "Iteration 75, loss = 0.43455311\n",
      "Iteration 76, loss = 0.43447835\n",
      "Iteration 77, loss = 0.43431553\n",
      "Iteration 78, loss = 0.43421823\n",
      "Iteration 79, loss = 0.43405034\n",
      "Iteration 80, loss = 0.43395452\n",
      "Iteration 81, loss = 0.43406756\n",
      "Iteration 82, loss = 0.43373402\n",
      "Iteration 83, loss = 0.43391456\n",
      "Iteration 84, loss = 0.43357454\n",
      "Iteration 85, loss = 0.43336549\n",
      "Iteration 86, loss = 0.43328145\n",
      "Iteration 87, loss = 0.43321421\n",
      "Iteration 88, loss = 0.43314334\n",
      "Iteration 89, loss = 0.43296736\n",
      "Iteration 90, loss = 0.43287527\n",
      "Iteration 91, loss = 0.43286775\n",
      "Iteration 92, loss = 0.43263755\n",
      "Iteration 93, loss = 0.43248445\n",
      "Iteration 94, loss = 0.43273223\n",
      "Iteration 95, loss = 0.43237590\n",
      "Iteration 96, loss = 0.43227084\n",
      "Iteration 97, loss = 0.43215470\n",
      "Iteration 98, loss = 0.43220400\n",
      "Iteration 99, loss = 0.43191726\n",
      "Iteration 100, loss = 0.43179213\n",
      "Iteration 101, loss = 0.43175279\n",
      "Iteration 102, loss = 0.43161409\n",
      "Iteration 103, loss = 0.43153122\n",
      "Iteration 104, loss = 0.43139348\n",
      "Iteration 105, loss = 0.43144547\n",
      "Iteration 106, loss = 0.43114689\n",
      "Iteration 107, loss = 0.43100645\n",
      "Iteration 108, loss = 0.43094209\n",
      "Iteration 109, loss = 0.43083965\n",
      "Iteration 110, loss = 0.43074941\n",
      "Iteration 111, loss = 0.43067414\n",
      "Iteration 112, loss = 0.43043144\n",
      "Iteration 113, loss = 0.43046305\n",
      "Iteration 114, loss = 0.43023989\n",
      "Iteration 115, loss = 0.43018281\n",
      "Iteration 116, loss = 0.43023037\n",
      "Iteration 117, loss = 0.42997108\n",
      "Iteration 118, loss = 0.42989077\n",
      "Iteration 119, loss = 0.42981471\n",
      "Iteration 120, loss = 0.42960657\n",
      "Iteration 121, loss = 0.42964522\n",
      "Iteration 122, loss = 0.42945746\n",
      "Iteration 123, loss = 0.42932216\n",
      "Iteration 124, loss = 0.42932376\n",
      "Iteration 125, loss = 0.42909829\n",
      "Iteration 126, loss = 0.42904096\n",
      "Iteration 127, loss = 0.42884852\n",
      "Iteration 128, loss = 0.42866661\n",
      "Iteration 129, loss = 0.42876790\n",
      "Iteration 130, loss = 0.42857040\n",
      "Iteration 131, loss = 0.42847196\n",
      "Iteration 132, loss = 0.42828916\n",
      "Iteration 133, loss = 0.42828167\n",
      "Iteration 134, loss = 0.42810543\n",
      "Iteration 135, loss = 0.42801969\n",
      "Iteration 136, loss = 0.42783585\n",
      "Iteration 137, loss = 0.42782225\n",
      "Iteration 138, loss = 0.42761088\n",
      "Iteration 139, loss = 0.42792685\n",
      "Iteration 140, loss = 0.42748231\n",
      "Iteration 141, loss = 0.42753744\n",
      "Iteration 142, loss = 0.42720560\n",
      "Iteration 143, loss = 0.42708941\n",
      "Iteration 144, loss = 0.42701844\n",
      "Iteration 145, loss = 0.42694062\n",
      "Iteration 146, loss = 0.42690995\n",
      "Iteration 147, loss = 0.42671981\n",
      "Iteration 148, loss = 0.42674947\n",
      "Iteration 149, loss = 0.42646796\n",
      "Iteration 150, loss = 0.42625616\n",
      "Iteration 151, loss = 0.42629892\n",
      "Iteration 152, loss = 0.42612020\n",
      "Iteration 153, loss = 0.42591855\n",
      "Iteration 154, loss = 0.42606372\n",
      "Iteration 155, loss = 0.42625295\n",
      "Iteration 156, loss = 0.42570949\n",
      "Iteration 157, loss = 0.42551621\n",
      "Iteration 158, loss = 0.42550949\n",
      "Iteration 159, loss = 0.42557927\n",
      "Iteration 160, loss = 0.42516861\n",
      "Iteration 161, loss = 0.42509294\n",
      "Iteration 162, loss = 0.42512446\n",
      "Iteration 163, loss = 0.42493768\n",
      "Iteration 164, loss = 0.42470283\n",
      "Iteration 165, loss = 0.42466559\n",
      "Iteration 166, loss = 0.42458713\n",
      "Iteration 167, loss = 0.42451829\n",
      "Iteration 168, loss = 0.42427674\n",
      "Iteration 169, loss = 0.42417411\n",
      "Iteration 170, loss = 0.42410026\n",
      "Iteration 171, loss = 0.42408652\n",
      "Iteration 172, loss = 0.42384058\n",
      "Iteration 173, loss = 0.42385026\n",
      "Iteration 174, loss = 0.42366123\n",
      "Iteration 175, loss = 0.42360834\n",
      "Iteration 176, loss = 0.42357580\n",
      "Iteration 177, loss = 0.42331957\n",
      "Iteration 178, loss = 0.42327803\n",
      "Iteration 179, loss = 0.42304769\n",
      "Iteration 180, loss = 0.42300201\n",
      "Iteration 181, loss = 0.42292174\n",
      "Iteration 182, loss = 0.42260962\n",
      "Iteration 183, loss = 0.42252558\n",
      "Iteration 184, loss = 0.42246169\n",
      "Iteration 185, loss = 0.42237358\n",
      "Iteration 186, loss = 0.42218244\n",
      "Iteration 187, loss = 0.42239632\n",
      "Iteration 188, loss = 0.42206526\n",
      "Iteration 189, loss = 0.42192017\n",
      "Iteration 190, loss = 0.42179274\n",
      "Iteration 191, loss = 0.42157694\n",
      "Iteration 192, loss = 0.42156288\n",
      "Iteration 193, loss = 0.42138215\n",
      "Iteration 194, loss = 0.42131296\n",
      "Iteration 195, loss = 0.42133381\n",
      "Iteration 196, loss = 0.42107231\n",
      "Iteration 197, loss = 0.42086224\n",
      "Iteration 198, loss = 0.42077951\n",
      "Iteration 199, loss = 0.42067890\n",
      "Iteration 200, loss = 0.42057830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76461857\n",
      "Iteration 2, loss = 0.71810828\n",
      "Iteration 3, loss = 0.67821531\n",
      "Iteration 4, loss = 0.64461735\n",
      "Iteration 5, loss = 0.61578539\n",
      "Iteration 6, loss = 0.59002665\n",
      "Iteration 7, loss = 0.56944838\n",
      "Iteration 8, loss = 0.55047627\n",
      "Iteration 9, loss = 0.53357105\n",
      "Iteration 10, loss = 0.51876503\n",
      "Iteration 11, loss = 0.50603548\n",
      "Iteration 12, loss = 0.49450402\n",
      "Iteration 13, loss = 0.48463789\n",
      "Iteration 14, loss = 0.47559372\n",
      "Iteration 15, loss = 0.46804171\n",
      "Iteration 16, loss = 0.46178237\n",
      "Iteration 17, loss = 0.45619100\n",
      "Iteration 18, loss = 0.45196519\n",
      "Iteration 19, loss = 0.44778076\n",
      "Iteration 20, loss = 0.44395626\n",
      "Iteration 21, loss = 0.44133373\n",
      "Iteration 22, loss = 0.43882625\n",
      "Iteration 23, loss = 0.43664097\n",
      "Iteration 24, loss = 0.43504595\n",
      "Iteration 25, loss = 0.43366711\n",
      "Iteration 26, loss = 0.43240322\n",
      "Iteration 27, loss = 0.43125083\n",
      "Iteration 28, loss = 0.43057095\n",
      "Iteration 29, loss = 0.42963907\n",
      "Iteration 30, loss = 0.42909245\n",
      "Iteration 31, loss = 0.42862496\n",
      "Iteration 32, loss = 0.42810460\n",
      "Iteration 33, loss = 0.42795619\n",
      "Iteration 34, loss = 0.42732317\n",
      "Iteration 35, loss = 0.42711000\n",
      "Iteration 36, loss = 0.42669122\n",
      "Iteration 37, loss = 0.42653823\n",
      "Iteration 38, loss = 0.42621466\n",
      "Iteration 39, loss = 0.42586925\n",
      "Iteration 40, loss = 0.42577283\n",
      "Iteration 41, loss = 0.42549683\n",
      "Iteration 42, loss = 0.42534871\n",
      "Iteration 43, loss = 0.42501452\n",
      "Iteration 44, loss = 0.42504901\n",
      "Iteration 45, loss = 0.42469804\n",
      "Iteration 46, loss = 0.42447657\n",
      "Iteration 47, loss = 0.42427742\n",
      "Iteration 48, loss = 0.42420435\n",
      "Iteration 49, loss = 0.42397769\n",
      "Iteration 50, loss = 0.42381967\n",
      "Iteration 51, loss = 0.42368837\n",
      "Iteration 52, loss = 0.42358340\n",
      "Iteration 53, loss = 0.42332569\n",
      "Iteration 54, loss = 0.42314010\n",
      "Iteration 55, loss = 0.42299981\n",
      "Iteration 56, loss = 0.42294562\n",
      "Iteration 57, loss = 0.42284822\n",
      "Iteration 58, loss = 0.42264448\n",
      "Iteration 59, loss = 0.42242626\n",
      "Iteration 60, loss = 0.42232399\n",
      "Iteration 61, loss = 0.42217292\n",
      "Iteration 62, loss = 0.42204640\n",
      "Iteration 63, loss = 0.42188497\n",
      "Iteration 64, loss = 0.42180711\n",
      "Iteration 65, loss = 0.42160216\n",
      "Iteration 66, loss = 0.42159175\n",
      "Iteration 67, loss = 0.42152331\n",
      "Iteration 68, loss = 0.42128026\n",
      "Iteration 69, loss = 0.42107049\n",
      "Iteration 70, loss = 0.42109927\n",
      "Iteration 71, loss = 0.42083595\n",
      "Iteration 72, loss = 0.42072649\n",
      "Iteration 73, loss = 0.42053675\n",
      "Iteration 74, loss = 0.42044580\n",
      "Iteration 75, loss = 0.42026503\n",
      "Iteration 76, loss = 0.42031478\n",
      "Iteration 77, loss = 0.41995567\n",
      "Iteration 78, loss = 0.41984231\n",
      "Iteration 79, loss = 0.41973070\n",
      "Iteration 80, loss = 0.41968729\n",
      "Iteration 81, loss = 0.41953515\n",
      "Iteration 82, loss = 0.41942351\n",
      "Iteration 83, loss = 0.41925110\n",
      "Iteration 84, loss = 0.41912730\n",
      "Iteration 85, loss = 0.41914651\n",
      "Iteration 86, loss = 0.41900322\n",
      "Iteration 87, loss = 0.41893393\n",
      "Iteration 88, loss = 0.41861335\n",
      "Iteration 89, loss = 0.41850678\n",
      "Iteration 90, loss = 0.41850125\n",
      "Iteration 91, loss = 0.41823829\n",
      "Iteration 92, loss = 0.41845967\n",
      "Iteration 93, loss = 0.41804898\n",
      "Iteration 94, loss = 0.41833040\n",
      "Iteration 95, loss = 0.41776110\n",
      "Iteration 96, loss = 0.41758154\n",
      "Iteration 97, loss = 0.41747768\n",
      "Iteration 98, loss = 0.41729560\n",
      "Iteration 99, loss = 0.41722313\n",
      "Iteration 100, loss = 0.41705125\n",
      "Iteration 101, loss = 0.41725698\n",
      "Iteration 102, loss = 0.41687548\n",
      "Iteration 103, loss = 0.41671286\n",
      "Iteration 104, loss = 0.41672063\n",
      "Iteration 105, loss = 0.41645609\n",
      "Iteration 106, loss = 0.41645978\n",
      "Iteration 107, loss = 0.41625798\n",
      "Iteration 108, loss = 0.41610848\n",
      "Iteration 109, loss = 0.41598295\n",
      "Iteration 110, loss = 0.41580385\n",
      "Iteration 111, loss = 0.41572507\n",
      "Iteration 112, loss = 0.41567374\n",
      "Iteration 113, loss = 0.41542188\n",
      "Iteration 114, loss = 0.41537388\n",
      "Iteration 115, loss = 0.41519348\n",
      "Iteration 116, loss = 0.41497169\n",
      "Iteration 117, loss = 0.41486429\n",
      "Iteration 118, loss = 0.41477897\n",
      "Iteration 119, loss = 0.41481284\n",
      "Iteration 120, loss = 0.41467029\n",
      "Iteration 121, loss = 0.41444488\n",
      "Iteration 122, loss = 0.41431944\n",
      "Iteration 123, loss = 0.41410030\n",
      "Iteration 124, loss = 0.41410607\n",
      "Iteration 125, loss = 0.41399719\n",
      "Iteration 126, loss = 0.41374906\n",
      "Iteration 127, loss = 0.41357569\n",
      "Iteration 128, loss = 0.41350268\n",
      "Iteration 129, loss = 0.41343226\n",
      "Iteration 130, loss = 0.41327965\n",
      "Iteration 131, loss = 0.41307077\n",
      "Iteration 132, loss = 0.41300803\n",
      "Iteration 133, loss = 0.41286378\n",
      "Iteration 134, loss = 0.41272926\n",
      "Iteration 135, loss = 0.41248088\n",
      "Iteration 136, loss = 0.41274969\n",
      "Iteration 137, loss = 0.41222426\n",
      "Iteration 138, loss = 0.41214063\n",
      "Iteration 139, loss = 0.41216520\n",
      "Iteration 140, loss = 0.41188613\n",
      "Iteration 141, loss = 0.41176515\n",
      "Iteration 142, loss = 0.41158875\n",
      "Iteration 143, loss = 0.41147526\n",
      "Iteration 144, loss = 0.41143666\n",
      "Iteration 145, loss = 0.41132196\n",
      "Iteration 146, loss = 0.41112701\n",
      "Iteration 147, loss = 0.41099968\n",
      "Iteration 148, loss = 0.41093325\n",
      "Iteration 149, loss = 0.41062103\n",
      "Iteration 150, loss = 0.41047547\n",
      "Iteration 151, loss = 0.41037057\n",
      "Iteration 152, loss = 0.41028149\n",
      "Iteration 153, loss = 0.41065497\n",
      "Iteration 154, loss = 0.41009754\n",
      "Iteration 155, loss = 0.40968277\n",
      "Iteration 156, loss = 0.40975363\n",
      "Iteration 157, loss = 0.40956158\n",
      "Iteration 158, loss = 0.40939286\n",
      "Iteration 159, loss = 0.40926050\n",
      "Iteration 160, loss = 0.40921385\n",
      "Iteration 161, loss = 0.40903146\n",
      "Iteration 162, loss = 0.40893606\n",
      "Iteration 163, loss = 0.40871478\n",
      "Iteration 164, loss = 0.40872381\n",
      "Iteration 165, loss = 0.40850811\n",
      "Iteration 166, loss = 0.40831779\n",
      "Iteration 167, loss = 0.40805106\n",
      "Iteration 168, loss = 0.40797706\n",
      "Iteration 169, loss = 0.40781695\n",
      "Iteration 170, loss = 0.40783082\n",
      "Iteration 171, loss = 0.40756732\n",
      "Iteration 172, loss = 0.40760012\n",
      "Iteration 173, loss = 0.40742048\n",
      "Iteration 174, loss = 0.40725160\n",
      "Iteration 175, loss = 0.40703586\n",
      "Iteration 176, loss = 0.40692100\n",
      "Iteration 177, loss = 0.40665380\n",
      "Iteration 178, loss = 0.40675444\n",
      "Iteration 179, loss = 0.40643767\n",
      "Iteration 180, loss = 0.40624247\n",
      "Iteration 181, loss = 0.40604709\n",
      "Iteration 182, loss = 0.40596225\n",
      "Iteration 183, loss = 0.40570368\n",
      "Iteration 184, loss = 0.40563960\n",
      "Iteration 185, loss = 0.40571569\n",
      "Iteration 186, loss = 0.40543625\n",
      "Iteration 187, loss = 0.40530603\n",
      "Iteration 188, loss = 0.40512238\n",
      "Iteration 189, loss = 0.40498453\n",
      "Iteration 190, loss = 0.40474299\n",
      "Iteration 191, loss = 0.40465847\n",
      "Iteration 192, loss = 0.40450249\n",
      "Iteration 193, loss = 0.40422998\n",
      "Iteration 194, loss = 0.40431688\n",
      "Iteration 195, loss = 0.40414709\n",
      "Iteration 196, loss = 0.40388955\n",
      "Iteration 197, loss = 0.40399438\n",
      "Iteration 198, loss = 0.40364059\n",
      "Iteration 199, loss = 0.40350445\n",
      "Iteration 200, loss = 0.40315916\n",
      "Iteration 1, loss = 0.76698668\n",
      "Iteration 2, loss = 0.71931575\n",
      "Iteration 3, loss = 0.68047286\n",
      "Iteration 4, loss = 0.64658256\n",
      "Iteration 5, loss = 0.61699206\n",
      "Iteration 6, loss = 0.59184022\n",
      "Iteration 7, loss = 0.57053122\n",
      "Iteration 8, loss = 0.55105828\n",
      "Iteration 9, loss = 0.53387083\n",
      "Iteration 10, loss = 0.51868375\n",
      "Iteration 11, loss = 0.50582741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.49403082\n",
      "Iteration 13, loss = 0.48395043\n",
      "Iteration 14, loss = 0.47457915\n",
      "Iteration 15, loss = 0.46699736\n",
      "Iteration 16, loss = 0.46028293\n",
      "Iteration 17, loss = 0.45471435\n",
      "Iteration 18, loss = 0.45016087\n",
      "Iteration 19, loss = 0.44543484\n",
      "Iteration 20, loss = 0.44204581\n",
      "Iteration 21, loss = 0.43889025\n",
      "Iteration 22, loss = 0.43630381\n",
      "Iteration 23, loss = 0.43387192\n",
      "Iteration 24, loss = 0.43205691\n",
      "Iteration 25, loss = 0.43060946\n",
      "Iteration 26, loss = 0.42914312\n",
      "Iteration 27, loss = 0.42798085\n",
      "Iteration 28, loss = 0.42715581\n",
      "Iteration 29, loss = 0.42628258\n",
      "Iteration 30, loss = 0.42549795\n",
      "Iteration 31, loss = 0.42508747\n",
      "Iteration 32, loss = 0.42460474\n",
      "Iteration 33, loss = 0.42434262\n",
      "Iteration 34, loss = 0.42371759\n",
      "Iteration 35, loss = 0.42331717\n",
      "Iteration 36, loss = 0.42319010\n",
      "Iteration 37, loss = 0.42287232\n",
      "Iteration 38, loss = 0.42251797\n",
      "Iteration 39, loss = 0.42230602\n",
      "Iteration 40, loss = 0.42223711\n",
      "Iteration 41, loss = 0.42193776\n",
      "Iteration 42, loss = 0.42179630\n",
      "Iteration 43, loss = 0.42159182\n",
      "Iteration 44, loss = 0.42152033\n",
      "Iteration 45, loss = 0.42122125\n",
      "Iteration 46, loss = 0.42107361\n",
      "Iteration 47, loss = 0.42088978\n",
      "Iteration 48, loss = 0.42079099\n",
      "Iteration 49, loss = 0.42064651\n",
      "Iteration 50, loss = 0.42046172\n",
      "Iteration 51, loss = 0.42034726\n",
      "Iteration 52, loss = 0.42027875\n",
      "Iteration 53, loss = 0.42007663\n",
      "Iteration 54, loss = 0.41992558\n",
      "Iteration 55, loss = 0.41979882\n",
      "Iteration 56, loss = 0.41966464\n",
      "Iteration 57, loss = 0.41959706\n",
      "Iteration 58, loss = 0.41953825\n",
      "Iteration 59, loss = 0.41929654\n",
      "Iteration 60, loss = 0.41914278\n",
      "Iteration 61, loss = 0.41909925\n",
      "Iteration 62, loss = 0.41893716\n",
      "Iteration 63, loss = 0.41889749\n",
      "Iteration 64, loss = 0.41884792\n",
      "Iteration 65, loss = 0.41864276\n",
      "Iteration 66, loss = 0.41853264\n",
      "Iteration 67, loss = 0.41838397\n",
      "Iteration 68, loss = 0.41821371\n",
      "Iteration 69, loss = 0.41810053\n",
      "Iteration 70, loss = 0.41808058\n",
      "Iteration 71, loss = 0.41789543\n",
      "Iteration 72, loss = 0.41776948\n",
      "Iteration 73, loss = 0.41755704\n",
      "Iteration 74, loss = 0.41754270\n",
      "Iteration 75, loss = 0.41738362\n",
      "Iteration 76, loss = 0.41729181\n",
      "Iteration 77, loss = 0.41716839\n",
      "Iteration 78, loss = 0.41700924\n",
      "Iteration 79, loss = 0.41685867\n",
      "Iteration 80, loss = 0.41678594\n",
      "Iteration 81, loss = 0.41665242\n",
      "Iteration 82, loss = 0.41652722\n",
      "Iteration 83, loss = 0.41639190\n",
      "Iteration 84, loss = 0.41631400\n",
      "Iteration 85, loss = 0.41631639\n",
      "Iteration 86, loss = 0.41602842\n",
      "Iteration 87, loss = 0.41613037\n",
      "Iteration 88, loss = 0.41592346\n",
      "Iteration 89, loss = 0.41580151\n",
      "Iteration 90, loss = 0.41576732\n",
      "Iteration 91, loss = 0.41550207\n",
      "Iteration 92, loss = 0.41565218\n",
      "Iteration 93, loss = 0.41534233\n",
      "Iteration 94, loss = 0.41529679\n",
      "Iteration 95, loss = 0.41504010\n",
      "Iteration 96, loss = 0.41494349\n",
      "Iteration 97, loss = 0.41495101\n",
      "Iteration 98, loss = 0.41471227\n",
      "Iteration 99, loss = 0.41466980\n",
      "Iteration 100, loss = 0.41438963\n",
      "Iteration 101, loss = 0.41461783\n",
      "Iteration 102, loss = 0.41437644\n",
      "Iteration 103, loss = 0.41415427\n",
      "Iteration 104, loss = 0.41415369\n",
      "Iteration 105, loss = 0.41387622\n",
      "Iteration 106, loss = 0.41382177\n",
      "Iteration 107, loss = 0.41374635\n",
      "Iteration 108, loss = 0.41372042\n",
      "Iteration 109, loss = 0.41360979\n",
      "Iteration 110, loss = 0.41340923\n",
      "Iteration 111, loss = 0.41317872\n",
      "Iteration 112, loss = 0.41314603\n",
      "Iteration 113, loss = 0.41300282\n",
      "Iteration 114, loss = 0.41294138\n",
      "Iteration 115, loss = 0.41281349\n",
      "Iteration 116, loss = 0.41261919\n",
      "Iteration 117, loss = 0.41251450\n",
      "Iteration 118, loss = 0.41249845\n",
      "Iteration 119, loss = 0.41247371\n",
      "Iteration 120, loss = 0.41226575\n",
      "Iteration 121, loss = 0.41218178\n",
      "Iteration 122, loss = 0.41204885\n",
      "Iteration 123, loss = 0.41181899\n",
      "Iteration 124, loss = 0.41179997\n",
      "Iteration 125, loss = 0.41171308\n",
      "Iteration 126, loss = 0.41155924\n",
      "Iteration 127, loss = 0.41145862\n",
      "Iteration 128, loss = 0.41128875\n",
      "Iteration 129, loss = 0.41119806\n",
      "Iteration 130, loss = 0.41114917\n",
      "Iteration 131, loss = 0.41089690\n",
      "Iteration 132, loss = 0.41087383\n",
      "Iteration 133, loss = 0.41077691\n",
      "Iteration 134, loss = 0.41062607\n",
      "Iteration 135, loss = 0.41046649\n",
      "Iteration 136, loss = 0.41042915\n",
      "Iteration 137, loss = 0.41054359\n",
      "Iteration 138, loss = 0.41026541\n",
      "Iteration 139, loss = 0.41015867\n",
      "Iteration 140, loss = 0.40998224\n",
      "Iteration 141, loss = 0.40987599\n",
      "Iteration 142, loss = 0.40964099\n",
      "Iteration 143, loss = 0.40947431\n",
      "Iteration 144, loss = 0.40951201\n",
      "Iteration 145, loss = 0.40942992\n",
      "Iteration 146, loss = 0.40937096\n",
      "Iteration 147, loss = 0.40907281\n",
      "Iteration 148, loss = 0.40905138\n",
      "Iteration 149, loss = 0.40880685\n",
      "Iteration 150, loss = 0.40869131\n",
      "Iteration 151, loss = 0.40856757\n",
      "Iteration 152, loss = 0.40864067\n",
      "Iteration 153, loss = 0.40876787\n",
      "Iteration 154, loss = 0.40824185\n",
      "Iteration 155, loss = 0.40808373\n",
      "Iteration 156, loss = 0.40805503\n",
      "Iteration 157, loss = 0.40785075\n",
      "Iteration 158, loss = 0.40773423\n",
      "Iteration 159, loss = 0.40760114\n",
      "Iteration 160, loss = 0.40756470\n",
      "Iteration 161, loss = 0.40747267\n",
      "Iteration 162, loss = 0.40737447\n",
      "Iteration 163, loss = 0.40720250\n",
      "Iteration 164, loss = 0.40707298\n",
      "Iteration 165, loss = 0.40686471\n",
      "Iteration 166, loss = 0.40681090\n",
      "Iteration 167, loss = 0.40660319\n",
      "Iteration 168, loss = 0.40665548\n",
      "Iteration 169, loss = 0.40644347\n",
      "Iteration 170, loss = 0.40638287\n",
      "Iteration 171, loss = 0.40611417\n",
      "Iteration 172, loss = 0.40606214\n",
      "Iteration 173, loss = 0.40612171\n",
      "Iteration 174, loss = 0.40584654\n",
      "Iteration 175, loss = 0.40579657\n",
      "Iteration 176, loss = 0.40555888\n",
      "Iteration 177, loss = 0.40544021\n",
      "Iteration 178, loss = 0.40535636\n",
      "Iteration 179, loss = 0.40522781\n",
      "Iteration 180, loss = 0.40498772\n",
      "Iteration 181, loss = 0.40486773\n",
      "Iteration 182, loss = 0.40475640\n",
      "Iteration 183, loss = 0.40456211\n",
      "Iteration 184, loss = 0.40444738\n",
      "Iteration 185, loss = 0.40442870\n",
      "Iteration 186, loss = 0.40436166\n",
      "Iteration 187, loss = 0.40406319\n",
      "Iteration 188, loss = 0.40403353\n",
      "Iteration 189, loss = 0.40392129\n",
      "Iteration 190, loss = 0.40380519\n",
      "Iteration 191, loss = 0.40353269\n",
      "Iteration 192, loss = 0.40364072\n",
      "Iteration 193, loss = 0.40319085\n",
      "Iteration 194, loss = 0.40324408\n",
      "Iteration 195, loss = 0.40325830\n",
      "Iteration 196, loss = 0.40285143\n",
      "Iteration 197, loss = 0.40282339\n",
      "Iteration 198, loss = 0.40271970\n",
      "Iteration 199, loss = 0.40266201\n",
      "Iteration 200, loss = 0.40230377\n",
      "Iteration 1, loss = 0.75977775\n",
      "Iteration 2, loss = 0.71338118\n",
      "Iteration 3, loss = 0.67397373\n",
      "Iteration 4, loss = 0.64118000\n",
      "Iteration 5, loss = 0.61291609\n",
      "Iteration 6, loss = 0.58912251\n",
      "Iteration 7, loss = 0.56969590\n",
      "Iteration 8, loss = 0.55182183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.53666622\n",
      "Iteration 10, loss = 0.52326531\n",
      "Iteration 11, loss = 0.51190507\n",
      "Iteration 12, loss = 0.50220708\n",
      "Iteration 13, loss = 0.49366151\n",
      "Iteration 14, loss = 0.48574896\n",
      "Iteration 15, loss = 0.47969172\n",
      "Iteration 16, loss = 0.47419528\n",
      "Iteration 17, loss = 0.47028418\n",
      "Iteration 18, loss = 0.46626132\n",
      "Iteration 19, loss = 0.46275930\n",
      "Iteration 20, loss = 0.46055236\n",
      "Iteration 21, loss = 0.45839741\n",
      "Iteration 22, loss = 0.45637320\n",
      "Iteration 23, loss = 0.45490479\n",
      "Iteration 24, loss = 0.45373051\n",
      "Iteration 25, loss = 0.45272731\n",
      "Iteration 26, loss = 0.45180684\n",
      "Iteration 27, loss = 0.45112329\n",
      "Iteration 28, loss = 0.45069400\n",
      "Iteration 29, loss = 0.45010131\n",
      "Iteration 30, loss = 0.44961647\n",
      "Iteration 31, loss = 0.44931499\n",
      "Iteration 32, loss = 0.44904142\n",
      "Iteration 33, loss = 0.44881140\n",
      "Iteration 34, loss = 0.44846938\n",
      "Iteration 35, loss = 0.44815970\n",
      "Iteration 36, loss = 0.44807669\n",
      "Iteration 37, loss = 0.44784330\n",
      "Iteration 38, loss = 0.44757018\n",
      "Iteration 39, loss = 0.44735477\n",
      "Iteration 40, loss = 0.44730593\n",
      "Iteration 41, loss = 0.44705182\n",
      "Iteration 42, loss = 0.44690810\n",
      "Iteration 43, loss = 0.44669258\n",
      "Iteration 44, loss = 0.44658457\n",
      "Iteration 45, loss = 0.44637255\n",
      "Iteration 46, loss = 0.44622999\n",
      "Iteration 47, loss = 0.44600411\n",
      "Iteration 48, loss = 0.44588374\n",
      "Iteration 49, loss = 0.44577344\n",
      "Iteration 50, loss = 0.44569833\n",
      "Iteration 51, loss = 0.44545129\n",
      "Iteration 52, loss = 0.44532419\n",
      "Iteration 53, loss = 0.44520507\n",
      "Iteration 54, loss = 0.44495658\n",
      "Iteration 55, loss = 0.44485045\n",
      "Iteration 56, loss = 0.44461176\n",
      "Iteration 57, loss = 0.44451278\n",
      "Iteration 58, loss = 0.44467440\n",
      "Iteration 59, loss = 0.44422579\n",
      "Iteration 60, loss = 0.44406685\n",
      "Iteration 61, loss = 0.44396955\n",
      "Iteration 62, loss = 0.44375128\n",
      "Iteration 63, loss = 0.44374307\n",
      "Iteration 64, loss = 0.44360201\n",
      "Iteration 65, loss = 0.44348602\n",
      "Iteration 66, loss = 0.44329784\n",
      "Iteration 67, loss = 0.44317036\n",
      "Iteration 68, loss = 0.44304527\n",
      "Iteration 69, loss = 0.44288343\n",
      "Iteration 70, loss = 0.44286259\n",
      "Iteration 71, loss = 0.44255869\n",
      "Iteration 72, loss = 0.44246207\n",
      "Iteration 73, loss = 0.44223908\n",
      "Iteration 74, loss = 0.44218932\n",
      "Iteration 75, loss = 0.44193066\n",
      "Iteration 76, loss = 0.44187541\n",
      "Iteration 77, loss = 0.44179889\n",
      "Iteration 78, loss = 0.44171200\n",
      "Iteration 79, loss = 0.44140243\n",
      "Iteration 80, loss = 0.44131036\n",
      "Iteration 81, loss = 0.44117603\n",
      "Iteration 82, loss = 0.44099335\n",
      "Iteration 83, loss = 0.44089249\n",
      "Iteration 84, loss = 0.44080386\n",
      "Iteration 85, loss = 0.44066637\n",
      "Iteration 86, loss = 0.44045407\n",
      "Iteration 87, loss = 0.44079628\n",
      "Iteration 88, loss = 0.44035768\n",
      "Iteration 89, loss = 0.44010768\n",
      "Iteration 90, loss = 0.43992851\n",
      "Iteration 91, loss = 0.43979248\n",
      "Iteration 92, loss = 0.43982276\n",
      "Iteration 93, loss = 0.43951091\n",
      "Iteration 94, loss = 0.43942954\n",
      "Iteration 95, loss = 0.43924440\n",
      "Iteration 96, loss = 0.43915751\n",
      "Iteration 97, loss = 0.43906413\n",
      "Iteration 98, loss = 0.43880458\n",
      "Iteration 99, loss = 0.43870295\n",
      "Iteration 100, loss = 0.43845478\n",
      "Iteration 101, loss = 0.43877066\n",
      "Iteration 102, loss = 0.43835535\n",
      "Iteration 103, loss = 0.43828169\n",
      "Iteration 104, loss = 0.43807898\n",
      "Iteration 105, loss = 0.43792317\n",
      "Iteration 106, loss = 0.43769519\n",
      "Iteration 107, loss = 0.43767809\n",
      "Iteration 108, loss = 0.43760198\n",
      "Iteration 109, loss = 0.43750622\n",
      "Iteration 110, loss = 0.43733996\n",
      "Iteration 111, loss = 0.43698358\n",
      "Iteration 112, loss = 0.43698888\n",
      "Iteration 113, loss = 0.43670532\n",
      "Iteration 114, loss = 0.43658303\n",
      "Iteration 115, loss = 0.43656366\n",
      "Iteration 116, loss = 0.43638372\n",
      "Iteration 117, loss = 0.43620684\n",
      "Iteration 118, loss = 0.43611030\n",
      "Iteration 119, loss = 0.43609093\n",
      "Iteration 120, loss = 0.43602216\n",
      "Iteration 121, loss = 0.43565880\n",
      "Iteration 122, loss = 0.43556530\n",
      "Iteration 123, loss = 0.43541884\n",
      "Iteration 124, loss = 0.43536628\n",
      "Iteration 125, loss = 0.43512903\n",
      "Iteration 126, loss = 0.43522229\n",
      "Iteration 127, loss = 0.43478009\n",
      "Iteration 128, loss = 0.43470622\n",
      "Iteration 129, loss = 0.43448431\n",
      "Iteration 130, loss = 0.43440538\n",
      "Iteration 131, loss = 0.43424580\n",
      "Iteration 132, loss = 0.43410286\n",
      "Iteration 133, loss = 0.43410689\n",
      "Iteration 134, loss = 0.43381356\n",
      "Iteration 135, loss = 0.43374910\n",
      "Iteration 136, loss = 0.43359445\n",
      "Iteration 137, loss = 0.43350217\n",
      "Iteration 138, loss = 0.43310787\n",
      "Iteration 139, loss = 0.43324521\n",
      "Iteration 140, loss = 0.43302707\n",
      "Iteration 141, loss = 0.43288165\n",
      "Iteration 142, loss = 0.43266535\n",
      "Iteration 143, loss = 0.43261318\n",
      "Iteration 144, loss = 0.43247530\n",
      "Iteration 145, loss = 0.43228384\n",
      "Iteration 146, loss = 0.43204937\n",
      "Iteration 147, loss = 0.43202092\n",
      "Iteration 148, loss = 0.43179755\n",
      "Iteration 149, loss = 0.43159693\n",
      "Iteration 150, loss = 0.43152496\n",
      "Iteration 151, loss = 0.43135089\n",
      "Iteration 152, loss = 0.43131747\n",
      "Iteration 153, loss = 0.43140723\n",
      "Iteration 154, loss = 0.43088123\n",
      "Iteration 155, loss = 0.43093320\n",
      "Iteration 156, loss = 0.43057101\n",
      "Iteration 157, loss = 0.43048034\n",
      "Iteration 158, loss = 0.43038591\n",
      "Iteration 159, loss = 0.43016337\n",
      "Iteration 160, loss = 0.43000459\n",
      "Iteration 161, loss = 0.42980935\n",
      "Iteration 162, loss = 0.42963310\n",
      "Iteration 163, loss = 0.42956065\n",
      "Iteration 164, loss = 0.42942475\n",
      "Iteration 165, loss = 0.42920761\n",
      "Iteration 166, loss = 0.42916893\n",
      "Iteration 167, loss = 0.42896248\n",
      "Iteration 168, loss = 0.42882020\n",
      "Iteration 169, loss = 0.42870889\n",
      "Iteration 170, loss = 0.42857218\n",
      "Iteration 171, loss = 0.42823776\n",
      "Iteration 172, loss = 0.42812462\n",
      "Iteration 173, loss = 0.42803376\n",
      "Iteration 174, loss = 0.42794704\n",
      "Iteration 175, loss = 0.42784307\n",
      "Iteration 176, loss = 0.42763458\n",
      "Iteration 177, loss = 0.42739187\n",
      "Iteration 178, loss = 0.42720017\n",
      "Iteration 179, loss = 0.42713009\n",
      "Iteration 180, loss = 0.42693461\n",
      "Iteration 181, loss = 0.42671417\n",
      "Iteration 182, loss = 0.42661776\n",
      "Iteration 183, loss = 0.42651210\n",
      "Iteration 184, loss = 0.42645375\n",
      "Iteration 185, loss = 0.42636061\n",
      "Iteration 186, loss = 0.42609408\n",
      "Iteration 187, loss = 0.42584718\n",
      "Iteration 188, loss = 0.42565780\n",
      "Iteration 189, loss = 0.42551983\n",
      "Iteration 190, loss = 0.42530550\n",
      "Iteration 191, loss = 0.42520151\n",
      "Iteration 192, loss = 0.42519500\n",
      "Iteration 193, loss = 0.42480160\n",
      "Iteration 194, loss = 0.42477938\n",
      "Iteration 195, loss = 0.42476550\n",
      "Iteration 196, loss = 0.42435930\n",
      "Iteration 197, loss = 0.42449098\n",
      "Iteration 198, loss = 0.42425100\n",
      "Iteration 199, loss = 0.42417529\n",
      "Iteration 200, loss = 0.42403744\n",
      "Iteration 1, loss = 0.75119036\n",
      "Iteration 2, loss = 0.70244548\n",
      "Iteration 3, loss = 0.65776226\n",
      "Iteration 4, loss = 0.62193710\n",
      "Iteration 5, loss = 0.59213843\n",
      "Iteration 6, loss = 0.56640384\n",
      "Iteration 7, loss = 0.54412222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.52633545\n",
      "Iteration 9, loss = 0.50965865\n",
      "Iteration 10, loss = 0.49607285\n",
      "Iteration 11, loss = 0.48394682\n",
      "Iteration 12, loss = 0.47372542\n",
      "Iteration 13, loss = 0.46361749\n",
      "Iteration 14, loss = 0.45574916\n",
      "Iteration 15, loss = 0.44866068\n",
      "Iteration 16, loss = 0.44239822\n",
      "Iteration 17, loss = 0.43687950\n",
      "Iteration 18, loss = 0.43303207\n",
      "Iteration 19, loss = 0.42896865\n",
      "Iteration 20, loss = 0.42567933\n",
      "Iteration 21, loss = 0.42310761\n",
      "Iteration 22, loss = 0.42056068\n",
      "Iteration 23, loss = 0.41899675\n",
      "Iteration 24, loss = 0.41733731\n",
      "Iteration 25, loss = 0.41623215\n",
      "Iteration 26, loss = 0.41517294\n",
      "Iteration 27, loss = 0.41420798\n",
      "Iteration 28, loss = 0.41352678\n",
      "Iteration 29, loss = 0.41290442\n",
      "Iteration 30, loss = 0.41236020\n",
      "Iteration 31, loss = 0.41191421\n",
      "Iteration 32, loss = 0.41169502\n",
      "Iteration 33, loss = 0.41133468\n",
      "Iteration 34, loss = 0.41099500\n",
      "Iteration 35, loss = 0.41078443\n",
      "Iteration 36, loss = 0.41059346\n",
      "Iteration 37, loss = 0.41034743\n",
      "Iteration 38, loss = 0.41016852\n",
      "Iteration 39, loss = 0.40999217\n",
      "Iteration 40, loss = 0.40987374\n",
      "Iteration 41, loss = 0.40972533\n",
      "Iteration 42, loss = 0.40963890\n",
      "Iteration 43, loss = 0.40953258\n",
      "Iteration 44, loss = 0.40936160\n",
      "Iteration 45, loss = 0.40920815\n",
      "Iteration 46, loss = 0.40912359\n",
      "Iteration 47, loss = 0.40891048\n",
      "Iteration 48, loss = 0.40877274\n",
      "Iteration 49, loss = 0.40863469\n",
      "Iteration 50, loss = 0.40850314\n",
      "Iteration 51, loss = 0.40844405\n",
      "Iteration 52, loss = 0.40831000\n",
      "Iteration 53, loss = 0.40809418\n",
      "Iteration 54, loss = 0.40805703\n",
      "Iteration 55, loss = 0.40789685\n",
      "Iteration 56, loss = 0.40777843\n",
      "Iteration 57, loss = 0.40773617\n",
      "Iteration 58, loss = 0.40765441\n",
      "Iteration 59, loss = 0.40734714\n",
      "Iteration 60, loss = 0.40722810\n",
      "Iteration 61, loss = 0.40731651\n",
      "Iteration 62, loss = 0.40735609\n",
      "Iteration 63, loss = 0.40705611\n",
      "Iteration 64, loss = 0.40686461\n",
      "Iteration 65, loss = 0.40658625\n",
      "Iteration 66, loss = 0.40655840\n",
      "Iteration 67, loss = 0.40634327\n",
      "Iteration 68, loss = 0.40639197\n",
      "Iteration 69, loss = 0.40611444\n",
      "Iteration 70, loss = 0.40605642\n",
      "Iteration 71, loss = 0.40588233\n",
      "Iteration 72, loss = 0.40579491\n",
      "Iteration 73, loss = 0.40562268\n",
      "Iteration 74, loss = 0.40551729\n",
      "Iteration 75, loss = 0.40540174\n",
      "Iteration 76, loss = 0.40533982\n",
      "Iteration 77, loss = 0.40512861\n",
      "Iteration 78, loss = 0.40510965\n",
      "Iteration 79, loss = 0.40484282\n",
      "Iteration 80, loss = 0.40478634\n",
      "Iteration 81, loss = 0.40468553\n",
      "Iteration 82, loss = 0.40450501\n",
      "Iteration 83, loss = 0.40452853\n",
      "Iteration 84, loss = 0.40431373\n",
      "Iteration 85, loss = 0.40412978\n",
      "Iteration 86, loss = 0.40417521\n",
      "Iteration 87, loss = 0.40388893\n",
      "Iteration 88, loss = 0.40390006\n",
      "Iteration 89, loss = 0.40361192\n",
      "Iteration 90, loss = 0.40353255\n",
      "Iteration 91, loss = 0.40348599\n",
      "Iteration 92, loss = 0.40331294\n",
      "Iteration 93, loss = 0.40311831\n",
      "Iteration 94, loss = 0.40306952\n",
      "Iteration 95, loss = 0.40281864\n",
      "Iteration 96, loss = 0.40302265\n",
      "Iteration 97, loss = 0.40259151\n",
      "Iteration 98, loss = 0.40250342\n",
      "Iteration 99, loss = 0.40233853\n",
      "Iteration 100, loss = 0.40235451\n",
      "Iteration 101, loss = 0.40209145\n",
      "Iteration 102, loss = 0.40199168\n",
      "Iteration 103, loss = 0.40177305\n",
      "Iteration 104, loss = 0.40174353\n",
      "Iteration 105, loss = 0.40152465\n",
      "Iteration 106, loss = 0.40142222\n",
      "Iteration 107, loss = 0.40131221\n",
      "Iteration 108, loss = 0.40133138\n",
      "Iteration 109, loss = 0.40101792\n",
      "Iteration 110, loss = 0.40097575\n",
      "Iteration 111, loss = 0.40077692\n",
      "Iteration 112, loss = 0.40055444\n",
      "Iteration 113, loss = 0.40053510\n",
      "Iteration 114, loss = 0.40027996\n",
      "Iteration 115, loss = 0.40020192\n",
      "Iteration 116, loss = 0.40007923\n",
      "Iteration 117, loss = 0.39986789\n",
      "Iteration 118, loss = 0.39981067\n",
      "Iteration 119, loss = 0.39965036\n",
      "Iteration 120, loss = 0.39948051\n",
      "Iteration 121, loss = 0.39946303\n",
      "Iteration 122, loss = 0.39919636\n",
      "Iteration 123, loss = 0.39916664\n",
      "Iteration 124, loss = 0.39912801\n",
      "Iteration 125, loss = 0.39892007\n",
      "Iteration 126, loss = 0.39866191\n",
      "Iteration 127, loss = 0.39851836\n",
      "Iteration 128, loss = 0.39836920\n",
      "Iteration 129, loss = 0.39821300\n",
      "Iteration 130, loss = 0.39806474\n",
      "Iteration 131, loss = 0.39807198\n",
      "Iteration 132, loss = 0.39776130\n",
      "Iteration 133, loss = 0.39765272\n",
      "Iteration 134, loss = 0.39751504\n",
      "Iteration 135, loss = 0.39731429\n",
      "Iteration 136, loss = 0.39715037\n",
      "Iteration 137, loss = 0.39714971\n",
      "Iteration 138, loss = 0.39686192\n",
      "Iteration 139, loss = 0.39690699\n",
      "Iteration 140, loss = 0.39674777\n",
      "Iteration 141, loss = 0.39648565\n",
      "Iteration 142, loss = 0.39629397\n",
      "Iteration 143, loss = 0.39620442\n",
      "Iteration 144, loss = 0.39599110\n",
      "Iteration 145, loss = 0.39612421\n",
      "Iteration 146, loss = 0.39583763\n",
      "Iteration 147, loss = 0.39556858\n",
      "Iteration 148, loss = 0.39556715\n",
      "Iteration 149, loss = 0.39532407\n",
      "Iteration 150, loss = 0.39510916\n",
      "Iteration 151, loss = 0.39509200\n",
      "Iteration 152, loss = 0.39480222\n",
      "Iteration 153, loss = 0.39467426\n",
      "Iteration 154, loss = 0.39461496\n",
      "Iteration 155, loss = 0.39479387\n",
      "Iteration 156, loss = 0.39413297\n",
      "Iteration 157, loss = 0.39399333\n",
      "Iteration 158, loss = 0.39390673\n",
      "Iteration 159, loss = 0.39370574\n",
      "Iteration 160, loss = 0.39358935\n",
      "Iteration 161, loss = 0.39334892\n",
      "Iteration 162, loss = 0.39335816\n",
      "Iteration 163, loss = 0.39310039\n",
      "Iteration 164, loss = 0.39291349\n",
      "Iteration 165, loss = 0.39288109\n",
      "Iteration 166, loss = 0.39266014\n",
      "Iteration 167, loss = 0.39251133\n",
      "Iteration 168, loss = 0.39227840\n",
      "Iteration 169, loss = 0.39224802\n",
      "Iteration 170, loss = 0.39193075\n",
      "Iteration 171, loss = 0.39172977\n",
      "Iteration 172, loss = 0.39170812\n",
      "Iteration 173, loss = 0.39159032\n",
      "Iteration 174, loss = 0.39129402\n",
      "Iteration 175, loss = 0.39105600\n",
      "Iteration 176, loss = 0.39135475\n",
      "Iteration 177, loss = 0.39093484\n",
      "Iteration 178, loss = 0.39071448\n",
      "Iteration 179, loss = 0.39058892\n",
      "Iteration 180, loss = 0.39030546\n",
      "Iteration 181, loss = 0.39031989\n",
      "Iteration 182, loss = 0.39000024\n",
      "Iteration 183, loss = 0.38995182\n",
      "Iteration 184, loss = 0.38971307\n",
      "Iteration 185, loss = 0.38963368\n",
      "Iteration 186, loss = 0.38936247\n",
      "Iteration 187, loss = 0.38935814\n",
      "Iteration 188, loss = 0.38900175\n",
      "Iteration 189, loss = 0.38882879\n",
      "Iteration 190, loss = 0.38880803\n",
      "Iteration 191, loss = 0.38841697\n",
      "Iteration 192, loss = 0.38839634\n",
      "Iteration 193, loss = 0.38818780\n",
      "Iteration 194, loss = 0.38815772\n",
      "Iteration 195, loss = 0.38802441\n",
      "Iteration 196, loss = 0.38770687\n",
      "Iteration 197, loss = 0.38738552\n",
      "Iteration 198, loss = 0.38736928\n",
      "Iteration 199, loss = 0.38716175\n",
      "Iteration 200, loss = 0.38702878\n",
      "Iteration 1, loss = 0.75832456\n",
      "Iteration 2, loss = 0.71171039\n",
      "Iteration 3, loss = 0.66908978\n",
      "Iteration 4, loss = 0.63545083\n",
      "Iteration 5, loss = 0.60680628\n",
      "Iteration 6, loss = 0.58225380\n",
      "Iteration 7, loss = 0.56040464\n",
      "Iteration 8, loss = 0.54365459\n",
      "Iteration 9, loss = 0.52775228\n",
      "Iteration 10, loss = 0.51479965\n",
      "Iteration 11, loss = 0.50303278\n",
      "Iteration 12, loss = 0.49322982\n",
      "Iteration 13, loss = 0.48382353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.47646565\n",
      "Iteration 15, loss = 0.46965282\n",
      "Iteration 16, loss = 0.46390666\n",
      "Iteration 17, loss = 0.45887594\n",
      "Iteration 18, loss = 0.45518417\n",
      "Iteration 19, loss = 0.45147907\n",
      "Iteration 20, loss = 0.44831786\n",
      "Iteration 21, loss = 0.44584141\n",
      "Iteration 22, loss = 0.44361769\n",
      "Iteration 23, loss = 0.44229591\n",
      "Iteration 24, loss = 0.44069308\n",
      "Iteration 25, loss = 0.43972276\n",
      "Iteration 26, loss = 0.43856770\n",
      "Iteration 27, loss = 0.43787131\n",
      "Iteration 28, loss = 0.43722117\n",
      "Iteration 29, loss = 0.43645116\n",
      "Iteration 30, loss = 0.43609639\n",
      "Iteration 31, loss = 0.43551459\n",
      "Iteration 32, loss = 0.43535283\n",
      "Iteration 33, loss = 0.43495016\n",
      "Iteration 34, loss = 0.43457925\n",
      "Iteration 35, loss = 0.43431085\n",
      "Iteration 36, loss = 0.43416272\n",
      "Iteration 37, loss = 0.43385617\n",
      "Iteration 38, loss = 0.43367446\n",
      "Iteration 39, loss = 0.43348782\n",
      "Iteration 40, loss = 0.43342159\n",
      "Iteration 41, loss = 0.43312504\n",
      "Iteration 42, loss = 0.43304791\n",
      "Iteration 43, loss = 0.43291141\n",
      "Iteration 44, loss = 0.43278707\n",
      "Iteration 45, loss = 0.43255913\n",
      "Iteration 46, loss = 0.43244030\n",
      "Iteration 47, loss = 0.43227622\n",
      "Iteration 48, loss = 0.43218995\n",
      "Iteration 49, loss = 0.43208251\n",
      "Iteration 50, loss = 0.43196295\n",
      "Iteration 51, loss = 0.43182412\n",
      "Iteration 52, loss = 0.43164683\n",
      "Iteration 53, loss = 0.43154447\n",
      "Iteration 54, loss = 0.43144968\n",
      "Iteration 55, loss = 0.43128637\n",
      "Iteration 56, loss = 0.43119653\n",
      "Iteration 57, loss = 0.43105918\n",
      "Iteration 58, loss = 0.43095178\n",
      "Iteration 59, loss = 0.43080071\n",
      "Iteration 60, loss = 0.43073039\n",
      "Iteration 61, loss = 0.43068113\n",
      "Iteration 62, loss = 0.43050218\n",
      "Iteration 63, loss = 0.43032608\n",
      "Iteration 64, loss = 0.43028896\n",
      "Iteration 65, loss = 0.43007265\n",
      "Iteration 66, loss = 0.43004319\n",
      "Iteration 67, loss = 0.42980009\n",
      "Iteration 68, loss = 0.42986655\n",
      "Iteration 69, loss = 0.42971458\n",
      "Iteration 70, loss = 0.42953243\n",
      "Iteration 71, loss = 0.42937826\n",
      "Iteration 72, loss = 0.42919914\n",
      "Iteration 73, loss = 0.42921312\n",
      "Iteration 74, loss = 0.42908183\n",
      "Iteration 75, loss = 0.42889643\n",
      "Iteration 76, loss = 0.42881326\n",
      "Iteration 77, loss = 0.42864137\n",
      "Iteration 78, loss = 0.42853673\n",
      "Iteration 79, loss = 0.42835868\n",
      "Iteration 80, loss = 0.42825301\n",
      "Iteration 81, loss = 0.42835717\n",
      "Iteration 82, loss = 0.42801414\n",
      "Iteration 83, loss = 0.42818806\n",
      "Iteration 84, loss = 0.42783339\n",
      "Iteration 85, loss = 0.42761297\n",
      "Iteration 86, loss = 0.42751908\n",
      "Iteration 87, loss = 0.42744036\n",
      "Iteration 88, loss = 0.42735742\n",
      "Iteration 89, loss = 0.42716826\n",
      "Iteration 90, loss = 0.42706531\n",
      "Iteration 91, loss = 0.42704405\n",
      "Iteration 92, loss = 0.42680241\n",
      "Iteration 93, loss = 0.42663561\n",
      "Iteration 94, loss = 0.42687294\n",
      "Iteration 95, loss = 0.42650003\n",
      "Iteration 96, loss = 0.42638021\n",
      "Iteration 97, loss = 0.42625089\n",
      "Iteration 98, loss = 0.42628804\n",
      "Iteration 99, loss = 0.42598435\n",
      "Iteration 100, loss = 0.42584507\n",
      "Iteration 101, loss = 0.42579035\n",
      "Iteration 102, loss = 0.42563571\n",
      "Iteration 103, loss = 0.42553876\n",
      "Iteration 104, loss = 0.42538356\n",
      "Iteration 105, loss = 0.42541670\n",
      "Iteration 106, loss = 0.42510380\n",
      "Iteration 107, loss = 0.42494520\n",
      "Iteration 108, loss = 0.42486613\n",
      "Iteration 109, loss = 0.42474291\n",
      "Iteration 110, loss = 0.42463552\n",
      "Iteration 111, loss = 0.42454258\n",
      "Iteration 112, loss = 0.42428126\n",
      "Iteration 113, loss = 0.42429561\n",
      "Iteration 114, loss = 0.42405354\n",
      "Iteration 115, loss = 0.42397731\n",
      "Iteration 116, loss = 0.42400499\n",
      "Iteration 117, loss = 0.42372603\n",
      "Iteration 118, loss = 0.42362567\n",
      "Iteration 119, loss = 0.42352972\n",
      "Iteration 120, loss = 0.42329969\n",
      "Iteration 121, loss = 0.42332145\n",
      "Iteration 122, loss = 0.42311123\n",
      "Iteration 123, loss = 0.42295521\n",
      "Iteration 124, loss = 0.42293622\n",
      "Iteration 125, loss = 0.42268475\n",
      "Iteration 126, loss = 0.42260736\n",
      "Iteration 127, loss = 0.42239074\n",
      "Iteration 128, loss = 0.42218532\n",
      "Iteration 129, loss = 0.42227088\n",
      "Iteration 130, loss = 0.42204903\n",
      "Iteration 131, loss = 0.42192339\n",
      "Iteration 132, loss = 0.42171876\n",
      "Iteration 133, loss = 0.42168535\n",
      "Iteration 134, loss = 0.42148389\n",
      "Iteration 135, loss = 0.42137394\n",
      "Iteration 136, loss = 0.42116455\n",
      "Iteration 137, loss = 0.42112799\n",
      "Iteration 138, loss = 0.42088920\n",
      "Iteration 139, loss = 0.42118739\n",
      "Iteration 140, loss = 0.42071229\n",
      "Iteration 141, loss = 0.42074136\n",
      "Iteration 142, loss = 0.42037979\n",
      "Iteration 143, loss = 0.42024110\n",
      "Iteration 144, loss = 0.42013896\n",
      "Iteration 145, loss = 0.42003779\n",
      "Iteration 146, loss = 0.41997541\n",
      "Iteration 147, loss = 0.41976172\n",
      "Iteration 148, loss = 0.41976130\n",
      "Iteration 149, loss = 0.41945519\n",
      "Iteration 150, loss = 0.41921402\n",
      "Iteration 151, loss = 0.41923311\n",
      "Iteration 152, loss = 0.41902584\n",
      "Iteration 153, loss = 0.41879022\n",
      "Iteration 154, loss = 0.41890437\n",
      "Iteration 155, loss = 0.41906926\n",
      "Iteration 156, loss = 0.41849188\n",
      "Iteration 157, loss = 0.41827165\n",
      "Iteration 158, loss = 0.41823477\n",
      "Iteration 159, loss = 0.41828071\n",
      "Iteration 160, loss = 0.41783146\n",
      "Iteration 161, loss = 0.41772492\n",
      "Iteration 162, loss = 0.41772731\n",
      "Iteration 163, loss = 0.41751204\n",
      "Iteration 164, loss = 0.41724267\n",
      "Iteration 165, loss = 0.41717906\n",
      "Iteration 166, loss = 0.41706908\n",
      "Iteration 167, loss = 0.41696108\n",
      "Iteration 168, loss = 0.41668694\n",
      "Iteration 169, loss = 0.41655511\n",
      "Iteration 170, loss = 0.41645120\n",
      "Iteration 171, loss = 0.41639993\n",
      "Iteration 172, loss = 0.41612656\n",
      "Iteration 173, loss = 0.41610258\n",
      "Iteration 174, loss = 0.41588099\n",
      "Iteration 175, loss = 0.41579163\n",
      "Iteration 176, loss = 0.41573125\n",
      "Iteration 177, loss = 0.41543041\n",
      "Iteration 178, loss = 0.41536297\n",
      "Iteration 179, loss = 0.41509700\n",
      "Iteration 180, loss = 0.41501808\n",
      "Iteration 181, loss = 0.41491652\n",
      "Iteration 182, loss = 0.41455405\n",
      "Iteration 183, loss = 0.41444035\n",
      "Iteration 184, loss = 0.41433950\n",
      "Iteration 185, loss = 0.41421429\n",
      "Iteration 186, loss = 0.41399110\n",
      "Iteration 187, loss = 0.41418659\n",
      "Iteration 188, loss = 0.41380807\n",
      "Iteration 189, loss = 0.41361941\n",
      "Iteration 190, loss = 0.41345939\n",
      "Iteration 191, loss = 0.41320752\n",
      "Iteration 192, loss = 0.41316211\n",
      "Iteration 193, loss = 0.41294116\n",
      "Iteration 194, loss = 0.41283627\n",
      "Iteration 195, loss = 0.41281803\n",
      "Iteration 196, loss = 0.41252444\n",
      "Iteration 197, loss = 0.41227496\n",
      "Iteration 198, loss = 0.41215980\n",
      "Iteration 199, loss = 0.41202066\n",
      "Iteration 200, loss = 0.41187958\n",
      "Iteration 1, loss = 0.75925580\n",
      "Iteration 2, loss = 0.71274133\n",
      "Iteration 3, loss = 0.67284115\n",
      "Iteration 4, loss = 0.63924087\n",
      "Iteration 5, loss = 0.61040600\n",
      "Iteration 6, loss = 0.58464224\n",
      "Iteration 7, loss = 0.56405912\n",
      "Iteration 8, loss = 0.54508177\n",
      "Iteration 9, loss = 0.52817072\n",
      "Iteration 10, loss = 0.51336129\n",
      "Iteration 11, loss = 0.50062726\n",
      "Iteration 12, loss = 0.48909053\n",
      "Iteration 13, loss = 0.47921894\n",
      "Iteration 14, loss = 0.47016732\n",
      "Iteration 15, loss = 0.46260794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.45634034\n",
      "Iteration 17, loss = 0.45074090\n",
      "Iteration 18, loss = 0.44650673\n",
      "Iteration 19, loss = 0.44231222\n",
      "Iteration 20, loss = 0.43847968\n",
      "Iteration 21, loss = 0.43584833\n",
      "Iteration 22, loss = 0.43333389\n",
      "Iteration 23, loss = 0.43114110\n",
      "Iteration 24, loss = 0.42953874\n",
      "Iteration 25, loss = 0.42815390\n",
      "Iteration 26, loss = 0.42688468\n",
      "Iteration 27, loss = 0.42572668\n",
      "Iteration 28, loss = 0.42504230\n",
      "Iteration 29, loss = 0.42410608\n",
      "Iteration 30, loss = 0.42355713\n",
      "Iteration 31, loss = 0.42308607\n",
      "Iteration 32, loss = 0.42256403\n",
      "Iteration 33, loss = 0.42241425\n",
      "Iteration 34, loss = 0.42177924\n",
      "Iteration 35, loss = 0.42156506\n",
      "Iteration 36, loss = 0.42114533\n",
      "Iteration 37, loss = 0.42099101\n",
      "Iteration 38, loss = 0.42066656\n",
      "Iteration 39, loss = 0.42032083\n",
      "Iteration 40, loss = 0.42022289\n",
      "Iteration 41, loss = 0.41994678\n",
      "Iteration 42, loss = 0.41979640\n",
      "Iteration 43, loss = 0.41946204\n",
      "Iteration 44, loss = 0.41949628\n",
      "Iteration 45, loss = 0.41914328\n",
      "Iteration 46, loss = 0.41892023\n",
      "Iteration 47, loss = 0.41871969\n",
      "Iteration 48, loss = 0.41864402\n",
      "Iteration 49, loss = 0.41841531\n",
      "Iteration 50, loss = 0.41825405\n",
      "Iteration 51, loss = 0.41812041\n",
      "Iteration 52, loss = 0.41801237\n",
      "Iteration 53, loss = 0.41775094\n",
      "Iteration 54, loss = 0.41756304\n",
      "Iteration 55, loss = 0.41741925\n",
      "Iteration 56, loss = 0.41736088\n",
      "Iteration 57, loss = 0.41725878\n",
      "Iteration 58, loss = 0.41704912\n",
      "Iteration 59, loss = 0.41682713\n",
      "Iteration 60, loss = 0.41672103\n",
      "Iteration 61, loss = 0.41656511\n",
      "Iteration 62, loss = 0.41643321\n",
      "Iteration 63, loss = 0.41626486\n",
      "Iteration 64, loss = 0.41618180\n",
      "Iteration 65, loss = 0.41597012\n",
      "Iteration 66, loss = 0.41595250\n",
      "Iteration 67, loss = 0.41587818\n",
      "Iteration 68, loss = 0.41562745\n",
      "Iteration 69, loss = 0.41540926\n",
      "Iteration 70, loss = 0.41543148\n",
      "Iteration 71, loss = 0.41515984\n",
      "Iteration 72, loss = 0.41504211\n",
      "Iteration 73, loss = 0.41484086\n",
      "Iteration 74, loss = 0.41474301\n",
      "Iteration 75, loss = 0.41455225\n",
      "Iteration 76, loss = 0.41459528\n",
      "Iteration 77, loss = 0.41422415\n",
      "Iteration 78, loss = 0.41410077\n",
      "Iteration 79, loss = 0.41397740\n",
      "Iteration 80, loss = 0.41392502\n",
      "Iteration 81, loss = 0.41376251\n",
      "Iteration 82, loss = 0.41363789\n",
      "Iteration 83, loss = 0.41345589\n",
      "Iteration 84, loss = 0.41331969\n",
      "Iteration 85, loss = 0.41332718\n",
      "Iteration 86, loss = 0.41316897\n",
      "Iteration 87, loss = 0.41309114\n",
      "Iteration 88, loss = 0.41275448\n",
      "Iteration 89, loss = 0.41263294\n",
      "Iteration 90, loss = 0.41261538\n",
      "Iteration 91, loss = 0.41233817\n",
      "Iteration 92, loss = 0.41254134\n",
      "Iteration 93, loss = 0.41211903\n",
      "Iteration 94, loss = 0.41239059\n",
      "Iteration 95, loss = 0.41180218\n",
      "Iteration 96, loss = 0.41160738\n",
      "Iteration 97, loss = 0.41148594\n",
      "Iteration 98, loss = 0.41128848\n",
      "Iteration 99, loss = 0.41119934\n",
      "Iteration 100, loss = 0.41101113\n",
      "Iteration 101, loss = 0.41120389\n",
      "Iteration 102, loss = 0.41080213\n",
      "Iteration 103, loss = 0.41062142\n",
      "Iteration 104, loss = 0.41060648\n",
      "Iteration 105, loss = 0.41032792\n",
      "Iteration 106, loss = 0.41031351\n",
      "Iteration 107, loss = 0.41009344\n",
      "Iteration 108, loss = 0.40992550\n",
      "Iteration 109, loss = 0.40977983\n",
      "Iteration 110, loss = 0.40958057\n",
      "Iteration 111, loss = 0.40948071\n",
      "Iteration 112, loss = 0.40940772\n",
      "Iteration 113, loss = 0.40913886\n",
      "Iteration 114, loss = 0.40906841\n",
      "Iteration 115, loss = 0.40886924\n",
      "Iteration 116, loss = 0.40862454\n",
      "Iteration 117, loss = 0.40849465\n",
      "Iteration 118, loss = 0.40838730\n",
      "Iteration 119, loss = 0.40839855\n",
      "Iteration 120, loss = 0.40823567\n",
      "Iteration 121, loss = 0.40798647\n",
      "Iteration 122, loss = 0.40783871\n",
      "Iteration 123, loss = 0.40759267\n",
      "Iteration 124, loss = 0.40757465\n",
      "Iteration 125, loss = 0.40744161\n",
      "Iteration 126, loss = 0.40716898\n",
      "Iteration 127, loss = 0.40696921\n",
      "Iteration 128, loss = 0.40687113\n",
      "Iteration 129, loss = 0.40677540\n",
      "Iteration 130, loss = 0.40659737\n",
      "Iteration 131, loss = 0.40636099\n",
      "Iteration 132, loss = 0.40627231\n",
      "Iteration 133, loss = 0.40610151\n",
      "Iteration 134, loss = 0.40594270\n",
      "Iteration 135, loss = 0.40566150\n",
      "Iteration 136, loss = 0.40590785\n",
      "Iteration 137, loss = 0.40534965\n",
      "Iteration 138, loss = 0.40523999\n",
      "Iteration 139, loss = 0.40523534\n",
      "Iteration 140, loss = 0.40492602\n",
      "Iteration 141, loss = 0.40477904\n",
      "Iteration 142, loss = 0.40456934\n",
      "Iteration 143, loss = 0.40442457\n",
      "Iteration 144, loss = 0.40436259\n",
      "Iteration 145, loss = 0.40421737\n",
      "Iteration 146, loss = 0.40398994\n",
      "Iteration 147, loss = 0.40383132\n",
      "Iteration 148, loss = 0.40373285\n",
      "Iteration 149, loss = 0.40338958\n",
      "Iteration 150, loss = 0.40321517\n",
      "Iteration 151, loss = 0.40308185\n",
      "Iteration 152, loss = 0.40296181\n",
      "Iteration 153, loss = 0.40330520\n",
      "Iteration 154, loss = 0.40271075\n",
      "Iteration 155, loss = 0.40226210\n",
      "Iteration 156, loss = 0.40230115\n",
      "Iteration 157, loss = 0.40207715\n",
      "Iteration 158, loss = 0.40187453\n",
      "Iteration 159, loss = 0.40170969\n",
      "Iteration 160, loss = 0.40162651\n",
      "Iteration 161, loss = 0.40141410\n",
      "Iteration 162, loss = 0.40128643\n",
      "Iteration 163, loss = 0.40102697\n",
      "Iteration 164, loss = 0.40099237\n",
      "Iteration 165, loss = 0.40074496\n",
      "Iteration 166, loss = 0.40053050\n",
      "Iteration 167, loss = 0.40022328\n",
      "Iteration 168, loss = 0.40011570\n",
      "Iteration 169, loss = 0.39992013\n",
      "Iteration 170, loss = 0.39989971\n",
      "Iteration 171, loss = 0.39959231\n",
      "Iteration 172, loss = 0.39959776\n",
      "Iteration 173, loss = 0.39937254\n",
      "Iteration 174, loss = 0.39916508\n",
      "Iteration 175, loss = 0.39891643\n",
      "Iteration 176, loss = 0.39876338\n",
      "Iteration 177, loss = 0.39845717\n",
      "Iteration 178, loss = 0.39852260\n",
      "Iteration 179, loss = 0.39816377\n",
      "Iteration 180, loss = 0.39793326\n",
      "Iteration 181, loss = 0.39770335\n",
      "Iteration 182, loss = 0.39757392\n",
      "Iteration 183, loss = 0.39727938\n",
      "Iteration 184, loss = 0.39717359\n",
      "Iteration 185, loss = 0.39721291\n",
      "Iteration 186, loss = 0.39690293\n",
      "Iteration 187, loss = 0.39672700\n",
      "Iteration 188, loss = 0.39650343\n",
      "Iteration 189, loss = 0.39633308\n",
      "Iteration 190, loss = 0.39604221\n",
      "Iteration 191, loss = 0.39592522\n",
      "Iteration 192, loss = 0.39573477\n",
      "Iteration 193, loss = 0.39541271\n",
      "Iteration 194, loss = 0.39546959\n",
      "Iteration 195, loss = 0.39526025\n",
      "Iteration 196, loss = 0.39495133\n",
      "Iteration 197, loss = 0.39502069\n",
      "Iteration 198, loss = 0.39462146\n",
      "Iteration 199, loss = 0.39444036\n",
      "Iteration 200, loss = 0.39405456\n",
      "Iteration 1, loss = 0.76162014\n",
      "Iteration 2, loss = 0.71395006\n",
      "Iteration 3, loss = 0.67510773\n",
      "Iteration 4, loss = 0.64121698\n",
      "Iteration 5, loss = 0.61162532\n",
      "Iteration 6, loss = 0.58647122\n",
      "Iteration 7, loss = 0.56515843\n",
      "Iteration 8, loss = 0.54568044\n",
      "Iteration 9, loss = 0.52848800\n",
      "Iteration 10, loss = 0.51329764\n",
      "Iteration 11, loss = 0.50043738\n",
      "Iteration 12, loss = 0.48863707\n",
      "Iteration 13, loss = 0.47855389\n",
      "Iteration 14, loss = 0.46917606\n",
      "Iteration 15, loss = 0.46158790\n",
      "Iteration 16, loss = 0.45486821\n",
      "Iteration 17, loss = 0.44929371\n",
      "Iteration 18, loss = 0.44473472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.44000009\n",
      "Iteration 20, loss = 0.43660628\n",
      "Iteration 21, loss = 0.43344395\n",
      "Iteration 22, loss = 0.43085293\n",
      "Iteration 23, loss = 0.42841577\n",
      "Iteration 24, loss = 0.42659647\n",
      "Iteration 25, loss = 0.42514503\n",
      "Iteration 26, loss = 0.42367574\n",
      "Iteration 27, loss = 0.42251075\n",
      "Iteration 28, loss = 0.42168396\n",
      "Iteration 29, loss = 0.42080892\n",
      "Iteration 30, loss = 0.42002364\n",
      "Iteration 31, loss = 0.41961428\n",
      "Iteration 32, loss = 0.41913147\n",
      "Iteration 33, loss = 0.41887045\n",
      "Iteration 34, loss = 0.41824601\n",
      "Iteration 35, loss = 0.41784690\n",
      "Iteration 36, loss = 0.41772247\n",
      "Iteration 37, loss = 0.41740640\n",
      "Iteration 38, loss = 0.41705388\n",
      "Iteration 39, loss = 0.41684399\n",
      "Iteration 40, loss = 0.41677747\n",
      "Iteration 41, loss = 0.41648069\n",
      "Iteration 42, loss = 0.41634041\n",
      "Iteration 43, loss = 0.41613864\n",
      "Iteration 44, loss = 0.41607032\n",
      "Iteration 45, loss = 0.41577224\n",
      "Iteration 46, loss = 0.41562564\n",
      "Iteration 47, loss = 0.41544361\n",
      "Iteration 48, loss = 0.41534689\n",
      "Iteration 49, loss = 0.41520264\n",
      "Iteration 50, loss = 0.41501851\n",
      "Iteration 51, loss = 0.41490496\n",
      "Iteration 52, loss = 0.41483717\n",
      "Iteration 53, loss = 0.41463435\n",
      "Iteration 54, loss = 0.41448395\n",
      "Iteration 55, loss = 0.41435733\n",
      "Iteration 56, loss = 0.41422265\n",
      "Iteration 57, loss = 0.41415416\n",
      "Iteration 58, loss = 0.41409455\n",
      "Iteration 59, loss = 0.41385033\n",
      "Iteration 60, loss = 0.41369608\n",
      "Iteration 61, loss = 0.41364965\n",
      "Iteration 62, loss = 0.41348600\n",
      "Iteration 63, loss = 0.41344461\n",
      "Iteration 64, loss = 0.41339266\n",
      "Iteration 65, loss = 0.41318384\n",
      "Iteration 66, loss = 0.41306960\n",
      "Iteration 67, loss = 0.41291770\n",
      "Iteration 68, loss = 0.41274353\n",
      "Iteration 69, loss = 0.41262646\n",
      "Iteration 70, loss = 0.41260336\n",
      "Iteration 71, loss = 0.41241218\n",
      "Iteration 72, loss = 0.41228095\n",
      "Iteration 73, loss = 0.41206310\n",
      "Iteration 74, loss = 0.41204438\n",
      "Iteration 75, loss = 0.41187849\n",
      "Iteration 76, loss = 0.41178018\n",
      "Iteration 77, loss = 0.41165065\n",
      "Iteration 78, loss = 0.41148491\n",
      "Iteration 79, loss = 0.41132729\n",
      "Iteration 80, loss = 0.41124716\n",
      "Iteration 81, loss = 0.41110591\n",
      "Iteration 82, loss = 0.41097227\n",
      "Iteration 83, loss = 0.41082922\n",
      "Iteration 84, loss = 0.41074306\n",
      "Iteration 85, loss = 0.41073770\n",
      "Iteration 86, loss = 0.41043923\n",
      "Iteration 87, loss = 0.41053682\n",
      "Iteration 88, loss = 0.41031485\n",
      "Iteration 89, loss = 0.41018276\n",
      "Iteration 90, loss = 0.41013977\n",
      "Iteration 91, loss = 0.40986338\n",
      "Iteration 92, loss = 0.40999788\n",
      "Iteration 93, loss = 0.40968275\n",
      "Iteration 94, loss = 0.40962614\n",
      "Iteration 95, loss = 0.40935542\n",
      "Iteration 96, loss = 0.40924826\n",
      "Iteration 97, loss = 0.40924337\n",
      "Iteration 98, loss = 0.40899168\n",
      "Iteration 99, loss = 0.40893600\n",
      "Iteration 100, loss = 0.40864166\n",
      "Iteration 101, loss = 0.40885910\n",
      "Iteration 102, loss = 0.40859987\n",
      "Iteration 103, loss = 0.40836482\n",
      "Iteration 104, loss = 0.40834533\n",
      "Iteration 105, loss = 0.40805694\n",
      "Iteration 106, loss = 0.40798833\n",
      "Iteration 107, loss = 0.40789748\n",
      "Iteration 108, loss = 0.40785903\n",
      "Iteration 109, loss = 0.40772875\n",
      "Iteration 110, loss = 0.40751156\n",
      "Iteration 111, loss = 0.40726331\n",
      "Iteration 112, loss = 0.40721151\n",
      "Iteration 113, loss = 0.40705374\n",
      "Iteration 114, loss = 0.40697341\n",
      "Iteration 115, loss = 0.40682594\n",
      "Iteration 116, loss = 0.40661627\n",
      "Iteration 117, loss = 0.40649390\n",
      "Iteration 118, loss = 0.40645542\n",
      "Iteration 119, loss = 0.40641639\n",
      "Iteration 120, loss = 0.40618601\n",
      "Iteration 121, loss = 0.40608514\n",
      "Iteration 122, loss = 0.40592832\n",
      "Iteration 123, loss = 0.40567719\n",
      "Iteration 124, loss = 0.40563873\n",
      "Iteration 125, loss = 0.40553267\n",
      "Iteration 126, loss = 0.40535461\n",
      "Iteration 127, loss = 0.40523251\n",
      "Iteration 128, loss = 0.40503912\n",
      "Iteration 129, loss = 0.40492429\n",
      "Iteration 130, loss = 0.40485407\n",
      "Iteration 131, loss = 0.40457422\n",
      "Iteration 132, loss = 0.40453176\n",
      "Iteration 133, loss = 0.40441106\n",
      "Iteration 134, loss = 0.40423741\n",
      "Iteration 135, loss = 0.40404837\n",
      "Iteration 136, loss = 0.40398508\n",
      "Iteration 137, loss = 0.40407400\n",
      "Iteration 138, loss = 0.40377217\n",
      "Iteration 139, loss = 0.40363388\n",
      "Iteration 140, loss = 0.40343406\n",
      "Iteration 141, loss = 0.40330334\n",
      "Iteration 142, loss = 0.40304042\n",
      "Iteration 143, loss = 0.40284689\n",
      "Iteration 144, loss = 0.40285986\n",
      "Iteration 145, loss = 0.40274835\n",
      "Iteration 146, loss = 0.40266281\n",
      "Iteration 147, loss = 0.40233181\n",
      "Iteration 148, loss = 0.40228323\n",
      "Iteration 149, loss = 0.40201160\n",
      "Iteration 150, loss = 0.40186503\n",
      "Iteration 151, loss = 0.40171049\n",
      "Iteration 152, loss = 0.40175858\n",
      "Iteration 153, loss = 0.40185728\n",
      "Iteration 154, loss = 0.40129325\n",
      "Iteration 155, loss = 0.40110511\n",
      "Iteration 156, loss = 0.40104225\n",
      "Iteration 157, loss = 0.40080889\n",
      "Iteration 158, loss = 0.40066116\n",
      "Iteration 159, loss = 0.40049427\n",
      "Iteration 160, loss = 0.40042652\n",
      "Iteration 161, loss = 0.40029959\n",
      "Iteration 162, loss = 0.40017089\n",
      "Iteration 163, loss = 0.39996527\n",
      "Iteration 164, loss = 0.39979336\n",
      "Iteration 165, loss = 0.39955392\n",
      "Iteration 166, loss = 0.39946992\n",
      "Iteration 167, loss = 0.39922656\n",
      "Iteration 168, loss = 0.39924487\n",
      "Iteration 169, loss = 0.39899522\n",
      "Iteration 170, loss = 0.39889903\n",
      "Iteration 171, loss = 0.39859018\n",
      "Iteration 172, loss = 0.39850296\n",
      "Iteration 173, loss = 0.39852792\n",
      "Iteration 174, loss = 0.39821081\n",
      "Iteration 175, loss = 0.39813231\n",
      "Iteration 176, loss = 0.39785049\n",
      "Iteration 177, loss = 0.39769413\n",
      "Iteration 178, loss = 0.39756950\n",
      "Iteration 179, loss = 0.39740728\n",
      "Iteration 180, loss = 0.39712439\n",
      "Iteration 181, loss = 0.39696699\n",
      "Iteration 182, loss = 0.39681519\n",
      "Iteration 183, loss = 0.39657970\n",
      "Iteration 184, loss = 0.39642274\n",
      "Iteration 185, loss = 0.39636589\n",
      "Iteration 186, loss = 0.39626040\n",
      "Iteration 187, loss = 0.39591510\n",
      "Iteration 188, loss = 0.39584586\n",
      "Iteration 189, loss = 0.39569323\n",
      "Iteration 190, loss = 0.39553429\n",
      "Iteration 191, loss = 0.39522100\n",
      "Iteration 192, loss = 0.39529325\n",
      "Iteration 193, loss = 0.39479157\n",
      "Iteration 194, loss = 0.39480780\n",
      "Iteration 195, loss = 0.39478172\n",
      "Iteration 196, loss = 0.39431907\n",
      "Iteration 197, loss = 0.39425127\n",
      "Iteration 198, loss = 0.39410598\n",
      "Iteration 199, loss = 0.39399821\n",
      "Iteration 200, loss = 0.39359271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75442427\n",
      "Iteration 2, loss = 0.70803020\n",
      "Iteration 3, loss = 0.66862049\n",
      "Iteration 4, loss = 0.63582109\n",
      "Iteration 5, loss = 0.60755083\n",
      "Iteration 6, loss = 0.58374915\n",
      "Iteration 7, loss = 0.56431470\n",
      "Iteration 8, loss = 0.54643150\n",
      "Iteration 9, loss = 0.53126811\n",
      "Iteration 10, loss = 0.51786176\n",
      "Iteration 11, loss = 0.50649546\n",
      "Iteration 12, loss = 0.49679102\n",
      "Iteration 13, loss = 0.48823918\n",
      "Iteration 14, loss = 0.48031866\n",
      "Iteration 15, loss = 0.47425185\n",
      "Iteration 16, loss = 0.46874799\n",
      "Iteration 17, loss = 0.46482924\n",
      "Iteration 18, loss = 0.46079837\n",
      "Iteration 19, loss = 0.45728654\n",
      "Iteration 20, loss = 0.45507408\n",
      "Iteration 21, loss = 0.45291126\n",
      "Iteration 22, loss = 0.45088138\n",
      "Iteration 23, loss = 0.44940744\n",
      "Iteration 24, loss = 0.44822869\n",
      "Iteration 25, loss = 0.44722240\n",
      "Iteration 26, loss = 0.44629803\n",
      "Iteration 27, loss = 0.44561259\n",
      "Iteration 28, loss = 0.44518183\n",
      "Iteration 29, loss = 0.44458657\n",
      "Iteration 30, loss = 0.44410195\n",
      "Iteration 31, loss = 0.44380084\n",
      "Iteration 32, loss = 0.44352698\n",
      "Iteration 33, loss = 0.44329762\n",
      "Iteration 34, loss = 0.44295503\n",
      "Iteration 35, loss = 0.44264666\n",
      "Iteration 36, loss = 0.44256458\n",
      "Iteration 37, loss = 0.44233219\n",
      "Iteration 38, loss = 0.44205944\n",
      "Iteration 39, loss = 0.44184466\n",
      "Iteration 40, loss = 0.44179700\n",
      "Iteration 41, loss = 0.44154258\n",
      "Iteration 42, loss = 0.44139890\n",
      "Iteration 43, loss = 0.44118367\n",
      "Iteration 44, loss = 0.44107569\n",
      "Iteration 45, loss = 0.44086260\n",
      "Iteration 46, loss = 0.44071880\n",
      "Iteration 47, loss = 0.44049257\n",
      "Iteration 48, loss = 0.44037084\n",
      "Iteration 49, loss = 0.44025909\n",
      "Iteration 50, loss = 0.44018186\n",
      "Iteration 51, loss = 0.43993274\n",
      "Iteration 52, loss = 0.43980323\n",
      "Iteration 53, loss = 0.43968179\n",
      "Iteration 54, loss = 0.43943121\n",
      "Iteration 55, loss = 0.43932192\n",
      "Iteration 56, loss = 0.43907868\n",
      "Iteration 57, loss = 0.43897596\n",
      "Iteration 58, loss = 0.43913512\n",
      "Iteration 59, loss = 0.43867992\n",
      "Iteration 60, loss = 0.43851774\n",
      "Iteration 61, loss = 0.43841581\n",
      "Iteration 62, loss = 0.43819191\n",
      "Iteration 63, loss = 0.43817790\n",
      "Iteration 64, loss = 0.43803119\n",
      "Iteration 65, loss = 0.43790882\n",
      "Iteration 66, loss = 0.43771400\n",
      "Iteration 67, loss = 0.43758214\n",
      "Iteration 68, loss = 0.43744820\n",
      "Iteration 69, loss = 0.43727883\n",
      "Iteration 70, loss = 0.43725240\n",
      "Iteration 71, loss = 0.43693794\n",
      "Iteration 72, loss = 0.43683248\n",
      "Iteration 73, loss = 0.43660157\n",
      "Iteration 74, loss = 0.43654354\n",
      "Iteration 75, loss = 0.43627440\n",
      "Iteration 76, loss = 0.43621073\n",
      "Iteration 77, loss = 0.43612404\n",
      "Iteration 78, loss = 0.43602670\n",
      "Iteration 79, loss = 0.43570727\n",
      "Iteration 80, loss = 0.43560422\n",
      "Iteration 81, loss = 0.43545866\n",
      "Iteration 82, loss = 0.43526386\n",
      "Iteration 83, loss = 0.43515149\n",
      "Iteration 84, loss = 0.43505209\n",
      "Iteration 85, loss = 0.43490346\n",
      "Iteration 86, loss = 0.43467696\n",
      "Iteration 87, loss = 0.43501318\n",
      "Iteration 88, loss = 0.43455529\n",
      "Iteration 89, loss = 0.43429149\n",
      "Iteration 90, loss = 0.43409804\n",
      "Iteration 91, loss = 0.43394665\n",
      "Iteration 92, loss = 0.43396080\n",
      "Iteration 93, loss = 0.43363639\n",
      "Iteration 94, loss = 0.43354060\n",
      "Iteration 95, loss = 0.43333802\n",
      "Iteration 96, loss = 0.43323735\n",
      "Iteration 97, loss = 0.43313029\n",
      "Iteration 98, loss = 0.43285165\n",
      "Iteration 99, loss = 0.43273329\n",
      "Iteration 100, loss = 0.43246640\n",
      "Iteration 101, loss = 0.43277102\n",
      "Iteration 102, loss = 0.43233421\n",
      "Iteration 103, loss = 0.43224102\n",
      "Iteration 104, loss = 0.43201897\n",
      "Iteration 105, loss = 0.43184732\n",
      "Iteration 106, loss = 0.43159943\n",
      "Iteration 107, loss = 0.43156382\n",
      "Iteration 108, loss = 0.43146918\n",
      "Iteration 109, loss = 0.43135155\n",
      "Iteration 110, loss = 0.43116562\n",
      "Iteration 111, loss = 0.43078771\n",
      "Iteration 112, loss = 0.43077023\n",
      "Iteration 113, loss = 0.43046701\n",
      "Iteration 114, loss = 0.43032309\n",
      "Iteration 115, loss = 0.43028046\n",
      "Iteration 116, loss = 0.43008150\n",
      "Iteration 117, loss = 0.42988055\n",
      "Iteration 118, loss = 0.42975883\n",
      "Iteration 119, loss = 0.42972280\n",
      "Iteration 120, loss = 0.42962508\n",
      "Iteration 121, loss = 0.42923943\n",
      "Iteration 122, loss = 0.42912292\n",
      "Iteration 123, loss = 0.42894796\n",
      "Iteration 124, loss = 0.42887856\n",
      "Iteration 125, loss = 0.42861032\n",
      "Iteration 126, loss = 0.42868583\n",
      "Iteration 127, loss = 0.42821252\n",
      "Iteration 128, loss = 0.42811275\n",
      "Iteration 129, loss = 0.42786016\n",
      "Iteration 130, loss = 0.42775674\n",
      "Iteration 131, loss = 0.42757188\n",
      "Iteration 132, loss = 0.42740187\n",
      "Iteration 133, loss = 0.42737500\n",
      "Iteration 134, loss = 0.42705772\n",
      "Iteration 135, loss = 0.42696462\n",
      "Iteration 136, loss = 0.42678116\n",
      "Iteration 137, loss = 0.42666018\n",
      "Iteration 138, loss = 0.42623433\n",
      "Iteration 139, loss = 0.42634011\n",
      "Iteration 140, loss = 0.42609205\n",
      "Iteration 141, loss = 0.42592171\n",
      "Iteration 142, loss = 0.42567153\n",
      "Iteration 143, loss = 0.42558842\n",
      "Iteration 144, loss = 0.42542322\n",
      "Iteration 145, loss = 0.42520055\n",
      "Iteration 146, loss = 0.42493076\n",
      "Iteration 147, loss = 0.42487340\n",
      "Iteration 148, loss = 0.42461854\n",
      "Iteration 149, loss = 0.42438149\n",
      "Iteration 150, loss = 0.42428471\n",
      "Iteration 151, loss = 0.42407388\n",
      "Iteration 152, loss = 0.42401548\n",
      "Iteration 153, loss = 0.42407066\n",
      "Iteration 154, loss = 0.42350426\n",
      "Iteration 155, loss = 0.42353166\n",
      "Iteration 156, loss = 0.42312833\n",
      "Iteration 157, loss = 0.42300443\n",
      "Iteration 158, loss = 0.42288069\n",
      "Iteration 159, loss = 0.42261914\n",
      "Iteration 160, loss = 0.42242586\n",
      "Iteration 161, loss = 0.42218757\n",
      "Iteration 162, loss = 0.42198335\n",
      "Iteration 163, loss = 0.42187465\n",
      "Iteration 164, loss = 0.42170568\n",
      "Iteration 165, loss = 0.42144714\n",
      "Iteration 166, loss = 0.42137737\n",
      "Iteration 167, loss = 0.42113434\n",
      "Iteration 168, loss = 0.42095290\n",
      "Iteration 169, loss = 0.42080987\n",
      "Iteration 170, loss = 0.42063489\n",
      "Iteration 171, loss = 0.42025767\n",
      "Iteration 172, loss = 0.42010644\n",
      "Iteration 173, loss = 0.41997494\n",
      "Iteration 174, loss = 0.41985390\n",
      "Iteration 175, loss = 0.41971105\n",
      "Iteration 176, loss = 0.41946506\n",
      "Iteration 177, loss = 0.41917772\n",
      "Iteration 178, loss = 0.41894615\n",
      "Iteration 179, loss = 0.41884738\n",
      "Iteration 180, loss = 0.41860682\n",
      "Iteration 181, loss = 0.41834385\n",
      "Iteration 182, loss = 0.41820518\n",
      "Iteration 183, loss = 0.41806751\n",
      "Iteration 184, loss = 0.41796441\n",
      "Iteration 185, loss = 0.41782257\n",
      "Iteration 186, loss = 0.41752844\n",
      "Iteration 187, loss = 0.41723619\n",
      "Iteration 188, loss = 0.41700611\n",
      "Iteration 189, loss = 0.41682847\n",
      "Iteration 190, loss = 0.41657302\n",
      "Iteration 191, loss = 0.41642783\n",
      "Iteration 192, loss = 0.41638565\n",
      "Iteration 193, loss = 0.41593795\n",
      "Iteration 194, loss = 0.41588253\n",
      "Iteration 195, loss = 0.41582744\n",
      "Iteration 196, loss = 0.41537603\n",
      "Iteration 197, loss = 0.41547250\n",
      "Iteration 198, loss = 0.41518620\n",
      "Iteration 199, loss = 0.41506482\n",
      "Iteration 200, loss = 0.41488463\n",
      "Iteration 1, loss = 0.75065251\n",
      "Iteration 2, loss = 0.70190708\n",
      "Iteration 3, loss = 0.65722314\n",
      "Iteration 4, loss = 0.62139734\n",
      "Iteration 5, loss = 0.59159805\n",
      "Iteration 6, loss = 0.56586290\n",
      "Iteration 7, loss = 0.54358053\n",
      "Iteration 8, loss = 0.52579304\n",
      "Iteration 9, loss = 0.50911544\n",
      "Iteration 10, loss = 0.49552867\n",
      "Iteration 11, loss = 0.48340195\n",
      "Iteration 12, loss = 0.47317948\n",
      "Iteration 13, loss = 0.46307054\n",
      "Iteration 14, loss = 0.45520104\n",
      "Iteration 15, loss = 0.44811142\n",
      "Iteration 16, loss = 0.44184751\n",
      "Iteration 17, loss = 0.43632745\n",
      "Iteration 18, loss = 0.43247853\n",
      "Iteration 19, loss = 0.42841379\n",
      "Iteration 20, loss = 0.42512293\n",
      "Iteration 21, loss = 0.42254999\n",
      "Iteration 22, loss = 0.42000174\n",
      "Iteration 23, loss = 0.41843672\n",
      "Iteration 24, loss = 0.41677612\n",
      "Iteration 25, loss = 0.41566999\n",
      "Iteration 26, loss = 0.41460980\n",
      "Iteration 27, loss = 0.41364390\n",
      "Iteration 28, loss = 0.41296190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.41233888\n",
      "Iteration 30, loss = 0.41179390\n",
      "Iteration 31, loss = 0.41134723\n",
      "Iteration 32, loss = 0.41112754\n",
      "Iteration 33, loss = 0.41076651\n",
      "Iteration 34, loss = 0.41042644\n",
      "Iteration 35, loss = 0.41021550\n",
      "Iteration 36, loss = 0.41002397\n",
      "Iteration 37, loss = 0.40977743\n",
      "Iteration 38, loss = 0.40959813\n",
      "Iteration 39, loss = 0.40942130\n",
      "Iteration 40, loss = 0.40930245\n",
      "Iteration 41, loss = 0.40915350\n",
      "Iteration 42, loss = 0.40906675\n",
      "Iteration 43, loss = 0.40895976\n",
      "Iteration 44, loss = 0.40878826\n",
      "Iteration 45, loss = 0.40863443\n",
      "Iteration 46, loss = 0.40854925\n",
      "Iteration 47, loss = 0.40833548\n",
      "Iteration 48, loss = 0.40819721\n",
      "Iteration 49, loss = 0.40805847\n",
      "Iteration 50, loss = 0.40792630\n",
      "Iteration 51, loss = 0.40786655\n",
      "Iteration 52, loss = 0.40773170\n",
      "Iteration 53, loss = 0.40751524\n",
      "Iteration 54, loss = 0.40747709\n",
      "Iteration 55, loss = 0.40731615\n",
      "Iteration 56, loss = 0.40719703\n",
      "Iteration 57, loss = 0.40715373\n",
      "Iteration 58, loss = 0.40707119\n",
      "Iteration 59, loss = 0.40676294\n",
      "Iteration 60, loss = 0.40664300\n",
      "Iteration 61, loss = 0.40673042\n",
      "Iteration 62, loss = 0.40676913\n",
      "Iteration 63, loss = 0.40646788\n",
      "Iteration 64, loss = 0.40627556\n",
      "Iteration 65, loss = 0.40599590\n",
      "Iteration 66, loss = 0.40596693\n",
      "Iteration 67, loss = 0.40575065\n",
      "Iteration 68, loss = 0.40579806\n",
      "Iteration 69, loss = 0.40551933\n",
      "Iteration 70, loss = 0.40546027\n",
      "Iteration 71, loss = 0.40528476\n",
      "Iteration 72, loss = 0.40519596\n",
      "Iteration 73, loss = 0.40502257\n",
      "Iteration 74, loss = 0.40491569\n",
      "Iteration 75, loss = 0.40479876\n",
      "Iteration 76, loss = 0.40473545\n",
      "Iteration 77, loss = 0.40452285\n",
      "Iteration 78, loss = 0.40450234\n",
      "Iteration 79, loss = 0.40423391\n",
      "Iteration 80, loss = 0.40417588\n",
      "Iteration 81, loss = 0.40407358\n",
      "Iteration 82, loss = 0.40389140\n",
      "Iteration 83, loss = 0.40391319\n",
      "Iteration 84, loss = 0.40369682\n",
      "Iteration 85, loss = 0.40351104\n",
      "Iteration 86, loss = 0.40355492\n",
      "Iteration 87, loss = 0.40326659\n",
      "Iteration 88, loss = 0.40327617\n",
      "Iteration 89, loss = 0.40298595\n",
      "Iteration 90, loss = 0.40290495\n",
      "Iteration 91, loss = 0.40285621\n",
      "Iteration 92, loss = 0.40268112\n",
      "Iteration 93, loss = 0.40248468\n",
      "Iteration 94, loss = 0.40243388\n",
      "Iteration 95, loss = 0.40218092\n",
      "Iteration 96, loss = 0.40238311\n",
      "Iteration 97, loss = 0.40194947\n",
      "Iteration 98, loss = 0.40185925\n",
      "Iteration 99, loss = 0.40169210\n",
      "Iteration 100, loss = 0.40170648\n",
      "Iteration 101, loss = 0.40144081\n",
      "Iteration 102, loss = 0.40133906\n",
      "Iteration 103, loss = 0.40111780\n",
      "Iteration 104, loss = 0.40108585\n",
      "Iteration 105, loss = 0.40086430\n",
      "Iteration 106, loss = 0.40075966\n",
      "Iteration 107, loss = 0.40064692\n",
      "Iteration 108, loss = 0.40066374\n",
      "Iteration 109, loss = 0.40034788\n",
      "Iteration 110, loss = 0.40030323\n",
      "Iteration 111, loss = 0.40010163\n",
      "Iteration 112, loss = 0.39987627\n",
      "Iteration 113, loss = 0.39985440\n",
      "Iteration 114, loss = 0.39959649\n",
      "Iteration 115, loss = 0.39951598\n",
      "Iteration 116, loss = 0.39938999\n",
      "Iteration 117, loss = 0.39917604\n",
      "Iteration 118, loss = 0.39911618\n",
      "Iteration 119, loss = 0.39895250\n",
      "Iteration 120, loss = 0.39877986\n",
      "Iteration 121, loss = 0.39875948\n",
      "Iteration 122, loss = 0.39848977\n",
      "Iteration 123, loss = 0.39845718\n",
      "Iteration 124, loss = 0.39841563\n",
      "Iteration 125, loss = 0.39820441\n",
      "Iteration 126, loss = 0.39794283\n",
      "Iteration 127, loss = 0.39779596\n",
      "Iteration 128, loss = 0.39764356\n",
      "Iteration 129, loss = 0.39748392\n",
      "Iteration 130, loss = 0.39733281\n",
      "Iteration 131, loss = 0.39733678\n",
      "Iteration 132, loss = 0.39702233\n",
      "Iteration 133, loss = 0.39691027\n",
      "Iteration 134, loss = 0.39676945\n",
      "Iteration 135, loss = 0.39656452\n",
      "Iteration 136, loss = 0.39639750\n",
      "Iteration 137, loss = 0.39639295\n",
      "Iteration 138, loss = 0.39610147\n",
      "Iteration 139, loss = 0.39614315\n",
      "Iteration 140, loss = 0.39597982\n",
      "Iteration 141, loss = 0.39571409\n",
      "Iteration 142, loss = 0.39551846\n",
      "Iteration 143, loss = 0.39542555\n",
      "Iteration 144, loss = 0.39520785\n",
      "Iteration 145, loss = 0.39533783\n",
      "Iteration 146, loss = 0.39504633\n",
      "Iteration 147, loss = 0.39477385\n",
      "Iteration 148, loss = 0.39476845\n",
      "Iteration 149, loss = 0.39452156\n",
      "Iteration 150, loss = 0.39430232\n",
      "Iteration 151, loss = 0.39428112\n",
      "Iteration 152, loss = 0.39398710\n",
      "Iteration 153, loss = 0.39385519\n",
      "Iteration 154, loss = 0.39379120\n",
      "Iteration 155, loss = 0.39396680\n",
      "Iteration 156, loss = 0.39330065\n",
      "Iteration 157, loss = 0.39315700\n",
      "Iteration 158, loss = 0.39306617\n",
      "Iteration 159, loss = 0.39286052\n",
      "Iteration 160, loss = 0.39274022\n",
      "Iteration 161, loss = 0.39249483\n",
      "Iteration 162, loss = 0.39249969\n",
      "Iteration 163, loss = 0.39223698\n",
      "Iteration 164, loss = 0.39204548\n",
      "Iteration 165, loss = 0.39200898\n",
      "Iteration 166, loss = 0.39178336\n",
      "Iteration 167, loss = 0.39162992\n",
      "Iteration 168, loss = 0.39139255\n",
      "Iteration 169, loss = 0.39135773\n",
      "Iteration 170, loss = 0.39103502\n",
      "Iteration 171, loss = 0.39082932\n",
      "Iteration 172, loss = 0.39080287\n",
      "Iteration 173, loss = 0.39068053\n",
      "Iteration 174, loss = 0.39037936\n",
      "Iteration 175, loss = 0.39013598\n",
      "Iteration 176, loss = 0.39043066\n",
      "Iteration 177, loss = 0.39000508\n",
      "Iteration 178, loss = 0.38978035\n",
      "Iteration 179, loss = 0.38965019\n",
      "Iteration 180, loss = 0.38936072\n",
      "Iteration 181, loss = 0.38937140\n",
      "Iteration 182, loss = 0.38904529\n",
      "Iteration 183, loss = 0.38899248\n",
      "Iteration 184, loss = 0.38874829\n",
      "Iteration 185, loss = 0.38866472\n",
      "Iteration 186, loss = 0.38838748\n",
      "Iteration 187, loss = 0.38837854\n",
      "Iteration 188, loss = 0.38801598\n",
      "Iteration 189, loss = 0.38783707\n",
      "Iteration 190, loss = 0.38781202\n",
      "Iteration 191, loss = 0.38741517\n",
      "Iteration 192, loss = 0.38738958\n",
      "Iteration 193, loss = 0.38717561\n",
      "Iteration 194, loss = 0.38714040\n",
      "Iteration 195, loss = 0.38700090\n",
      "Iteration 196, loss = 0.38667841\n",
      "Iteration 197, loss = 0.38635074\n",
      "Iteration 198, loss = 0.38632943\n",
      "Iteration 199, loss = 0.38611620\n",
      "Iteration 200, loss = 0.38597764\n",
      "Iteration 1, loss = 0.75778688\n",
      "Iteration 2, loss = 0.71117249\n",
      "Iteration 3, loss = 0.66855183\n",
      "Iteration 4, loss = 0.63491254\n",
      "Iteration 5, loss = 0.60626754\n",
      "Iteration 6, loss = 0.58171472\n",
      "Iteration 7, loss = 0.55986479\n",
      "Iteration 8, loss = 0.54311391\n",
      "Iteration 9, loss = 0.52721065\n",
      "Iteration 10, loss = 0.51425698\n",
      "Iteration 11, loss = 0.50248936\n",
      "Iteration 12, loss = 0.49268548\n",
      "Iteration 13, loss = 0.48327816\n",
      "Iteration 14, loss = 0.47591923\n",
      "Iteration 15, loss = 0.46910536\n",
      "Iteration 16, loss = 0.46335780\n",
      "Iteration 17, loss = 0.45832567\n",
      "Iteration 18, loss = 0.45463252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.45092600\n",
      "Iteration 20, loss = 0.44776320\n",
      "Iteration 21, loss = 0.44528549\n",
      "Iteration 22, loss = 0.44306035\n",
      "Iteration 23, loss = 0.44173745\n",
      "Iteration 24, loss = 0.44013331\n",
      "Iteration 25, loss = 0.43916187\n",
      "Iteration 26, loss = 0.43800563\n",
      "Iteration 27, loss = 0.43730819\n",
      "Iteration 28, loss = 0.43665713\n",
      "Iteration 29, loss = 0.43588618\n",
      "Iteration 30, loss = 0.43553051\n",
      "Iteration 31, loss = 0.43494773\n",
      "Iteration 32, loss = 0.43478533\n",
      "Iteration 33, loss = 0.43438185\n",
      "Iteration 34, loss = 0.43401018\n",
      "Iteration 35, loss = 0.43374113\n",
      "Iteration 36, loss = 0.43359232\n",
      "Iteration 37, loss = 0.43328496\n",
      "Iteration 38, loss = 0.43310258\n",
      "Iteration 39, loss = 0.43291521\n",
      "Iteration 40, loss = 0.43284847\n",
      "Iteration 41, loss = 0.43255086\n",
      "Iteration 42, loss = 0.43247325\n",
      "Iteration 43, loss = 0.43233591\n",
      "Iteration 44, loss = 0.43221085\n",
      "Iteration 45, loss = 0.43198218\n",
      "Iteration 46, loss = 0.43186241\n",
      "Iteration 47, loss = 0.43169749\n",
      "Iteration 48, loss = 0.43161046\n",
      "Iteration 49, loss = 0.43150209\n",
      "Iteration 50, loss = 0.43138178\n",
      "Iteration 51, loss = 0.43124193\n",
      "Iteration 52, loss = 0.43106365\n",
      "Iteration 53, loss = 0.43096038\n",
      "Iteration 54, loss = 0.43086446\n",
      "Iteration 55, loss = 0.43070022\n",
      "Iteration 56, loss = 0.43060941\n",
      "Iteration 57, loss = 0.43047086\n",
      "Iteration 58, loss = 0.43036234\n",
      "Iteration 59, loss = 0.43021022\n",
      "Iteration 60, loss = 0.43013869\n",
      "Iteration 61, loss = 0.43008812\n",
      "Iteration 62, loss = 0.42990812\n",
      "Iteration 63, loss = 0.42973072\n",
      "Iteration 64, loss = 0.42969239\n",
      "Iteration 65, loss = 0.42947470\n",
      "Iteration 66, loss = 0.42944395\n",
      "Iteration 67, loss = 0.42919955\n",
      "Iteration 68, loss = 0.42926450\n",
      "Iteration 69, loss = 0.42911135\n",
      "Iteration 70, loss = 0.42892779\n",
      "Iteration 71, loss = 0.42877197\n",
      "Iteration 72, loss = 0.42859139\n",
      "Iteration 73, loss = 0.42860411\n",
      "Iteration 74, loss = 0.42847101\n",
      "Iteration 75, loss = 0.42828402\n",
      "Iteration 76, loss = 0.42819927\n",
      "Iteration 77, loss = 0.42802575\n",
      "Iteration 78, loss = 0.42791960\n",
      "Iteration 79, loss = 0.42773974\n",
      "Iteration 80, loss = 0.42763232\n",
      "Iteration 81, loss = 0.42773480\n",
      "Iteration 82, loss = 0.42739007\n",
      "Iteration 83, loss = 0.42756254\n",
      "Iteration 84, loss = 0.42720566\n",
      "Iteration 85, loss = 0.42698328\n",
      "Iteration 86, loss = 0.42688765\n",
      "Iteration 87, loss = 0.42680698\n",
      "Iteration 88, loss = 0.42672201\n",
      "Iteration 89, loss = 0.42653076\n",
      "Iteration 90, loss = 0.42642592\n",
      "Iteration 91, loss = 0.42640247\n",
      "Iteration 92, loss = 0.42615887\n",
      "Iteration 93, loss = 0.42598989\n",
      "Iteration 94, loss = 0.42622531\n",
      "Iteration 95, loss = 0.42584994\n",
      "Iteration 96, loss = 0.42572783\n",
      "Iteration 97, loss = 0.42559629\n",
      "Iteration 98, loss = 0.42563140\n",
      "Iteration 99, loss = 0.42532522\n",
      "Iteration 100, loss = 0.42518370\n",
      "Iteration 101, loss = 0.42512663\n",
      "Iteration 102, loss = 0.42496951\n",
      "Iteration 103, loss = 0.42487026\n",
      "Iteration 104, loss = 0.42471249\n",
      "Iteration 105, loss = 0.42474283\n",
      "Iteration 106, loss = 0.42442780\n",
      "Iteration 107, loss = 0.42426639\n",
      "Iteration 108, loss = 0.42418501\n",
      "Iteration 109, loss = 0.42405880\n",
      "Iteration 110, loss = 0.42394887\n",
      "Iteration 111, loss = 0.42385324\n",
      "Iteration 112, loss = 0.42358916\n",
      "Iteration 113, loss = 0.42360091\n",
      "Iteration 114, loss = 0.42335599\n",
      "Iteration 115, loss = 0.42327700\n",
      "Iteration 116, loss = 0.42330178\n",
      "Iteration 117, loss = 0.42301989\n",
      "Iteration 118, loss = 0.42291657\n",
      "Iteration 119, loss = 0.42281775\n",
      "Iteration 120, loss = 0.42258455\n",
      "Iteration 121, loss = 0.42260372\n",
      "Iteration 122, loss = 0.42239031\n",
      "Iteration 123, loss = 0.42223131\n",
      "Iteration 124, loss = 0.42220930\n",
      "Iteration 125, loss = 0.42195431\n",
      "Iteration 126, loss = 0.42187401\n",
      "Iteration 127, loss = 0.42165394\n",
      "Iteration 128, loss = 0.42144519\n",
      "Iteration 129, loss = 0.42152818\n",
      "Iteration 130, loss = 0.42130294\n",
      "Iteration 131, loss = 0.42117360\n",
      "Iteration 132, loss = 0.42096582\n",
      "Iteration 133, loss = 0.42092870\n",
      "Iteration 134, loss = 0.42072375\n",
      "Iteration 135, loss = 0.42061047\n",
      "Iteration 136, loss = 0.42039752\n",
      "Iteration 137, loss = 0.42035758\n",
      "Iteration 138, loss = 0.42011497\n",
      "Iteration 139, loss = 0.42041031\n",
      "Iteration 140, loss = 0.41993115\n",
      "Iteration 141, loss = 0.41995665\n",
      "Iteration 142, loss = 0.41959108\n",
      "Iteration 143, loss = 0.41944916\n",
      "Iteration 144, loss = 0.41934271\n",
      "Iteration 145, loss = 0.41923818\n",
      "Iteration 146, loss = 0.41917154\n",
      "Iteration 147, loss = 0.41895443\n",
      "Iteration 148, loss = 0.41894987\n",
      "Iteration 149, loss = 0.41864035\n",
      "Iteration 150, loss = 0.41839505\n",
      "Iteration 151, loss = 0.41841066\n",
      "Iteration 152, loss = 0.41819950\n",
      "Iteration 153, loss = 0.41795935\n",
      "Iteration 154, loss = 0.41806929\n",
      "Iteration 155, loss = 0.41823070\n",
      "Iteration 156, loss = 0.41764863\n",
      "Iteration 157, loss = 0.41742473\n",
      "Iteration 158, loss = 0.41738362\n",
      "Iteration 159, loss = 0.41742599\n",
      "Iteration 160, loss = 0.41697177\n",
      "Iteration 161, loss = 0.41686098\n",
      "Iteration 162, loss = 0.41685931\n",
      "Iteration 163, loss = 0.41664010\n",
      "Iteration 164, loss = 0.41636606\n",
      "Iteration 165, loss = 0.41629866\n",
      "Iteration 166, loss = 0.41618434\n",
      "Iteration 167, loss = 0.41607119\n",
      "Iteration 168, loss = 0.41579254\n",
      "Iteration 169, loss = 0.41565664\n",
      "Iteration 170, loss = 0.41554847\n",
      "Iteration 171, loss = 0.41549222\n",
      "Iteration 172, loss = 0.41521486\n",
      "Iteration 173, loss = 0.41518626\n",
      "Iteration 174, loss = 0.41496045\n",
      "Iteration 175, loss = 0.41486594\n",
      "Iteration 176, loss = 0.41480175\n",
      "Iteration 177, loss = 0.41449509\n",
      "Iteration 178, loss = 0.41442368\n",
      "Iteration 179, loss = 0.41415290\n",
      "Iteration 180, loss = 0.41406941\n",
      "Iteration 181, loss = 0.41396441\n",
      "Iteration 182, loss = 0.41359575\n",
      "Iteration 183, loss = 0.41347773\n",
      "Iteration 184, loss = 0.41337185\n",
      "Iteration 185, loss = 0.41324141\n",
      "Iteration 186, loss = 0.41301391\n",
      "Iteration 187, loss = 0.41320637\n",
      "Iteration 188, loss = 0.41282153\n",
      "Iteration 189, loss = 0.41262731\n",
      "Iteration 190, loss = 0.41246272\n",
      "Iteration 191, loss = 0.41220585\n",
      "Iteration 192, loss = 0.41215593\n",
      "Iteration 193, loss = 0.41192965\n",
      "Iteration 194, loss = 0.41181982\n",
      "Iteration 195, loss = 0.41179616\n",
      "Iteration 196, loss = 0.41149825\n",
      "Iteration 197, loss = 0.41124335\n",
      "Iteration 198, loss = 0.41112347\n",
      "Iteration 199, loss = 0.41097907\n",
      "Iteration 200, loss = 0.41083255\n",
      "Iteration 1, loss = 0.75871916\n",
      "Iteration 2, loss = 0.71220444\n",
      "Iteration 3, loss = 0.67230378\n",
      "Iteration 4, loss = 0.63870312\n",
      "Iteration 5, loss = 0.60986775\n",
      "Iteration 6, loss = 0.58410315\n",
      "Iteration 7, loss = 0.56351916\n",
      "Iteration 8, loss = 0.54454088\n",
      "Iteration 9, loss = 0.52762883\n",
      "Iteration 10, loss = 0.51281857\n",
      "Iteration 11, loss = 0.50008362\n",
      "Iteration 12, loss = 0.48854588\n",
      "Iteration 13, loss = 0.47867323\n",
      "Iteration 14, loss = 0.46962033\n",
      "Iteration 15, loss = 0.46205966\n",
      "Iteration 16, loss = 0.45579066\n",
      "Iteration 17, loss = 0.45018981\n",
      "Iteration 18, loss = 0.44595421\n",
      "Iteration 19, loss = 0.44175809\n",
      "Iteration 20, loss = 0.43792413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.43529129\n",
      "Iteration 22, loss = 0.43277552\n",
      "Iteration 23, loss = 0.43058135\n",
      "Iteration 24, loss = 0.42897762\n",
      "Iteration 25, loss = 0.42759153\n",
      "Iteration 26, loss = 0.42632114\n",
      "Iteration 27, loss = 0.42516192\n",
      "Iteration 28, loss = 0.42447645\n",
      "Iteration 29, loss = 0.42353914\n",
      "Iteration 30, loss = 0.42298930\n",
      "Iteration 31, loss = 0.42251722\n",
      "Iteration 32, loss = 0.42199436\n",
      "Iteration 33, loss = 0.42184377\n",
      "Iteration 34, loss = 0.42120792\n",
      "Iteration 35, loss = 0.42099297\n",
      "Iteration 36, loss = 0.42057251\n",
      "Iteration 37, loss = 0.42041737\n",
      "Iteration 38, loss = 0.42009218\n",
      "Iteration 39, loss = 0.41974577\n",
      "Iteration 40, loss = 0.41964702\n",
      "Iteration 41, loss = 0.41937021\n",
      "Iteration 42, loss = 0.41921894\n",
      "Iteration 43, loss = 0.41888390\n",
      "Iteration 44, loss = 0.41891743\n",
      "Iteration 45, loss = 0.41856355\n",
      "Iteration 46, loss = 0.41833966\n",
      "Iteration 47, loss = 0.41813831\n",
      "Iteration 48, loss = 0.41806168\n",
      "Iteration 49, loss = 0.41783209\n",
      "Iteration 50, loss = 0.41766982\n",
      "Iteration 51, loss = 0.41753525\n",
      "Iteration 52, loss = 0.41742621\n",
      "Iteration 53, loss = 0.41716371\n",
      "Iteration 54, loss = 0.41697488\n",
      "Iteration 55, loss = 0.41683005\n",
      "Iteration 56, loss = 0.41677054\n",
      "Iteration 57, loss = 0.41666726\n",
      "Iteration 58, loss = 0.41645630\n",
      "Iteration 59, loss = 0.41623324\n",
      "Iteration 60, loss = 0.41612604\n",
      "Iteration 61, loss = 0.41596893\n",
      "Iteration 62, loss = 0.41583576\n",
      "Iteration 63, loss = 0.41566601\n",
      "Iteration 64, loss = 0.41558170\n",
      "Iteration 65, loss = 0.41536863\n",
      "Iteration 66, loss = 0.41534955\n",
      "Iteration 67, loss = 0.41527392\n",
      "Iteration 68, loss = 0.41502168\n",
      "Iteration 69, loss = 0.41480193\n",
      "Iteration 70, loss = 0.41482275\n",
      "Iteration 71, loss = 0.41454955\n",
      "Iteration 72, loss = 0.41443025\n",
      "Iteration 73, loss = 0.41422710\n",
      "Iteration 74, loss = 0.41412781\n",
      "Iteration 75, loss = 0.41393529\n",
      "Iteration 76, loss = 0.41397693\n",
      "Iteration 77, loss = 0.41360383\n",
      "Iteration 78, loss = 0.41347867\n",
      "Iteration 79, loss = 0.41335338\n",
      "Iteration 80, loss = 0.41329934\n",
      "Iteration 81, loss = 0.41313503\n",
      "Iteration 82, loss = 0.41300833\n",
      "Iteration 83, loss = 0.41282461\n",
      "Iteration 84, loss = 0.41268638\n",
      "Iteration 85, loss = 0.41269193\n",
      "Iteration 86, loss = 0.41253140\n",
      "Iteration 87, loss = 0.41245196\n",
      "Iteration 88, loss = 0.41211288\n",
      "Iteration 89, loss = 0.41198906\n",
      "Iteration 90, loss = 0.41196949\n",
      "Iteration 91, loss = 0.41169006\n",
      "Iteration 92, loss = 0.41189056\n",
      "Iteration 93, loss = 0.41146632\n",
      "Iteration 94, loss = 0.41173608\n",
      "Iteration 95, loss = 0.41114493\n",
      "Iteration 96, loss = 0.41094782\n",
      "Iteration 97, loss = 0.41082378\n",
      "Iteration 98, loss = 0.41062397\n",
      "Iteration 99, loss = 0.41053233\n",
      "Iteration 100, loss = 0.41034165\n",
      "Iteration 101, loss = 0.41053227\n",
      "Iteration 102, loss = 0.41012767\n",
      "Iteration 103, loss = 0.40994429\n",
      "Iteration 104, loss = 0.40992619\n",
      "Iteration 105, loss = 0.40964537\n",
      "Iteration 106, loss = 0.40962830\n",
      "Iteration 107, loss = 0.40940552\n",
      "Iteration 108, loss = 0.40923490\n",
      "Iteration 109, loss = 0.40908632\n",
      "Iteration 110, loss = 0.40888415\n",
      "Iteration 111, loss = 0.40878130\n",
      "Iteration 112, loss = 0.40870524\n",
      "Iteration 113, loss = 0.40843378\n",
      "Iteration 114, loss = 0.40836015\n",
      "Iteration 115, loss = 0.40815820\n",
      "Iteration 116, loss = 0.40791033\n",
      "Iteration 117, loss = 0.40777723\n",
      "Iteration 118, loss = 0.40766671\n",
      "Iteration 119, loss = 0.40767471\n",
      "Iteration 120, loss = 0.40750893\n",
      "Iteration 121, loss = 0.40725637\n",
      "Iteration 122, loss = 0.40710541\n",
      "Iteration 123, loss = 0.40685573\n",
      "Iteration 124, loss = 0.40683436\n",
      "Iteration 125, loss = 0.40669789\n",
      "Iteration 126, loss = 0.40642184\n",
      "Iteration 127, loss = 0.40621840\n",
      "Iteration 128, loss = 0.40611686\n",
      "Iteration 129, loss = 0.40601754\n",
      "Iteration 130, loss = 0.40583600\n",
      "Iteration 131, loss = 0.40559581\n",
      "Iteration 132, loss = 0.40550354\n",
      "Iteration 133, loss = 0.40532897\n",
      "Iteration 134, loss = 0.40516672\n",
      "Iteration 135, loss = 0.40488119\n",
      "Iteration 136, loss = 0.40512425\n",
      "Iteration 137, loss = 0.40456173\n",
      "Iteration 138, loss = 0.40444836\n",
      "Iteration 139, loss = 0.40443969\n",
      "Iteration 140, loss = 0.40412629\n",
      "Iteration 141, loss = 0.40397564\n",
      "Iteration 142, loss = 0.40376148\n",
      "Iteration 143, loss = 0.40361245\n",
      "Iteration 144, loss = 0.40354709\n",
      "Iteration 145, loss = 0.40339766\n",
      "Iteration 146, loss = 0.40316591\n",
      "Iteration 147, loss = 0.40300299\n",
      "Iteration 148, loss = 0.40290015\n",
      "Iteration 149, loss = 0.40255267\n",
      "Iteration 150, loss = 0.40237425\n",
      "Iteration 151, loss = 0.40223694\n",
      "Iteration 152, loss = 0.40211268\n",
      "Iteration 153, loss = 0.40245181\n",
      "Iteration 154, loss = 0.40185247\n",
      "Iteration 155, loss = 0.40139942\n",
      "Iteration 156, loss = 0.40143400\n",
      "Iteration 157, loss = 0.40120563\n",
      "Iteration 158, loss = 0.40099847\n",
      "Iteration 159, loss = 0.40082920\n",
      "Iteration 160, loss = 0.40074116\n",
      "Iteration 161, loss = 0.40052448\n",
      "Iteration 162, loss = 0.40039248\n",
      "Iteration 163, loss = 0.40012789\n",
      "Iteration 164, loss = 0.40008768\n",
      "Iteration 165, loss = 0.39983579\n",
      "Iteration 166, loss = 0.39961773\n",
      "Iteration 167, loss = 0.39930519\n",
      "Iteration 168, loss = 0.39919303\n",
      "Iteration 169, loss = 0.39899260\n",
      "Iteration 170, loss = 0.39896758\n",
      "Iteration 171, loss = 0.39865441\n",
      "Iteration 172, loss = 0.39865581\n",
      "Iteration 173, loss = 0.39842470\n",
      "Iteration 174, loss = 0.39821210\n",
      "Iteration 175, loss = 0.39795887\n",
      "Iteration 176, loss = 0.39780072\n",
      "Iteration 177, loss = 0.39748927\n",
      "Iteration 178, loss = 0.39754993\n",
      "Iteration 179, loss = 0.39718552\n",
      "Iteration 180, loss = 0.39695024\n",
      "Iteration 181, loss = 0.39671556\n",
      "Iteration 182, loss = 0.39658017\n",
      "Iteration 183, loss = 0.39628085\n",
      "Iteration 184, loss = 0.39616948\n",
      "Iteration 185, loss = 0.39620367\n",
      "Iteration 186, loss = 0.39588938\n",
      "Iteration 187, loss = 0.39570745\n",
      "Iteration 188, loss = 0.39547860\n",
      "Iteration 189, loss = 0.39530365\n",
      "Iteration 190, loss = 0.39500644\n",
      "Iteration 191, loss = 0.39488474\n",
      "Iteration 192, loss = 0.39468956\n",
      "Iteration 193, loss = 0.39436103\n",
      "Iteration 194, loss = 0.39441376\n",
      "Iteration 195, loss = 0.39419897\n",
      "Iteration 196, loss = 0.39388338\n",
      "Iteration 197, loss = 0.39394784\n",
      "Iteration 198, loss = 0.39354242\n",
      "Iteration 199, loss = 0.39335544\n",
      "Iteration 200, loss = 0.39296425\n",
      "Iteration 1, loss = 0.76108340\n",
      "Iteration 2, loss = 0.71341317\n",
      "Iteration 3, loss = 0.67457061\n",
      "Iteration 4, loss = 0.64067955\n",
      "Iteration 5, loss = 0.61108747\n",
      "Iteration 6, loss = 0.58593284\n",
      "Iteration 7, loss = 0.56461930\n",
      "Iteration 8, loss = 0.54514041\n",
      "Iteration 9, loss = 0.52794703\n",
      "Iteration 10, loss = 0.51275585\n",
      "Iteration 11, loss = 0.49989471\n",
      "Iteration 12, loss = 0.48809354\n",
      "Iteration 13, loss = 0.47800957\n",
      "Iteration 14, loss = 0.46863058\n",
      "Iteration 15, loss = 0.46104126\n",
      "Iteration 16, loss = 0.45432051\n",
      "Iteration 17, loss = 0.44874487\n",
      "Iteration 18, loss = 0.44418479\n",
      "Iteration 19, loss = 0.43944874\n",
      "Iteration 20, loss = 0.43605388\n",
      "Iteration 21, loss = 0.43289031\n",
      "Iteration 22, loss = 0.43029823\n",
      "Iteration 23, loss = 0.42785997\n",
      "Iteration 24, loss = 0.42603965\n",
      "Iteration 25, loss = 0.42458720\n",
      "Iteration 26, loss = 0.42311700\n",
      "Iteration 27, loss = 0.42195112\n",
      "Iteration 28, loss = 0.42112353\n",
      "Iteration 29, loss = 0.42024767\n",
      "Iteration 30, loss = 0.41946169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.41905178\n",
      "Iteration 32, loss = 0.41856831\n",
      "Iteration 33, loss = 0.41830674\n",
      "Iteration 34, loss = 0.41768171\n",
      "Iteration 35, loss = 0.41728209\n",
      "Iteration 36, loss = 0.41715725\n",
      "Iteration 37, loss = 0.41684070\n",
      "Iteration 38, loss = 0.41648770\n",
      "Iteration 39, loss = 0.41627736\n",
      "Iteration 40, loss = 0.41621040\n",
      "Iteration 41, loss = 0.41591322\n",
      "Iteration 42, loss = 0.41577238\n",
      "Iteration 43, loss = 0.41557021\n",
      "Iteration 44, loss = 0.41550153\n",
      "Iteration 45, loss = 0.41520287\n",
      "Iteration 46, loss = 0.41505570\n",
      "Iteration 47, loss = 0.41487316\n",
      "Iteration 48, loss = 0.41477596\n",
      "Iteration 49, loss = 0.41463104\n",
      "Iteration 50, loss = 0.41444628\n",
      "Iteration 51, loss = 0.41433213\n",
      "Iteration 52, loss = 0.41426371\n",
      "Iteration 53, loss = 0.41406012\n",
      "Iteration 54, loss = 0.41390908\n",
      "Iteration 55, loss = 0.41378177\n",
      "Iteration 56, loss = 0.41364632\n",
      "Iteration 57, loss = 0.41357703\n",
      "Iteration 58, loss = 0.41351662\n",
      "Iteration 59, loss = 0.41327143\n",
      "Iteration 60, loss = 0.41311641\n",
      "Iteration 61, loss = 0.41306896\n",
      "Iteration 62, loss = 0.41290443\n",
      "Iteration 63, loss = 0.41286214\n",
      "Iteration 64, loss = 0.41280921\n",
      "Iteration 65, loss = 0.41259931\n",
      "Iteration 66, loss = 0.41248392\n",
      "Iteration 67, loss = 0.41233095\n",
      "Iteration 68, loss = 0.41215564\n",
      "Iteration 69, loss = 0.41203744\n",
      "Iteration 70, loss = 0.41201329\n",
      "Iteration 71, loss = 0.41182077\n",
      "Iteration 72, loss = 0.41168825\n",
      "Iteration 73, loss = 0.41146910\n",
      "Iteration 74, loss = 0.41144921\n",
      "Iteration 75, loss = 0.41128186\n",
      "Iteration 76, loss = 0.41118214\n",
      "Iteration 77, loss = 0.41105122\n",
      "Iteration 78, loss = 0.41088405\n",
      "Iteration 79, loss = 0.41072497\n",
      "Iteration 80, loss = 0.41064331\n",
      "Iteration 81, loss = 0.41050051\n",
      "Iteration 82, loss = 0.41036522\n",
      "Iteration 83, loss = 0.41022061\n",
      "Iteration 84, loss = 0.41013283\n",
      "Iteration 85, loss = 0.41012587\n",
      "Iteration 86, loss = 0.40982554\n",
      "Iteration 87, loss = 0.40992192\n",
      "Iteration 88, loss = 0.40969761\n",
      "Iteration 89, loss = 0.40956368\n",
      "Iteration 90, loss = 0.40951900\n",
      "Iteration 91, loss = 0.40924069\n",
      "Iteration 92, loss = 0.40937277\n",
      "Iteration 93, loss = 0.40905632\n",
      "Iteration 94, loss = 0.40899776\n",
      "Iteration 95, loss = 0.40872478\n",
      "Iteration 96, loss = 0.40861574\n",
      "Iteration 97, loss = 0.40860875\n",
      "Iteration 98, loss = 0.40835493\n",
      "Iteration 99, loss = 0.40829704\n",
      "Iteration 100, loss = 0.40800042\n",
      "Iteration 101, loss = 0.40821594\n",
      "Iteration 102, loss = 0.40795402\n",
      "Iteration 103, loss = 0.40771682\n",
      "Iteration 104, loss = 0.40769450\n",
      "Iteration 105, loss = 0.40740417\n",
      "Iteration 106, loss = 0.40733324\n",
      "Iteration 107, loss = 0.40723994\n",
      "Iteration 108, loss = 0.40719932\n",
      "Iteration 109, loss = 0.40706615\n",
      "Iteration 110, loss = 0.40684639\n",
      "Iteration 111, loss = 0.40659541\n",
      "Iteration 112, loss = 0.40654074\n",
      "Iteration 113, loss = 0.40638060\n",
      "Iteration 114, loss = 0.40629737\n",
      "Iteration 115, loss = 0.40614700\n",
      "Iteration 116, loss = 0.40593484\n",
      "Iteration 117, loss = 0.40580972\n",
      "Iteration 118, loss = 0.40576800\n",
      "Iteration 119, loss = 0.40572652\n",
      "Iteration 120, loss = 0.40549295\n",
      "Iteration 121, loss = 0.40538940\n",
      "Iteration 122, loss = 0.40522913\n",
      "Iteration 123, loss = 0.40497484\n",
      "Iteration 124, loss = 0.40493342\n",
      "Iteration 125, loss = 0.40482438\n",
      "Iteration 126, loss = 0.40464285\n",
      "Iteration 127, loss = 0.40451759\n",
      "Iteration 128, loss = 0.40432075\n",
      "Iteration 129, loss = 0.40420238\n",
      "Iteration 130, loss = 0.40412902\n",
      "Iteration 131, loss = 0.40384523\n",
      "Iteration 132, loss = 0.40379982\n",
      "Iteration 133, loss = 0.40367557\n",
      "Iteration 134, loss = 0.40349849\n",
      "Iteration 135, loss = 0.40330535\n",
      "Iteration 136, loss = 0.40323830\n",
      "Iteration 137, loss = 0.40332361\n",
      "Iteration 138, loss = 0.40301817\n",
      "Iteration 139, loss = 0.40287553\n",
      "Iteration 140, loss = 0.40267224\n",
      "Iteration 141, loss = 0.40253791\n",
      "Iteration 142, loss = 0.40227093\n",
      "Iteration 143, loss = 0.40207355\n",
      "Iteration 144, loss = 0.40208283\n",
      "Iteration 145, loss = 0.40196714\n",
      "Iteration 146, loss = 0.40187782\n",
      "Iteration 147, loss = 0.40154219\n",
      "Iteration 148, loss = 0.40148966\n",
      "Iteration 149, loss = 0.40121415\n",
      "Iteration 150, loss = 0.40106321\n",
      "Iteration 151, loss = 0.40090424\n",
      "Iteration 152, loss = 0.40094854\n",
      "Iteration 153, loss = 0.40104317\n",
      "Iteration 154, loss = 0.40047393\n",
      "Iteration 155, loss = 0.40028158\n",
      "Iteration 156, loss = 0.40021382\n",
      "Iteration 157, loss = 0.39997633\n",
      "Iteration 158, loss = 0.39982417\n",
      "Iteration 159, loss = 0.39965247\n",
      "Iteration 160, loss = 0.39958023\n",
      "Iteration 161, loss = 0.39944840\n",
      "Iteration 162, loss = 0.39931529\n",
      "Iteration 163, loss = 0.39910502\n",
      "Iteration 164, loss = 0.39892722\n",
      "Iteration 165, loss = 0.39868330\n",
      "Iteration 166, loss = 0.39859486\n",
      "Iteration 167, loss = 0.39834659\n",
      "Iteration 168, loss = 0.39836001\n",
      "Iteration 169, loss = 0.39810504\n",
      "Iteration 170, loss = 0.39800385\n",
      "Iteration 171, loss = 0.39768957\n",
      "Iteration 172, loss = 0.39759731\n",
      "Iteration 173, loss = 0.39761745\n",
      "Iteration 174, loss = 0.39729450\n",
      "Iteration 175, loss = 0.39721164\n",
      "Iteration 176, loss = 0.39692385\n",
      "Iteration 177, loss = 0.39676224\n",
      "Iteration 178, loss = 0.39663201\n",
      "Iteration 179, loss = 0.39646483\n",
      "Iteration 180, loss = 0.39617622\n",
      "Iteration 181, loss = 0.39601349\n",
      "Iteration 182, loss = 0.39585596\n",
      "Iteration 183, loss = 0.39561487\n",
      "Iteration 184, loss = 0.39545208\n",
      "Iteration 185, loss = 0.39538973\n",
      "Iteration 186, loss = 0.39527880\n",
      "Iteration 187, loss = 0.39492719\n",
      "Iteration 188, loss = 0.39485251\n",
      "Iteration 189, loss = 0.39469430\n",
      "Iteration 190, loss = 0.39452911\n",
      "Iteration 191, loss = 0.39421035\n",
      "Iteration 192, loss = 0.39427738\n",
      "Iteration 193, loss = 0.39376883\n",
      "Iteration 194, loss = 0.39377967\n",
      "Iteration 195, loss = 0.39374801\n",
      "Iteration 196, loss = 0.39327794\n",
      "Iteration 197, loss = 0.39320453\n",
      "Iteration 198, loss = 0.39305326\n",
      "Iteration 199, loss = 0.39293876\n",
      "Iteration 200, loss = 0.39252677\n",
      "Iteration 1, loss = 0.75388633\n",
      "Iteration 2, loss = 0.70749206\n",
      "Iteration 3, loss = 0.66808270\n",
      "Iteration 4, loss = 0.63528311\n",
      "Iteration 5, loss = 0.60701258\n",
      "Iteration 6, loss = 0.58321035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.56377509\n",
      "Iteration 8, loss = 0.54589088\n",
      "Iteration 9, loss = 0.53072645\n",
      "Iteration 10, loss = 0.51731920\n",
      "Iteration 11, loss = 0.50595180\n",
      "Iteration 12, loss = 0.49624627\n",
      "Iteration 13, loss = 0.48769328\n",
      "Iteration 14, loss = 0.47977143\n",
      "Iteration 15, loss = 0.47370309\n",
      "Iteration 16, loss = 0.46819787\n",
      "Iteration 17, loss = 0.46427774\n",
      "Iteration 18, loss = 0.46024545\n",
      "Iteration 19, loss = 0.45673199\n",
      "Iteration 20, loss = 0.45451833\n",
      "Iteration 21, loss = 0.45235407\n",
      "Iteration 22, loss = 0.45032300\n",
      "Iteration 23, loss = 0.44884784\n",
      "Iteration 24, loss = 0.44766801\n",
      "Iteration 25, loss = 0.44666076\n",
      "Iteration 26, loss = 0.44573534\n",
      "Iteration 27, loss = 0.44504905\n",
      "Iteration 28, loss = 0.44461749\n",
      "Iteration 29, loss = 0.44402130\n",
      "Iteration 30, loss = 0.44353604\n",
      "Iteration 31, loss = 0.44323429\n",
      "Iteration 32, loss = 0.44295971\n",
      "Iteration 33, loss = 0.44272975\n",
      "Iteration 34, loss = 0.44238644\n",
      "Iteration 35, loss = 0.44207753\n",
      "Iteration 36, loss = 0.44199486\n",
      "Iteration 37, loss = 0.44176190\n",
      "Iteration 38, loss = 0.44148852\n",
      "Iteration 39, loss = 0.44127313\n",
      "Iteration 40, loss = 0.44122490\n",
      "Iteration 41, loss = 0.44096978\n",
      "Iteration 42, loss = 0.44082542\n",
      "Iteration 43, loss = 0.44060953\n",
      "Iteration 44, loss = 0.44050086\n",
      "Iteration 45, loss = 0.44028698\n",
      "Iteration 46, loss = 0.44014236\n",
      "Iteration 47, loss = 0.43991540\n",
      "Iteration 48, loss = 0.43979285\n",
      "Iteration 49, loss = 0.43968023\n",
      "Iteration 50, loss = 0.43960208\n",
      "Iteration 51, loss = 0.43935205\n",
      "Iteration 52, loss = 0.43922159\n",
      "Iteration 53, loss = 0.43909921\n",
      "Iteration 54, loss = 0.43884772\n",
      "Iteration 55, loss = 0.43873741\n",
      "Iteration 56, loss = 0.43849299\n",
      "Iteration 57, loss = 0.43838918\n",
      "Iteration 58, loss = 0.43854736\n",
      "Iteration 59, loss = 0.43809078\n",
      "Iteration 60, loss = 0.43792757\n",
      "Iteration 61, loss = 0.43782443\n",
      "Iteration 62, loss = 0.43759925\n",
      "Iteration 63, loss = 0.43758391\n",
      "Iteration 64, loss = 0.43743590\n",
      "Iteration 65, loss = 0.43731215\n",
      "Iteration 66, loss = 0.43711593\n",
      "Iteration 67, loss = 0.43698287\n",
      "Iteration 68, loss = 0.43684729\n",
      "Iteration 69, loss = 0.43667642\n",
      "Iteration 70, loss = 0.43664871\n",
      "Iteration 71, loss = 0.43633241\n",
      "Iteration 72, loss = 0.43622531\n",
      "Iteration 73, loss = 0.43599286\n",
      "Iteration 74, loss = 0.43593325\n",
      "Iteration 75, loss = 0.43566227\n",
      "Iteration 76, loss = 0.43559704\n",
      "Iteration 77, loss = 0.43550850\n",
      "Iteration 78, loss = 0.43540935\n",
      "Iteration 79, loss = 0.43508817\n",
      "Iteration 80, loss = 0.43498324\n",
      "Iteration 81, loss = 0.43483575\n",
      "Iteration 82, loss = 0.43463894\n",
      "Iteration 83, loss = 0.43452464\n",
      "Iteration 84, loss = 0.43442337\n",
      "Iteration 85, loss = 0.43427281\n",
      "Iteration 86, loss = 0.43404408\n",
      "Iteration 87, loss = 0.43437894\n",
      "Iteration 88, loss = 0.43391827\n",
      "Iteration 89, loss = 0.43365227\n",
      "Iteration 90, loss = 0.43345660\n",
      "Iteration 91, loss = 0.43330284\n",
      "Iteration 92, loss = 0.43331455\n",
      "Iteration 93, loss = 0.43298806\n",
      "Iteration 94, loss = 0.43288999\n",
      "Iteration 95, loss = 0.43268480\n",
      "Iteration 96, loss = 0.43258193\n",
      "Iteration 97, loss = 0.43247267\n",
      "Iteration 98, loss = 0.43219126\n",
      "Iteration 99, loss = 0.43207037\n",
      "Iteration 100, loss = 0.43180074\n",
      "Iteration 101, loss = 0.43210346\n",
      "Iteration 102, loss = 0.43166359\n",
      "Iteration 103, loss = 0.43156755\n",
      "Iteration 104, loss = 0.43134268\n",
      "Iteration 105, loss = 0.43116859\n",
      "Iteration 106, loss = 0.43091780\n",
      "Iteration 107, loss = 0.43087945\n",
      "Iteration 108, loss = 0.43078205\n",
      "Iteration 109, loss = 0.43066133\n",
      "Iteration 110, loss = 0.43047258\n",
      "Iteration 111, loss = 0.43009160\n",
      "Iteration 112, loss = 0.43007089\n",
      "Iteration 113, loss = 0.42976479\n",
      "Iteration 114, loss = 0.42961783\n",
      "Iteration 115, loss = 0.42957188\n",
      "Iteration 116, loss = 0.42937008\n",
      "Iteration 117, loss = 0.42916578\n",
      "Iteration 118, loss = 0.42904058\n",
      "Iteration 119, loss = 0.42900193\n",
      "Iteration 120, loss = 0.42890035\n",
      "Iteration 121, loss = 0.42851155\n",
      "Iteration 122, loss = 0.42839176\n",
      "Iteration 123, loss = 0.42821292\n",
      "Iteration 124, loss = 0.42814092\n",
      "Iteration 125, loss = 0.42786854\n",
      "Iteration 126, loss = 0.42794134\n",
      "Iteration 127, loss = 0.42746391\n",
      "Iteration 128, loss = 0.42736059\n",
      "Iteration 129, loss = 0.42710385\n",
      "Iteration 130, loss = 0.42699705\n",
      "Iteration 131, loss = 0.42680862\n",
      "Iteration 132, loss = 0.42663493\n",
      "Iteration 133, loss = 0.42660387\n",
      "Iteration 134, loss = 0.42628320\n",
      "Iteration 135, loss = 0.42618620\n",
      "Iteration 136, loss = 0.42599874\n",
      "Iteration 137, loss = 0.42587389\n",
      "Iteration 138, loss = 0.42544384\n",
      "Iteration 139, loss = 0.42554535\n",
      "Iteration 140, loss = 0.42529328\n",
      "Iteration 141, loss = 0.42511950\n",
      "Iteration 142, loss = 0.42486468\n",
      "Iteration 143, loss = 0.42477743\n",
      "Iteration 144, loss = 0.42460849\n",
      "Iteration 145, loss = 0.42438159\n",
      "Iteration 146, loss = 0.42410723\n",
      "Iteration 147, loss = 0.42404585\n",
      "Iteration 148, loss = 0.42378673\n",
      "Iteration 149, loss = 0.42354494\n",
      "Iteration 150, loss = 0.42344466\n",
      "Iteration 151, loss = 0.42322895\n",
      "Iteration 152, loss = 0.42316702\n",
      "Iteration 153, loss = 0.42321764\n",
      "Iteration 154, loss = 0.42264602\n",
      "Iteration 155, loss = 0.42266998\n",
      "Iteration 156, loss = 0.42226133\n",
      "Iteration 157, loss = 0.42213302\n",
      "Iteration 158, loss = 0.42200521\n",
      "Iteration 159, loss = 0.42173851\n",
      "Iteration 160, loss = 0.42154065\n",
      "Iteration 161, loss = 0.42129683\n",
      "Iteration 162, loss = 0.42108881\n",
      "Iteration 163, loss = 0.42097539\n",
      "Iteration 164, loss = 0.42080189\n",
      "Iteration 165, loss = 0.42053794\n",
      "Iteration 166, loss = 0.42046392\n",
      "Iteration 167, loss = 0.42021605\n",
      "Iteration 168, loss = 0.42002956\n",
      "Iteration 169, loss = 0.41988228\n",
      "Iteration 170, loss = 0.41970214\n",
      "Iteration 171, loss = 0.41931952\n",
      "Iteration 172, loss = 0.41916324\n",
      "Iteration 173, loss = 0.41902658\n",
      "Iteration 174, loss = 0.41890076\n",
      "Iteration 175, loss = 0.41875271\n",
      "Iteration 176, loss = 0.41850189\n",
      "Iteration 177, loss = 0.41820885\n",
      "Iteration 178, loss = 0.41797215\n",
      "Iteration 179, loss = 0.41786933\n",
      "Iteration 180, loss = 0.41762314\n",
      "Iteration 181, loss = 0.41735449\n",
      "Iteration 182, loss = 0.41721046\n",
      "Iteration 183, loss = 0.41706827\n",
      "Iteration 184, loss = 0.41695937\n",
      "Iteration 185, loss = 0.41681137\n",
      "Iteration 186, loss = 0.41651337\n",
      "Iteration 187, loss = 0.41621537\n",
      "Iteration 188, loss = 0.41598000\n",
      "Iteration 189, loss = 0.41579710\n",
      "Iteration 190, loss = 0.41553623\n",
      "Iteration 191, loss = 0.41538570\n",
      "Iteration 192, loss = 0.41533880\n",
      "Iteration 193, loss = 0.41488422\n",
      "Iteration 194, loss = 0.41482406\n",
      "Iteration 195, loss = 0.41476370\n",
      "Iteration 196, loss = 0.41430657\n",
      "Iteration 197, loss = 0.41439803\n",
      "Iteration 198, loss = 0.41410582\n",
      "Iteration 199, loss = 0.41397873\n",
      "Iteration 200, loss = 0.41379253\n",
      "Iteration 1, loss = 0.75059882\n",
      "Iteration 2, loss = 0.70185336\n",
      "Iteration 3, loss = 0.65716935\n",
      "Iteration 4, loss = 0.62134349\n",
      "Iteration 5, loss = 0.59154412\n",
      "Iteration 6, loss = 0.56580888\n",
      "Iteration 7, loss = 0.54352641\n",
      "Iteration 8, loss = 0.52573882\n",
      "Iteration 9, loss = 0.50906111\n",
      "Iteration 10, loss = 0.49547423\n",
      "Iteration 11, loss = 0.48334743\n",
      "Iteration 12, loss = 0.47312484\n",
      "Iteration 13, loss = 0.46301579\n",
      "Iteration 14, loss = 0.45514618\n",
      "Iteration 15, loss = 0.44805645\n",
      "Iteration 16, loss = 0.44179239\n",
      "Iteration 17, loss = 0.43627220\n",
      "Iteration 18, loss = 0.43242312\n",
      "Iteration 19, loss = 0.42835825\n",
      "Iteration 20, loss = 0.42506724\n",
      "Iteration 21, loss = 0.42249417\n",
      "Iteration 22, loss = 0.41994577\n",
      "Iteration 23, loss = 0.41838065\n",
      "Iteration 24, loss = 0.41671992\n",
      "Iteration 25, loss = 0.41561368\n",
      "Iteration 26, loss = 0.41455339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.41358739\n",
      "Iteration 28, loss = 0.41290530\n",
      "Iteration 29, loss = 0.41228221\n",
      "Iteration 30, loss = 0.41173714\n",
      "Iteration 31, loss = 0.41129039\n",
      "Iteration 32, loss = 0.41107065\n",
      "Iteration 33, loss = 0.41070955\n",
      "Iteration 34, loss = 0.41036943\n",
      "Iteration 35, loss = 0.41015844\n",
      "Iteration 36, loss = 0.40996685\n",
      "Iteration 37, loss = 0.40972025\n",
      "Iteration 38, loss = 0.40954090\n",
      "Iteration 39, loss = 0.40936402\n",
      "Iteration 40, loss = 0.40924512\n",
      "Iteration 41, loss = 0.40909611\n",
      "Iteration 42, loss = 0.40900932\n",
      "Iteration 43, loss = 0.40890225\n",
      "Iteration 44, loss = 0.40873070\n",
      "Iteration 45, loss = 0.40857681\n",
      "Iteration 46, loss = 0.40849157\n",
      "Iteration 47, loss = 0.40827772\n",
      "Iteration 48, loss = 0.40813939\n",
      "Iteration 49, loss = 0.40800058\n",
      "Iteration 50, loss = 0.40786834\n",
      "Iteration 51, loss = 0.40780852\n",
      "Iteration 52, loss = 0.40767358\n",
      "Iteration 53, loss = 0.40745704\n",
      "Iteration 54, loss = 0.40741879\n",
      "Iteration 55, loss = 0.40725777\n",
      "Iteration 56, loss = 0.40713857\n",
      "Iteration 57, loss = 0.40709515\n",
      "Iteration 58, loss = 0.40701253\n",
      "Iteration 59, loss = 0.40670417\n",
      "Iteration 60, loss = 0.40658414\n",
      "Iteration 61, loss = 0.40667145\n",
      "Iteration 62, loss = 0.40671006\n",
      "Iteration 63, loss = 0.40640868\n",
      "Iteration 64, loss = 0.40621627\n",
      "Iteration 65, loss = 0.40593647\n",
      "Iteration 66, loss = 0.40590738\n",
      "Iteration 67, loss = 0.40569098\n",
      "Iteration 68, loss = 0.40573825\n",
      "Iteration 69, loss = 0.40545939\n",
      "Iteration 70, loss = 0.40540022\n",
      "Iteration 71, loss = 0.40522456\n",
      "Iteration 72, loss = 0.40513561\n",
      "Iteration 73, loss = 0.40496209\n",
      "Iteration 74, loss = 0.40485506\n",
      "Iteration 75, loss = 0.40473799\n",
      "Iteration 76, loss = 0.40467452\n",
      "Iteration 77, loss = 0.40446178\n",
      "Iteration 78, loss = 0.40444110\n",
      "Iteration 79, loss = 0.40417251\n",
      "Iteration 80, loss = 0.40411431\n",
      "Iteration 81, loss = 0.40401185\n",
      "Iteration 82, loss = 0.40382950\n",
      "Iteration 83, loss = 0.40385110\n",
      "Iteration 84, loss = 0.40363458\n",
      "Iteration 85, loss = 0.40344860\n",
      "Iteration 86, loss = 0.40349231\n",
      "Iteration 87, loss = 0.40320377\n",
      "Iteration 88, loss = 0.40321318\n",
      "Iteration 89, loss = 0.40292275\n",
      "Iteration 90, loss = 0.40284158\n",
      "Iteration 91, loss = 0.40279261\n",
      "Iteration 92, loss = 0.40261731\n",
      "Iteration 93, loss = 0.40242067\n",
      "Iteration 94, loss = 0.40236966\n",
      "Iteration 95, loss = 0.40211649\n",
      "Iteration 96, loss = 0.40231849\n",
      "Iteration 97, loss = 0.40188459\n",
      "Iteration 98, loss = 0.40179414\n",
      "Iteration 99, loss = 0.40162675\n",
      "Iteration 100, loss = 0.40164096\n",
      "Iteration 101, loss = 0.40137503\n",
      "Iteration 102, loss = 0.40127307\n",
      "Iteration 103, loss = 0.40105153\n",
      "Iteration 104, loss = 0.40101934\n",
      "Iteration 105, loss = 0.40079751\n",
      "Iteration 106, loss = 0.40069263\n",
      "Iteration 107, loss = 0.40057961\n",
      "Iteration 108, loss = 0.40059618\n",
      "Iteration 109, loss = 0.40028007\n",
      "Iteration 110, loss = 0.40023516\n",
      "Iteration 111, loss = 0.40003327\n",
      "Iteration 112, loss = 0.39980762\n",
      "Iteration 113, loss = 0.39978549\n",
      "Iteration 114, loss = 0.39952728\n",
      "Iteration 115, loss = 0.39944652\n",
      "Iteration 116, loss = 0.39932019\n",
      "Iteration 117, loss = 0.39910597\n",
      "Iteration 118, loss = 0.39904583\n",
      "Iteration 119, loss = 0.39888179\n",
      "Iteration 120, loss = 0.39870887\n",
      "Iteration 121, loss = 0.39868819\n",
      "Iteration 122, loss = 0.39841816\n",
      "Iteration 123, loss = 0.39838528\n",
      "Iteration 124, loss = 0.39834342\n",
      "Iteration 125, loss = 0.39813186\n",
      "Iteration 126, loss = 0.39786993\n",
      "Iteration 127, loss = 0.39772271\n",
      "Iteration 128, loss = 0.39756997\n",
      "Iteration 129, loss = 0.39740998\n",
      "Iteration 130, loss = 0.39725857\n",
      "Iteration 131, loss = 0.39726220\n",
      "Iteration 132, loss = 0.39694736\n",
      "Iteration 133, loss = 0.39683494\n",
      "Iteration 134, loss = 0.39669379\n",
      "Iteration 135, loss = 0.39648843\n",
      "Iteration 136, loss = 0.39632109\n",
      "Iteration 137, loss = 0.39631614\n",
      "Iteration 138, loss = 0.39602427\n",
      "Iteration 139, loss = 0.39606560\n",
      "Iteration 140, loss = 0.39590184\n",
      "Iteration 141, loss = 0.39563574\n",
      "Iteration 142, loss = 0.39543970\n",
      "Iteration 143, loss = 0.39534643\n",
      "Iteration 144, loss = 0.39512828\n",
      "Iteration 145, loss = 0.39525794\n",
      "Iteration 146, loss = 0.39496593\n",
      "Iteration 147, loss = 0.39469309\n",
      "Iteration 148, loss = 0.39468728\n",
      "Iteration 149, loss = 0.39444000\n",
      "Iteration 150, loss = 0.39422030\n",
      "Iteration 151, loss = 0.39419869\n",
      "Iteration 152, loss = 0.39390423\n",
      "Iteration 153, loss = 0.39377191\n",
      "Iteration 154, loss = 0.39370743\n",
      "Iteration 155, loss = 0.39388269\n",
      "Iteration 156, loss = 0.39321599\n",
      "Iteration 157, loss = 0.39307193\n",
      "Iteration 158, loss = 0.39298066\n",
      "Iteration 159, loss = 0.39277453\n",
      "Iteration 160, loss = 0.39265382\n",
      "Iteration 161, loss = 0.39240792\n",
      "Iteration 162, loss = 0.39241232\n",
      "Iteration 163, loss = 0.39214911\n",
      "Iteration 164, loss = 0.39195712\n",
      "Iteration 165, loss = 0.39192020\n",
      "Iteration 166, loss = 0.39169409\n",
      "Iteration 167, loss = 0.39154018\n",
      "Iteration 168, loss = 0.39130235\n",
      "Iteration 169, loss = 0.39126706\n",
      "Iteration 170, loss = 0.39094379\n",
      "Iteration 171, loss = 0.39073761\n",
      "Iteration 172, loss = 0.39071065\n",
      "Iteration 173, loss = 0.39058784\n",
      "Iteration 174, loss = 0.39028618\n",
      "Iteration 175, loss = 0.39004224\n",
      "Iteration 176, loss = 0.39033649\n",
      "Iteration 177, loss = 0.38991033\n",
      "Iteration 178, loss = 0.38968514\n",
      "Iteration 179, loss = 0.38955451\n",
      "Iteration 180, loss = 0.38926442\n",
      "Iteration 181, loss = 0.38927470\n",
      "Iteration 182, loss = 0.38894793\n",
      "Iteration 183, loss = 0.38889467\n",
      "Iteration 184, loss = 0.38864991\n",
      "Iteration 185, loss = 0.38856590\n",
      "Iteration 186, loss = 0.38828804\n",
      "Iteration 187, loss = 0.38827862\n",
      "Iteration 188, loss = 0.38791542\n",
      "Iteration 189, loss = 0.38773591\n",
      "Iteration 190, loss = 0.38771040\n",
      "Iteration 191, loss = 0.38731295\n",
      "Iteration 192, loss = 0.38728685\n",
      "Iteration 193, loss = 0.38707231\n",
      "Iteration 194, loss = 0.38703658\n",
      "Iteration 195, loss = 0.38689644\n",
      "Iteration 196, loss = 0.38657344\n",
      "Iteration 197, loss = 0.38624512\n",
      "Iteration 198, loss = 0.38622327\n",
      "Iteration 199, loss = 0.38600946\n",
      "Iteration 200, loss = 0.38587031\n",
      "Iteration 1, loss = 0.75773311\n",
      "Iteration 2, loss = 0.71111868\n",
      "Iteration 3, loss = 0.66849798\n",
      "Iteration 4, loss = 0.63485861\n",
      "Iteration 5, loss = 0.60621354\n",
      "Iteration 6, loss = 0.58166065\n",
      "Iteration 7, loss = 0.55981063\n",
      "Iteration 8, loss = 0.54305966\n",
      "Iteration 9, loss = 0.52715629\n",
      "Iteration 10, loss = 0.51420251\n",
      "Iteration 11, loss = 0.50243482\n",
      "Iteration 12, loss = 0.49263085\n",
      "Iteration 13, loss = 0.48322343\n",
      "Iteration 14, loss = 0.47586439\n",
      "Iteration 15, loss = 0.46905042\n",
      "Iteration 16, loss = 0.46330273\n",
      "Iteration 17, loss = 0.45827046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.45457718\n",
      "Iteration 19, loss = 0.45087052\n",
      "Iteration 20, loss = 0.44770756\n",
      "Iteration 21, loss = 0.44522973\n",
      "Iteration 22, loss = 0.44300445\n",
      "Iteration 23, loss = 0.44168143\n",
      "Iteration 24, loss = 0.44007716\n",
      "Iteration 25, loss = 0.43910560\n",
      "Iteration 26, loss = 0.43794924\n",
      "Iteration 27, loss = 0.43725169\n",
      "Iteration 28, loss = 0.43660053\n",
      "Iteration 29, loss = 0.43582948\n",
      "Iteration 30, loss = 0.43547372\n",
      "Iteration 31, loss = 0.43489083\n",
      "Iteration 32, loss = 0.43472836\n",
      "Iteration 33, loss = 0.43432479\n",
      "Iteration 34, loss = 0.43395304\n",
      "Iteration 35, loss = 0.43368391\n",
      "Iteration 36, loss = 0.43353503\n",
      "Iteration 37, loss = 0.43322758\n",
      "Iteration 38, loss = 0.43304513\n",
      "Iteration 39, loss = 0.43285767\n",
      "Iteration 40, loss = 0.43279087\n",
      "Iteration 41, loss = 0.43249315\n",
      "Iteration 42, loss = 0.43241548\n",
      "Iteration 43, loss = 0.43227805\n",
      "Iteration 44, loss = 0.43215291\n",
      "Iteration 45, loss = 0.43192415\n",
      "Iteration 46, loss = 0.43180429\n",
      "Iteration 47, loss = 0.43163927\n",
      "Iteration 48, loss = 0.43155216\n",
      "Iteration 49, loss = 0.43144369\n",
      "Iteration 50, loss = 0.43132329\n",
      "Iteration 51, loss = 0.43118332\n",
      "Iteration 52, loss = 0.43100494\n",
      "Iteration 53, loss = 0.43090156\n",
      "Iteration 54, loss = 0.43080553\n",
      "Iteration 55, loss = 0.43064118\n",
      "Iteration 56, loss = 0.43055027\n",
      "Iteration 57, loss = 0.43041159\n",
      "Iteration 58, loss = 0.43030294\n",
      "Iteration 59, loss = 0.43015071\n",
      "Iteration 60, loss = 0.43007904\n",
      "Iteration 61, loss = 0.43002834\n",
      "Iteration 62, loss = 0.42984823\n",
      "Iteration 63, loss = 0.42967068\n",
      "Iteration 64, loss = 0.42963222\n",
      "Iteration 65, loss = 0.42941439\n",
      "Iteration 66, loss = 0.42938349\n",
      "Iteration 67, loss = 0.42913895\n",
      "Iteration 68, loss = 0.42920374\n",
      "Iteration 69, loss = 0.42905046\n",
      "Iteration 70, loss = 0.42886675\n",
      "Iteration 71, loss = 0.42871075\n",
      "Iteration 72, loss = 0.42853002\n",
      "Iteration 73, loss = 0.42854260\n",
      "Iteration 74, loss = 0.42840931\n",
      "Iteration 75, loss = 0.42822215\n",
      "Iteration 76, loss = 0.42813723\n",
      "Iteration 77, loss = 0.42796353\n",
      "Iteration 78, loss = 0.42785723\n",
      "Iteration 79, loss = 0.42767717\n",
      "Iteration 80, loss = 0.42756956\n",
      "Iteration 81, loss = 0.42767186\n",
      "Iteration 82, loss = 0.42732695\n",
      "Iteration 83, loss = 0.42749927\n",
      "Iteration 84, loss = 0.42714214\n",
      "Iteration 85, loss = 0.42691956\n",
      "Iteration 86, loss = 0.42682375\n",
      "Iteration 87, loss = 0.42674287\n",
      "Iteration 88, loss = 0.42665768\n",
      "Iteration 89, loss = 0.42646621\n",
      "Iteration 90, loss = 0.42636117\n",
      "Iteration 91, loss = 0.42633748\n",
      "Iteration 92, loss = 0.42609368\n",
      "Iteration 93, loss = 0.42592447\n",
      "Iteration 94, loss = 0.42615968\n",
      "Iteration 95, loss = 0.42578406\n",
      "Iteration 96, loss = 0.42566170\n",
      "Iteration 97, loss = 0.42552992\n",
      "Iteration 98, loss = 0.42556482\n",
      "Iteration 99, loss = 0.42525837\n",
      "Iteration 100, loss = 0.42511661\n",
      "Iteration 101, loss = 0.42505930\n",
      "Iteration 102, loss = 0.42490192\n",
      "Iteration 103, loss = 0.42480243\n",
      "Iteration 104, loss = 0.42464439\n",
      "Iteration 105, loss = 0.42467443\n",
      "Iteration 106, loss = 0.42435917\n",
      "Iteration 107, loss = 0.42419747\n",
      "Iteration 108, loss = 0.42411584\n",
      "Iteration 109, loss = 0.42398931\n",
      "Iteration 110, loss = 0.42387911\n",
      "Iteration 111, loss = 0.42378321\n",
      "Iteration 112, loss = 0.42351883\n",
      "Iteration 113, loss = 0.42353030\n",
      "Iteration 114, loss = 0.42328508\n",
      "Iteration 115, loss = 0.42320580\n",
      "Iteration 116, loss = 0.42323028\n",
      "Iteration 117, loss = 0.42294807\n",
      "Iteration 118, loss = 0.42284444\n",
      "Iteration 119, loss = 0.42274531\n",
      "Iteration 120, loss = 0.42251179\n",
      "Iteration 121, loss = 0.42253068\n",
      "Iteration 122, loss = 0.42231694\n",
      "Iteration 123, loss = 0.42215762\n",
      "Iteration 124, loss = 0.42213529\n",
      "Iteration 125, loss = 0.42187993\n",
      "Iteration 126, loss = 0.42179932\n",
      "Iteration 127, loss = 0.42157889\n",
      "Iteration 128, loss = 0.42136979\n",
      "Iteration 129, loss = 0.42145251\n",
      "Iteration 130, loss = 0.42122691\n",
      "Iteration 131, loss = 0.42109718\n",
      "Iteration 132, loss = 0.42088908\n",
      "Iteration 133, loss = 0.42085156\n",
      "Iteration 134, loss = 0.42064624\n",
      "Iteration 135, loss = 0.42053261\n",
      "Iteration 136, loss = 0.42031929\n",
      "Iteration 137, loss = 0.42027900\n",
      "Iteration 138, loss = 0.42003598\n",
      "Iteration 139, loss = 0.42033103\n",
      "Iteration 140, loss = 0.41985143\n",
      "Iteration 141, loss = 0.41987656\n",
      "Iteration 142, loss = 0.41951057\n",
      "Iteration 143, loss = 0.41936831\n",
      "Iteration 144, loss = 0.41926141\n",
      "Iteration 145, loss = 0.41915652\n",
      "Iteration 146, loss = 0.41908944\n",
      "Iteration 147, loss = 0.41887196\n",
      "Iteration 148, loss = 0.41886697\n",
      "Iteration 149, loss = 0.41855709\n",
      "Iteration 150, loss = 0.41831136\n",
      "Iteration 151, loss = 0.41832659\n",
      "Iteration 152, loss = 0.41811503\n",
      "Iteration 153, loss = 0.41787440\n",
      "Iteration 154, loss = 0.41798390\n",
      "Iteration 155, loss = 0.41814494\n",
      "Iteration 156, loss = 0.41756239\n",
      "Iteration 157, loss = 0.41733810\n",
      "Iteration 158, loss = 0.41729654\n",
      "Iteration 159, loss = 0.41733854\n",
      "Iteration 160, loss = 0.41688380\n",
      "Iteration 161, loss = 0.41677256\n",
      "Iteration 162, loss = 0.41677047\n",
      "Iteration 163, loss = 0.41655084\n",
      "Iteration 164, loss = 0.41627631\n",
      "Iteration 165, loss = 0.41620852\n",
      "Iteration 166, loss = 0.41609374\n",
      "Iteration 167, loss = 0.41598005\n",
      "Iteration 168, loss = 0.41570093\n",
      "Iteration 169, loss = 0.41556460\n",
      "Iteration 170, loss = 0.41545598\n",
      "Iteration 171, loss = 0.41539921\n",
      "Iteration 172, loss = 0.41512143\n",
      "Iteration 173, loss = 0.41509235\n",
      "Iteration 174, loss = 0.41486609\n",
      "Iteration 175, loss = 0.41477105\n",
      "Iteration 176, loss = 0.41470646\n",
      "Iteration 177, loss = 0.41439919\n",
      "Iteration 178, loss = 0.41432736\n",
      "Iteration 179, loss = 0.41405608\n",
      "Iteration 180, loss = 0.41397211\n",
      "Iteration 181, loss = 0.41386674\n",
      "Iteration 182, loss = 0.41349744\n",
      "Iteration 183, loss = 0.41337897\n",
      "Iteration 184, loss = 0.41327256\n",
      "Iteration 185, loss = 0.41314157\n",
      "Iteration 186, loss = 0.41291362\n",
      "Iteration 187, loss = 0.41310576\n",
      "Iteration 188, loss = 0.41272026\n",
      "Iteration 189, loss = 0.41252546\n",
      "Iteration 190, loss = 0.41236039\n",
      "Iteration 191, loss = 0.41210300\n",
      "Iteration 192, loss = 0.41205261\n",
      "Iteration 193, loss = 0.41182578\n",
      "Iteration 194, loss = 0.41171543\n",
      "Iteration 195, loss = 0.41169119\n",
      "Iteration 196, loss = 0.41139283\n",
      "Iteration 197, loss = 0.41113737\n",
      "Iteration 198, loss = 0.41101700\n",
      "Iteration 199, loss = 0.41087205\n",
      "Iteration 200, loss = 0.41072496\n",
      "Iteration 1, loss = 0.75866550\n",
      "Iteration 2, loss = 0.71215075\n",
      "Iteration 3, loss = 0.67225003\n",
      "Iteration 4, loss = 0.63864933\n",
      "Iteration 5, loss = 0.60981390\n",
      "Iteration 6, loss = 0.58404922\n",
      "Iteration 7, loss = 0.56346513\n",
      "Iteration 8, loss = 0.54448675\n",
      "Iteration 9, loss = 0.52757460\n",
      "Iteration 10, loss = 0.51276425\n",
      "Iteration 11, loss = 0.50002920\n",
      "Iteration 12, loss = 0.48849135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.47861860\n",
      "Iteration 14, loss = 0.46956556\n",
      "Iteration 15, loss = 0.46200475\n",
      "Iteration 16, loss = 0.45573561\n",
      "Iteration 17, loss = 0.45013461\n",
      "Iteration 18, loss = 0.44589886\n",
      "Iteration 19, loss = 0.44170258\n",
      "Iteration 20, loss = 0.43786847\n",
      "Iteration 21, loss = 0.43523547\n",
      "Iteration 22, loss = 0.43271957\n",
      "Iteration 23, loss = 0.43052526\n",
      "Iteration 24, loss = 0.42892139\n",
      "Iteration 25, loss = 0.42753517\n",
      "Iteration 26, loss = 0.42626465\n",
      "Iteration 27, loss = 0.42510531\n",
      "Iteration 28, loss = 0.42441971\n",
      "Iteration 29, loss = 0.42348229\n",
      "Iteration 30, loss = 0.42293235\n",
      "Iteration 31, loss = 0.42246017\n",
      "Iteration 32, loss = 0.42193722\n",
      "Iteration 33, loss = 0.42178654\n",
      "Iteration 34, loss = 0.42115060\n",
      "Iteration 35, loss = 0.42093556\n",
      "Iteration 36, loss = 0.42051502\n",
      "Iteration 37, loss = 0.42035980\n",
      "Iteration 38, loss = 0.42003452\n",
      "Iteration 39, loss = 0.41968803\n",
      "Iteration 40, loss = 0.41958919\n",
      "Iteration 41, loss = 0.41931230\n",
      "Iteration 42, loss = 0.41916094\n",
      "Iteration 43, loss = 0.41882581\n",
      "Iteration 44, loss = 0.41885927\n",
      "Iteration 45, loss = 0.41850529\n",
      "Iteration 46, loss = 0.41828131\n",
      "Iteration 47, loss = 0.41807987\n",
      "Iteration 48, loss = 0.41800313\n",
      "Iteration 49, loss = 0.41777344\n",
      "Iteration 50, loss = 0.41761106\n",
      "Iteration 51, loss = 0.41747639\n",
      "Iteration 52, loss = 0.41736725\n",
      "Iteration 53, loss = 0.41710463\n",
      "Iteration 54, loss = 0.41691570\n",
      "Iteration 55, loss = 0.41677075\n",
      "Iteration 56, loss = 0.41671112\n",
      "Iteration 57, loss = 0.41660771\n",
      "Iteration 58, loss = 0.41639661\n",
      "Iteration 59, loss = 0.41617343\n",
      "Iteration 60, loss = 0.41606611\n",
      "Iteration 61, loss = 0.41590887\n",
      "Iteration 62, loss = 0.41577556\n",
      "Iteration 63, loss = 0.41560566\n",
      "Iteration 64, loss = 0.41552122\n",
      "Iteration 65, loss = 0.41530800\n",
      "Iteration 66, loss = 0.41528876\n",
      "Iteration 67, loss = 0.41521300\n",
      "Iteration 68, loss = 0.41496059\n",
      "Iteration 69, loss = 0.41474067\n",
      "Iteration 70, loss = 0.41476135\n",
      "Iteration 71, loss = 0.41448797\n",
      "Iteration 72, loss = 0.41436851\n",
      "Iteration 73, loss = 0.41416515\n",
      "Iteration 74, loss = 0.41406571\n",
      "Iteration 75, loss = 0.41387301\n",
      "Iteration 76, loss = 0.41391449\n",
      "Iteration 77, loss = 0.41354118\n",
      "Iteration 78, loss = 0.41341584\n",
      "Iteration 79, loss = 0.41329034\n",
      "Iteration 80, loss = 0.41323613\n",
      "Iteration 81, loss = 0.41307162\n",
      "Iteration 82, loss = 0.41294470\n",
      "Iteration 83, loss = 0.41276079\n",
      "Iteration 84, loss = 0.41262236\n",
      "Iteration 85, loss = 0.41262769\n",
      "Iteration 86, loss = 0.41246693\n",
      "Iteration 87, loss = 0.41238731\n",
      "Iteration 88, loss = 0.41204798\n",
      "Iteration 89, loss = 0.41192391\n",
      "Iteration 90, loss = 0.41190413\n",
      "Iteration 91, loss = 0.41162447\n",
      "Iteration 92, loss = 0.41182469\n",
      "Iteration 93, loss = 0.41140024\n",
      "Iteration 94, loss = 0.41166981\n",
      "Iteration 95, loss = 0.41107837\n",
      "Iteration 96, loss = 0.41088102\n",
      "Iteration 97, loss = 0.41075670\n",
      "Iteration 98, loss = 0.41055664\n",
      "Iteration 99, loss = 0.41046474\n",
      "Iteration 100, loss = 0.41027380\n",
      "Iteration 101, loss = 0.41046419\n",
      "Iteration 102, loss = 0.41005929\n",
      "Iteration 103, loss = 0.40987564\n",
      "Iteration 104, loss = 0.40985720\n",
      "Iteration 105, loss = 0.40957615\n",
      "Iteration 106, loss = 0.40955880\n",
      "Iteration 107, loss = 0.40933573\n",
      "Iteration 108, loss = 0.40916482\n",
      "Iteration 109, loss = 0.40901594\n",
      "Iteration 110, loss = 0.40881346\n",
      "Iteration 111, loss = 0.40871029\n",
      "Iteration 112, loss = 0.40863391\n",
      "Iteration 113, loss = 0.40836218\n",
      "Iteration 114, loss = 0.40828822\n",
      "Iteration 115, loss = 0.40808597\n",
      "Iteration 116, loss = 0.40783777\n",
      "Iteration 117, loss = 0.40770434\n",
      "Iteration 118, loss = 0.40759348\n",
      "Iteration 119, loss = 0.40760114\n",
      "Iteration 120, loss = 0.40743505\n",
      "Iteration 121, loss = 0.40718214\n",
      "Iteration 122, loss = 0.40703084\n",
      "Iteration 123, loss = 0.40678078\n",
      "Iteration 124, loss = 0.40675907\n",
      "Iteration 125, loss = 0.40662223\n",
      "Iteration 126, loss = 0.40634582\n",
      "Iteration 127, loss = 0.40614200\n",
      "Iteration 128, loss = 0.40604010\n",
      "Iteration 129, loss = 0.40594040\n",
      "Iteration 130, loss = 0.40575849\n",
      "Iteration 131, loss = 0.40551790\n",
      "Iteration 132, loss = 0.40542525\n",
      "Iteration 133, loss = 0.40525029\n",
      "Iteration 134, loss = 0.40508768\n",
      "Iteration 135, loss = 0.40480170\n",
      "Iteration 136, loss = 0.40504441\n",
      "Iteration 137, loss = 0.40448144\n",
      "Iteration 138, loss = 0.40436769\n",
      "Iteration 139, loss = 0.40435860\n",
      "Iteration 140, loss = 0.40404477\n",
      "Iteration 141, loss = 0.40389373\n",
      "Iteration 142, loss = 0.40367911\n",
      "Iteration 143, loss = 0.40352963\n",
      "Iteration 144, loss = 0.40346391\n",
      "Iteration 145, loss = 0.40331404\n",
      "Iteration 146, loss = 0.40308184\n",
      "Iteration 147, loss = 0.40291848\n",
      "Iteration 148, loss = 0.40281518\n",
      "Iteration 149, loss = 0.40246726\n",
      "Iteration 150, loss = 0.40228842\n",
      "Iteration 151, loss = 0.40215069\n",
      "Iteration 152, loss = 0.40202599\n",
      "Iteration 153, loss = 0.40236467\n",
      "Iteration 154, loss = 0.40176482\n",
      "Iteration 155, loss = 0.40131131\n",
      "Iteration 156, loss = 0.40134543\n",
      "Iteration 157, loss = 0.40111660\n",
      "Iteration 158, loss = 0.40090897\n",
      "Iteration 159, loss = 0.40073924\n",
      "Iteration 160, loss = 0.40065069\n",
      "Iteration 161, loss = 0.40043357\n",
      "Iteration 162, loss = 0.40030111\n",
      "Iteration 163, loss = 0.40003598\n",
      "Iteration 164, loss = 0.39999519\n",
      "Iteration 165, loss = 0.39974283\n",
      "Iteration 166, loss = 0.39952440\n",
      "Iteration 167, loss = 0.39921130\n",
      "Iteration 168, loss = 0.39909867\n",
      "Iteration 169, loss = 0.39889773\n",
      "Iteration 170, loss = 0.39887224\n",
      "Iteration 171, loss = 0.39855846\n",
      "Iteration 172, loss = 0.39855944\n",
      "Iteration 173, loss = 0.39832772\n",
      "Iteration 174, loss = 0.39811458\n",
      "Iteration 175, loss = 0.39786087\n",
      "Iteration 176, loss = 0.39770219\n",
      "Iteration 177, loss = 0.39739019\n",
      "Iteration 178, loss = 0.39745035\n",
      "Iteration 179, loss = 0.39708537\n",
      "Iteration 180, loss = 0.39684959\n",
      "Iteration 181, loss = 0.39661441\n",
      "Iteration 182, loss = 0.39647841\n",
      "Iteration 183, loss = 0.39617859\n",
      "Iteration 184, loss = 0.39606664\n",
      "Iteration 185, loss = 0.39610030\n",
      "Iteration 186, loss = 0.39578556\n",
      "Iteration 187, loss = 0.39560301\n",
      "Iteration 188, loss = 0.39537361\n",
      "Iteration 189, loss = 0.39519818\n",
      "Iteration 190, loss = 0.39490031\n",
      "Iteration 191, loss = 0.39477811\n",
      "Iteration 192, loss = 0.39458245\n",
      "Iteration 193, loss = 0.39425324\n",
      "Iteration 194, loss = 0.39430555\n",
      "Iteration 195, loss = 0.39409019\n",
      "Iteration 196, loss = 0.39377391\n",
      "Iteration 197, loss = 0.39383786\n",
      "Iteration 198, loss = 0.39343179\n",
      "Iteration 199, loss = 0.39324421\n",
      "Iteration 200, loss = 0.39285246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76102973\n",
      "Iteration 2, loss = 0.71335948\n",
      "Iteration 3, loss = 0.67451689\n",
      "Iteration 4, loss = 0.64062580\n",
      "Iteration 5, loss = 0.61103367\n",
      "Iteration 6, loss = 0.58587898\n",
      "Iteration 7, loss = 0.56456537\n",
      "Iteration 8, loss = 0.54508638\n",
      "Iteration 9, loss = 0.52789291\n",
      "Iteration 10, loss = 0.51270164\n",
      "Iteration 11, loss = 0.49984041\n",
      "Iteration 12, loss = 0.48803914\n",
      "Iteration 13, loss = 0.47795509\n",
      "Iteration 14, loss = 0.46857597\n",
      "Iteration 15, loss = 0.46098654\n",
      "Iteration 16, loss = 0.45426567\n",
      "Iteration 17, loss = 0.44868992\n",
      "Iteration 18, loss = 0.44412972\n",
      "Iteration 19, loss = 0.43939352\n",
      "Iteration 20, loss = 0.43599855\n",
      "Iteration 21, loss = 0.43283484\n",
      "Iteration 22, loss = 0.43024266\n",
      "Iteration 23, loss = 0.42780428\n",
      "Iteration 24, loss = 0.42598385\n",
      "Iteration 25, loss = 0.42453129\n",
      "Iteration 26, loss = 0.42306100\n",
      "Iteration 27, loss = 0.42189502\n",
      "Iteration 28, loss = 0.42106734\n",
      "Iteration 29, loss = 0.42019139\n",
      "Iteration 30, loss = 0.41940534\n",
      "Iteration 31, loss = 0.41899537\n",
      "Iteration 32, loss = 0.41851183\n",
      "Iteration 33, loss = 0.41825019\n",
      "Iteration 34, loss = 0.41762510\n",
      "Iteration 35, loss = 0.41722541\n",
      "Iteration 36, loss = 0.41710053\n",
      "Iteration 37, loss = 0.41678392\n",
      "Iteration 38, loss = 0.41643087\n",
      "Iteration 39, loss = 0.41622047\n",
      "Iteration 40, loss = 0.41615346\n",
      "Iteration 41, loss = 0.41585623\n",
      "Iteration 42, loss = 0.41571533\n",
      "Iteration 43, loss = 0.41551311\n",
      "Iteration 44, loss = 0.41544439\n",
      "Iteration 45, loss = 0.41514566\n",
      "Iteration 46, loss = 0.41499843\n",
      "Iteration 47, loss = 0.41481583\n",
      "Iteration 48, loss = 0.41471858\n",
      "Iteration 49, loss = 0.41457358\n",
      "Iteration 50, loss = 0.41438874\n",
      "Iteration 51, loss = 0.41427453\n",
      "Iteration 52, loss = 0.41420604\n",
      "Iteration 53, loss = 0.41400236\n",
      "Iteration 54, loss = 0.41385125\n",
      "Iteration 55, loss = 0.41372386\n",
      "Iteration 56, loss = 0.41358832\n",
      "Iteration 57, loss = 0.41351895\n",
      "Iteration 58, loss = 0.41345845\n",
      "Iteration 59, loss = 0.41321316\n",
      "Iteration 60, loss = 0.41305805\n",
      "Iteration 61, loss = 0.41301049\n",
      "Iteration 62, loss = 0.41284586\n",
      "Iteration 63, loss = 0.41280348\n",
      "Iteration 64, loss = 0.41275043\n",
      "Iteration 65, loss = 0.41254042\n",
      "Iteration 66, loss = 0.41242490\n",
      "Iteration 67, loss = 0.41227182\n",
      "Iteration 68, loss = 0.41209639\n",
      "Iteration 69, loss = 0.41197806\n",
      "Iteration 70, loss = 0.41195380\n",
      "Iteration 71, loss = 0.41176114\n",
      "Iteration 72, loss = 0.41162848\n",
      "Iteration 73, loss = 0.41140919\n",
      "Iteration 74, loss = 0.41138917\n",
      "Iteration 75, loss = 0.41122166\n",
      "Iteration 76, loss = 0.41112180\n",
      "Iteration 77, loss = 0.41099073\n",
      "Iteration 78, loss = 0.41082340\n",
      "Iteration 79, loss = 0.41066416\n",
      "Iteration 80, loss = 0.41058234\n",
      "Iteration 81, loss = 0.41043938\n",
      "Iteration 82, loss = 0.41030391\n",
      "Iteration 83, loss = 0.41015914\n",
      "Iteration 84, loss = 0.41007118\n",
      "Iteration 85, loss = 0.41006405\n",
      "Iteration 86, loss = 0.40976353\n",
      "Iteration 87, loss = 0.40985977\n",
      "Iteration 88, loss = 0.40963521\n",
      "Iteration 89, loss = 0.40950110\n",
      "Iteration 90, loss = 0.40945623\n",
      "Iteration 91, loss = 0.40917772\n",
      "Iteration 92, loss = 0.40930955\n",
      "Iteration 93, loss = 0.40899296\n",
      "Iteration 94, loss = 0.40893419\n",
      "Iteration 95, loss = 0.40866098\n",
      "Iteration 96, loss = 0.40855173\n",
      "Iteration 97, loss = 0.40854452\n",
      "Iteration 98, loss = 0.40829047\n",
      "Iteration 99, loss = 0.40823235\n",
      "Iteration 100, loss = 0.40793549\n",
      "Iteration 101, loss = 0.40815081\n",
      "Iteration 102, loss = 0.40788860\n",
      "Iteration 103, loss = 0.40765118\n",
      "Iteration 104, loss = 0.40762857\n",
      "Iteration 105, loss = 0.40733803\n",
      "Iteration 106, loss = 0.40726686\n",
      "Iteration 107, loss = 0.40717330\n",
      "Iteration 108, loss = 0.40713245\n",
      "Iteration 109, loss = 0.40699898\n",
      "Iteration 110, loss = 0.40677894\n",
      "Iteration 111, loss = 0.40652768\n",
      "Iteration 112, loss = 0.40647271\n",
      "Iteration 113, loss = 0.40631232\n",
      "Iteration 114, loss = 0.40622878\n",
      "Iteration 115, loss = 0.40607812\n",
      "Iteration 116, loss = 0.40586570\n",
      "Iteration 117, loss = 0.40574029\n",
      "Iteration 118, loss = 0.40569823\n",
      "Iteration 119, loss = 0.40565648\n",
      "Iteration 120, loss = 0.40542258\n",
      "Iteration 121, loss = 0.40531875\n",
      "Iteration 122, loss = 0.40515812\n",
      "Iteration 123, loss = 0.40490350\n",
      "Iteration 124, loss = 0.40486177\n",
      "Iteration 125, loss = 0.40475241\n",
      "Iteration 126, loss = 0.40457052\n",
      "Iteration 127, loss = 0.40444494\n",
      "Iteration 128, loss = 0.40424774\n",
      "Iteration 129, loss = 0.40412900\n",
      "Iteration 130, loss = 0.40405530\n",
      "Iteration 131, loss = 0.40377111\n",
      "Iteration 132, loss = 0.40372539\n",
      "Iteration 133, loss = 0.40360076\n",
      "Iteration 134, loss = 0.40342333\n",
      "Iteration 135, loss = 0.40322976\n",
      "Iteration 136, loss = 0.40316232\n",
      "Iteration 137, loss = 0.40324725\n",
      "Iteration 138, loss = 0.40294143\n",
      "Iteration 139, loss = 0.40279834\n",
      "Iteration 140, loss = 0.40259469\n",
      "Iteration 141, loss = 0.40245997\n",
      "Iteration 142, loss = 0.40219258\n",
      "Iteration 143, loss = 0.40199479\n",
      "Iteration 144, loss = 0.40200369\n",
      "Iteration 145, loss = 0.40188756\n",
      "Iteration 146, loss = 0.40179784\n",
      "Iteration 147, loss = 0.40146174\n",
      "Iteration 148, loss = 0.40140879\n",
      "Iteration 149, loss = 0.40113287\n",
      "Iteration 150, loss = 0.40098148\n",
      "Iteration 151, loss = 0.40082205\n",
      "Iteration 152, loss = 0.40086595\n",
      "Iteration 153, loss = 0.40096015\n",
      "Iteration 154, loss = 0.40039037\n",
      "Iteration 155, loss = 0.40019759\n",
      "Iteration 156, loss = 0.40012931\n",
      "Iteration 157, loss = 0.39989140\n",
      "Iteration 158, loss = 0.39973877\n",
      "Iteration 159, loss = 0.39956657\n",
      "Iteration 160, loss = 0.39949386\n",
      "Iteration 161, loss = 0.39936152\n",
      "Iteration 162, loss = 0.39922795\n",
      "Iteration 163, loss = 0.39901719\n",
      "Iteration 164, loss = 0.39883879\n",
      "Iteration 165, loss = 0.39859439\n",
      "Iteration 166, loss = 0.39850550\n",
      "Iteration 167, loss = 0.39825671\n",
      "Iteration 168, loss = 0.39826962\n",
      "Iteration 169, loss = 0.39801410\n",
      "Iteration 170, loss = 0.39791239\n",
      "Iteration 171, loss = 0.39759754\n",
      "Iteration 172, loss = 0.39750475\n",
      "Iteration 173, loss = 0.39752440\n",
      "Iteration 174, loss = 0.39720084\n",
      "Iteration 175, loss = 0.39711753\n",
      "Iteration 176, loss = 0.39682911\n",
      "Iteration 177, loss = 0.39666696\n",
      "Iteration 178, loss = 0.39653614\n",
      "Iteration 179, loss = 0.39636845\n",
      "Iteration 180, loss = 0.39607924\n",
      "Iteration 181, loss = 0.39591596\n",
      "Iteration 182, loss = 0.39575784\n",
      "Iteration 183, loss = 0.39551617\n",
      "Iteration 184, loss = 0.39535277\n",
      "Iteration 185, loss = 0.39528985\n",
      "Iteration 186, loss = 0.39517835\n",
      "Iteration 187, loss = 0.39482609\n",
      "Iteration 188, loss = 0.39475084\n",
      "Iteration 189, loss = 0.39459206\n",
      "Iteration 190, loss = 0.39442622\n",
      "Iteration 191, loss = 0.39410689\n",
      "Iteration 192, loss = 0.39417338\n",
      "Iteration 193, loss = 0.39366412\n",
      "Iteration 194, loss = 0.39367440\n",
      "Iteration 195, loss = 0.39364216\n",
      "Iteration 196, loss = 0.39317132\n",
      "Iteration 197, loss = 0.39309733\n",
      "Iteration 198, loss = 0.39294544\n",
      "Iteration 199, loss = 0.39283024\n",
      "Iteration 200, loss = 0.39241757\n",
      "Iteration 1, loss = 0.75383256\n",
      "Iteration 2, loss = 0.70743824\n",
      "Iteration 3, loss = 0.66802887\n",
      "Iteration 4, loss = 0.63522922\n",
      "Iteration 5, loss = 0.60695864\n",
      "Iteration 6, loss = 0.58315632\n",
      "Iteration 7, loss = 0.56372096\n",
      "Iteration 8, loss = 0.54583664\n",
      "Iteration 9, loss = 0.53067210\n",
      "Iteration 10, loss = 0.51726475\n",
      "Iteration 11, loss = 0.50589725\n",
      "Iteration 12, loss = 0.49619160\n",
      "Iteration 13, loss = 0.48763850\n",
      "Iteration 14, loss = 0.47971652\n",
      "Iteration 15, loss = 0.47364804\n",
      "Iteration 16, loss = 0.46814269\n",
      "Iteration 17, loss = 0.46422242\n",
      "Iteration 18, loss = 0.46019000\n",
      "Iteration 19, loss = 0.45667638\n",
      "Iteration 20, loss = 0.45446260\n",
      "Iteration 21, loss = 0.45229821\n",
      "Iteration 22, loss = 0.45026701\n",
      "Iteration 23, loss = 0.44879174\n",
      "Iteration 24, loss = 0.44761180\n",
      "Iteration 25, loss = 0.44660445\n",
      "Iteration 26, loss = 0.44567892\n",
      "Iteration 27, loss = 0.44499255\n",
      "Iteration 28, loss = 0.44456090\n",
      "Iteration 29, loss = 0.44396461\n",
      "Iteration 30, loss = 0.44347928\n",
      "Iteration 31, loss = 0.44317747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.44290281\n",
      "Iteration 33, loss = 0.44267279\n",
      "Iteration 34, loss = 0.44232939\n",
      "Iteration 35, loss = 0.44202043\n",
      "Iteration 36, loss = 0.44193769\n",
      "Iteration 37, loss = 0.44170467\n",
      "Iteration 38, loss = 0.44143122\n",
      "Iteration 39, loss = 0.44121576\n",
      "Iteration 40, loss = 0.44116747\n",
      "Iteration 41, loss = 0.44091227\n",
      "Iteration 42, loss = 0.44076784\n",
      "Iteration 43, loss = 0.44055188\n",
      "Iteration 44, loss = 0.44044313\n",
      "Iteration 45, loss = 0.44022916\n",
      "Iteration 46, loss = 0.44008445\n",
      "Iteration 47, loss = 0.43985742\n",
      "Iteration 48, loss = 0.43973477\n",
      "Iteration 49, loss = 0.43962207\n",
      "Iteration 50, loss = 0.43954382\n",
      "Iteration 51, loss = 0.43929369\n",
      "Iteration 52, loss = 0.43916313\n",
      "Iteration 53, loss = 0.43904064\n",
      "Iteration 54, loss = 0.43878905\n",
      "Iteration 55, loss = 0.43867864\n",
      "Iteration 56, loss = 0.43843409\n",
      "Iteration 57, loss = 0.43833016\n",
      "Iteration 58, loss = 0.43848824\n",
      "Iteration 59, loss = 0.43803151\n",
      "Iteration 60, loss = 0.43786819\n",
      "Iteration 61, loss = 0.43776493\n",
      "Iteration 62, loss = 0.43753960\n",
      "Iteration 63, loss = 0.43752413\n",
      "Iteration 64, loss = 0.43737598\n",
      "Iteration 65, loss = 0.43725209\n",
      "Iteration 66, loss = 0.43705572\n",
      "Iteration 67, loss = 0.43692254\n",
      "Iteration 68, loss = 0.43678678\n",
      "Iteration 69, loss = 0.43661575\n",
      "Iteration 70, loss = 0.43658790\n",
      "Iteration 71, loss = 0.43627142\n",
      "Iteration 72, loss = 0.43616414\n",
      "Iteration 73, loss = 0.43593153\n",
      "Iteration 74, loss = 0.43587176\n",
      "Iteration 75, loss = 0.43560058\n",
      "Iteration 76, loss = 0.43553519\n",
      "Iteration 77, loss = 0.43544646\n",
      "Iteration 78, loss = 0.43534712\n",
      "Iteration 79, loss = 0.43502575\n",
      "Iteration 80, loss = 0.43492063\n",
      "Iteration 81, loss = 0.43477294\n",
      "Iteration 82, loss = 0.43457592\n",
      "Iteration 83, loss = 0.43446142\n",
      "Iteration 84, loss = 0.43435995\n",
      "Iteration 85, loss = 0.43420919\n",
      "Iteration 86, loss = 0.43398022\n",
      "Iteration 87, loss = 0.43431494\n",
      "Iteration 88, loss = 0.43385399\n",
      "Iteration 89, loss = 0.43358776\n",
      "Iteration 90, loss = 0.43339186\n",
      "Iteration 91, loss = 0.43323785\n",
      "Iteration 92, loss = 0.43324931\n",
      "Iteration 93, loss = 0.43292261\n",
      "Iteration 94, loss = 0.43282430\n",
      "Iteration 95, loss = 0.43261884\n",
      "Iteration 96, loss = 0.43251573\n",
      "Iteration 97, loss = 0.43240625\n",
      "Iteration 98, loss = 0.43212456\n",
      "Iteration 99, loss = 0.43200340\n",
      "Iteration 100, loss = 0.43173349\n",
      "Iteration 101, loss = 0.43203601\n",
      "Iteration 102, loss = 0.43159583\n",
      "Iteration 103, loss = 0.43149950\n",
      "Iteration 104, loss = 0.43127433\n",
      "Iteration 105, loss = 0.43109999\n",
      "Iteration 106, loss = 0.43084890\n",
      "Iteration 107, loss = 0.43081027\n",
      "Iteration 108, loss = 0.43071258\n",
      "Iteration 109, loss = 0.43059155\n",
      "Iteration 110, loss = 0.43040250\n",
      "Iteration 111, loss = 0.43002121\n",
      "Iteration 112, loss = 0.43000016\n",
      "Iteration 113, loss = 0.42969377\n",
      "Iteration 114, loss = 0.42954650\n",
      "Iteration 115, loss = 0.42950020\n",
      "Iteration 116, loss = 0.42929810\n",
      "Iteration 117, loss = 0.42909346\n",
      "Iteration 118, loss = 0.42896791\n",
      "Iteration 119, loss = 0.42892899\n",
      "Iteration 120, loss = 0.42882701\n",
      "Iteration 121, loss = 0.42843789\n",
      "Iteration 122, loss = 0.42831776\n",
      "Iteration 123, loss = 0.42813852\n",
      "Iteration 124, loss = 0.42806625\n",
      "Iteration 125, loss = 0.42779345\n",
      "Iteration 126, loss = 0.42786596\n",
      "Iteration 127, loss = 0.42738812\n",
      "Iteration 128, loss = 0.42728443\n",
      "Iteration 129, loss = 0.42702726\n",
      "Iteration 130, loss = 0.42692011\n",
      "Iteration 131, loss = 0.42673132\n",
      "Iteration 132, loss = 0.42655725\n",
      "Iteration 133, loss = 0.42652576\n",
      "Iteration 134, loss = 0.42620474\n",
      "Iteration 135, loss = 0.42610734\n",
      "Iteration 136, loss = 0.42591947\n",
      "Iteration 137, loss = 0.42579422\n",
      "Iteration 138, loss = 0.42536375\n",
      "Iteration 139, loss = 0.42546481\n",
      "Iteration 140, loss = 0.42521234\n",
      "Iteration 141, loss = 0.42503820\n",
      "Iteration 142, loss = 0.42478291\n",
      "Iteration 143, loss = 0.42469523\n",
      "Iteration 144, loss = 0.42452590\n",
      "Iteration 145, loss = 0.42429857\n",
      "Iteration 146, loss = 0.42402374\n",
      "Iteration 147, loss = 0.42396195\n",
      "Iteration 148, loss = 0.42370239\n",
      "Iteration 149, loss = 0.42346012\n",
      "Iteration 150, loss = 0.42335948\n",
      "Iteration 151, loss = 0.42314327\n",
      "Iteration 152, loss = 0.42308097\n",
      "Iteration 153, loss = 0.42313112\n",
      "Iteration 154, loss = 0.42255897\n",
      "Iteration 155, loss = 0.42258258\n",
      "Iteration 156, loss = 0.42217338\n",
      "Iteration 157, loss = 0.42204462\n",
      "Iteration 158, loss = 0.42191639\n",
      "Iteration 159, loss = 0.42164916\n",
      "Iteration 160, loss = 0.42145083\n",
      "Iteration 161, loss = 0.42120645\n",
      "Iteration 162, loss = 0.42099804\n",
      "Iteration 163, loss = 0.42088414\n",
      "Iteration 164, loss = 0.42071016\n",
      "Iteration 165, loss = 0.42044566\n",
      "Iteration 166, loss = 0.42037120\n",
      "Iteration 167, loss = 0.42012283\n",
      "Iteration 168, loss = 0.41993583\n",
      "Iteration 169, loss = 0.41978811\n",
      "Iteration 170, loss = 0.41960744\n",
      "Iteration 171, loss = 0.41922426\n",
      "Iteration 172, loss = 0.41906747\n",
      "Iteration 173, loss = 0.41893029\n",
      "Iteration 174, loss = 0.41880397\n",
      "Iteration 175, loss = 0.41865538\n",
      "Iteration 176, loss = 0.41840407\n",
      "Iteration 177, loss = 0.41811044\n",
      "Iteration 178, loss = 0.41787321\n",
      "Iteration 179, loss = 0.41776997\n",
      "Iteration 180, loss = 0.41752321\n",
      "Iteration 181, loss = 0.41725398\n",
      "Iteration 182, loss = 0.41710940\n",
      "Iteration 183, loss = 0.41696674\n",
      "Iteration 184, loss = 0.41685724\n",
      "Iteration 185, loss = 0.41670862\n",
      "Iteration 186, loss = 0.41641021\n",
      "Iteration 187, loss = 0.41611161\n",
      "Iteration 188, loss = 0.41587570\n",
      "Iteration 189, loss = 0.41569227\n",
      "Iteration 190, loss = 0.41543083\n",
      "Iteration 191, loss = 0.41527975\n",
      "Iteration 192, loss = 0.41523238\n",
      "Iteration 193, loss = 0.41477708\n",
      "Iteration 194, loss = 0.41471643\n",
      "Iteration 195, loss = 0.41465553\n",
      "Iteration 196, loss = 0.41419781\n",
      "Iteration 197, loss = 0.41428875\n",
      "Iteration 198, loss = 0.41399593\n",
      "Iteration 199, loss = 0.41386826\n",
      "Iteration 200, loss = 0.41368142\n",
      "Iteration 1, loss = 0.75059346\n",
      "Iteration 2, loss = 0.70184799\n",
      "Iteration 3, loss = 0.65716398\n",
      "Iteration 4, loss = 0.62133811\n",
      "Iteration 5, loss = 0.59153873\n",
      "Iteration 6, loss = 0.56580348\n",
      "Iteration 7, loss = 0.54352100\n",
      "Iteration 8, loss = 0.52573340\n",
      "Iteration 9, loss = 0.50905567\n",
      "Iteration 10, loss = 0.49546878\n",
      "Iteration 11, loss = 0.48334197\n",
      "Iteration 12, loss = 0.47311938\n",
      "Iteration 13, loss = 0.46301032\n",
      "Iteration 14, loss = 0.45514069\n",
      "Iteration 15, loss = 0.44805095\n",
      "Iteration 16, loss = 0.44178688\n",
      "Iteration 17, loss = 0.43626667\n",
      "Iteration 18, loss = 0.43241758\n",
      "Iteration 19, loss = 0.42835269\n",
      "Iteration 20, loss = 0.42506166\n",
      "Iteration 21, loss = 0.42248859\n",
      "Iteration 22, loss = 0.41994018\n",
      "Iteration 23, loss = 0.41837504\n",
      "Iteration 24, loss = 0.41671430\n",
      "Iteration 25, loss = 0.41560805\n",
      "Iteration 26, loss = 0.41454775\n",
      "Iteration 27, loss = 0.41358174\n",
      "Iteration 28, loss = 0.41289964\n",
      "Iteration 29, loss = 0.41227654\n",
      "Iteration 30, loss = 0.41173146\n",
      "Iteration 31, loss = 0.41128471\n",
      "Iteration 32, loss = 0.41106496\n",
      "Iteration 33, loss = 0.41070385\n",
      "Iteration 34, loss = 0.41036372\n",
      "Iteration 35, loss = 0.41015273\n",
      "Iteration 36, loss = 0.40996113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.40971453\n",
      "Iteration 38, loss = 0.40953518\n",
      "Iteration 39, loss = 0.40935829\n",
      "Iteration 40, loss = 0.40923938\n",
      "Iteration 41, loss = 0.40909036\n",
      "Iteration 42, loss = 0.40900358\n",
      "Iteration 43, loss = 0.40889650\n",
      "Iteration 44, loss = 0.40872494\n",
      "Iteration 45, loss = 0.40857105\n",
      "Iteration 46, loss = 0.40848580\n",
      "Iteration 47, loss = 0.40827195\n",
      "Iteration 48, loss = 0.40813361\n",
      "Iteration 49, loss = 0.40799478\n",
      "Iteration 50, loss = 0.40786254\n",
      "Iteration 51, loss = 0.40780271\n",
      "Iteration 52, loss = 0.40766776\n",
      "Iteration 53, loss = 0.40745122\n",
      "Iteration 54, loss = 0.40741295\n",
      "Iteration 55, loss = 0.40725193\n",
      "Iteration 56, loss = 0.40713272\n",
      "Iteration 57, loss = 0.40708929\n",
      "Iteration 58, loss = 0.40700666\n",
      "Iteration 59, loss = 0.40669829\n",
      "Iteration 60, loss = 0.40657825\n",
      "Iteration 61, loss = 0.40666555\n",
      "Iteration 62, loss = 0.40670415\n",
      "Iteration 63, loss = 0.40640275\n",
      "Iteration 64, loss = 0.40621033\n",
      "Iteration 65, loss = 0.40593053\n",
      "Iteration 66, loss = 0.40590142\n",
      "Iteration 67, loss = 0.40568501\n",
      "Iteration 68, loss = 0.40573226\n",
      "Iteration 69, loss = 0.40545340\n",
      "Iteration 70, loss = 0.40539421\n",
      "Iteration 71, loss = 0.40521854\n",
      "Iteration 72, loss = 0.40512957\n",
      "Iteration 73, loss = 0.40495604\n",
      "Iteration 74, loss = 0.40484900\n",
      "Iteration 75, loss = 0.40473191\n",
      "Iteration 76, loss = 0.40466843\n",
      "Iteration 77, loss = 0.40445567\n",
      "Iteration 78, loss = 0.40443497\n",
      "Iteration 79, loss = 0.40416636\n",
      "Iteration 80, loss = 0.40410815\n",
      "Iteration 81, loss = 0.40400567\n",
      "Iteration 82, loss = 0.40382331\n",
      "Iteration 83, loss = 0.40384489\n",
      "Iteration 84, loss = 0.40362834\n",
      "Iteration 85, loss = 0.40344235\n",
      "Iteration 86, loss = 0.40348605\n",
      "Iteration 87, loss = 0.40319748\n",
      "Iteration 88, loss = 0.40320688\n",
      "Iteration 89, loss = 0.40291642\n",
      "Iteration 90, loss = 0.40283523\n",
      "Iteration 91, loss = 0.40278625\n",
      "Iteration 92, loss = 0.40261092\n",
      "Iteration 93, loss = 0.40241426\n",
      "Iteration 94, loss = 0.40236323\n",
      "Iteration 95, loss = 0.40211004\n",
      "Iteration 96, loss = 0.40231202\n",
      "Iteration 97, loss = 0.40187809\n",
      "Iteration 98, loss = 0.40178763\n",
      "Iteration 99, loss = 0.40162021\n",
      "Iteration 100, loss = 0.40163441\n",
      "Iteration 101, loss = 0.40136844\n",
      "Iteration 102, loss = 0.40126647\n",
      "Iteration 103, loss = 0.40104490\n",
      "Iteration 104, loss = 0.40101268\n",
      "Iteration 105, loss = 0.40079082\n",
      "Iteration 106, loss = 0.40068592\n",
      "Iteration 107, loss = 0.40057287\n",
      "Iteration 108, loss = 0.40058942\n",
      "Iteration 109, loss = 0.40027328\n",
      "Iteration 110, loss = 0.40022835\n",
      "Iteration 111, loss = 0.40002643\n",
      "Iteration 112, loss = 0.39980074\n",
      "Iteration 113, loss = 0.39977858\n",
      "Iteration 114, loss = 0.39952035\n",
      "Iteration 115, loss = 0.39943957\n",
      "Iteration 116, loss = 0.39931320\n",
      "Iteration 117, loss = 0.39909895\n",
      "Iteration 118, loss = 0.39903879\n",
      "Iteration 119, loss = 0.39887472\n",
      "Iteration 120, loss = 0.39870176\n",
      "Iteration 121, loss = 0.39868105\n",
      "Iteration 122, loss = 0.39841099\n",
      "Iteration 123, loss = 0.39837808\n",
      "Iteration 124, loss = 0.39833619\n",
      "Iteration 125, loss = 0.39812459\n",
      "Iteration 126, loss = 0.39786263\n",
      "Iteration 127, loss = 0.39771537\n",
      "Iteration 128, loss = 0.39756260\n",
      "Iteration 129, loss = 0.39740257\n",
      "Iteration 130, loss = 0.39725113\n",
      "Iteration 131, loss = 0.39725473\n",
      "Iteration 132, loss = 0.39693985\n",
      "Iteration 133, loss = 0.39682740\n",
      "Iteration 134, loss = 0.39668621\n",
      "Iteration 135, loss = 0.39648080\n",
      "Iteration 136, loss = 0.39631344\n",
      "Iteration 137, loss = 0.39630844\n",
      "Iteration 138, loss = 0.39601653\n",
      "Iteration 139, loss = 0.39605783\n",
      "Iteration 140, loss = 0.39589403\n",
      "Iteration 141, loss = 0.39562789\n",
      "Iteration 142, loss = 0.39543181\n",
      "Iteration 143, loss = 0.39533851\n",
      "Iteration 144, loss = 0.39512031\n",
      "Iteration 145, loss = 0.39524994\n",
      "Iteration 146, loss = 0.39495788\n",
      "Iteration 147, loss = 0.39468500\n",
      "Iteration 148, loss = 0.39467915\n",
      "Iteration 149, loss = 0.39443183\n",
      "Iteration 150, loss = 0.39421209\n",
      "Iteration 151, loss = 0.39419043\n",
      "Iteration 152, loss = 0.39389593\n",
      "Iteration 153, loss = 0.39376357\n",
      "Iteration 154, loss = 0.39369904\n",
      "Iteration 155, loss = 0.39387427\n",
      "Iteration 156, loss = 0.39320751\n",
      "Iteration 157, loss = 0.39306341\n",
      "Iteration 158, loss = 0.39297210\n",
      "Iteration 159, loss = 0.39276592\n",
      "Iteration 160, loss = 0.39264516\n",
      "Iteration 161, loss = 0.39239922\n",
      "Iteration 162, loss = 0.39240357\n",
      "Iteration 163, loss = 0.39214030\n",
      "Iteration 164, loss = 0.39194827\n",
      "Iteration 165, loss = 0.39191131\n",
      "Iteration 166, loss = 0.39168515\n",
      "Iteration 167, loss = 0.39153119\n",
      "Iteration 168, loss = 0.39129331\n",
      "Iteration 169, loss = 0.39125798\n",
      "Iteration 170, loss = 0.39093465\n",
      "Iteration 171, loss = 0.39072842\n",
      "Iteration 172, loss = 0.39070141\n",
      "Iteration 173, loss = 0.39057856\n",
      "Iteration 174, loss = 0.39027684\n",
      "Iteration 175, loss = 0.39003285\n",
      "Iteration 176, loss = 0.39032705\n",
      "Iteration 177, loss = 0.38990084\n",
      "Iteration 178, loss = 0.38967560\n",
      "Iteration 179, loss = 0.38954492\n",
      "Iteration 180, loss = 0.38925477\n",
      "Iteration 181, loss = 0.38926501\n",
      "Iteration 182, loss = 0.38893818\n",
      "Iteration 183, loss = 0.38888486\n",
      "Iteration 184, loss = 0.38864005\n",
      "Iteration 185, loss = 0.38855600\n",
      "Iteration 186, loss = 0.38827808\n",
      "Iteration 187, loss = 0.38826861\n",
      "Iteration 188, loss = 0.38790535\n",
      "Iteration 189, loss = 0.38772577\n",
      "Iteration 190, loss = 0.38770022\n",
      "Iteration 191, loss = 0.38730271\n",
      "Iteration 192, loss = 0.38727656\n",
      "Iteration 193, loss = 0.38706196\n",
      "Iteration 194, loss = 0.38702618\n",
      "Iteration 195, loss = 0.38688597\n",
      "Iteration 196, loss = 0.38656292\n",
      "Iteration 197, loss = 0.38623453\n",
      "Iteration 198, loss = 0.38621264\n",
      "Iteration 199, loss = 0.38599877\n",
      "Iteration 200, loss = 0.38585956\n",
      "Iteration 1, loss = 0.75772774\n",
      "Iteration 2, loss = 0.71111330\n",
      "Iteration 3, loss = 0.66849259\n",
      "Iteration 4, loss = 0.63485322\n",
      "Iteration 5, loss = 0.60620814\n",
      "Iteration 6, loss = 0.58165524\n",
      "Iteration 7, loss = 0.55980521\n",
      "Iteration 8, loss = 0.54305423\n",
      "Iteration 9, loss = 0.52715085\n",
      "Iteration 10, loss = 0.51419706\n",
      "Iteration 11, loss = 0.50242937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.49262539\n",
      "Iteration 13, loss = 0.48321795\n",
      "Iteration 14, loss = 0.47585890\n",
      "Iteration 15, loss = 0.46904493\n",
      "Iteration 16, loss = 0.46329722\n",
      "Iteration 17, loss = 0.45826493\n",
      "Iteration 18, loss = 0.45457165\n",
      "Iteration 19, loss = 0.45086497\n",
      "Iteration 20, loss = 0.44770200\n",
      "Iteration 21, loss = 0.44522416\n",
      "Iteration 22, loss = 0.44299886\n",
      "Iteration 23, loss = 0.44167583\n",
      "Iteration 24, loss = 0.44007154\n",
      "Iteration 25, loss = 0.43909998\n",
      "Iteration 26, loss = 0.43794360\n",
      "Iteration 27, loss = 0.43724603\n",
      "Iteration 28, loss = 0.43659487\n",
      "Iteration 29, loss = 0.43582381\n",
      "Iteration 30, loss = 0.43546803\n",
      "Iteration 31, loss = 0.43488513\n",
      "Iteration 32, loss = 0.43472266\n",
      "Iteration 33, loss = 0.43431908\n",
      "Iteration 34, loss = 0.43394732\n",
      "Iteration 35, loss = 0.43367819\n",
      "Iteration 36, loss = 0.43352930\n",
      "Iteration 37, loss = 0.43322184\n",
      "Iteration 38, loss = 0.43303938\n",
      "Iteration 39, loss = 0.43285192\n",
      "Iteration 40, loss = 0.43278511\n",
      "Iteration 41, loss = 0.43248738\n",
      "Iteration 42, loss = 0.43240970\n",
      "Iteration 43, loss = 0.43227226\n",
      "Iteration 44, loss = 0.43214712\n",
      "Iteration 45, loss = 0.43191835\n",
      "Iteration 46, loss = 0.43179847\n",
      "Iteration 47, loss = 0.43163344\n",
      "Iteration 48, loss = 0.43154632\n",
      "Iteration 49, loss = 0.43143784\n",
      "Iteration 50, loss = 0.43131743\n",
      "Iteration 51, loss = 0.43117746\n",
      "Iteration 52, loss = 0.43099906\n",
      "Iteration 53, loss = 0.43089568\n",
      "Iteration 54, loss = 0.43079963\n",
      "Iteration 55, loss = 0.43063528\n",
      "Iteration 56, loss = 0.43054435\n",
      "Iteration 57, loss = 0.43040566\n",
      "Iteration 58, loss = 0.43029700\n",
      "Iteration 59, loss = 0.43014475\n",
      "Iteration 60, loss = 0.43007308\n",
      "Iteration 61, loss = 0.43002236\n",
      "Iteration 62, loss = 0.42984223\n",
      "Iteration 63, loss = 0.42966467\n",
      "Iteration 64, loss = 0.42962619\n",
      "Iteration 65, loss = 0.42940835\n",
      "Iteration 66, loss = 0.42937744\n",
      "Iteration 67, loss = 0.42913288\n",
      "Iteration 68, loss = 0.42919766\n",
      "Iteration 69, loss = 0.42904436\n",
      "Iteration 70, loss = 0.42886064\n",
      "Iteration 71, loss = 0.42870462\n",
      "Iteration 72, loss = 0.42852387\n",
      "Iteration 73, loss = 0.42853644\n",
      "Iteration 74, loss = 0.42840313\n",
      "Iteration 75, loss = 0.42821596\n",
      "Iteration 76, loss = 0.42813102\n",
      "Iteration 77, loss = 0.42795730\n",
      "Iteration 78, loss = 0.42785098\n",
      "Iteration 79, loss = 0.42767090\n",
      "Iteration 80, loss = 0.42756328\n",
      "Iteration 81, loss = 0.42766556\n",
      "Iteration 82, loss = 0.42732063\n",
      "Iteration 83, loss = 0.42749293\n",
      "Iteration 84, loss = 0.42713579\n",
      "Iteration 85, loss = 0.42691318\n",
      "Iteration 86, loss = 0.42681735\n",
      "Iteration 87, loss = 0.42673645\n",
      "Iteration 88, loss = 0.42665124\n",
      "Iteration 89, loss = 0.42645975\n",
      "Iteration 90, loss = 0.42635469\n",
      "Iteration 91, loss = 0.42633098\n",
      "Iteration 92, loss = 0.42608715\n",
      "Iteration 93, loss = 0.42591792\n",
      "Iteration 94, loss = 0.42615311\n",
      "Iteration 95, loss = 0.42577746\n",
      "Iteration 96, loss = 0.42565508\n",
      "Iteration 97, loss = 0.42552328\n",
      "Iteration 98, loss = 0.42555816\n",
      "Iteration 99, loss = 0.42525168\n",
      "Iteration 100, loss = 0.42510990\n",
      "Iteration 101, loss = 0.42505255\n",
      "Iteration 102, loss = 0.42489515\n",
      "Iteration 103, loss = 0.42479563\n",
      "Iteration 104, loss = 0.42463756\n",
      "Iteration 105, loss = 0.42466758\n",
      "Iteration 106, loss = 0.42435229\n",
      "Iteration 107, loss = 0.42419056\n",
      "Iteration 108, loss = 0.42410891\n",
      "Iteration 109, loss = 0.42398235\n",
      "Iteration 110, loss = 0.42387213\n",
      "Iteration 111, loss = 0.42377619\n",
      "Iteration 112, loss = 0.42351179\n",
      "Iteration 113, loss = 0.42352323\n",
      "Iteration 114, loss = 0.42327798\n",
      "Iteration 115, loss = 0.42319867\n",
      "Iteration 116, loss = 0.42322311\n",
      "Iteration 117, loss = 0.42294088\n",
      "Iteration 118, loss = 0.42283722\n",
      "Iteration 119, loss = 0.42273806\n",
      "Iteration 120, loss = 0.42250450\n",
      "Iteration 121, loss = 0.42252336\n",
      "Iteration 122, loss = 0.42230958\n",
      "Iteration 123, loss = 0.42215024\n",
      "Iteration 124, loss = 0.42212787\n",
      "Iteration 125, loss = 0.42187248\n",
      "Iteration 126, loss = 0.42179184\n",
      "Iteration 127, loss = 0.42157137\n",
      "Iteration 128, loss = 0.42136224\n",
      "Iteration 129, loss = 0.42144493\n",
      "Iteration 130, loss = 0.42121929\n",
      "Iteration 131, loss = 0.42108952\n",
      "Iteration 132, loss = 0.42088138\n",
      "Iteration 133, loss = 0.42084383\n",
      "Iteration 134, loss = 0.42063848\n",
      "Iteration 135, loss = 0.42052481\n",
      "Iteration 136, loss = 0.42031145\n",
      "Iteration 137, loss = 0.42027112\n",
      "Iteration 138, loss = 0.42002807\n",
      "Iteration 139, loss = 0.42032308\n",
      "Iteration 140, loss = 0.41984344\n",
      "Iteration 141, loss = 0.41986853\n",
      "Iteration 142, loss = 0.41950250\n",
      "Iteration 143, loss = 0.41936020\n",
      "Iteration 144, loss = 0.41925326\n",
      "Iteration 145, loss = 0.41914834\n",
      "Iteration 146, loss = 0.41908121\n",
      "Iteration 147, loss = 0.41886369\n",
      "Iteration 148, loss = 0.41885866\n",
      "Iteration 149, loss = 0.41854875\n",
      "Iteration 150, loss = 0.41830297\n",
      "Iteration 151, loss = 0.41831817\n",
      "Iteration 152, loss = 0.41810656\n",
      "Iteration 153, loss = 0.41786589\n",
      "Iteration 154, loss = 0.41797534\n",
      "Iteration 155, loss = 0.41813635\n",
      "Iteration 156, loss = 0.41755374\n",
      "Iteration 157, loss = 0.41732941\n",
      "Iteration 158, loss = 0.41728781\n",
      "Iteration 159, loss = 0.41732978\n",
      "Iteration 160, loss = 0.41687498\n",
      "Iteration 161, loss = 0.41676369\n",
      "Iteration 162, loss = 0.41676156\n",
      "Iteration 163, loss = 0.41654190\n",
      "Iteration 164, loss = 0.41626731\n",
      "Iteration 165, loss = 0.41619948\n",
      "Iteration 166, loss = 0.41608466\n",
      "Iteration 167, loss = 0.41597091\n",
      "Iteration 168, loss = 0.41569174\n",
      "Iteration 169, loss = 0.41555538\n",
      "Iteration 170, loss = 0.41544671\n",
      "Iteration 171, loss = 0.41538988\n",
      "Iteration 172, loss = 0.41511206\n",
      "Iteration 173, loss = 0.41508293\n",
      "Iteration 174, loss = 0.41485663\n",
      "Iteration 175, loss = 0.41476153\n",
      "Iteration 176, loss = 0.41469690\n",
      "Iteration 177, loss = 0.41438957\n",
      "Iteration 178, loss = 0.41431770\n",
      "Iteration 179, loss = 0.41404637\n",
      "Iteration 180, loss = 0.41396235\n",
      "Iteration 181, loss = 0.41385695\n",
      "Iteration 182, loss = 0.41348758\n",
      "Iteration 183, loss = 0.41336906\n",
      "Iteration 184, loss = 0.41326260\n",
      "Iteration 185, loss = 0.41313156\n",
      "Iteration 186, loss = 0.41290357\n",
      "Iteration 187, loss = 0.41309567\n",
      "Iteration 188, loss = 0.41271011\n",
      "Iteration 189, loss = 0.41251525\n",
      "Iteration 190, loss = 0.41235013\n",
      "Iteration 191, loss = 0.41209268\n",
      "Iteration 192, loss = 0.41204224\n",
      "Iteration 193, loss = 0.41181536\n",
      "Iteration 194, loss = 0.41170496\n",
      "Iteration 195, loss = 0.41168066\n",
      "Iteration 196, loss = 0.41138226\n",
      "Iteration 197, loss = 0.41112674\n",
      "Iteration 198, loss = 0.41100632\n",
      "Iteration 199, loss = 0.41086131\n",
      "Iteration 200, loss = 0.41071417\n",
      "Iteration 1, loss = 0.75866013\n",
      "Iteration 2, loss = 0.71214538\n",
      "Iteration 3, loss = 0.67224466\n",
      "Iteration 4, loss = 0.63864395\n",
      "Iteration 5, loss = 0.60980852\n",
      "Iteration 6, loss = 0.58404382\n",
      "Iteration 7, loss = 0.56345973\n",
      "Iteration 8, loss = 0.54448134\n",
      "Iteration 9, loss = 0.52756917\n",
      "Iteration 10, loss = 0.51275882\n",
      "Iteration 11, loss = 0.50002376\n",
      "Iteration 12, loss = 0.48848590\n",
      "Iteration 13, loss = 0.47861313\n",
      "Iteration 14, loss = 0.46956008\n",
      "Iteration 15, loss = 0.46199926\n",
      "Iteration 16, loss = 0.45573010\n",
      "Iteration 17, loss = 0.45012909\n",
      "Iteration 18, loss = 0.44589333\n",
      "Iteration 19, loss = 0.44169702\n",
      "Iteration 20, loss = 0.43786290\n",
      "Iteration 21, loss = 0.43522989\n",
      "Iteration 22, loss = 0.43271398\n",
      "Iteration 23, loss = 0.43051965\n",
      "Iteration 24, loss = 0.42891576\n",
      "Iteration 25, loss = 0.42752953\n",
      "Iteration 26, loss = 0.42625900\n",
      "Iteration 27, loss = 0.42509964\n",
      "Iteration 28, loss = 0.42441404\n",
      "Iteration 29, loss = 0.42347660\n",
      "Iteration 30, loss = 0.42292666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.42245446\n",
      "Iteration 32, loss = 0.42193151\n",
      "Iteration 33, loss = 0.42178081\n",
      "Iteration 34, loss = 0.42114486\n",
      "Iteration 35, loss = 0.42092982\n",
      "Iteration 36, loss = 0.42050927\n",
      "Iteration 37, loss = 0.42035404\n",
      "Iteration 38, loss = 0.42002875\n",
      "Iteration 39, loss = 0.41968225\n",
      "Iteration 40, loss = 0.41958341\n",
      "Iteration 41, loss = 0.41930651\n",
      "Iteration 42, loss = 0.41915513\n",
      "Iteration 43, loss = 0.41882000\n",
      "Iteration 44, loss = 0.41885345\n",
      "Iteration 45, loss = 0.41849946\n",
      "Iteration 46, loss = 0.41827548\n",
      "Iteration 47, loss = 0.41807402\n",
      "Iteration 48, loss = 0.41799727\n",
      "Iteration 49, loss = 0.41776757\n",
      "Iteration 50, loss = 0.41760519\n",
      "Iteration 51, loss = 0.41747050\n",
      "Iteration 52, loss = 0.41736135\n",
      "Iteration 53, loss = 0.41709871\n",
      "Iteration 54, loss = 0.41690977\n",
      "Iteration 55, loss = 0.41676482\n",
      "Iteration 56, loss = 0.41670518\n",
      "Iteration 57, loss = 0.41660175\n",
      "Iteration 58, loss = 0.41639064\n",
      "Iteration 59, loss = 0.41616744\n",
      "Iteration 60, loss = 0.41606012\n",
      "Iteration 61, loss = 0.41590286\n",
      "Iteration 62, loss = 0.41576954\n",
      "Iteration 63, loss = 0.41559962\n",
      "Iteration 64, loss = 0.41551517\n",
      "Iteration 65, loss = 0.41530193\n",
      "Iteration 66, loss = 0.41528268\n",
      "Iteration 67, loss = 0.41520690\n",
      "Iteration 68, loss = 0.41495448\n",
      "Iteration 69, loss = 0.41473454\n",
      "Iteration 70, loss = 0.41475520\n",
      "Iteration 71, loss = 0.41448181\n",
      "Iteration 72, loss = 0.41436233\n",
      "Iteration 73, loss = 0.41415895\n",
      "Iteration 74, loss = 0.41405949\n",
      "Iteration 75, loss = 0.41386677\n",
      "Iteration 76, loss = 0.41390824\n",
      "Iteration 77, loss = 0.41353491\n",
      "Iteration 78, loss = 0.41340955\n",
      "Iteration 79, loss = 0.41328403\n",
      "Iteration 80, loss = 0.41322980\n",
      "Iteration 81, loss = 0.41306527\n",
      "Iteration 82, loss = 0.41293833\n",
      "Iteration 83, loss = 0.41275441\n",
      "Iteration 84, loss = 0.41261595\n",
      "Iteration 85, loss = 0.41262126\n",
      "Iteration 86, loss = 0.41246047\n",
      "Iteration 87, loss = 0.41238084\n",
      "Iteration 88, loss = 0.41204148\n",
      "Iteration 89, loss = 0.41191739\n",
      "Iteration 90, loss = 0.41189758\n",
      "Iteration 91, loss = 0.41161790\n",
      "Iteration 92, loss = 0.41181809\n",
      "Iteration 93, loss = 0.41139362\n",
      "Iteration 94, loss = 0.41166317\n",
      "Iteration 95, loss = 0.41107171\n",
      "Iteration 96, loss = 0.41087433\n",
      "Iteration 97, loss = 0.41074998\n",
      "Iteration 98, loss = 0.41054990\n",
      "Iteration 99, loss = 0.41045797\n",
      "Iteration 100, loss = 0.41026700\n",
      "Iteration 101, loss = 0.41045737\n",
      "Iteration 102, loss = 0.41005244\n",
      "Iteration 103, loss = 0.40986876\n",
      "Iteration 104, loss = 0.40985029\n",
      "Iteration 105, loss = 0.40956921\n",
      "Iteration 106, loss = 0.40955184\n",
      "Iteration 107, loss = 0.40932874\n",
      "Iteration 108, loss = 0.40915781\n",
      "Iteration 109, loss = 0.40900889\n",
      "Iteration 110, loss = 0.40880638\n",
      "Iteration 111, loss = 0.40870318\n",
      "Iteration 112, loss = 0.40862677\n",
      "Iteration 113, loss = 0.40835500\n",
      "Iteration 114, loss = 0.40828101\n",
      "Iteration 115, loss = 0.40807874\n",
      "Iteration 116, loss = 0.40783050\n",
      "Iteration 117, loss = 0.40769703\n",
      "Iteration 118, loss = 0.40758615\n",
      "Iteration 119, loss = 0.40759377\n",
      "Iteration 120, loss = 0.40742765\n",
      "Iteration 121, loss = 0.40717471\n",
      "Iteration 122, loss = 0.40702337\n",
      "Iteration 123, loss = 0.40677328\n",
      "Iteration 124, loss = 0.40675152\n",
      "Iteration 125, loss = 0.40661465\n",
      "Iteration 126, loss = 0.40633821\n",
      "Iteration 127, loss = 0.40613435\n",
      "Iteration 128, loss = 0.40603241\n",
      "Iteration 129, loss = 0.40593267\n",
      "Iteration 130, loss = 0.40575073\n",
      "Iteration 131, loss = 0.40551010\n",
      "Iteration 132, loss = 0.40541741\n",
      "Iteration 133, loss = 0.40524240\n",
      "Iteration 134, loss = 0.40507976\n",
      "Iteration 135, loss = 0.40479374\n",
      "Iteration 136, loss = 0.40503641\n",
      "Iteration 137, loss = 0.40447339\n",
      "Iteration 138, loss = 0.40435960\n",
      "Iteration 139, loss = 0.40435047\n",
      "Iteration 140, loss = 0.40403660\n",
      "Iteration 141, loss = 0.40388552\n",
      "Iteration 142, loss = 0.40367086\n",
      "Iteration 143, loss = 0.40352133\n",
      "Iteration 144, loss = 0.40345558\n",
      "Iteration 145, loss = 0.40330566\n",
      "Iteration 146, loss = 0.40307342\n",
      "Iteration 147, loss = 0.40291001\n",
      "Iteration 148, loss = 0.40280666\n",
      "Iteration 149, loss = 0.40245870\n",
      "Iteration 150, loss = 0.40227982\n",
      "Iteration 151, loss = 0.40214205\n",
      "Iteration 152, loss = 0.40201730\n",
      "Iteration 153, loss = 0.40235594\n",
      "Iteration 154, loss = 0.40175604\n",
      "Iteration 155, loss = 0.40130248\n",
      "Iteration 156, loss = 0.40133655\n",
      "Iteration 157, loss = 0.40110768\n",
      "Iteration 158, loss = 0.40090000\n",
      "Iteration 159, loss = 0.40073022\n",
      "Iteration 160, loss = 0.40064162\n",
      "Iteration 161, loss = 0.40042445\n",
      "Iteration 162, loss = 0.40029195\n",
      "Iteration 163, loss = 0.40002677\n",
      "Iteration 164, loss = 0.39998592\n",
      "Iteration 165, loss = 0.39973352\n",
      "Iteration 166, loss = 0.39951504\n",
      "Iteration 167, loss = 0.39920189\n",
      "Iteration 168, loss = 0.39908921\n",
      "Iteration 169, loss = 0.39888822\n",
      "Iteration 170, loss = 0.39886268\n",
      "Iteration 171, loss = 0.39854884\n",
      "Iteration 172, loss = 0.39854978\n",
      "Iteration 173, loss = 0.39831799\n",
      "Iteration 174, loss = 0.39810480\n",
      "Iteration 175, loss = 0.39785105\n",
      "Iteration 176, loss = 0.39769231\n",
      "Iteration 177, loss = 0.39738026\n",
      "Iteration 178, loss = 0.39744037\n",
      "Iteration 179, loss = 0.39707533\n",
      "Iteration 180, loss = 0.39683950\n",
      "Iteration 181, loss = 0.39660427\n",
      "Iteration 182, loss = 0.39646821\n",
      "Iteration 183, loss = 0.39616834\n",
      "Iteration 184, loss = 0.39605633\n",
      "Iteration 185, loss = 0.39608993\n",
      "Iteration 186, loss = 0.39577515\n",
      "Iteration 187, loss = 0.39559254\n",
      "Iteration 188, loss = 0.39536308\n",
      "Iteration 189, loss = 0.39518760\n",
      "Iteration 190, loss = 0.39488967\n",
      "Iteration 191, loss = 0.39476742\n",
      "Iteration 192, loss = 0.39457171\n",
      "Iteration 193, loss = 0.39424243\n",
      "Iteration 194, loss = 0.39429470\n",
      "Iteration 195, loss = 0.39407928\n",
      "Iteration 196, loss = 0.39376293\n",
      "Iteration 197, loss = 0.39382683\n",
      "Iteration 198, loss = 0.39342070\n",
      "Iteration 199, loss = 0.39323305\n",
      "Iteration 200, loss = 0.39284125\n",
      "Iteration 1, loss = 0.76102436\n",
      "Iteration 2, loss = 0.71335411\n",
      "Iteration 3, loss = 0.67451152\n",
      "Iteration 4, loss = 0.64062042\n",
      "Iteration 5, loss = 0.61102829\n",
      "Iteration 6, loss = 0.58587360\n",
      "Iteration 7, loss = 0.56455997\n",
      "Iteration 8, loss = 0.54508098\n",
      "Iteration 9, loss = 0.52788750\n",
      "Iteration 10, loss = 0.51269622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.49983498\n",
      "Iteration 12, loss = 0.48803370\n",
      "Iteration 13, loss = 0.47794964\n",
      "Iteration 14, loss = 0.46857051\n",
      "Iteration 15, loss = 0.46098107\n",
      "Iteration 16, loss = 0.45426019\n",
      "Iteration 17, loss = 0.44868442\n",
      "Iteration 18, loss = 0.44412421\n",
      "Iteration 19, loss = 0.43938800\n",
      "Iteration 20, loss = 0.43599302\n",
      "Iteration 21, loss = 0.43282930\n",
      "Iteration 22, loss = 0.43023710\n",
      "Iteration 23, loss = 0.42779871\n",
      "Iteration 24, loss = 0.42597827\n",
      "Iteration 25, loss = 0.42452570\n",
      "Iteration 26, loss = 0.42305540\n",
      "Iteration 27, loss = 0.42188941\n",
      "Iteration 28, loss = 0.42106172\n",
      "Iteration 29, loss = 0.42018576\n",
      "Iteration 30, loss = 0.41939970\n",
      "Iteration 31, loss = 0.41898973\n",
      "Iteration 32, loss = 0.41850618\n",
      "Iteration 33, loss = 0.41824454\n",
      "Iteration 34, loss = 0.41761944\n",
      "Iteration 35, loss = 0.41721974\n",
      "Iteration 36, loss = 0.41709486\n",
      "Iteration 37, loss = 0.41677824\n",
      "Iteration 38, loss = 0.41642518\n",
      "Iteration 39, loss = 0.41621478\n",
      "Iteration 40, loss = 0.41614777\n",
      "Iteration 41, loss = 0.41585053\n",
      "Iteration 42, loss = 0.41570963\n",
      "Iteration 43, loss = 0.41550740\n",
      "Iteration 44, loss = 0.41543868\n",
      "Iteration 45, loss = 0.41513994\n",
      "Iteration 46, loss = 0.41499270\n",
      "Iteration 47, loss = 0.41481010\n",
      "Iteration 48, loss = 0.41471284\n",
      "Iteration 49, loss = 0.41456783\n",
      "Iteration 50, loss = 0.41438299\n",
      "Iteration 51, loss = 0.41426877\n",
      "Iteration 52, loss = 0.41420027\n",
      "Iteration 53, loss = 0.41399658\n",
      "Iteration 54, loss = 0.41384546\n",
      "Iteration 55, loss = 0.41371807\n",
      "Iteration 56, loss = 0.41358252\n",
      "Iteration 57, loss = 0.41351314\n",
      "Iteration 58, loss = 0.41345263\n",
      "Iteration 59, loss = 0.41320733\n",
      "Iteration 60, loss = 0.41305221\n",
      "Iteration 61, loss = 0.41300464\n",
      "Iteration 62, loss = 0.41284000\n",
      "Iteration 63, loss = 0.41279760\n",
      "Iteration 64, loss = 0.41274455\n",
      "Iteration 65, loss = 0.41253452\n",
      "Iteration 66, loss = 0.41241900\n",
      "Iteration 67, loss = 0.41226590\n",
      "Iteration 68, loss = 0.41209046\n",
      "Iteration 69, loss = 0.41197212\n",
      "Iteration 70, loss = 0.41194784\n",
      "Iteration 71, loss = 0.41175517\n",
      "Iteration 72, loss = 0.41162249\n",
      "Iteration 73, loss = 0.41140319\n",
      "Iteration 74, loss = 0.41138316\n",
      "Iteration 75, loss = 0.41121563\n",
      "Iteration 76, loss = 0.41111576\n",
      "Iteration 77, loss = 0.41098467\n",
      "Iteration 78, loss = 0.41081733\n",
      "Iteration 79, loss = 0.41065808\n",
      "Iteration 80, loss = 0.41057624\n",
      "Iteration 81, loss = 0.41043326\n",
      "Iteration 82, loss = 0.41029778\n",
      "Iteration 83, loss = 0.41015298\n",
      "Iteration 84, loss = 0.41006501\n",
      "Iteration 85, loss = 0.41005786\n",
      "Iteration 86, loss = 0.40975732\n",
      "Iteration 87, loss = 0.40985355\n",
      "Iteration 88, loss = 0.40962897\n",
      "Iteration 89, loss = 0.40949483\n",
      "Iteration 90, loss = 0.40944995\n",
      "Iteration 91, loss = 0.40917142\n",
      "Iteration 92, loss = 0.40930322\n",
      "Iteration 93, loss = 0.40898661\n",
      "Iteration 94, loss = 0.40892783\n",
      "Iteration 95, loss = 0.40865459\n",
      "Iteration 96, loss = 0.40854532\n",
      "Iteration 97, loss = 0.40853809\n",
      "Iteration 98, loss = 0.40828402\n",
      "Iteration 99, loss = 0.40822588\n",
      "Iteration 100, loss = 0.40792899\n",
      "Iteration 101, loss = 0.40814429\n",
      "Iteration 102, loss = 0.40788205\n",
      "Iteration 103, loss = 0.40764461\n",
      "Iteration 104, loss = 0.40762197\n",
      "Iteration 105, loss = 0.40733141\n",
      "Iteration 106, loss = 0.40726021\n",
      "Iteration 107, loss = 0.40716662\n",
      "Iteration 108, loss = 0.40712575\n",
      "Iteration 109, loss = 0.40699225\n",
      "Iteration 110, loss = 0.40677219\n",
      "Iteration 111, loss = 0.40652090\n",
      "Iteration 112, loss = 0.40646590\n",
      "Iteration 113, loss = 0.40630549\n",
      "Iteration 114, loss = 0.40622192\n",
      "Iteration 115, loss = 0.40607122\n",
      "Iteration 116, loss = 0.40585877\n",
      "Iteration 117, loss = 0.40573333\n",
      "Iteration 118, loss = 0.40569124\n",
      "Iteration 119, loss = 0.40564947\n",
      "Iteration 120, loss = 0.40541553\n",
      "Iteration 121, loss = 0.40531167\n",
      "Iteration 122, loss = 0.40515101\n",
      "Iteration 123, loss = 0.40489635\n",
      "Iteration 124, loss = 0.40485459\n",
      "Iteration 125, loss = 0.40474521\n",
      "Iteration 126, loss = 0.40456328\n",
      "Iteration 127, loss = 0.40443766\n",
      "Iteration 128, loss = 0.40424042\n",
      "Iteration 129, loss = 0.40412165\n",
      "Iteration 130, loss = 0.40404792\n",
      "Iteration 131, loss = 0.40376369\n",
      "Iteration 132, loss = 0.40371793\n",
      "Iteration 133, loss = 0.40359327\n",
      "Iteration 134, loss = 0.40341580\n",
      "Iteration 135, loss = 0.40322219\n",
      "Iteration 136, loss = 0.40315471\n",
      "Iteration 137, loss = 0.40323960\n",
      "Iteration 138, loss = 0.40293374\n",
      "Iteration 139, loss = 0.40279060\n",
      "Iteration 140, loss = 0.40258692\n",
      "Iteration 141, loss = 0.40245217\n",
      "Iteration 142, loss = 0.40218473\n",
      "Iteration 143, loss = 0.40198690\n",
      "Iteration 144, loss = 0.40199576\n",
      "Iteration 145, loss = 0.40187959\n",
      "Iteration 146, loss = 0.40178983\n",
      "Iteration 147, loss = 0.40145368\n",
      "Iteration 148, loss = 0.40140069\n",
      "Iteration 149, loss = 0.40112473\n",
      "Iteration 150, loss = 0.40097329\n",
      "Iteration 151, loss = 0.40081381\n",
      "Iteration 152, loss = 0.40085767\n",
      "Iteration 153, loss = 0.40095184\n",
      "Iteration 154, loss = 0.40038200\n",
      "Iteration 155, loss = 0.40018917\n",
      "Iteration 156, loss = 0.40012085\n",
      "Iteration 157, loss = 0.39988289\n",
      "Iteration 158, loss = 0.39973022\n",
      "Iteration 159, loss = 0.39955797\n",
      "Iteration 160, loss = 0.39948520\n",
      "Iteration 161, loss = 0.39935282\n",
      "Iteration 162, loss = 0.39921920\n",
      "Iteration 163, loss = 0.39900839\n",
      "Iteration 164, loss = 0.39882993\n",
      "Iteration 165, loss = 0.39858549\n",
      "Iteration 166, loss = 0.39849654\n",
      "Iteration 167, loss = 0.39824771\n",
      "Iteration 168, loss = 0.39826056\n",
      "Iteration 169, loss = 0.39800498\n",
      "Iteration 170, loss = 0.39790323\n",
      "Iteration 171, loss = 0.39758832\n",
      "Iteration 172, loss = 0.39749548\n",
      "Iteration 173, loss = 0.39751508\n",
      "Iteration 174, loss = 0.39719146\n",
      "Iteration 175, loss = 0.39710810\n",
      "Iteration 176, loss = 0.39681962\n",
      "Iteration 177, loss = 0.39665741\n",
      "Iteration 178, loss = 0.39652653\n",
      "Iteration 179, loss = 0.39635879\n",
      "Iteration 180, loss = 0.39606952\n",
      "Iteration 181, loss = 0.39590618\n",
      "Iteration 182, loss = 0.39574800\n",
      "Iteration 183, loss = 0.39550628\n",
      "Iteration 184, loss = 0.39534282\n",
      "Iteration 185, loss = 0.39527984\n",
      "Iteration 186, loss = 0.39516828\n",
      "Iteration 187, loss = 0.39481595\n",
      "Iteration 188, loss = 0.39474065\n",
      "Iteration 189, loss = 0.39458181\n",
      "Iteration 190, loss = 0.39441590\n",
      "Iteration 191, loss = 0.39409652\n",
      "Iteration 192, loss = 0.39416295\n",
      "Iteration 193, loss = 0.39365362\n",
      "Iteration 194, loss = 0.39366385\n",
      "Iteration 195, loss = 0.39363155\n",
      "Iteration 196, loss = 0.39316063\n",
      "Iteration 197, loss = 0.39308658\n",
      "Iteration 198, loss = 0.39293463\n",
      "Iteration 199, loss = 0.39281936\n",
      "Iteration 200, loss = 0.39240663\n",
      "Iteration 1, loss = 0.75382718\n",
      "Iteration 2, loss = 0.70743286\n",
      "Iteration 3, loss = 0.66802348\n",
      "Iteration 4, loss = 0.63522383\n",
      "Iteration 5, loss = 0.60695324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.58315092\n",
      "Iteration 7, loss = 0.56371555\n",
      "Iteration 8, loss = 0.54583122\n",
      "Iteration 9, loss = 0.53066666\n",
      "Iteration 10, loss = 0.51725930\n",
      "Iteration 11, loss = 0.50589179\n",
      "Iteration 12, loss = 0.49618613\n",
      "Iteration 13, loss = 0.48763302\n",
      "Iteration 14, loss = 0.47971103\n",
      "Iteration 15, loss = 0.47364253\n",
      "Iteration 16, loss = 0.46813717\n",
      "Iteration 17, loss = 0.46421689\n",
      "Iteration 18, loss = 0.46018446\n",
      "Iteration 19, loss = 0.45667082\n",
      "Iteration 20, loss = 0.45445703\n",
      "Iteration 21, loss = 0.45229262\n",
      "Iteration 22, loss = 0.45026142\n",
      "Iteration 23, loss = 0.44878613\n",
      "Iteration 24, loss = 0.44760618\n",
      "Iteration 25, loss = 0.44659882\n",
      "Iteration 26, loss = 0.44567328\n",
      "Iteration 27, loss = 0.44498689\n",
      "Iteration 28, loss = 0.44455524\n",
      "Iteration 29, loss = 0.44395894\n",
      "Iteration 30, loss = 0.44347360\n",
      "Iteration 31, loss = 0.44317179\n",
      "Iteration 32, loss = 0.44289712\n",
      "Iteration 33, loss = 0.44266709\n",
      "Iteration 34, loss = 0.44232369\n",
      "Iteration 35, loss = 0.44201472\n",
      "Iteration 36, loss = 0.44193197\n",
      "Iteration 37, loss = 0.44169894\n",
      "Iteration 38, loss = 0.44142549\n",
      "Iteration 39, loss = 0.44121002\n",
      "Iteration 40, loss = 0.44116173\n",
      "Iteration 41, loss = 0.44090651\n",
      "Iteration 42, loss = 0.44076208\n",
      "Iteration 43, loss = 0.44054611\n",
      "Iteration 44, loss = 0.44043736\n",
      "Iteration 45, loss = 0.44022338\n",
      "Iteration 46, loss = 0.44007866\n",
      "Iteration 47, loss = 0.43985162\n",
      "Iteration 48, loss = 0.43972896\n",
      "Iteration 49, loss = 0.43961625\n",
      "Iteration 50, loss = 0.43953799\n",
      "Iteration 51, loss = 0.43928785\n",
      "Iteration 52, loss = 0.43915728\n",
      "Iteration 53, loss = 0.43903478\n",
      "Iteration 54, loss = 0.43878319\n",
      "Iteration 55, loss = 0.43867276\n",
      "Iteration 56, loss = 0.43842819\n",
      "Iteration 57, loss = 0.43832426\n",
      "Iteration 58, loss = 0.43848232\n",
      "Iteration 59, loss = 0.43802558\n",
      "Iteration 60, loss = 0.43786225\n",
      "Iteration 61, loss = 0.43775897\n",
      "Iteration 62, loss = 0.43753364\n",
      "Iteration 63, loss = 0.43751815\n",
      "Iteration 64, loss = 0.43736998\n",
      "Iteration 65, loss = 0.43724607\n",
      "Iteration 66, loss = 0.43704970\n",
      "Iteration 67, loss = 0.43691650\n",
      "Iteration 68, loss = 0.43678073\n",
      "Iteration 69, loss = 0.43660968\n",
      "Iteration 70, loss = 0.43658182\n",
      "Iteration 71, loss = 0.43626531\n",
      "Iteration 72, loss = 0.43615802\n",
      "Iteration 73, loss = 0.43592539\n",
      "Iteration 74, loss = 0.43586560\n",
      "Iteration 75, loss = 0.43559441\n",
      "Iteration 76, loss = 0.43552899\n",
      "Iteration 77, loss = 0.43544025\n",
      "Iteration 78, loss = 0.43534089\n",
      "Iteration 79, loss = 0.43501951\n",
      "Iteration 80, loss = 0.43491436\n",
      "Iteration 81, loss = 0.43476665\n",
      "Iteration 82, loss = 0.43456961\n",
      "Iteration 83, loss = 0.43445509\n",
      "Iteration 84, loss = 0.43435360\n",
      "Iteration 85, loss = 0.43420282\n",
      "Iteration 86, loss = 0.43397383\n",
      "Iteration 87, loss = 0.43430854\n",
      "Iteration 88, loss = 0.43384755\n",
      "Iteration 89, loss = 0.43358130\n",
      "Iteration 90, loss = 0.43338538\n",
      "Iteration 91, loss = 0.43323135\n",
      "Iteration 92, loss = 0.43324278\n",
      "Iteration 93, loss = 0.43291606\n",
      "Iteration 94, loss = 0.43281772\n",
      "Iteration 95, loss = 0.43261224\n",
      "Iteration 96, loss = 0.43250911\n",
      "Iteration 97, loss = 0.43239961\n",
      "Iteration 98, loss = 0.43211788\n",
      "Iteration 99, loss = 0.43199669\n",
      "Iteration 100, loss = 0.43172676\n",
      "Iteration 101, loss = 0.43202926\n",
      "Iteration 102, loss = 0.43158905\n",
      "Iteration 103, loss = 0.43149269\n",
      "Iteration 104, loss = 0.43126749\n",
      "Iteration 105, loss = 0.43109312\n",
      "Iteration 106, loss = 0.43084200\n",
      "Iteration 107, loss = 0.43080334\n",
      "Iteration 108, loss = 0.43070563\n",
      "Iteration 109, loss = 0.43058456\n",
      "Iteration 110, loss = 0.43039549\n",
      "Iteration 111, loss = 0.43001416\n",
      "Iteration 112, loss = 0.42999308\n",
      "Iteration 113, loss = 0.42968666\n",
      "Iteration 114, loss = 0.42953935\n",
      "Iteration 115, loss = 0.42949302\n",
      "Iteration 116, loss = 0.42929090\n",
      "Iteration 117, loss = 0.42908622\n",
      "Iteration 118, loss = 0.42896063\n",
      "Iteration 119, loss = 0.42892168\n",
      "Iteration 120, loss = 0.42881967\n",
      "Iteration 121, loss = 0.42843051\n",
      "Iteration 122, loss = 0.42831035\n",
      "Iteration 123, loss = 0.42813107\n",
      "Iteration 124, loss = 0.42805877\n",
      "Iteration 125, loss = 0.42778593\n",
      "Iteration 126, loss = 0.42785842\n",
      "Iteration 127, loss = 0.42738053\n",
      "Iteration 128, loss = 0.42727680\n",
      "Iteration 129, loss = 0.42701959\n",
      "Iteration 130, loss = 0.42691241\n",
      "Iteration 131, loss = 0.42672358\n",
      "Iteration 132, loss = 0.42654947\n",
      "Iteration 133, loss = 0.42651794\n",
      "Iteration 134, loss = 0.42619688\n",
      "Iteration 135, loss = 0.42609944\n",
      "Iteration 136, loss = 0.42591153\n",
      "Iteration 137, loss = 0.42578625\n",
      "Iteration 138, loss = 0.42535572\n",
      "Iteration 139, loss = 0.42545674\n",
      "Iteration 140, loss = 0.42520423\n",
      "Iteration 141, loss = 0.42503006\n",
      "Iteration 142, loss = 0.42477472\n",
      "Iteration 143, loss = 0.42468699\n",
      "Iteration 144, loss = 0.42451763\n",
      "Iteration 145, loss = 0.42429025\n",
      "Iteration 146, loss = 0.42401538\n",
      "Iteration 147, loss = 0.42395355\n",
      "Iteration 148, loss = 0.42369395\n",
      "Iteration 149, loss = 0.42345163\n",
      "Iteration 150, loss = 0.42335095\n",
      "Iteration 151, loss = 0.42313469\n",
      "Iteration 152, loss = 0.42307235\n",
      "Iteration 153, loss = 0.42312245\n",
      "Iteration 154, loss = 0.42255025\n",
      "Iteration 155, loss = 0.42257382\n",
      "Iteration 156, loss = 0.42216456\n",
      "Iteration 157, loss = 0.42203577\n",
      "Iteration 158, loss = 0.42190749\n",
      "Iteration 159, loss = 0.42164021\n",
      "Iteration 160, loss = 0.42144183\n",
      "Iteration 161, loss = 0.42119740\n",
      "Iteration 162, loss = 0.42098894\n",
      "Iteration 163, loss = 0.42087499\n",
      "Iteration 164, loss = 0.42070097\n",
      "Iteration 165, loss = 0.42043641\n",
      "Iteration 166, loss = 0.42036191\n",
      "Iteration 167, loss = 0.42011349\n",
      "Iteration 168, loss = 0.41992644\n",
      "Iteration 169, loss = 0.41977867\n",
      "Iteration 170, loss = 0.41959795\n",
      "Iteration 171, loss = 0.41921472\n",
      "Iteration 172, loss = 0.41905788\n",
      "Iteration 173, loss = 0.41892064\n",
      "Iteration 174, loss = 0.41879428\n",
      "Iteration 175, loss = 0.41864563\n",
      "Iteration 176, loss = 0.41839427\n",
      "Iteration 177, loss = 0.41810058\n",
      "Iteration 178, loss = 0.41786330\n",
      "Iteration 179, loss = 0.41776002\n",
      "Iteration 180, loss = 0.41751320\n",
      "Iteration 181, loss = 0.41724391\n",
      "Iteration 182, loss = 0.41709927\n",
      "Iteration 183, loss = 0.41695656\n",
      "Iteration 184, loss = 0.41684701\n",
      "Iteration 185, loss = 0.41669832\n",
      "Iteration 186, loss = 0.41639987\n",
      "Iteration 187, loss = 0.41610122\n",
      "Iteration 188, loss = 0.41586525\n",
      "Iteration 189, loss = 0.41568176\n",
      "Iteration 190, loss = 0.41542027\n",
      "Iteration 191, loss = 0.41526914\n",
      "Iteration 192, loss = 0.41522171\n",
      "Iteration 193, loss = 0.41476634\n",
      "Iteration 194, loss = 0.41470564\n",
      "Iteration 195, loss = 0.41464469\n",
      "Iteration 196, loss = 0.41418691\n",
      "Iteration 197, loss = 0.41427780\n",
      "Iteration 198, loss = 0.41398492\n",
      "Iteration 199, loss = 0.41385719\n",
      "Iteration 200, loss = 0.41367029\n",
      "Iteration 1, loss = 0.75059292\n",
      "Iteration 2, loss = 0.70184745\n",
      "Iteration 3, loss = 0.65716344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.62133757\n",
      "Iteration 5, loss = 0.59153819\n",
      "Iteration 6, loss = 0.56580294\n",
      "Iteration 7, loss = 0.54352046\n",
      "Iteration 8, loss = 0.52573286\n",
      "Iteration 9, loss = 0.50905513\n",
      "Iteration 10, loss = 0.49546824\n",
      "Iteration 11, loss = 0.48334143\n",
      "Iteration 12, loss = 0.47311883\n",
      "Iteration 13, loss = 0.46300977\n",
      "Iteration 14, loss = 0.45514014\n",
      "Iteration 15, loss = 0.44805040\n",
      "Iteration 16, loss = 0.44178632\n",
      "Iteration 17, loss = 0.43626612\n",
      "Iteration 18, loss = 0.43241703\n",
      "Iteration 19, loss = 0.42835214\n",
      "Iteration 20, loss = 0.42506111\n",
      "Iteration 21, loss = 0.42248803\n",
      "Iteration 22, loss = 0.41993962\n",
      "Iteration 23, loss = 0.41837448\n",
      "Iteration 24, loss = 0.41671374\n",
      "Iteration 25, loss = 0.41560749\n",
      "Iteration 26, loss = 0.41454719\n",
      "Iteration 27, loss = 0.41358117\n",
      "Iteration 28, loss = 0.41289907\n",
      "Iteration 29, loss = 0.41227597\n",
      "Iteration 30, loss = 0.41173090\n",
      "Iteration 31, loss = 0.41128414\n",
      "Iteration 32, loss = 0.41106439\n",
      "Iteration 33, loss = 0.41070328\n",
      "Iteration 34, loss = 0.41036315\n",
      "Iteration 35, loss = 0.41015216\n",
      "Iteration 36, loss = 0.40996056\n",
      "Iteration 37, loss = 0.40971396\n",
      "Iteration 38, loss = 0.40953461\n",
      "Iteration 39, loss = 0.40935771\n",
      "Iteration 40, loss = 0.40923881\n",
      "Iteration 41, loss = 0.40908979\n",
      "Iteration 42, loss = 0.40900300\n",
      "Iteration 43, loss = 0.40889592\n",
      "Iteration 44, loss = 0.40872436\n",
      "Iteration 45, loss = 0.40857047\n",
      "Iteration 46, loss = 0.40848522\n",
      "Iteration 47, loss = 0.40827137\n",
      "Iteration 48, loss = 0.40813303\n",
      "Iteration 49, loss = 0.40799420\n",
      "Iteration 50, loss = 0.40786196\n",
      "Iteration 51, loss = 0.40780213\n",
      "Iteration 52, loss = 0.40766718\n",
      "Iteration 53, loss = 0.40745064\n",
      "Iteration 54, loss = 0.40741237\n",
      "Iteration 55, loss = 0.40725134\n",
      "Iteration 56, loss = 0.40713213\n",
      "Iteration 57, loss = 0.40708870\n",
      "Iteration 58, loss = 0.40700607\n",
      "Iteration 59, loss = 0.40669770\n",
      "Iteration 60, loss = 0.40657766\n",
      "Iteration 61, loss = 0.40666496\n",
      "Iteration 62, loss = 0.40670356\n",
      "Iteration 63, loss = 0.40640216\n",
      "Iteration 64, loss = 0.40620974\n",
      "Iteration 65, loss = 0.40592993\n",
      "Iteration 66, loss = 0.40590083\n",
      "Iteration 67, loss = 0.40568441\n",
      "Iteration 68, loss = 0.40573166\n",
      "Iteration 69, loss = 0.40545280\n",
      "Iteration 70, loss = 0.40539361\n",
      "Iteration 71, loss = 0.40521793\n",
      "Iteration 72, loss = 0.40512897\n",
      "Iteration 73, loss = 0.40495544\n",
      "Iteration 74, loss = 0.40484839\n",
      "Iteration 75, loss = 0.40473130\n",
      "Iteration 76, loss = 0.40466782\n",
      "Iteration 77, loss = 0.40445506\n",
      "Iteration 78, loss = 0.40443436\n",
      "Iteration 79, loss = 0.40416575\n",
      "Iteration 80, loss = 0.40410753\n",
      "Iteration 81, loss = 0.40400505\n",
      "Iteration 82, loss = 0.40382269\n",
      "Iteration 83, loss = 0.40384427\n",
      "Iteration 84, loss = 0.40362772\n",
      "Iteration 85, loss = 0.40344172\n",
      "Iteration 86, loss = 0.40348542\n",
      "Iteration 87, loss = 0.40319686\n",
      "Iteration 88, loss = 0.40320625\n",
      "Iteration 89, loss = 0.40291579\n",
      "Iteration 90, loss = 0.40283460\n",
      "Iteration 91, loss = 0.40278561\n",
      "Iteration 92, loss = 0.40261028\n",
      "Iteration 93, loss = 0.40241362\n",
      "Iteration 94, loss = 0.40236259\n",
      "Iteration 95, loss = 0.40210939\n",
      "Iteration 96, loss = 0.40231137\n",
      "Iteration 97, loss = 0.40187744\n",
      "Iteration 98, loss = 0.40178697\n",
      "Iteration 99, loss = 0.40161956\n",
      "Iteration 100, loss = 0.40163375\n",
      "Iteration 101, loss = 0.40136778\n",
      "Iteration 102, loss = 0.40126581\n",
      "Iteration 103, loss = 0.40104424\n",
      "Iteration 104, loss = 0.40101201\n",
      "Iteration 105, loss = 0.40079015\n",
      "Iteration 106, loss = 0.40068525\n",
      "Iteration 107, loss = 0.40057219\n",
      "Iteration 108, loss = 0.40058874\n",
      "Iteration 109, loss = 0.40027260\n",
      "Iteration 110, loss = 0.40022767\n",
      "Iteration 111, loss = 0.40002575\n",
      "Iteration 112, loss = 0.39980005\n",
      "Iteration 113, loss = 0.39977789\n",
      "Iteration 114, loss = 0.39951966\n",
      "Iteration 115, loss = 0.39943887\n",
      "Iteration 116, loss = 0.39931250\n",
      "Iteration 117, loss = 0.39909825\n",
      "Iteration 118, loss = 0.39903808\n",
      "Iteration 119, loss = 0.39887401\n",
      "Iteration 120, loss = 0.39870105\n",
      "Iteration 121, loss = 0.39868033\n",
      "Iteration 122, loss = 0.39841027\n",
      "Iteration 123, loss = 0.39837736\n",
      "Iteration 124, loss = 0.39833546\n",
      "Iteration 125, loss = 0.39812387\n",
      "Iteration 126, loss = 0.39786190\n",
      "Iteration 127, loss = 0.39771464\n",
      "Iteration 128, loss = 0.39756186\n",
      "Iteration 129, loss = 0.39740183\n",
      "Iteration 130, loss = 0.39725039\n",
      "Iteration 131, loss = 0.39725399\n",
      "Iteration 132, loss = 0.39693910\n",
      "Iteration 133, loss = 0.39682664\n",
      "Iteration 134, loss = 0.39668545\n",
      "Iteration 135, loss = 0.39648004\n",
      "Iteration 136, loss = 0.39631267\n",
      "Iteration 137, loss = 0.39630767\n",
      "Iteration 138, loss = 0.39601576\n",
      "Iteration 139, loss = 0.39605706\n",
      "Iteration 140, loss = 0.39589325\n",
      "Iteration 141, loss = 0.39562711\n",
      "Iteration 142, loss = 0.39543102\n",
      "Iteration 143, loss = 0.39533772\n",
      "Iteration 144, loss = 0.39511952\n",
      "Iteration 145, loss = 0.39524914\n",
      "Iteration 146, loss = 0.39495707\n",
      "Iteration 147, loss = 0.39468419\n",
      "Iteration 148, loss = 0.39467833\n",
      "Iteration 149, loss = 0.39443101\n",
      "Iteration 150, loss = 0.39421127\n",
      "Iteration 151, loss = 0.39418961\n",
      "Iteration 152, loss = 0.39389510\n",
      "Iteration 153, loss = 0.39376273\n",
      "Iteration 154, loss = 0.39369820\n",
      "Iteration 155, loss = 0.39387342\n",
      "Iteration 156, loss = 0.39320666\n",
      "Iteration 157, loss = 0.39306256\n",
      "Iteration 158, loss = 0.39297124\n",
      "Iteration 159, loss = 0.39276506\n",
      "Iteration 160, loss = 0.39264430\n",
      "Iteration 161, loss = 0.39239835\n",
      "Iteration 162, loss = 0.39240270\n",
      "Iteration 163, loss = 0.39213942\n",
      "Iteration 164, loss = 0.39194739\n",
      "Iteration 165, loss = 0.39191042\n",
      "Iteration 166, loss = 0.39168426\n",
      "Iteration 167, loss = 0.39153029\n",
      "Iteration 168, loss = 0.39129240\n",
      "Iteration 169, loss = 0.39125707\n",
      "Iteration 170, loss = 0.39093374\n",
      "Iteration 171, loss = 0.39072750\n",
      "Iteration 172, loss = 0.39070049\n",
      "Iteration 173, loss = 0.39057763\n",
      "Iteration 174, loss = 0.39027591\n",
      "Iteration 175, loss = 0.39003191\n",
      "Iteration 176, loss = 0.39032611\n",
      "Iteration 177, loss = 0.38989989\n",
      "Iteration 178, loss = 0.38967465\n",
      "Iteration 179, loss = 0.38954396\n",
      "Iteration 180, loss = 0.38925380\n",
      "Iteration 181, loss = 0.38926404\n",
      "Iteration 182, loss = 0.38893720\n",
      "Iteration 183, loss = 0.38888388\n",
      "Iteration 184, loss = 0.38863907\n",
      "Iteration 185, loss = 0.38855501\n",
      "Iteration 186, loss = 0.38827708\n",
      "Iteration 187, loss = 0.38826761\n",
      "Iteration 188, loss = 0.38790434\n",
      "Iteration 189, loss = 0.38772475\n",
      "Iteration 190, loss = 0.38769920\n",
      "Iteration 191, loss = 0.38730169\n",
      "Iteration 192, loss = 0.38727553\n",
      "Iteration 193, loss = 0.38706093\n",
      "Iteration 194, loss = 0.38702514\n",
      "Iteration 195, loss = 0.38688493\n",
      "Iteration 196, loss = 0.38656187\n",
      "Iteration 197, loss = 0.38623348\n",
      "Iteration 198, loss = 0.38621157\n",
      "Iteration 199, loss = 0.38599770\n",
      "Iteration 200, loss = 0.38585848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75772720\n",
      "Iteration 2, loss = 0.71111276\n",
      "Iteration 3, loss = 0.66849205\n",
      "Iteration 4, loss = 0.63485268\n",
      "Iteration 5, loss = 0.60620760\n",
      "Iteration 6, loss = 0.58165470\n",
      "Iteration 7, loss = 0.55980467\n",
      "Iteration 8, loss = 0.54305369\n",
      "Iteration 9, loss = 0.52715031\n",
      "Iteration 10, loss = 0.51419652\n",
      "Iteration 11, loss = 0.50242882\n",
      "Iteration 12, loss = 0.49262484\n",
      "Iteration 13, loss = 0.48321740\n",
      "Iteration 14, loss = 0.47585836\n",
      "Iteration 15, loss = 0.46904438\n",
      "Iteration 16, loss = 0.46329667\n",
      "Iteration 17, loss = 0.45826438\n",
      "Iteration 18, loss = 0.45457110\n",
      "Iteration 19, loss = 0.45086441\n",
      "Iteration 20, loss = 0.44770144\n",
      "Iteration 21, loss = 0.44522360\n",
      "Iteration 22, loss = 0.44299830\n",
      "Iteration 23, loss = 0.44167527\n",
      "Iteration 24, loss = 0.44007098\n",
      "Iteration 25, loss = 0.43909941\n",
      "Iteration 26, loss = 0.43794303\n",
      "Iteration 27, loss = 0.43724547\n",
      "Iteration 28, loss = 0.43659431\n",
      "Iteration 29, loss = 0.43582324\n",
      "Iteration 30, loss = 0.43546747\n",
      "Iteration 31, loss = 0.43488457\n",
      "Iteration 32, loss = 0.43472209\n",
      "Iteration 33, loss = 0.43431851\n",
      "Iteration 34, loss = 0.43394675\n",
      "Iteration 35, loss = 0.43367762\n",
      "Iteration 36, loss = 0.43352873\n",
      "Iteration 37, loss = 0.43322127\n",
      "Iteration 38, loss = 0.43303880\n",
      "Iteration 39, loss = 0.43285134\n",
      "Iteration 40, loss = 0.43278453\n",
      "Iteration 41, loss = 0.43248680\n",
      "Iteration 42, loss = 0.43240912\n",
      "Iteration 43, loss = 0.43227169\n",
      "Iteration 44, loss = 0.43214654\n",
      "Iteration 45, loss = 0.43191777\n",
      "Iteration 46, loss = 0.43179789\n",
      "Iteration 47, loss = 0.43163286\n",
      "Iteration 48, loss = 0.43154574\n",
      "Iteration 49, loss = 0.43143726\n",
      "Iteration 50, loss = 0.43131685\n",
      "Iteration 51, loss = 0.43117687\n",
      "Iteration 52, loss = 0.43099847\n",
      "Iteration 53, loss = 0.43089509\n",
      "Iteration 54, loss = 0.43079904\n",
      "Iteration 55, loss = 0.43063469\n",
      "Iteration 56, loss = 0.43054376\n",
      "Iteration 57, loss = 0.43040506\n",
      "Iteration 58, loss = 0.43029641\n",
      "Iteration 59, loss = 0.43014416\n",
      "Iteration 60, loss = 0.43007248\n",
      "Iteration 61, loss = 0.43002176\n",
      "Iteration 62, loss = 0.42984163\n",
      "Iteration 63, loss = 0.42966407\n",
      "Iteration 64, loss = 0.42962559\n",
      "Iteration 65, loss = 0.42940775\n",
      "Iteration 66, loss = 0.42937683\n",
      "Iteration 67, loss = 0.42913228\n",
      "Iteration 68, loss = 0.42919705\n",
      "Iteration 69, loss = 0.42904375\n",
      "Iteration 70, loss = 0.42886003\n",
      "Iteration 71, loss = 0.42870401\n",
      "Iteration 72, loss = 0.42852326\n",
      "Iteration 73, loss = 0.42853583\n",
      "Iteration 74, loss = 0.42840251\n",
      "Iteration 75, loss = 0.42821534\n",
      "Iteration 76, loss = 0.42813039\n",
      "Iteration 77, loss = 0.42795668\n",
      "Iteration 78, loss = 0.42785036\n",
      "Iteration 79, loss = 0.42767028\n",
      "Iteration 80, loss = 0.42756265\n",
      "Iteration 81, loss = 0.42766493\n",
      "Iteration 82, loss = 0.42731999\n",
      "Iteration 83, loss = 0.42749230\n",
      "Iteration 84, loss = 0.42713515\n",
      "Iteration 85, loss = 0.42691254\n",
      "Iteration 86, loss = 0.42681671\n",
      "Iteration 87, loss = 0.42673580\n",
      "Iteration 88, loss = 0.42665060\n",
      "Iteration 89, loss = 0.42645910\n",
      "Iteration 90, loss = 0.42635404\n",
      "Iteration 91, loss = 0.42633033\n",
      "Iteration 92, loss = 0.42608650\n",
      "Iteration 93, loss = 0.42591726\n",
      "Iteration 94, loss = 0.42615246\n",
      "Iteration 95, loss = 0.42577680\n",
      "Iteration 96, loss = 0.42565442\n",
      "Iteration 97, loss = 0.42552261\n",
      "Iteration 98, loss = 0.42555749\n",
      "Iteration 99, loss = 0.42525101\n",
      "Iteration 100, loss = 0.42510922\n",
      "Iteration 101, loss = 0.42505188\n",
      "Iteration 102, loss = 0.42489447\n",
      "Iteration 103, loss = 0.42479496\n",
      "Iteration 104, loss = 0.42463688\n",
      "Iteration 105, loss = 0.42466690\n",
      "Iteration 106, loss = 0.42435161\n",
      "Iteration 107, loss = 0.42418987\n",
      "Iteration 108, loss = 0.42410822\n",
      "Iteration 109, loss = 0.42398166\n",
      "Iteration 110, loss = 0.42387143\n",
      "Iteration 111, loss = 0.42377549\n",
      "Iteration 112, loss = 0.42351108\n",
      "Iteration 113, loss = 0.42352252\n",
      "Iteration 114, loss = 0.42327727\n",
      "Iteration 115, loss = 0.42319796\n",
      "Iteration 116, loss = 0.42322240\n",
      "Iteration 117, loss = 0.42294016\n",
      "Iteration 118, loss = 0.42283649\n",
      "Iteration 119, loss = 0.42273733\n",
      "Iteration 120, loss = 0.42250377\n",
      "Iteration 121, loss = 0.42252263\n",
      "Iteration 122, loss = 0.42230885\n",
      "Iteration 123, loss = 0.42214950\n",
      "Iteration 124, loss = 0.42212713\n",
      "Iteration 125, loss = 0.42187173\n",
      "Iteration 126, loss = 0.42179109\n",
      "Iteration 127, loss = 0.42157062\n",
      "Iteration 128, loss = 0.42136148\n",
      "Iteration 129, loss = 0.42144417\n",
      "Iteration 130, loss = 0.42121853\n",
      "Iteration 131, loss = 0.42108876\n",
      "Iteration 132, loss = 0.42088062\n",
      "Iteration 133, loss = 0.42084306\n",
      "Iteration 134, loss = 0.42063770\n",
      "Iteration 135, loss = 0.42052403\n",
      "Iteration 136, loss = 0.42031066\n",
      "Iteration 137, loss = 0.42027034\n",
      "Iteration 138, loss = 0.42002727\n",
      "Iteration 139, loss = 0.42032229\n",
      "Iteration 140, loss = 0.41984264\n",
      "Iteration 141, loss = 0.41986773\n",
      "Iteration 142, loss = 0.41950169\n",
      "Iteration 143, loss = 0.41935939\n",
      "Iteration 144, loss = 0.41925244\n",
      "Iteration 145, loss = 0.41914752\n",
      "Iteration 146, loss = 0.41908039\n",
      "Iteration 147, loss = 0.41886286\n",
      "Iteration 148, loss = 0.41885783\n",
      "Iteration 149, loss = 0.41854791\n",
      "Iteration 150, loss = 0.41830213\n",
      "Iteration 151, loss = 0.41831732\n",
      "Iteration 152, loss = 0.41810571\n",
      "Iteration 153, loss = 0.41786503\n",
      "Iteration 154, loss = 0.41797448\n",
      "Iteration 155, loss = 0.41813549\n",
      "Iteration 156, loss = 0.41755288\n",
      "Iteration 157, loss = 0.41732855\n",
      "Iteration 158, loss = 0.41728694\n",
      "Iteration 159, loss = 0.41732890\n",
      "Iteration 160, loss = 0.41687410\n",
      "Iteration 161, loss = 0.41676281\n",
      "Iteration 162, loss = 0.41676067\n",
      "Iteration 163, loss = 0.41654100\n",
      "Iteration 164, loss = 0.41626641\n",
      "Iteration 165, loss = 0.41619857\n",
      "Iteration 166, loss = 0.41608375\n",
      "Iteration 167, loss = 0.41597000\n",
      "Iteration 168, loss = 0.41569082\n",
      "Iteration 169, loss = 0.41555445\n",
      "Iteration 170, loss = 0.41544578\n",
      "Iteration 171, loss = 0.41538895\n",
      "Iteration 172, loss = 0.41511113\n",
      "Iteration 173, loss = 0.41508199\n",
      "Iteration 174, loss = 0.41485568\n",
      "Iteration 175, loss = 0.41476058\n",
      "Iteration 176, loss = 0.41469595\n",
      "Iteration 177, loss = 0.41438861\n",
      "Iteration 178, loss = 0.41431674\n",
      "Iteration 179, loss = 0.41404540\n",
      "Iteration 180, loss = 0.41396137\n",
      "Iteration 181, loss = 0.41385597\n",
      "Iteration 182, loss = 0.41348660\n",
      "Iteration 183, loss = 0.41336807\n",
      "Iteration 184, loss = 0.41326161\n",
      "Iteration 185, loss = 0.41313056\n",
      "Iteration 186, loss = 0.41290256\n",
      "Iteration 187, loss = 0.41309467\n",
      "Iteration 188, loss = 0.41270909\n",
      "Iteration 189, loss = 0.41251423\n",
      "Iteration 190, loss = 0.41234910\n",
      "Iteration 191, loss = 0.41209165\n",
      "Iteration 192, loss = 0.41204121\n",
      "Iteration 193, loss = 0.41181432\n",
      "Iteration 194, loss = 0.41170391\n",
      "Iteration 195, loss = 0.41167961\n",
      "Iteration 196, loss = 0.41138120\n",
      "Iteration 197, loss = 0.41112568\n",
      "Iteration 198, loss = 0.41100525\n",
      "Iteration 199, loss = 0.41086024\n",
      "Iteration 200, loss = 0.41071309\n",
      "Iteration 1, loss = 0.75865959\n",
      "Iteration 2, loss = 0.71214484\n",
      "Iteration 3, loss = 0.67224412\n",
      "Iteration 4, loss = 0.63864341\n",
      "Iteration 5, loss = 0.60980798\n",
      "Iteration 6, loss = 0.58404328\n",
      "Iteration 7, loss = 0.56345919\n",
      "Iteration 8, loss = 0.54448080\n",
      "Iteration 9, loss = 0.52756863\n",
      "Iteration 10, loss = 0.51275827\n",
      "Iteration 11, loss = 0.50002321\n",
      "Iteration 12, loss = 0.48848535\n",
      "Iteration 13, loss = 0.47861258\n",
      "Iteration 14, loss = 0.46955953\n",
      "Iteration 15, loss = 0.46199871\n",
      "Iteration 16, loss = 0.45572955\n",
      "Iteration 17, loss = 0.45012854\n",
      "Iteration 18, loss = 0.44589278\n",
      "Iteration 19, loss = 0.44169647\n",
      "Iteration 20, loss = 0.43786235\n",
      "Iteration 21, loss = 0.43522933\n",
      "Iteration 22, loss = 0.43271342\n",
      "Iteration 23, loss = 0.43051909\n",
      "Iteration 24, loss = 0.42891520\n",
      "Iteration 25, loss = 0.42752897\n",
      "Iteration 26, loss = 0.42625844\n",
      "Iteration 27, loss = 0.42509908\n",
      "Iteration 28, loss = 0.42441347\n",
      "Iteration 29, loss = 0.42347603\n",
      "Iteration 30, loss = 0.42292609\n",
      "Iteration 31, loss = 0.42245389\n",
      "Iteration 32, loss = 0.42193093\n",
      "Iteration 33, loss = 0.42178024\n",
      "Iteration 34, loss = 0.42114429\n",
      "Iteration 35, loss = 0.42092925\n",
      "Iteration 36, loss = 0.42050869\n",
      "Iteration 37, loss = 0.42035346\n",
      "Iteration 38, loss = 0.42002817\n",
      "Iteration 39, loss = 0.41968167\n",
      "Iteration 40, loss = 0.41958283\n",
      "Iteration 41, loss = 0.41930593\n",
      "Iteration 42, loss = 0.41915455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.41881942\n",
      "Iteration 44, loss = 0.41885287\n",
      "Iteration 45, loss = 0.41849888\n",
      "Iteration 46, loss = 0.41827489\n",
      "Iteration 47, loss = 0.41807344\n",
      "Iteration 48, loss = 0.41799669\n",
      "Iteration 49, loss = 0.41776699\n",
      "Iteration 50, loss = 0.41760460\n",
      "Iteration 51, loss = 0.41746991\n",
      "Iteration 52, loss = 0.41736076\n",
      "Iteration 53, loss = 0.41709812\n",
      "Iteration 54, loss = 0.41690918\n",
      "Iteration 55, loss = 0.41676422\n",
      "Iteration 56, loss = 0.41670458\n",
      "Iteration 57, loss = 0.41660115\n",
      "Iteration 58, loss = 0.41639004\n",
      "Iteration 59, loss = 0.41616684\n",
      "Iteration 60, loss = 0.41605952\n",
      "Iteration 61, loss = 0.41590226\n",
      "Iteration 62, loss = 0.41576894\n",
      "Iteration 63, loss = 0.41559902\n",
      "Iteration 64, loss = 0.41551457\n",
      "Iteration 65, loss = 0.41530132\n",
      "Iteration 66, loss = 0.41528207\n",
      "Iteration 67, loss = 0.41520629\n",
      "Iteration 68, loss = 0.41495387\n",
      "Iteration 69, loss = 0.41473393\n",
      "Iteration 70, loss = 0.41475459\n",
      "Iteration 71, loss = 0.41448119\n",
      "Iteration 72, loss = 0.41436171\n",
      "Iteration 73, loss = 0.41415833\n",
      "Iteration 74, loss = 0.41405887\n",
      "Iteration 75, loss = 0.41386615\n",
      "Iteration 76, loss = 0.41390762\n",
      "Iteration 77, loss = 0.41353429\n",
      "Iteration 78, loss = 0.41340892\n",
      "Iteration 79, loss = 0.41328340\n",
      "Iteration 80, loss = 0.41322916\n",
      "Iteration 81, loss = 0.41306464\n",
      "Iteration 82, loss = 0.41293769\n",
      "Iteration 83, loss = 0.41275377\n",
      "Iteration 84, loss = 0.41261530\n",
      "Iteration 85, loss = 0.41262062\n",
      "Iteration 86, loss = 0.41245983\n",
      "Iteration 87, loss = 0.41238019\n",
      "Iteration 88, loss = 0.41204083\n",
      "Iteration 89, loss = 0.41191674\n",
      "Iteration 90, loss = 0.41189693\n",
      "Iteration 91, loss = 0.41161724\n",
      "Iteration 92, loss = 0.41181743\n",
      "Iteration 93, loss = 0.41139296\n",
      "Iteration 94, loss = 0.41166251\n",
      "Iteration 95, loss = 0.41107104\n",
      "Iteration 96, loss = 0.41087366\n",
      "Iteration 97, loss = 0.41074931\n",
      "Iteration 98, loss = 0.41054922\n",
      "Iteration 99, loss = 0.41045729\n",
      "Iteration 100, loss = 0.41026632\n",
      "Iteration 101, loss = 0.41045669\n",
      "Iteration 102, loss = 0.41005176\n",
      "Iteration 103, loss = 0.40986807\n",
      "Iteration 104, loss = 0.40984960\n",
      "Iteration 105, loss = 0.40956852\n",
      "Iteration 106, loss = 0.40955114\n",
      "Iteration 107, loss = 0.40932804\n",
      "Iteration 108, loss = 0.40915710\n",
      "Iteration 109, loss = 0.40900818\n",
      "Iteration 110, loss = 0.40880567\n",
      "Iteration 111, loss = 0.40870247\n",
      "Iteration 112, loss = 0.40862605\n",
      "Iteration 113, loss = 0.40835429\n",
      "Iteration 114, loss = 0.40828029\n",
      "Iteration 115, loss = 0.40807801\n",
      "Iteration 116, loss = 0.40782977\n",
      "Iteration 117, loss = 0.40769630\n",
      "Iteration 118, loss = 0.40758541\n",
      "Iteration 119, loss = 0.40759303\n",
      "Iteration 120, loss = 0.40742691\n",
      "Iteration 121, loss = 0.40717396\n",
      "Iteration 122, loss = 0.40702262\n",
      "Iteration 123, loss = 0.40677253\n",
      "Iteration 124, loss = 0.40675077\n",
      "Iteration 125, loss = 0.40661389\n",
      "Iteration 126, loss = 0.40633744\n",
      "Iteration 127, loss = 0.40613358\n",
      "Iteration 128, loss = 0.40603164\n",
      "Iteration 129, loss = 0.40593190\n",
      "Iteration 130, loss = 0.40574995\n",
      "Iteration 131, loss = 0.40550932\n",
      "Iteration 132, loss = 0.40541663\n",
      "Iteration 133, loss = 0.40524162\n",
      "Iteration 134, loss = 0.40507897\n",
      "Iteration 135, loss = 0.40479294\n",
      "Iteration 136, loss = 0.40503561\n",
      "Iteration 137, loss = 0.40447259\n",
      "Iteration 138, loss = 0.40435879\n",
      "Iteration 139, loss = 0.40434966\n",
      "Iteration 140, loss = 0.40403578\n",
      "Iteration 141, loss = 0.40388470\n",
      "Iteration 142, loss = 0.40367003\n",
      "Iteration 143, loss = 0.40352050\n",
      "Iteration 144, loss = 0.40345474\n",
      "Iteration 145, loss = 0.40330483\n",
      "Iteration 146, loss = 0.40307257\n",
      "Iteration 147, loss = 0.40290916\n",
      "Iteration 148, loss = 0.40280581\n",
      "Iteration 149, loss = 0.40245784\n",
      "Iteration 150, loss = 0.40227895\n",
      "Iteration 151, loss = 0.40214118\n",
      "Iteration 152, loss = 0.40201643\n",
      "Iteration 153, loss = 0.40235507\n",
      "Iteration 154, loss = 0.40175516\n",
      "Iteration 155, loss = 0.40130160\n",
      "Iteration 156, loss = 0.40133567\n",
      "Iteration 157, loss = 0.40110679\n",
      "Iteration 158, loss = 0.40089910\n",
      "Iteration 159, loss = 0.40072932\n",
      "Iteration 160, loss = 0.40064071\n",
      "Iteration 161, loss = 0.40042354\n",
      "Iteration 162, loss = 0.40029104\n",
      "Iteration 163, loss = 0.40002585\n",
      "Iteration 164, loss = 0.39998499\n",
      "Iteration 165, loss = 0.39973259\n",
      "Iteration 166, loss = 0.39951411\n",
      "Iteration 167, loss = 0.39920095\n",
      "Iteration 168, loss = 0.39908826\n",
      "Iteration 169, loss = 0.39888727\n",
      "Iteration 170, loss = 0.39886172\n",
      "Iteration 171, loss = 0.39854787\n",
      "Iteration 172, loss = 0.39854881\n",
      "Iteration 173, loss = 0.39831702\n",
      "Iteration 174, loss = 0.39810382\n",
      "Iteration 175, loss = 0.39785006\n",
      "Iteration 176, loss = 0.39769132\n",
      "Iteration 177, loss = 0.39737927\n",
      "Iteration 178, loss = 0.39743937\n",
      "Iteration 179, loss = 0.39707433\n",
      "Iteration 180, loss = 0.39683849\n",
      "Iteration 181, loss = 0.39660326\n",
      "Iteration 182, loss = 0.39646719\n",
      "Iteration 183, loss = 0.39616732\n",
      "Iteration 184, loss = 0.39605530\n",
      "Iteration 185, loss = 0.39608889\n",
      "Iteration 186, loss = 0.39577411\n",
      "Iteration 187, loss = 0.39559149\n",
      "Iteration 188, loss = 0.39536203\n",
      "Iteration 189, loss = 0.39518655\n",
      "Iteration 190, loss = 0.39488861\n",
      "Iteration 191, loss = 0.39476636\n",
      "Iteration 192, loss = 0.39457063\n",
      "Iteration 193, loss = 0.39424135\n",
      "Iteration 194, loss = 0.39429361\n",
      "Iteration 195, loss = 0.39407819\n",
      "Iteration 196, loss = 0.39376183\n",
      "Iteration 197, loss = 0.39382573\n",
      "Iteration 198, loss = 0.39341959\n",
      "Iteration 199, loss = 0.39323194\n",
      "Iteration 200, loss = 0.39284013\n",
      "Iteration 1, loss = 0.76102383\n",
      "Iteration 2, loss = 0.71335358\n",
      "Iteration 3, loss = 0.67451099\n",
      "Iteration 4, loss = 0.64061988\n",
      "Iteration 5, loss = 0.61102775\n",
      "Iteration 6, loss = 0.58587306\n",
      "Iteration 7, loss = 0.56455943\n",
      "Iteration 8, loss = 0.54508044\n",
      "Iteration 9, loss = 0.52788695\n",
      "Iteration 10, loss = 0.51269567\n",
      "Iteration 11, loss = 0.49983443\n",
      "Iteration 12, loss = 0.48803315\n",
      "Iteration 13, loss = 0.47794910\n",
      "Iteration 14, loss = 0.46856997\n",
      "Iteration 15, loss = 0.46098052\n",
      "Iteration 16, loss = 0.45425964\n",
      "Iteration 17, loss = 0.44868387\n",
      "Iteration 18, loss = 0.44412366\n",
      "Iteration 19, loss = 0.43938745\n",
      "Iteration 20, loss = 0.43599246\n",
      "Iteration 21, loss = 0.43282874\n",
      "Iteration 22, loss = 0.43023655\n",
      "Iteration 23, loss = 0.42779815\n",
      "Iteration 24, loss = 0.42597771\n",
      "Iteration 25, loss = 0.42452514\n",
      "Iteration 26, loss = 0.42305483\n",
      "Iteration 27, loss = 0.42188885\n",
      "Iteration 28, loss = 0.42106116\n",
      "Iteration 29, loss = 0.42018520\n",
      "Iteration 30, loss = 0.41939913\n",
      "Iteration 31, loss = 0.41898916\n",
      "Iteration 32, loss = 0.41850561\n",
      "Iteration 33, loss = 0.41824397\n",
      "Iteration 34, loss = 0.41761887\n",
      "Iteration 35, loss = 0.41721918\n",
      "Iteration 36, loss = 0.41709429\n",
      "Iteration 37, loss = 0.41677767\n",
      "Iteration 38, loss = 0.41642462\n",
      "Iteration 39, loss = 0.41621421\n",
      "Iteration 40, loss = 0.41614720\n",
      "Iteration 41, loss = 0.41584996\n",
      "Iteration 42, loss = 0.41570905\n",
      "Iteration 43, loss = 0.41550683\n",
      "Iteration 44, loss = 0.41543810\n",
      "Iteration 45, loss = 0.41513937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.41499212\n",
      "Iteration 47, loss = 0.41480952\n",
      "Iteration 48, loss = 0.41471226\n",
      "Iteration 49, loss = 0.41456725\n",
      "Iteration 50, loss = 0.41438241\n",
      "Iteration 51, loss = 0.41426819\n",
      "Iteration 52, loss = 0.41419969\n",
      "Iteration 53, loss = 0.41399600\n",
      "Iteration 54, loss = 0.41384488\n",
      "Iteration 55, loss = 0.41371749\n",
      "Iteration 56, loss = 0.41358194\n",
      "Iteration 57, loss = 0.41351256\n",
      "Iteration 58, loss = 0.41345205\n",
      "Iteration 59, loss = 0.41320674\n",
      "Iteration 60, loss = 0.41305163\n",
      "Iteration 61, loss = 0.41300406\n",
      "Iteration 62, loss = 0.41283942\n",
      "Iteration 63, loss = 0.41279702\n",
      "Iteration 64, loss = 0.41274396\n",
      "Iteration 65, loss = 0.41253393\n",
      "Iteration 66, loss = 0.41241841\n",
      "Iteration 67, loss = 0.41226531\n",
      "Iteration 68, loss = 0.41208987\n",
      "Iteration 69, loss = 0.41197153\n",
      "Iteration 70, loss = 0.41194725\n",
      "Iteration 71, loss = 0.41175457\n",
      "Iteration 72, loss = 0.41162190\n",
      "Iteration 73, loss = 0.41140259\n",
      "Iteration 74, loss = 0.41138256\n",
      "Iteration 75, loss = 0.41121503\n",
      "Iteration 76, loss = 0.41111515\n",
      "Iteration 77, loss = 0.41098407\n",
      "Iteration 78, loss = 0.41081672\n",
      "Iteration 79, loss = 0.41065747\n",
      "Iteration 80, loss = 0.41057563\n",
      "Iteration 81, loss = 0.41043265\n",
      "Iteration 82, loss = 0.41029716\n",
      "Iteration 83, loss = 0.41015237\n",
      "Iteration 84, loss = 0.41006439\n",
      "Iteration 85, loss = 0.41005724\n",
      "Iteration 86, loss = 0.40975670\n",
      "Iteration 87, loss = 0.40985293\n",
      "Iteration 88, loss = 0.40962834\n",
      "Iteration 89, loss = 0.40949420\n",
      "Iteration 90, loss = 0.40944932\n",
      "Iteration 91, loss = 0.40917079\n",
      "Iteration 92, loss = 0.40930259\n",
      "Iteration 93, loss = 0.40898598\n",
      "Iteration 94, loss = 0.40892719\n",
      "Iteration 95, loss = 0.40865395\n",
      "Iteration 96, loss = 0.40854468\n",
      "Iteration 97, loss = 0.40853745\n",
      "Iteration 98, loss = 0.40828338\n",
      "Iteration 99, loss = 0.40822523\n",
      "Iteration 100, loss = 0.40792834\n",
      "Iteration 101, loss = 0.40814363\n",
      "Iteration 102, loss = 0.40788140\n",
      "Iteration 103, loss = 0.40764395\n",
      "Iteration 104, loss = 0.40762131\n",
      "Iteration 105, loss = 0.40733075\n",
      "Iteration 106, loss = 0.40725955\n",
      "Iteration 107, loss = 0.40716595\n",
      "Iteration 108, loss = 0.40712508\n",
      "Iteration 109, loss = 0.40699158\n",
      "Iteration 110, loss = 0.40677151\n",
      "Iteration 111, loss = 0.40652022\n",
      "Iteration 112, loss = 0.40646522\n",
      "Iteration 113, loss = 0.40630480\n",
      "Iteration 114, loss = 0.40622123\n",
      "Iteration 115, loss = 0.40607053\n",
      "Iteration 116, loss = 0.40585808\n",
      "Iteration 117, loss = 0.40573264\n",
      "Iteration 118, loss = 0.40569054\n",
      "Iteration 119, loss = 0.40564877\n",
      "Iteration 120, loss = 0.40541483\n",
      "Iteration 121, loss = 0.40531096\n",
      "Iteration 122, loss = 0.40515030\n",
      "Iteration 123, loss = 0.40489564\n",
      "Iteration 124, loss = 0.40485387\n",
      "Iteration 125, loss = 0.40474449\n",
      "Iteration 126, loss = 0.40456255\n",
      "Iteration 127, loss = 0.40443693\n",
      "Iteration 128, loss = 0.40423969\n",
      "Iteration 129, loss = 0.40412091\n",
      "Iteration 130, loss = 0.40404718\n",
      "Iteration 131, loss = 0.40376294\n",
      "Iteration 132, loss = 0.40371719\n",
      "Iteration 133, loss = 0.40359252\n",
      "Iteration 134, loss = 0.40341505\n",
      "Iteration 135, loss = 0.40322143\n",
      "Iteration 136, loss = 0.40315395\n",
      "Iteration 137, loss = 0.40323883\n",
      "Iteration 138, loss = 0.40293297\n",
      "Iteration 139, loss = 0.40278983\n",
      "Iteration 140, loss = 0.40258614\n",
      "Iteration 141, loss = 0.40245138\n",
      "Iteration 142, loss = 0.40218394\n",
      "Iteration 143, loss = 0.40198611\n",
      "Iteration 144, loss = 0.40199497\n",
      "Iteration 145, loss = 0.40187879\n",
      "Iteration 146, loss = 0.40178903\n",
      "Iteration 147, loss = 0.40145287\n",
      "Iteration 148, loss = 0.40139988\n",
      "Iteration 149, loss = 0.40112391\n",
      "Iteration 150, loss = 0.40097247\n",
      "Iteration 151, loss = 0.40081299\n",
      "Iteration 152, loss = 0.40085684\n",
      "Iteration 153, loss = 0.40095100\n",
      "Iteration 154, loss = 0.40038116\n",
      "Iteration 155, loss = 0.40018833\n",
      "Iteration 156, loss = 0.40012000\n",
      "Iteration 157, loss = 0.39988204\n",
      "Iteration 158, loss = 0.39972936\n",
      "Iteration 159, loss = 0.39955711\n",
      "Iteration 160, loss = 0.39948434\n",
      "Iteration 161, loss = 0.39935195\n",
      "Iteration 162, loss = 0.39921832\n",
      "Iteration 163, loss = 0.39900751\n",
      "Iteration 164, loss = 0.39882904\n",
      "Iteration 165, loss = 0.39858459\n",
      "Iteration 166, loss = 0.39849564\n",
      "Iteration 167, loss = 0.39824681\n",
      "Iteration 168, loss = 0.39825966\n",
      "Iteration 169, loss = 0.39800407\n",
      "Iteration 170, loss = 0.39790231\n",
      "Iteration 171, loss = 0.39758740\n",
      "Iteration 172, loss = 0.39749455\n",
      "Iteration 173, loss = 0.39751415\n",
      "Iteration 174, loss = 0.39719052\n",
      "Iteration 175, loss = 0.39710715\n",
      "Iteration 176, loss = 0.39681867\n",
      "Iteration 177, loss = 0.39665645\n",
      "Iteration 178, loss = 0.39652557\n",
      "Iteration 179, loss = 0.39635783\n",
      "Iteration 180, loss = 0.39606855\n",
      "Iteration 181, loss = 0.39590521\n",
      "Iteration 182, loss = 0.39574702\n",
      "Iteration 183, loss = 0.39550529\n",
      "Iteration 184, loss = 0.39534182\n",
      "Iteration 185, loss = 0.39527884\n",
      "Iteration 186, loss = 0.39516727\n",
      "Iteration 187, loss = 0.39481494\n",
      "Iteration 188, loss = 0.39473963\n",
      "Iteration 189, loss = 0.39458079\n",
      "Iteration 190, loss = 0.39441487\n",
      "Iteration 191, loss = 0.39409548\n",
      "Iteration 192, loss = 0.39416191\n",
      "Iteration 193, loss = 0.39365257\n",
      "Iteration 194, loss = 0.39366279\n",
      "Iteration 195, loss = 0.39363049\n",
      "Iteration 196, loss = 0.39315957\n",
      "Iteration 197, loss = 0.39308551\n",
      "Iteration 198, loss = 0.39293355\n",
      "Iteration 199, loss = 0.39281827\n",
      "Iteration 200, loss = 0.39240553\n",
      "Iteration 1, loss = 0.75382664\n",
      "Iteration 2, loss = 0.70743232\n",
      "Iteration 3, loss = 0.66802294\n",
      "Iteration 4, loss = 0.63522330\n",
      "Iteration 5, loss = 0.60695270\n",
      "Iteration 6, loss = 0.58315038\n",
      "Iteration 7, loss = 0.56371501\n",
      "Iteration 8, loss = 0.54583068\n",
      "Iteration 9, loss = 0.53066612\n",
      "Iteration 10, loss = 0.51725875\n",
      "Iteration 11, loss = 0.50589124\n",
      "Iteration 12, loss = 0.49618558\n",
      "Iteration 13, loss = 0.48763248\n",
      "Iteration 14, loss = 0.47971048\n",
      "Iteration 15, loss = 0.47364198\n",
      "Iteration 16, loss = 0.46813662\n",
      "Iteration 17, loss = 0.46421634\n",
      "Iteration 18, loss = 0.46018390\n",
      "Iteration 19, loss = 0.45667027\n",
      "Iteration 20, loss = 0.45445647\n",
      "Iteration 21, loss = 0.45229206\n",
      "Iteration 22, loss = 0.45026086\n",
      "Iteration 23, loss = 0.44878557\n",
      "Iteration 24, loss = 0.44760562\n",
      "Iteration 25, loss = 0.44659826\n",
      "Iteration 26, loss = 0.44567271\n",
      "Iteration 27, loss = 0.44498633\n",
      "Iteration 28, loss = 0.44455467\n",
      "Iteration 29, loss = 0.44395837\n",
      "Iteration 30, loss = 0.44347304\n",
      "Iteration 31, loss = 0.44317122\n",
      "Iteration 32, loss = 0.44289655\n",
      "Iteration 33, loss = 0.44266652\n",
      "Iteration 34, loss = 0.44232312\n",
      "Iteration 35, loss = 0.44201415\n",
      "Iteration 36, loss = 0.44193140\n",
      "Iteration 37, loss = 0.44169837\n",
      "Iteration 38, loss = 0.44142491\n",
      "Iteration 39, loss = 0.44120945\n",
      "Iteration 40, loss = 0.44116115\n",
      "Iteration 41, loss = 0.44090594\n",
      "Iteration 42, loss = 0.44076150\n",
      "Iteration 43, loss = 0.44054553\n",
      "Iteration 44, loss = 0.44043678\n",
      "Iteration 45, loss = 0.44022280\n",
      "Iteration 46, loss = 0.44007808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.43985104\n",
      "Iteration 48, loss = 0.43972838\n",
      "Iteration 49, loss = 0.43961566\n",
      "Iteration 50, loss = 0.43953741\n",
      "Iteration 51, loss = 0.43928726\n",
      "Iteration 52, loss = 0.43915669\n",
      "Iteration 53, loss = 0.43903420\n",
      "Iteration 54, loss = 0.43878260\n",
      "Iteration 55, loss = 0.43867217\n",
      "Iteration 56, loss = 0.43842760\n",
      "Iteration 57, loss = 0.43832367\n",
      "Iteration 58, loss = 0.43848173\n",
      "Iteration 59, loss = 0.43802499\n",
      "Iteration 60, loss = 0.43786165\n",
      "Iteration 61, loss = 0.43775838\n",
      "Iteration 62, loss = 0.43753304\n",
      "Iteration 63, loss = 0.43751755\n",
      "Iteration 64, loss = 0.43736938\n",
      "Iteration 65, loss = 0.43724547\n",
      "Iteration 66, loss = 0.43704909\n",
      "Iteration 67, loss = 0.43691589\n",
      "Iteration 68, loss = 0.43678012\n",
      "Iteration 69, loss = 0.43660907\n",
      "Iteration 70, loss = 0.43658121\n",
      "Iteration 71, loss = 0.43626470\n",
      "Iteration 72, loss = 0.43615740\n",
      "Iteration 73, loss = 0.43592477\n",
      "Iteration 74, loss = 0.43586499\n",
      "Iteration 75, loss = 0.43559379\n",
      "Iteration 76, loss = 0.43552838\n",
      "Iteration 77, loss = 0.43543963\n",
      "Iteration 78, loss = 0.43534026\n",
      "Iteration 79, loss = 0.43501888\n",
      "Iteration 80, loss = 0.43491374\n",
      "Iteration 81, loss = 0.43476602\n",
      "Iteration 82, loss = 0.43456898\n",
      "Iteration 83, loss = 0.43445446\n",
      "Iteration 84, loss = 0.43435297\n",
      "Iteration 85, loss = 0.43420218\n",
      "Iteration 86, loss = 0.43397319\n",
      "Iteration 87, loss = 0.43430790\n",
      "Iteration 88, loss = 0.43384691\n",
      "Iteration 89, loss = 0.43358066\n",
      "Iteration 90, loss = 0.43338473\n",
      "Iteration 91, loss = 0.43323070\n",
      "Iteration 92, loss = 0.43324213\n",
      "Iteration 93, loss = 0.43291540\n",
      "Iteration 94, loss = 0.43281706\n",
      "Iteration 95, loss = 0.43261158\n",
      "Iteration 96, loss = 0.43250845\n",
      "Iteration 97, loss = 0.43239894\n",
      "Iteration 98, loss = 0.43211721\n",
      "Iteration 99, loss = 0.43199602\n",
      "Iteration 100, loss = 0.43172609\n",
      "Iteration 101, loss = 0.43202858\n",
      "Iteration 102, loss = 0.43158837\n",
      "Iteration 103, loss = 0.43149200\n",
      "Iteration 104, loss = 0.43126680\n",
      "Iteration 105, loss = 0.43109243\n",
      "Iteration 106, loss = 0.43084131\n",
      "Iteration 107, loss = 0.43080265\n",
      "Iteration 108, loss = 0.43070493\n",
      "Iteration 109, loss = 0.43058386\n",
      "Iteration 110, loss = 0.43039478\n",
      "Iteration 111, loss = 0.43001345\n",
      "Iteration 112, loss = 0.42999237\n",
      "Iteration 113, loss = 0.42968594\n",
      "Iteration 114, loss = 0.42953864\n",
      "Iteration 115, loss = 0.42949230\n",
      "Iteration 116, loss = 0.42929018\n",
      "Iteration 117, loss = 0.42908550\n",
      "Iteration 118, loss = 0.42895991\n",
      "Iteration 119, loss = 0.42892095\n",
      "Iteration 120, loss = 0.42881893\n",
      "Iteration 121, loss = 0.42842977\n",
      "Iteration 122, loss = 0.42830961\n",
      "Iteration 123, loss = 0.42813032\n",
      "Iteration 124, loss = 0.42805802\n",
      "Iteration 125, loss = 0.42778517\n",
      "Iteration 126, loss = 0.42785766\n",
      "Iteration 127, loss = 0.42737977\n",
      "Iteration 128, loss = 0.42727604\n",
      "Iteration 129, loss = 0.42701883\n",
      "Iteration 130, loss = 0.42691164\n",
      "Iteration 131, loss = 0.42672280\n",
      "Iteration 132, loss = 0.42654869\n",
      "Iteration 133, loss = 0.42651716\n",
      "Iteration 134, loss = 0.42619610\n",
      "Iteration 135, loss = 0.42609865\n",
      "Iteration 136, loss = 0.42591074\n",
      "Iteration 137, loss = 0.42578545\n",
      "Iteration 138, loss = 0.42535492\n",
      "Iteration 139, loss = 0.42545594\n",
      "Iteration 140, loss = 0.42520342\n",
      "Iteration 141, loss = 0.42502925\n",
      "Iteration 142, loss = 0.42477390\n",
      "Iteration 143, loss = 0.42468617\n",
      "Iteration 144, loss = 0.42451680\n",
      "Iteration 145, loss = 0.42428942\n",
      "Iteration 146, loss = 0.42401454\n",
      "Iteration 147, loss = 0.42395271\n",
      "Iteration 148, loss = 0.42369310\n",
      "Iteration 149, loss = 0.42345078\n",
      "Iteration 150, loss = 0.42335009\n",
      "Iteration 151, loss = 0.42313383\n",
      "Iteration 152, loss = 0.42307149\n",
      "Iteration 153, loss = 0.42312159\n",
      "Iteration 154, loss = 0.42254938\n",
      "Iteration 155, loss = 0.42257295\n",
      "Iteration 156, loss = 0.42216368\n",
      "Iteration 157, loss = 0.42203488\n",
      "Iteration 158, loss = 0.42190660\n",
      "Iteration 159, loss = 0.42163932\n",
      "Iteration 160, loss = 0.42144093\n",
      "Iteration 161, loss = 0.42119649\n",
      "Iteration 162, loss = 0.42098803\n",
      "Iteration 163, loss = 0.42087408\n",
      "Iteration 164, loss = 0.42070005\n",
      "Iteration 165, loss = 0.42043549\n",
      "Iteration 166, loss = 0.42036098\n",
      "Iteration 167, loss = 0.42011256\n",
      "Iteration 168, loss = 0.41992550\n",
      "Iteration 169, loss = 0.41977773\n",
      "Iteration 170, loss = 0.41959700\n",
      "Iteration 171, loss = 0.41921376\n",
      "Iteration 172, loss = 0.41905692\n",
      "Iteration 173, loss = 0.41891967\n",
      "Iteration 174, loss = 0.41879331\n",
      "Iteration 175, loss = 0.41864466\n",
      "Iteration 176, loss = 0.41839328\n",
      "Iteration 177, loss = 0.41809959\n",
      "Iteration 178, loss = 0.41786231\n",
      "Iteration 179, loss = 0.41775902\n",
      "Iteration 180, loss = 0.41751220\n",
      "Iteration 181, loss = 0.41724290\n",
      "Iteration 182, loss = 0.41709826\n",
      "Iteration 183, loss = 0.41695555\n",
      "Iteration 184, loss = 0.41684599\n",
      "Iteration 185, loss = 0.41669729\n",
      "Iteration 186, loss = 0.41639884\n",
      "Iteration 187, loss = 0.41610018\n",
      "Iteration 188, loss = 0.41586421\n",
      "Iteration 189, loss = 0.41568071\n",
      "Iteration 190, loss = 0.41541921\n",
      "Iteration 191, loss = 0.41526807\n",
      "Iteration 192, loss = 0.41522065\n",
      "Iteration 193, loss = 0.41476527\n",
      "Iteration 194, loss = 0.41470456\n",
      "Iteration 195, loss = 0.41464360\n",
      "Iteration 196, loss = 0.41418582\n",
      "Iteration 197, loss = 0.41427671\n",
      "Iteration 198, loss = 0.41398382\n",
      "Iteration 199, loss = 0.41385608\n",
      "Iteration 200, loss = 0.41366918\n",
      "Iteration 1, loss = 0.79669760\n",
      "Iteration 2, loss = 0.75557697\n",
      "Iteration 3, loss = 0.71469950\n",
      "Iteration 4, loss = 0.67924961\n",
      "Iteration 5, loss = 0.64721967\n",
      "Iteration 6, loss = 0.61889362\n",
      "Iteration 7, loss = 0.59331554\n",
      "Iteration 8, loss = 0.57202345\n",
      "Iteration 9, loss = 0.55230062\n",
      "Iteration 10, loss = 0.53594039\n",
      "Iteration 11, loss = 0.52158961\n",
      "Iteration 12, loss = 0.50934520\n",
      "Iteration 13, loss = 0.49746176\n",
      "Iteration 14, loss = 0.48858625\n",
      "Iteration 15, loss = 0.48011948\n",
      "Iteration 16, loss = 0.47240743\n",
      "Iteration 17, loss = 0.46530660\n",
      "Iteration 18, loss = 0.45971048\n",
      "Iteration 19, loss = 0.45392696\n",
      "Iteration 20, loss = 0.44880472\n",
      "Iteration 21, loss = 0.44408580\n",
      "Iteration 22, loss = 0.43977611\n",
      "Iteration 23, loss = 0.43610622\n",
      "Iteration 24, loss = 0.43251204\n",
      "Iteration 25, loss = 0.42926684\n",
      "Iteration 26, loss = 0.42635471\n",
      "Iteration 27, loss = 0.42354868\n",
      "Iteration 28, loss = 0.42124048\n",
      "Iteration 29, loss = 0.41885753\n",
      "Iteration 30, loss = 0.41671698\n",
      "Iteration 31, loss = 0.41484184\n",
      "Iteration 32, loss = 0.41322445\n",
      "Iteration 33, loss = 0.41150185\n",
      "Iteration 34, loss = 0.41005960\n",
      "Iteration 35, loss = 0.40865096\n",
      "Iteration 36, loss = 0.40745207\n",
      "Iteration 37, loss = 0.40618268\n",
      "Iteration 38, loss = 0.40511460\n",
      "Iteration 39, loss = 0.40407324\n",
      "Iteration 40, loss = 0.40308614\n",
      "Iteration 41, loss = 0.40214118\n",
      "Iteration 42, loss = 0.40125116\n",
      "Iteration 43, loss = 0.40040948\n",
      "Iteration 44, loss = 0.39969445\n",
      "Iteration 45, loss = 0.39881351\n",
      "Iteration 46, loss = 0.39815272\n",
      "Iteration 47, loss = 0.39742329\n",
      "Iteration 48, loss = 0.39667854\n",
      "Iteration 49, loss = 0.39600425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.39535771\n",
      "Iteration 51, loss = 0.39479927\n",
      "Iteration 52, loss = 0.39412193\n",
      "Iteration 53, loss = 0.39348737\n",
      "Iteration 54, loss = 0.39298633\n",
      "Iteration 55, loss = 0.39237622\n",
      "Iteration 56, loss = 0.39184025\n",
      "Iteration 57, loss = 0.39134928\n",
      "Iteration 58, loss = 0.39079303\n",
      "Iteration 59, loss = 0.39020004\n",
      "Iteration 60, loss = 0.38970487\n",
      "Iteration 61, loss = 0.38935210\n",
      "Iteration 62, loss = 0.38893147\n",
      "Iteration 63, loss = 0.38841325\n",
      "Iteration 64, loss = 0.38787025\n",
      "Iteration 65, loss = 0.38731909\n",
      "Iteration 66, loss = 0.38695092\n",
      "Iteration 67, loss = 0.38641755\n",
      "Iteration 68, loss = 0.38614709\n",
      "Iteration 69, loss = 0.38556306\n",
      "Iteration 70, loss = 0.38520689\n",
      "Iteration 71, loss = 0.38479027\n",
      "Iteration 72, loss = 0.38431671\n",
      "Iteration 73, loss = 0.38396334\n",
      "Iteration 74, loss = 0.38353864\n",
      "Iteration 75, loss = 0.38317797\n",
      "Iteration 76, loss = 0.38280910\n",
      "Iteration 77, loss = 0.38236642\n",
      "Iteration 78, loss = 0.38203892\n",
      "Iteration 79, loss = 0.38160361\n",
      "Iteration 80, loss = 0.38126295\n",
      "Iteration 81, loss = 0.38100087\n",
      "Iteration 82, loss = 0.38051987\n",
      "Iteration 83, loss = 0.38027118\n",
      "Iteration 84, loss = 0.37985466\n",
      "Iteration 85, loss = 0.37944812\n",
      "Iteration 86, loss = 0.37922483\n",
      "Iteration 87, loss = 0.37879295\n",
      "Iteration 88, loss = 0.37860399\n",
      "Iteration 89, loss = 0.37813731\n",
      "Iteration 90, loss = 0.37783048\n",
      "Iteration 91, loss = 0.37760975\n",
      "Iteration 92, loss = 0.37726407\n",
      "Iteration 93, loss = 0.37688092\n",
      "Iteration 94, loss = 0.37664709\n",
      "Iteration 95, loss = 0.37627011\n",
      "Iteration 96, loss = 0.37617697\n",
      "Iteration 97, loss = 0.37570618\n",
      "Iteration 98, loss = 0.37546580\n",
      "Iteration 99, loss = 0.37513636\n",
      "Iteration 100, loss = 0.37492585\n",
      "Iteration 101, loss = 0.37455688\n",
      "Iteration 102, loss = 0.37433807\n",
      "Iteration 103, loss = 0.37398203\n",
      "Iteration 104, loss = 0.37378894\n",
      "Iteration 105, loss = 0.37337225\n",
      "Iteration 106, loss = 0.37315392\n",
      "Iteration 107, loss = 0.37288594\n",
      "Iteration 108, loss = 0.37268326\n",
      "Iteration 109, loss = 0.37234294\n",
      "Iteration 110, loss = 0.37211377\n",
      "Iteration 111, loss = 0.37180141\n",
      "Iteration 112, loss = 0.37146582\n",
      "Iteration 113, loss = 0.37127249\n",
      "Iteration 114, loss = 0.37098043\n",
      "Iteration 115, loss = 0.37079108\n",
      "Iteration 116, loss = 0.37053570\n",
      "Iteration 117, loss = 0.37021577\n",
      "Iteration 118, loss = 0.37000437\n",
      "Iteration 119, loss = 0.36971978\n",
      "Iteration 120, loss = 0.36944006\n",
      "Iteration 121, loss = 0.36926522\n",
      "Iteration 122, loss = 0.36897057\n",
      "Iteration 123, loss = 0.36872537\n",
      "Iteration 124, loss = 0.36854034\n",
      "Iteration 125, loss = 0.36827624\n",
      "Iteration 126, loss = 0.36796157\n",
      "Iteration 127, loss = 0.36780650\n",
      "Iteration 128, loss = 0.36749135\n",
      "Iteration 129, loss = 0.36725852\n",
      "Iteration 130, loss = 0.36701838\n",
      "Iteration 131, loss = 0.36686883\n",
      "Iteration 132, loss = 0.36658430\n",
      "Iteration 133, loss = 0.36631909\n",
      "Iteration 134, loss = 0.36612163\n",
      "Iteration 135, loss = 0.36583841\n",
      "Iteration 136, loss = 0.36564619\n",
      "Iteration 137, loss = 0.36553809\n",
      "Iteration 138, loss = 0.36516578\n",
      "Iteration 139, loss = 0.36505663\n",
      "Iteration 140, loss = 0.36480934\n",
      "Iteration 141, loss = 0.36456529\n",
      "Iteration 142, loss = 0.36434083\n",
      "Iteration 143, loss = 0.36416045\n",
      "Iteration 144, loss = 0.36389162\n",
      "Iteration 145, loss = 0.36388653\n",
      "Iteration 146, loss = 0.36362555\n",
      "Iteration 147, loss = 0.36329666\n",
      "Iteration 148, loss = 0.36324799\n",
      "Iteration 149, loss = 0.36293384\n",
      "Iteration 150, loss = 0.36266341\n",
      "Iteration 151, loss = 0.36258638\n",
      "Iteration 152, loss = 0.36227713\n",
      "Iteration 153, loss = 0.36207737\n",
      "Iteration 154, loss = 0.36203305\n",
      "Iteration 155, loss = 0.36189197\n",
      "Iteration 156, loss = 0.36144865\n",
      "Iteration 157, loss = 0.36125486\n",
      "Iteration 158, loss = 0.36111263\n",
      "Iteration 159, loss = 0.36087217\n",
      "Iteration 160, loss = 0.36073282\n",
      "Iteration 161, loss = 0.36049298\n",
      "Iteration 162, loss = 0.36036918\n",
      "Iteration 163, loss = 0.36009390\n",
      "Iteration 164, loss = 0.35994984\n",
      "Iteration 165, loss = 0.35981267\n",
      "Iteration 166, loss = 0.35963608\n",
      "Iteration 167, loss = 0.35944360\n",
      "Iteration 168, loss = 0.35916915\n",
      "Iteration 169, loss = 0.35903233\n",
      "Iteration 170, loss = 0.35880193\n",
      "Iteration 171, loss = 0.35857206\n",
      "Iteration 172, loss = 0.35856606\n",
      "Iteration 173, loss = 0.35842148\n",
      "Iteration 174, loss = 0.35803099\n",
      "Iteration 175, loss = 0.35789799\n",
      "Iteration 176, loss = 0.35798087\n",
      "Iteration 177, loss = 0.35769945\n",
      "Iteration 178, loss = 0.35738800\n",
      "Iteration 179, loss = 0.35731097\n",
      "Iteration 180, loss = 0.35696026\n",
      "Iteration 181, loss = 0.35706948\n",
      "Iteration 182, loss = 0.35666672\n",
      "Iteration 183, loss = 0.35655904\n",
      "Iteration 184, loss = 0.35638839\n",
      "Iteration 185, loss = 0.35624235\n",
      "Iteration 186, loss = 0.35599433\n",
      "Iteration 187, loss = 0.35595252\n",
      "Iteration 188, loss = 0.35558002\n",
      "Iteration 189, loss = 0.35542146\n",
      "Iteration 190, loss = 0.35537418\n",
      "Iteration 191, loss = 0.35502664\n",
      "Iteration 192, loss = 0.35496094\n",
      "Iteration 193, loss = 0.35479334\n",
      "Iteration 194, loss = 0.35465569\n",
      "Iteration 195, loss = 0.35448446\n",
      "Iteration 196, loss = 0.35428383\n",
      "Iteration 197, loss = 0.35403354\n",
      "Iteration 198, loss = 0.35396345\n",
      "Iteration 199, loss = 0.35378990\n",
      "Iteration 200, loss = 0.35360000\n",
      "Iteration 1, loss = 0.80032318\n",
      "Iteration 2, loss = 0.75976639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.72007953\n",
      "Iteration 4, loss = 0.68679759\n",
      "Iteration 5, loss = 0.65667709\n",
      "Iteration 6, loss = 0.63014574\n",
      "Iteration 7, loss = 0.60602193\n",
      "Iteration 8, loss = 0.58675447\n",
      "Iteration 9, loss = 0.56866078\n",
      "Iteration 10, loss = 0.55328420\n",
      "Iteration 11, loss = 0.54021222\n",
      "Iteration 12, loss = 0.52914660\n",
      "Iteration 13, loss = 0.51867313\n",
      "Iteration 14, loss = 0.51011968\n",
      "Iteration 15, loss = 0.50232890\n",
      "Iteration 16, loss = 0.49517305\n",
      "Iteration 17, loss = 0.48863727\n",
      "Iteration 18, loss = 0.48306034\n",
      "Iteration 19, loss = 0.47753875\n",
      "Iteration 20, loss = 0.47250611\n",
      "Iteration 21, loss = 0.46788387\n",
      "Iteration 22, loss = 0.46372481\n",
      "Iteration 23, loss = 0.46032271\n",
      "Iteration 24, loss = 0.45670468\n",
      "Iteration 25, loss = 0.45361849\n",
      "Iteration 26, loss = 0.45058892\n",
      "Iteration 27, loss = 0.44799808\n",
      "Iteration 28, loss = 0.44581345\n",
      "Iteration 29, loss = 0.44343880\n",
      "Iteration 30, loss = 0.44144084\n",
      "Iteration 31, loss = 0.43942618\n",
      "Iteration 32, loss = 0.43798873\n",
      "Iteration 33, loss = 0.43641247\n",
      "Iteration 34, loss = 0.43482102\n",
      "Iteration 35, loss = 0.43350932\n",
      "Iteration 36, loss = 0.43240488\n",
      "Iteration 37, loss = 0.43120499\n",
      "Iteration 38, loss = 0.43023281\n",
      "Iteration 39, loss = 0.42926244\n",
      "Iteration 40, loss = 0.42849106\n",
      "Iteration 41, loss = 0.42749611\n",
      "Iteration 42, loss = 0.42665644\n",
      "Iteration 43, loss = 0.42592306\n",
      "Iteration 44, loss = 0.42530831\n",
      "Iteration 45, loss = 0.42451361\n",
      "Iteration 46, loss = 0.42389757\n",
      "Iteration 47, loss = 0.42323684\n",
      "Iteration 48, loss = 0.42267969\n",
      "Iteration 49, loss = 0.42205685\n",
      "Iteration 50, loss = 0.42149392\n",
      "Iteration 51, loss = 0.42098444\n",
      "Iteration 52, loss = 0.42037886\n",
      "Iteration 53, loss = 0.41981918\n",
      "Iteration 54, loss = 0.41943426\n",
      "Iteration 55, loss = 0.41886234\n",
      "Iteration 56, loss = 0.41837771\n",
      "Iteration 57, loss = 0.41793320\n",
      "Iteration 58, loss = 0.41746145\n",
      "Iteration 59, loss = 0.41700926\n",
      "Iteration 60, loss = 0.41656322\n",
      "Iteration 61, loss = 0.41617811\n",
      "Iteration 62, loss = 0.41571665\n",
      "Iteration 63, loss = 0.41535120\n",
      "Iteration 64, loss = 0.41488929\n",
      "Iteration 65, loss = 0.41441820\n",
      "Iteration 66, loss = 0.41413584\n",
      "Iteration 67, loss = 0.41361771\n",
      "Iteration 68, loss = 0.41338540\n",
      "Iteration 69, loss = 0.41291625\n",
      "Iteration 70, loss = 0.41253687\n",
      "Iteration 71, loss = 0.41215049\n",
      "Iteration 72, loss = 0.41171926\n",
      "Iteration 73, loss = 0.41146914\n",
      "Iteration 74, loss = 0.41108628\n",
      "Iteration 75, loss = 0.41065874\n",
      "Iteration 76, loss = 0.41039356\n",
      "Iteration 77, loss = 0.40995878\n",
      "Iteration 78, loss = 0.40962124\n",
      "Iteration 79, loss = 0.40923533\n",
      "Iteration 80, loss = 0.40891009\n",
      "Iteration 81, loss = 0.40871838\n",
      "Iteration 82, loss = 0.40822652\n",
      "Iteration 83, loss = 0.40809714\n",
      "Iteration 84, loss = 0.40765774\n",
      "Iteration 85, loss = 0.40726300\n",
      "Iteration 86, loss = 0.40695050\n",
      "Iteration 87, loss = 0.40667775\n",
      "Iteration 88, loss = 0.40643198\n",
      "Iteration 89, loss = 0.40610790\n",
      "Iteration 90, loss = 0.40574674\n",
      "Iteration 91, loss = 0.40558307\n",
      "Iteration 92, loss = 0.40520609\n",
      "Iteration 93, loss = 0.40484669\n",
      "Iteration 94, loss = 0.40474494\n",
      "Iteration 95, loss = 0.40436676\n",
      "Iteration 96, loss = 0.40412636\n",
      "Iteration 97, loss = 0.40387385\n",
      "Iteration 98, loss = 0.40364960\n",
      "Iteration 99, loss = 0.40328893\n",
      "Iteration 100, loss = 0.40304844\n",
      "Iteration 101, loss = 0.40276922\n",
      "Iteration 102, loss = 0.40250746\n",
      "Iteration 103, loss = 0.40227864\n",
      "Iteration 104, loss = 0.40201945\n",
      "Iteration 105, loss = 0.40177757\n",
      "Iteration 106, loss = 0.40144172\n",
      "Iteration 107, loss = 0.40117154\n",
      "Iteration 108, loss = 0.40095290\n",
      "Iteration 109, loss = 0.40067975\n",
      "Iteration 110, loss = 0.40050726\n",
      "Iteration 111, loss = 0.40023780\n",
      "Iteration 112, loss = 0.39987382\n",
      "Iteration 113, loss = 0.39973072\n",
      "Iteration 114, loss = 0.39946597\n",
      "Iteration 115, loss = 0.39921500\n",
      "Iteration 116, loss = 0.39910204\n",
      "Iteration 117, loss = 0.39873420\n",
      "Iteration 118, loss = 0.39857290\n",
      "Iteration 119, loss = 0.39832810\n",
      "Iteration 120, loss = 0.39805925\n",
      "Iteration 121, loss = 0.39785005\n",
      "Iteration 122, loss = 0.39759383\n",
      "Iteration 123, loss = 0.39735952\n",
      "Iteration 124, loss = 0.39713534\n",
      "Iteration 125, loss = 0.39692517\n",
      "Iteration 126, loss = 0.39669790\n",
      "Iteration 127, loss = 0.39648269\n",
      "Iteration 128, loss = 0.39619930\n",
      "Iteration 129, loss = 0.39602993\n",
      "Iteration 130, loss = 0.39581474\n",
      "Iteration 131, loss = 0.39557949\n",
      "Iteration 132, loss = 0.39534469\n",
      "Iteration 133, loss = 0.39519113\n",
      "Iteration 134, loss = 0.39494041\n",
      "Iteration 135, loss = 0.39475876\n",
      "Iteration 136, loss = 0.39442328\n",
      "Iteration 137, loss = 0.39438685\n",
      "Iteration 138, loss = 0.39406145\n",
      "Iteration 139, loss = 0.39408111\n",
      "Iteration 140, loss = 0.39366329\n",
      "Iteration 141, loss = 0.39353858\n",
      "Iteration 142, loss = 0.39325451\n",
      "Iteration 143, loss = 0.39301168\n",
      "Iteration 144, loss = 0.39285984\n",
      "Iteration 145, loss = 0.39263645\n",
      "Iteration 146, loss = 0.39250215\n",
      "Iteration 147, loss = 0.39230080\n",
      "Iteration 148, loss = 0.39216412\n",
      "Iteration 149, loss = 0.39180574\n",
      "Iteration 150, loss = 0.39152310\n",
      "Iteration 151, loss = 0.39140330\n",
      "Iteration 152, loss = 0.39113151\n",
      "Iteration 153, loss = 0.39091997\n",
      "Iteration 154, loss = 0.39088507\n",
      "Iteration 155, loss = 0.39080718\n",
      "Iteration 156, loss = 0.39037204\n",
      "Iteration 157, loss = 0.39006531\n",
      "Iteration 158, loss = 0.38995149\n",
      "Iteration 159, loss = 0.38986005\n",
      "Iteration 160, loss = 0.38949399\n",
      "Iteration 161, loss = 0.38934452\n",
      "Iteration 162, loss = 0.38922719\n",
      "Iteration 163, loss = 0.38893604\n",
      "Iteration 164, loss = 0.38870073\n",
      "Iteration 165, loss = 0.38856129\n",
      "Iteration 166, loss = 0.38849039\n",
      "Iteration 167, loss = 0.38826949\n",
      "Iteration 168, loss = 0.38801831\n",
      "Iteration 169, loss = 0.38776021\n",
      "Iteration 170, loss = 0.38761988\n",
      "Iteration 171, loss = 0.38749014\n",
      "Iteration 172, loss = 0.38727701\n",
      "Iteration 173, loss = 0.38713354\n",
      "Iteration 174, loss = 0.38693221\n",
      "Iteration 175, loss = 0.38672449\n",
      "Iteration 176, loss = 0.38661425\n",
      "Iteration 177, loss = 0.38639276\n",
      "Iteration 178, loss = 0.38613264\n",
      "Iteration 179, loss = 0.38596233\n",
      "Iteration 180, loss = 0.38575075\n",
      "Iteration 181, loss = 0.38571537\n",
      "Iteration 182, loss = 0.38532967\n",
      "Iteration 183, loss = 0.38509408\n",
      "Iteration 184, loss = 0.38498433\n",
      "Iteration 185, loss = 0.38482933\n",
      "Iteration 186, loss = 0.38460072\n",
      "Iteration 187, loss = 0.38465255\n",
      "Iteration 188, loss = 0.38426048\n",
      "Iteration 189, loss = 0.38404663\n",
      "Iteration 190, loss = 0.38393551\n",
      "Iteration 191, loss = 0.38362485\n",
      "Iteration 192, loss = 0.38357388\n",
      "Iteration 193, loss = 0.38328454\n",
      "Iteration 194, loss = 0.38318795\n",
      "Iteration 195, loss = 0.38300627\n",
      "Iteration 196, loss = 0.38280720\n",
      "Iteration 197, loss = 0.38257733\n",
      "Iteration 198, loss = 0.38239391\n",
      "Iteration 199, loss = 0.38226339\n",
      "Iteration 200, loss = 0.38205623\n",
      "Iteration 1, loss = 0.79907644\n",
      "Iteration 2, loss = 0.75805898\n",
      "Iteration 3, loss = 0.72018387\n",
      "Iteration 4, loss = 0.68639335\n",
      "Iteration 5, loss = 0.65642308\n",
      "Iteration 6, loss = 0.62903665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.60615117\n",
      "Iteration 8, loss = 0.58594438\n",
      "Iteration 9, loss = 0.56812910\n",
      "Iteration 10, loss = 0.55190390\n",
      "Iteration 11, loss = 0.53903109\n",
      "Iteration 12, loss = 0.52691609\n",
      "Iteration 13, loss = 0.51668254\n",
      "Iteration 14, loss = 0.50702108\n",
      "Iteration 15, loss = 0.49856563\n",
      "Iteration 16, loss = 0.49108096\n",
      "Iteration 17, loss = 0.48403912\n",
      "Iteration 18, loss = 0.47814778\n",
      "Iteration 19, loss = 0.47217755\n",
      "Iteration 20, loss = 0.46639673\n",
      "Iteration 21, loss = 0.46172279\n",
      "Iteration 22, loss = 0.45713825\n",
      "Iteration 23, loss = 0.45275449\n",
      "Iteration 24, loss = 0.44901364\n",
      "Iteration 25, loss = 0.44539194\n",
      "Iteration 26, loss = 0.44209888\n",
      "Iteration 27, loss = 0.43887782\n",
      "Iteration 28, loss = 0.43628618\n",
      "Iteration 29, loss = 0.43351934\n",
      "Iteration 30, loss = 0.43131012\n",
      "Iteration 31, loss = 0.42919823\n",
      "Iteration 32, loss = 0.42719047\n",
      "Iteration 33, loss = 0.42544984\n",
      "Iteration 34, loss = 0.42361015\n",
      "Iteration 35, loss = 0.42200641\n",
      "Iteration 36, loss = 0.42057655\n",
      "Iteration 37, loss = 0.41925375\n",
      "Iteration 38, loss = 0.41786294\n",
      "Iteration 39, loss = 0.41660394\n",
      "Iteration 40, loss = 0.41546555\n",
      "Iteration 41, loss = 0.41435744\n",
      "Iteration 42, loss = 0.41329999\n",
      "Iteration 43, loss = 0.41227568\n",
      "Iteration 44, loss = 0.41140488\n",
      "Iteration 45, loss = 0.41038995\n",
      "Iteration 46, loss = 0.40951959\n",
      "Iteration 47, loss = 0.40860714\n",
      "Iteration 48, loss = 0.40779882\n",
      "Iteration 49, loss = 0.40703217\n",
      "Iteration 50, loss = 0.40625949\n",
      "Iteration 51, loss = 0.40546440\n",
      "Iteration 52, loss = 0.40476449\n",
      "Iteration 53, loss = 0.40406787\n",
      "Iteration 54, loss = 0.40334053\n",
      "Iteration 55, loss = 0.40266435\n",
      "Iteration 56, loss = 0.40206028\n",
      "Iteration 57, loss = 0.40138958\n",
      "Iteration 58, loss = 0.40072007\n",
      "Iteration 59, loss = 0.40006803\n",
      "Iteration 60, loss = 0.39950722\n",
      "Iteration 61, loss = 0.39887802\n",
      "Iteration 62, loss = 0.39833742\n",
      "Iteration 63, loss = 0.39769359\n",
      "Iteration 64, loss = 0.39724174\n",
      "Iteration 65, loss = 0.39664007\n",
      "Iteration 66, loss = 0.39618612\n",
      "Iteration 67, loss = 0.39575190\n",
      "Iteration 68, loss = 0.39513518\n",
      "Iteration 69, loss = 0.39453915\n",
      "Iteration 70, loss = 0.39413291\n",
      "Iteration 71, loss = 0.39354475\n",
      "Iteration 72, loss = 0.39306893\n",
      "Iteration 73, loss = 0.39258797\n",
      "Iteration 74, loss = 0.39211574\n",
      "Iteration 75, loss = 0.39166646\n",
      "Iteration 76, loss = 0.39125783\n",
      "Iteration 77, loss = 0.39068452\n",
      "Iteration 78, loss = 0.39031588\n",
      "Iteration 79, loss = 0.38986191\n",
      "Iteration 80, loss = 0.38944770\n",
      "Iteration 81, loss = 0.38903644\n",
      "Iteration 82, loss = 0.38865562\n",
      "Iteration 83, loss = 0.38820144\n",
      "Iteration 84, loss = 0.38780887\n",
      "Iteration 85, loss = 0.38743463\n",
      "Iteration 86, loss = 0.38701820\n",
      "Iteration 87, loss = 0.38663366\n",
      "Iteration 88, loss = 0.38619021\n",
      "Iteration 89, loss = 0.38578021\n",
      "Iteration 90, loss = 0.38549202\n",
      "Iteration 91, loss = 0.38502333\n",
      "Iteration 92, loss = 0.38493979\n",
      "Iteration 93, loss = 0.38436316\n",
      "Iteration 94, loss = 0.38430085\n",
      "Iteration 95, loss = 0.38367839\n",
      "Iteration 96, loss = 0.38330895\n",
      "Iteration 97, loss = 0.38301127\n",
      "Iteration 98, loss = 0.38263127\n",
      "Iteration 99, loss = 0.38236187\n",
      "Iteration 100, loss = 0.38200640\n",
      "Iteration 101, loss = 0.38187145\n",
      "Iteration 102, loss = 0.38147831\n",
      "Iteration 103, loss = 0.38113749\n",
      "Iteration 104, loss = 0.38097677\n",
      "Iteration 105, loss = 0.38055190\n",
      "Iteration 106, loss = 0.38029760\n",
      "Iteration 107, loss = 0.38000038\n",
      "Iteration 108, loss = 0.37974422\n",
      "Iteration 109, loss = 0.37940781\n",
      "Iteration 110, loss = 0.37911636\n",
      "Iteration 111, loss = 0.37883583\n",
      "Iteration 112, loss = 0.37862346\n",
      "Iteration 113, loss = 0.37826678\n",
      "Iteration 114, loss = 0.37805372\n",
      "Iteration 115, loss = 0.37777286\n",
      "Iteration 116, loss = 0.37741169\n",
      "Iteration 117, loss = 0.37715191\n",
      "Iteration 118, loss = 0.37690157\n",
      "Iteration 119, loss = 0.37670578\n",
      "Iteration 120, loss = 0.37643872\n",
      "Iteration 121, loss = 0.37614539\n",
      "Iteration 122, loss = 0.37590021\n",
      "Iteration 123, loss = 0.37556818\n",
      "Iteration 124, loss = 0.37530833\n",
      "Iteration 125, loss = 0.37512692\n",
      "Iteration 126, loss = 0.37476898\n",
      "Iteration 127, loss = 0.37445931\n",
      "Iteration 128, loss = 0.37431085\n",
      "Iteration 129, loss = 0.37417113\n",
      "Iteration 130, loss = 0.37375989\n",
      "Iteration 131, loss = 0.37355737\n",
      "Iteration 132, loss = 0.37330773\n",
      "Iteration 133, loss = 0.37307451\n",
      "Iteration 134, loss = 0.37284956\n",
      "Iteration 135, loss = 0.37251164\n",
      "Iteration 136, loss = 0.37249172\n",
      "Iteration 137, loss = 0.37204122\n",
      "Iteration 138, loss = 0.37188750\n",
      "Iteration 139, loss = 0.37173124\n",
      "Iteration 140, loss = 0.37147067\n",
      "Iteration 141, loss = 0.37119997\n",
      "Iteration 142, loss = 0.37104365\n",
      "Iteration 143, loss = 0.37080188\n",
      "Iteration 144, loss = 0.37061092\n",
      "Iteration 145, loss = 0.37042669\n",
      "Iteration 146, loss = 0.37014665\n",
      "Iteration 147, loss = 0.36998422\n",
      "Iteration 148, loss = 0.36979836\n",
      "Iteration 149, loss = 0.36949399\n",
      "Iteration 150, loss = 0.36923376\n",
      "Iteration 151, loss = 0.36907191\n",
      "Iteration 152, loss = 0.36899548\n",
      "Iteration 153, loss = 0.36895418\n",
      "Iteration 154, loss = 0.36859515\n",
      "Iteration 155, loss = 0.36815311\n",
      "Iteration 156, loss = 0.36807487\n",
      "Iteration 157, loss = 0.36786126\n",
      "Iteration 158, loss = 0.36762121\n",
      "Iteration 159, loss = 0.36746306\n",
      "Iteration 160, loss = 0.36727836\n",
      "Iteration 161, loss = 0.36707719\n",
      "Iteration 162, loss = 0.36686642\n",
      "Iteration 163, loss = 0.36662459\n",
      "Iteration 164, loss = 0.36649936\n",
      "Iteration 165, loss = 0.36633995\n",
      "Iteration 166, loss = 0.36608039\n",
      "Iteration 167, loss = 0.36578096\n",
      "Iteration 168, loss = 0.36560837\n",
      "Iteration 169, loss = 0.36545597\n",
      "Iteration 170, loss = 0.36537406\n",
      "Iteration 171, loss = 0.36503972\n",
      "Iteration 172, loss = 0.36501001\n",
      "Iteration 173, loss = 0.36480095\n",
      "Iteration 174, loss = 0.36456612\n",
      "Iteration 175, loss = 0.36434114\n",
      "Iteration 176, loss = 0.36415501\n",
      "Iteration 177, loss = 0.36387636\n",
      "Iteration 178, loss = 0.36392722\n",
      "Iteration 179, loss = 0.36353426\n",
      "Iteration 180, loss = 0.36337139\n",
      "Iteration 181, loss = 0.36317386\n",
      "Iteration 182, loss = 0.36300553\n",
      "Iteration 183, loss = 0.36272549\n",
      "Iteration 184, loss = 0.36266731\n",
      "Iteration 185, loss = 0.36251521\n",
      "Iteration 186, loss = 0.36231080\n",
      "Iteration 187, loss = 0.36213774\n",
      "Iteration 188, loss = 0.36189020\n",
      "Iteration 189, loss = 0.36173569\n",
      "Iteration 190, loss = 0.36151063\n",
      "Iteration 191, loss = 0.36135795\n",
      "Iteration 192, loss = 0.36113279\n",
      "Iteration 193, loss = 0.36094593\n",
      "Iteration 194, loss = 0.36091881\n",
      "Iteration 195, loss = 0.36070830\n",
      "Iteration 196, loss = 0.36038865\n",
      "Iteration 197, loss = 0.36038877\n",
      "Iteration 198, loss = 0.36003514\n",
      "Iteration 199, loss = 0.35992339\n",
      "Iteration 200, loss = 0.35958614\n",
      "Iteration 1, loss = 0.79433606\n",
      "Iteration 2, loss = 0.75464226\n",
      "Iteration 3, loss = 0.71766889\n",
      "Iteration 4, loss = 0.68463338\n",
      "Iteration 5, loss = 0.65437914\n",
      "Iteration 6, loss = 0.62761933\n",
      "Iteration 7, loss = 0.60417337\n",
      "Iteration 8, loss = 0.58312254\n",
      "Iteration 9, loss = 0.56478461\n",
      "Iteration 10, loss = 0.54808187\n",
      "Iteration 11, loss = 0.53447950\n",
      "Iteration 12, loss = 0.52191300\n",
      "Iteration 13, loss = 0.51130674\n",
      "Iteration 14, loss = 0.50103004\n",
      "Iteration 15, loss = 0.49235603\n",
      "Iteration 16, loss = 0.48443740\n",
      "Iteration 17, loss = 0.47727989\n",
      "Iteration 18, loss = 0.47109976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.46462814\n",
      "Iteration 20, loss = 0.45911666\n",
      "Iteration 21, loss = 0.45393085\n",
      "Iteration 22, loss = 0.44925347\n",
      "Iteration 23, loss = 0.44470152\n",
      "Iteration 24, loss = 0.44079814\n",
      "Iteration 25, loss = 0.43709199\n",
      "Iteration 26, loss = 0.43366157\n",
      "Iteration 27, loss = 0.43052879\n",
      "Iteration 28, loss = 0.42781290\n",
      "Iteration 29, loss = 0.42506948\n",
      "Iteration 30, loss = 0.42270514\n",
      "Iteration 31, loss = 0.42065215\n",
      "Iteration 32, loss = 0.41858876\n",
      "Iteration 33, loss = 0.41681935\n",
      "Iteration 34, loss = 0.41498452\n",
      "Iteration 35, loss = 0.41330910\n",
      "Iteration 36, loss = 0.41204567\n",
      "Iteration 37, loss = 0.41062982\n",
      "Iteration 38, loss = 0.40921699\n",
      "Iteration 39, loss = 0.40806217\n",
      "Iteration 40, loss = 0.40692912\n",
      "Iteration 41, loss = 0.40578435\n",
      "Iteration 42, loss = 0.40477487\n",
      "Iteration 43, loss = 0.40385330\n",
      "Iteration 44, loss = 0.40293777\n",
      "Iteration 45, loss = 0.40198079\n",
      "Iteration 46, loss = 0.40103449\n",
      "Iteration 47, loss = 0.40020283\n",
      "Iteration 48, loss = 0.39941825\n",
      "Iteration 49, loss = 0.39864542\n",
      "Iteration 50, loss = 0.39781984\n",
      "Iteration 51, loss = 0.39702271\n",
      "Iteration 52, loss = 0.39639426\n",
      "Iteration 53, loss = 0.39570737\n",
      "Iteration 54, loss = 0.39493956\n",
      "Iteration 55, loss = 0.39432095\n",
      "Iteration 56, loss = 0.39369678\n",
      "Iteration 57, loss = 0.39308553\n",
      "Iteration 58, loss = 0.39247959\n",
      "Iteration 59, loss = 0.39184569\n",
      "Iteration 60, loss = 0.39125995\n",
      "Iteration 61, loss = 0.39073307\n",
      "Iteration 62, loss = 0.39016577\n",
      "Iteration 63, loss = 0.38962582\n",
      "Iteration 64, loss = 0.38923067\n",
      "Iteration 65, loss = 0.38858417\n",
      "Iteration 66, loss = 0.38809010\n",
      "Iteration 67, loss = 0.38759003\n",
      "Iteration 68, loss = 0.38706479\n",
      "Iteration 69, loss = 0.38659932\n",
      "Iteration 70, loss = 0.38611212\n",
      "Iteration 71, loss = 0.38562052\n",
      "Iteration 72, loss = 0.38515813\n",
      "Iteration 73, loss = 0.38465774\n",
      "Iteration 74, loss = 0.38424921\n",
      "Iteration 75, loss = 0.38380628\n",
      "Iteration 76, loss = 0.38333877\n",
      "Iteration 77, loss = 0.38289066\n",
      "Iteration 78, loss = 0.38253571\n",
      "Iteration 79, loss = 0.38199209\n",
      "Iteration 80, loss = 0.38161118\n",
      "Iteration 81, loss = 0.38118700\n",
      "Iteration 82, loss = 0.38078723\n",
      "Iteration 83, loss = 0.38045605\n",
      "Iteration 84, loss = 0.38003710\n",
      "Iteration 85, loss = 0.37967016\n",
      "Iteration 86, loss = 0.37915533\n",
      "Iteration 87, loss = 0.37892138\n",
      "Iteration 88, loss = 0.37847173\n",
      "Iteration 89, loss = 0.37812333\n",
      "Iteration 90, loss = 0.37786183\n",
      "Iteration 91, loss = 0.37736253\n",
      "Iteration 92, loss = 0.37723483\n",
      "Iteration 93, loss = 0.37670701\n",
      "Iteration 94, loss = 0.37640183\n",
      "Iteration 95, loss = 0.37593140\n",
      "Iteration 96, loss = 0.37560722\n",
      "Iteration 97, loss = 0.37535723\n",
      "Iteration 98, loss = 0.37494003\n",
      "Iteration 99, loss = 0.37465343\n",
      "Iteration 100, loss = 0.37420412\n",
      "Iteration 101, loss = 0.37411449\n",
      "Iteration 102, loss = 0.37378781\n",
      "Iteration 103, loss = 0.37340677\n",
      "Iteration 104, loss = 0.37318301\n",
      "Iteration 105, loss = 0.37272326\n",
      "Iteration 106, loss = 0.37247578\n",
      "Iteration 107, loss = 0.37222242\n",
      "Iteration 108, loss = 0.37197531\n",
      "Iteration 109, loss = 0.37166365\n",
      "Iteration 110, loss = 0.37130886\n",
      "Iteration 111, loss = 0.37096685\n",
      "Iteration 112, loss = 0.37077366\n",
      "Iteration 113, loss = 0.37043450\n",
      "Iteration 114, loss = 0.37021366\n",
      "Iteration 115, loss = 0.36992025\n",
      "Iteration 116, loss = 0.36956378\n",
      "Iteration 117, loss = 0.36931472\n",
      "Iteration 118, loss = 0.36910877\n",
      "Iteration 119, loss = 0.36890452\n",
      "Iteration 120, loss = 0.36861274\n",
      "Iteration 121, loss = 0.36834769\n",
      "Iteration 122, loss = 0.36807546\n",
      "Iteration 123, loss = 0.36776660\n",
      "Iteration 124, loss = 0.36757687\n",
      "Iteration 125, loss = 0.36731202\n",
      "Iteration 126, loss = 0.36703358\n",
      "Iteration 127, loss = 0.36680311\n",
      "Iteration 128, loss = 0.36657365\n",
      "Iteration 129, loss = 0.36638662\n",
      "Iteration 130, loss = 0.36610702\n",
      "Iteration 131, loss = 0.36578660\n",
      "Iteration 132, loss = 0.36561328\n",
      "Iteration 133, loss = 0.36535602\n",
      "Iteration 134, loss = 0.36506741\n",
      "Iteration 135, loss = 0.36482083\n",
      "Iteration 136, loss = 0.36461660\n",
      "Iteration 137, loss = 0.36446915\n",
      "Iteration 138, loss = 0.36427369\n",
      "Iteration 139, loss = 0.36395060\n",
      "Iteration 140, loss = 0.36367244\n",
      "Iteration 141, loss = 0.36344627\n",
      "Iteration 142, loss = 0.36322087\n",
      "Iteration 143, loss = 0.36288847\n",
      "Iteration 144, loss = 0.36269024\n",
      "Iteration 145, loss = 0.36252910\n",
      "Iteration 146, loss = 0.36232630\n",
      "Iteration 147, loss = 0.36204204\n",
      "Iteration 148, loss = 0.36175744\n",
      "Iteration 149, loss = 0.36150897\n",
      "Iteration 150, loss = 0.36128574\n",
      "Iteration 151, loss = 0.36104612\n",
      "Iteration 152, loss = 0.36098407\n",
      "Iteration 153, loss = 0.36091059\n",
      "Iteration 154, loss = 0.36045178\n",
      "Iteration 155, loss = 0.36023247\n",
      "Iteration 156, loss = 0.36003901\n",
      "Iteration 157, loss = 0.35979222\n",
      "Iteration 158, loss = 0.35956884\n",
      "Iteration 159, loss = 0.35931837\n",
      "Iteration 160, loss = 0.35921634\n",
      "Iteration 161, loss = 0.35898171\n",
      "Iteration 162, loss = 0.35873506\n",
      "Iteration 163, loss = 0.35857521\n",
      "Iteration 164, loss = 0.35831635\n",
      "Iteration 165, loss = 0.35810003\n",
      "Iteration 166, loss = 0.35792137\n",
      "Iteration 167, loss = 0.35766792\n",
      "Iteration 168, loss = 0.35754666\n",
      "Iteration 169, loss = 0.35727813\n",
      "Iteration 170, loss = 0.35713009\n",
      "Iteration 171, loss = 0.35681643\n",
      "Iteration 172, loss = 0.35670442\n",
      "Iteration 173, loss = 0.35654512\n",
      "Iteration 174, loss = 0.35636358\n",
      "Iteration 175, loss = 0.35611796\n",
      "Iteration 176, loss = 0.35585630\n",
      "Iteration 177, loss = 0.35561683\n",
      "Iteration 178, loss = 0.35551322\n",
      "Iteration 179, loss = 0.35524922\n",
      "Iteration 180, loss = 0.35502081\n",
      "Iteration 181, loss = 0.35481801\n",
      "Iteration 182, loss = 0.35465812\n",
      "Iteration 183, loss = 0.35440606\n",
      "Iteration 184, loss = 0.35420402\n",
      "Iteration 185, loss = 0.35407125\n",
      "Iteration 186, loss = 0.35390294\n",
      "Iteration 187, loss = 0.35367945\n",
      "Iteration 188, loss = 0.35351126\n",
      "Iteration 189, loss = 0.35333928\n",
      "Iteration 190, loss = 0.35312693\n",
      "Iteration 191, loss = 0.35287650\n",
      "Iteration 192, loss = 0.35276777\n",
      "Iteration 193, loss = 0.35249827\n",
      "Iteration 194, loss = 0.35243721\n",
      "Iteration 195, loss = 0.35229063\n",
      "Iteration 196, loss = 0.35192900\n",
      "Iteration 197, loss = 0.35194078\n",
      "Iteration 198, loss = 0.35170984\n",
      "Iteration 199, loss = 0.35165955\n",
      "Iteration 200, loss = 0.35125538\n",
      "Iteration 1, loss = 0.79395106\n",
      "Iteration 2, loss = 0.75421289\n",
      "Iteration 3, loss = 0.71713602\n",
      "Iteration 4, loss = 0.68453685\n",
      "Iteration 5, loss = 0.65529408\n",
      "Iteration 6, loss = 0.62982410\n",
      "Iteration 7, loss = 0.60738284\n",
      "Iteration 8, loss = 0.58743655\n",
      "Iteration 9, loss = 0.57013560\n",
      "Iteration 10, loss = 0.55462790\n",
      "Iteration 11, loss = 0.54179184\n",
      "Iteration 12, loss = 0.53080996\n",
      "Iteration 13, loss = 0.52133752\n",
      "Iteration 14, loss = 0.51200902\n",
      "Iteration 15, loss = 0.50460571\n",
      "Iteration 16, loss = 0.49773151\n",
      "Iteration 17, loss = 0.49190009\n",
      "Iteration 18, loss = 0.48611127\n",
      "Iteration 19, loss = 0.48076618\n",
      "Iteration 20, loss = 0.47631531\n",
      "Iteration 21, loss = 0.47206355\n",
      "Iteration 22, loss = 0.46810433\n",
      "Iteration 23, loss = 0.46437129\n",
      "Iteration 24, loss = 0.46115295\n",
      "Iteration 25, loss = 0.45807083\n",
      "Iteration 26, loss = 0.45537111\n",
      "Iteration 27, loss = 0.45284940\n",
      "Iteration 28, loss = 0.45054349\n",
      "Iteration 29, loss = 0.44840487\n",
      "Iteration 30, loss = 0.44639784\n",
      "Iteration 31, loss = 0.44465146\n",
      "Iteration 32, loss = 0.44300194\n",
      "Iteration 33, loss = 0.44148906\n",
      "Iteration 34, loss = 0.43993574\n",
      "Iteration 35, loss = 0.43856423\n",
      "Iteration 36, loss = 0.43744594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.43616993\n",
      "Iteration 38, loss = 0.43505815\n",
      "Iteration 39, loss = 0.43400366\n",
      "Iteration 40, loss = 0.43308945\n",
      "Iteration 41, loss = 0.43209766\n",
      "Iteration 42, loss = 0.43119726\n",
      "Iteration 43, loss = 0.43032380\n",
      "Iteration 44, loss = 0.42950762\n",
      "Iteration 45, loss = 0.42870102\n",
      "Iteration 46, loss = 0.42787165\n",
      "Iteration 47, loss = 0.42714409\n",
      "Iteration 48, loss = 0.42630294\n",
      "Iteration 49, loss = 0.42563191\n",
      "Iteration 50, loss = 0.42491675\n",
      "Iteration 51, loss = 0.42413926\n",
      "Iteration 52, loss = 0.42350255\n",
      "Iteration 53, loss = 0.42284151\n",
      "Iteration 54, loss = 0.42212519\n",
      "Iteration 55, loss = 0.42145449\n",
      "Iteration 56, loss = 0.42082092\n",
      "Iteration 57, loss = 0.42019162\n",
      "Iteration 58, loss = 0.41966147\n",
      "Iteration 59, loss = 0.41893169\n",
      "Iteration 60, loss = 0.41832641\n",
      "Iteration 61, loss = 0.41774603\n",
      "Iteration 62, loss = 0.41714679\n",
      "Iteration 63, loss = 0.41661245\n",
      "Iteration 64, loss = 0.41607209\n",
      "Iteration 65, loss = 0.41555915\n",
      "Iteration 66, loss = 0.41500459\n",
      "Iteration 67, loss = 0.41442610\n",
      "Iteration 68, loss = 0.41390774\n",
      "Iteration 69, loss = 0.41344160\n",
      "Iteration 70, loss = 0.41287246\n",
      "Iteration 71, loss = 0.41227416\n",
      "Iteration 72, loss = 0.41176426\n",
      "Iteration 73, loss = 0.41118662\n",
      "Iteration 74, loss = 0.41075578\n",
      "Iteration 75, loss = 0.41016354\n",
      "Iteration 76, loss = 0.40972713\n",
      "Iteration 77, loss = 0.40925481\n",
      "Iteration 78, loss = 0.40888280\n",
      "Iteration 79, loss = 0.40822396\n",
      "Iteration 80, loss = 0.40782144\n",
      "Iteration 81, loss = 0.40737002\n",
      "Iteration 82, loss = 0.40685941\n",
      "Iteration 83, loss = 0.40651216\n",
      "Iteration 84, loss = 0.40599181\n",
      "Iteration 85, loss = 0.40556906\n",
      "Iteration 86, loss = 0.40506412\n",
      "Iteration 87, loss = 0.40483554\n",
      "Iteration 88, loss = 0.40431604\n",
      "Iteration 89, loss = 0.40378357\n",
      "Iteration 90, loss = 0.40343715\n",
      "Iteration 91, loss = 0.40299823\n",
      "Iteration 92, loss = 0.40270380\n",
      "Iteration 93, loss = 0.40215068\n",
      "Iteration 94, loss = 0.40180836\n",
      "Iteration 95, loss = 0.40136121\n",
      "Iteration 96, loss = 0.40100923\n",
      "Iteration 97, loss = 0.40068687\n",
      "Iteration 98, loss = 0.40022674\n",
      "Iteration 99, loss = 0.39986511\n",
      "Iteration 100, loss = 0.39944527\n",
      "Iteration 101, loss = 0.39933382\n",
      "Iteration 102, loss = 0.39881913\n",
      "Iteration 103, loss = 0.39851591\n",
      "Iteration 104, loss = 0.39814505\n",
      "Iteration 105, loss = 0.39775537\n",
      "Iteration 106, loss = 0.39735712\n",
      "Iteration 107, loss = 0.39704814\n",
      "Iteration 108, loss = 0.39674108\n",
      "Iteration 109, loss = 0.39646066\n",
      "Iteration 110, loss = 0.39599234\n",
      "Iteration 111, loss = 0.39559821\n",
      "Iteration 112, loss = 0.39536383\n",
      "Iteration 113, loss = 0.39497271\n",
      "Iteration 114, loss = 0.39463761\n",
      "Iteration 115, loss = 0.39438249\n",
      "Iteration 116, loss = 0.39401268\n",
      "Iteration 117, loss = 0.39370154\n",
      "Iteration 118, loss = 0.39339186\n",
      "Iteration 119, loss = 0.39314455\n",
      "Iteration 120, loss = 0.39296489\n",
      "Iteration 121, loss = 0.39245870\n",
      "Iteration 122, loss = 0.39221193\n",
      "Iteration 123, loss = 0.39188330\n",
      "Iteration 124, loss = 0.39169324\n",
      "Iteration 125, loss = 0.39127917\n",
      "Iteration 126, loss = 0.39113684\n",
      "Iteration 127, loss = 0.39069672\n",
      "Iteration 128, loss = 0.39041545\n",
      "Iteration 129, loss = 0.39013181\n",
      "Iteration 130, loss = 0.38987298\n",
      "Iteration 131, loss = 0.38952590\n",
      "Iteration 132, loss = 0.38924106\n",
      "Iteration 133, loss = 0.38897437\n",
      "Iteration 134, loss = 0.38864971\n",
      "Iteration 135, loss = 0.38841295\n",
      "Iteration 136, loss = 0.38812454\n",
      "Iteration 137, loss = 0.38786802\n",
      "Iteration 138, loss = 0.38746891\n",
      "Iteration 139, loss = 0.38739600\n",
      "Iteration 140, loss = 0.38707995\n",
      "Iteration 141, loss = 0.38671443\n",
      "Iteration 142, loss = 0.38646943\n",
      "Iteration 143, loss = 0.38617697\n",
      "Iteration 144, loss = 0.38593139\n",
      "Iteration 145, loss = 0.38567833\n",
      "Iteration 146, loss = 0.38536030\n",
      "Iteration 147, loss = 0.38520293\n",
      "Iteration 148, loss = 0.38479538\n",
      "Iteration 149, loss = 0.38453102\n",
      "Iteration 150, loss = 0.38423184\n",
      "Iteration 151, loss = 0.38399454\n",
      "Iteration 152, loss = 0.38379433\n",
      "Iteration 153, loss = 0.38373400\n",
      "Iteration 154, loss = 0.38321586\n",
      "Iteration 155, loss = 0.38301533\n",
      "Iteration 156, loss = 0.38266709\n",
      "Iteration 157, loss = 0.38244044\n",
      "Iteration 158, loss = 0.38214889\n",
      "Iteration 159, loss = 0.38185910\n",
      "Iteration 160, loss = 0.38169377\n",
      "Iteration 161, loss = 0.38131349\n",
      "Iteration 162, loss = 0.38101156\n",
      "Iteration 163, loss = 0.38082487\n",
      "Iteration 164, loss = 0.38059182\n",
      "Iteration 165, loss = 0.38029621\n",
      "Iteration 166, loss = 0.38022048\n",
      "Iteration 167, loss = 0.37990520\n",
      "Iteration 168, loss = 0.37955962\n",
      "Iteration 169, loss = 0.37937812\n",
      "Iteration 170, loss = 0.37911504\n",
      "Iteration 171, loss = 0.37876842\n",
      "Iteration 172, loss = 0.37853881\n",
      "Iteration 173, loss = 0.37829790\n",
      "Iteration 174, loss = 0.37807621\n",
      "Iteration 175, loss = 0.37784514\n",
      "Iteration 176, loss = 0.37754842\n",
      "Iteration 177, loss = 0.37720565\n",
      "Iteration 178, loss = 0.37692177\n",
      "Iteration 179, loss = 0.37674444\n",
      "Iteration 180, loss = 0.37648443\n",
      "Iteration 181, loss = 0.37617779\n",
      "Iteration 182, loss = 0.37598964\n",
      "Iteration 183, loss = 0.37582627\n",
      "Iteration 184, loss = 0.37552478\n",
      "Iteration 185, loss = 0.37534763\n",
      "Iteration 186, loss = 0.37495961\n",
      "Iteration 187, loss = 0.37470180\n",
      "Iteration 188, loss = 0.37444987\n",
      "Iteration 189, loss = 0.37414414\n",
      "Iteration 190, loss = 0.37388442\n",
      "Iteration 191, loss = 0.37363398\n",
      "Iteration 192, loss = 0.37351242\n",
      "Iteration 193, loss = 0.37314267\n",
      "Iteration 194, loss = 0.37304073\n",
      "Iteration 195, loss = 0.37281307\n",
      "Iteration 196, loss = 0.37247259\n",
      "Iteration 197, loss = 0.37242076\n",
      "Iteration 198, loss = 0.37209261\n",
      "Iteration 199, loss = 0.37195067\n",
      "Iteration 200, loss = 0.37183058\n",
      "Iteration 1, loss = 0.79133751\n",
      "Iteration 2, loss = 0.75021921\n",
      "Iteration 3, loss = 0.70933755\n",
      "Iteration 4, loss = 0.67387716\n",
      "Iteration 5, loss = 0.64183440\n",
      "Iteration 6, loss = 0.61349471\n",
      "Iteration 7, loss = 0.58789971\n",
      "Iteration 8, loss = 0.56659064\n",
      "Iteration 9, loss = 0.54684879\n",
      "Iteration 10, loss = 0.53046746\n",
      "Iteration 11, loss = 0.51609590\n",
      "Iteration 12, loss = 0.50382646\n",
      "Iteration 13, loss = 0.49191861\n",
      "Iteration 14, loss = 0.48301614\n",
      "Iteration 15, loss = 0.47452323\n",
      "Iteration 16, loss = 0.46678905\n",
      "Iteration 17, loss = 0.45965925\n",
      "Iteration 18, loss = 0.45404288\n",
      "Iteration 19, loss = 0.44823299\n",
      "Iteration 20, loss = 0.44308620\n",
      "Iteration 21, loss = 0.43833941\n",
      "Iteration 22, loss = 0.43400276\n",
      "Iteration 23, loss = 0.43030987\n",
      "Iteration 24, loss = 0.42668996\n",
      "Iteration 25, loss = 0.42342089\n",
      "Iteration 26, loss = 0.42048711\n",
      "Iteration 27, loss = 0.41766305\n",
      "Iteration 28, loss = 0.41533415\n",
      "Iteration 29, loss = 0.41293011\n",
      "Iteration 30, loss = 0.41076878\n",
      "Iteration 31, loss = 0.40887231\n",
      "Iteration 32, loss = 0.40723548\n",
      "Iteration 33, loss = 0.40548897\n",
      "Iteration 34, loss = 0.40402663\n",
      "Iteration 35, loss = 0.40259876\n",
      "Iteration 36, loss = 0.40138473\n",
      "Iteration 37, loss = 0.40009935\n",
      "Iteration 38, loss = 0.39901923\n",
      "Iteration 39, loss = 0.39796394\n",
      "Iteration 40, loss = 0.39696018\n",
      "Iteration 41, loss = 0.39600002\n",
      "Iteration 42, loss = 0.39509477\n",
      "Iteration 43, loss = 0.39423316\n",
      "Iteration 44, loss = 0.39351010\n",
      "Iteration 45, loss = 0.39261347\n",
      "Iteration 46, loss = 0.39194041\n",
      "Iteration 47, loss = 0.39119736\n",
      "Iteration 48, loss = 0.39043730\n",
      "Iteration 49, loss = 0.38974823\n",
      "Iteration 50, loss = 0.38909076\n",
      "Iteration 51, loss = 0.38851778\n",
      "Iteration 52, loss = 0.38781970\n",
      "Iteration 53, loss = 0.38716433\n",
      "Iteration 54, loss = 0.38664207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.38601388\n",
      "Iteration 56, loss = 0.38545899\n",
      "Iteration 57, loss = 0.38494308\n",
      "Iteration 58, loss = 0.38436859\n",
      "Iteration 59, loss = 0.38375489\n",
      "Iteration 60, loss = 0.38324015\n",
      "Iteration 61, loss = 0.38287330\n",
      "Iteration 62, loss = 0.38243555\n",
      "Iteration 63, loss = 0.38189466\n",
      "Iteration 64, loss = 0.38132728\n",
      "Iteration 65, loss = 0.38075276\n",
      "Iteration 66, loss = 0.38036947\n",
      "Iteration 67, loss = 0.37981887\n",
      "Iteration 68, loss = 0.37952791\n",
      "Iteration 69, loss = 0.37892052\n",
      "Iteration 70, loss = 0.37854733\n",
      "Iteration 71, loss = 0.37811474\n",
      "Iteration 72, loss = 0.37761751\n",
      "Iteration 73, loss = 0.37724211\n",
      "Iteration 74, loss = 0.37679514\n",
      "Iteration 75, loss = 0.37641278\n",
      "Iteration 76, loss = 0.37602741\n",
      "Iteration 77, loss = 0.37556741\n",
      "Iteration 78, loss = 0.37521572\n",
      "Iteration 79, loss = 0.37476055\n",
      "Iteration 80, loss = 0.37439359\n",
      "Iteration 81, loss = 0.37411059\n",
      "Iteration 82, loss = 0.37360515\n",
      "Iteration 83, loss = 0.37333515\n",
      "Iteration 84, loss = 0.37288710\n",
      "Iteration 85, loss = 0.37245768\n",
      "Iteration 86, loss = 0.37221739\n",
      "Iteration 87, loss = 0.37177074\n",
      "Iteration 88, loss = 0.37156177\n",
      "Iteration 89, loss = 0.37107477\n",
      "Iteration 90, loss = 0.37074959\n",
      "Iteration 91, loss = 0.37051238\n",
      "Iteration 92, loss = 0.37015345\n",
      "Iteration 93, loss = 0.36974368\n",
      "Iteration 94, loss = 0.36949289\n",
      "Iteration 95, loss = 0.36910013\n",
      "Iteration 96, loss = 0.36898497\n",
      "Iteration 97, loss = 0.36850128\n",
      "Iteration 98, loss = 0.36823816\n",
      "Iteration 99, loss = 0.36789009\n",
      "Iteration 100, loss = 0.36766101\n",
      "Iteration 101, loss = 0.36728064\n",
      "Iteration 102, loss = 0.36704972\n",
      "Iteration 103, loss = 0.36668348\n",
      "Iteration 104, loss = 0.36648548\n",
      "Iteration 105, loss = 0.36605334\n",
      "Iteration 106, loss = 0.36582996\n",
      "Iteration 107, loss = 0.36554329\n",
      "Iteration 108, loss = 0.36532712\n",
      "Iteration 109, loss = 0.36496752\n",
      "Iteration 110, loss = 0.36471920\n",
      "Iteration 111, loss = 0.36438918\n",
      "Iteration 112, loss = 0.36403483\n",
      "Iteration 113, loss = 0.36382467\n",
      "Iteration 114, loss = 0.36351137\n",
      "Iteration 115, loss = 0.36331155\n",
      "Iteration 116, loss = 0.36302702\n",
      "Iteration 117, loss = 0.36269799\n",
      "Iteration 118, loss = 0.36246353\n",
      "Iteration 119, loss = 0.36215401\n",
      "Iteration 120, loss = 0.36185783\n",
      "Iteration 121, loss = 0.36166412\n",
      "Iteration 122, loss = 0.36135392\n",
      "Iteration 123, loss = 0.36108600\n",
      "Iteration 124, loss = 0.36088211\n",
      "Iteration 125, loss = 0.36059465\n",
      "Iteration 126, loss = 0.36026131\n",
      "Iteration 127, loss = 0.36007925\n",
      "Iteration 128, loss = 0.35972864\n",
      "Iteration 129, loss = 0.35946656\n",
      "Iteration 130, loss = 0.35920022\n",
      "Iteration 131, loss = 0.35902321\n",
      "Iteration 132, loss = 0.35871488\n",
      "Iteration 133, loss = 0.35841803\n",
      "Iteration 134, loss = 0.35819415\n",
      "Iteration 135, loss = 0.35788559\n",
      "Iteration 136, loss = 0.35767285\n",
      "Iteration 137, loss = 0.35754061\n",
      "Iteration 138, loss = 0.35714181\n",
      "Iteration 139, loss = 0.35700310\n",
      "Iteration 140, loss = 0.35673139\n",
      "Iteration 141, loss = 0.35645877\n",
      "Iteration 142, loss = 0.35619942\n",
      "Iteration 143, loss = 0.35599264\n",
      "Iteration 144, loss = 0.35569202\n",
      "Iteration 145, loss = 0.35564832\n",
      "Iteration 146, loss = 0.35536354\n",
      "Iteration 147, loss = 0.35500461\n",
      "Iteration 148, loss = 0.35493424\n",
      "Iteration 149, loss = 0.35457969\n",
      "Iteration 150, loss = 0.35429440\n",
      "Iteration 151, loss = 0.35419311\n",
      "Iteration 152, loss = 0.35385283\n",
      "Iteration 153, loss = 0.35361832\n",
      "Iteration 154, loss = 0.35355557\n",
      "Iteration 155, loss = 0.35338663\n",
      "Iteration 156, loss = 0.35291242\n",
      "Iteration 157, loss = 0.35270304\n",
      "Iteration 158, loss = 0.35254115\n",
      "Iteration 159, loss = 0.35228078\n",
      "Iteration 160, loss = 0.35212517\n",
      "Iteration 161, loss = 0.35184551\n",
      "Iteration 162, loss = 0.35170602\n",
      "Iteration 163, loss = 0.35140505\n",
      "Iteration 164, loss = 0.35123534\n",
      "Iteration 165, loss = 0.35108322\n",
      "Iteration 166, loss = 0.35088016\n",
      "Iteration 167, loss = 0.35066705\n",
      "Iteration 168, loss = 0.35037188\n",
      "Iteration 169, loss = 0.35020008\n",
      "Iteration 170, loss = 0.34995498\n",
      "Iteration 171, loss = 0.34970450\n",
      "Iteration 172, loss = 0.34967366\n",
      "Iteration 173, loss = 0.34951632\n",
      "Iteration 174, loss = 0.34910550\n",
      "Iteration 175, loss = 0.34895299\n",
      "Iteration 176, loss = 0.34903414\n",
      "Iteration 177, loss = 0.34873338\n",
      "Iteration 178, loss = 0.34840190\n",
      "Iteration 179, loss = 0.34830247\n",
      "Iteration 180, loss = 0.34792471\n",
      "Iteration 181, loss = 0.34803552\n",
      "Iteration 182, loss = 0.34761268\n",
      "Iteration 183, loss = 0.34748144\n",
      "Iteration 184, loss = 0.34728780\n",
      "Iteration 185, loss = 0.34712021\n",
      "Iteration 186, loss = 0.34684550\n",
      "Iteration 187, loss = 0.34679531\n",
      "Iteration 188, loss = 0.34639547\n",
      "Iteration 189, loss = 0.34620360\n",
      "Iteration 190, loss = 0.34610855\n",
      "Iteration 191, loss = 0.34574627\n",
      "Iteration 192, loss = 0.34565542\n",
      "Iteration 193, loss = 0.34545397\n",
      "Iteration 194, loss = 0.34528603\n",
      "Iteration 195, loss = 0.34508667\n",
      "Iteration 196, loss = 0.34486905\n",
      "Iteration 197, loss = 0.34457903\n",
      "Iteration 198, loss = 0.34449174\n",
      "Iteration 199, loss = 0.34428513\n",
      "Iteration 200, loss = 0.34407077\n",
      "Iteration 1, loss = 0.79495178\n",
      "Iteration 2, loss = 0.75439763\n",
      "Iteration 3, loss = 0.71470615\n",
      "Iteration 4, loss = 0.68141833\n",
      "Iteration 5, loss = 0.65128674\n",
      "Iteration 6, loss = 0.62474260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.60060353\n",
      "Iteration 8, loss = 0.58131721\n",
      "Iteration 9, loss = 0.56320566\n",
      "Iteration 10, loss = 0.54780497\n",
      "Iteration 11, loss = 0.53470953\n",
      "Iteration 12, loss = 0.52362192\n",
      "Iteration 13, loss = 0.51312125\n",
      "Iteration 14, loss = 0.50454352\n",
      "Iteration 15, loss = 0.49672683\n",
      "Iteration 16, loss = 0.48954239\n",
      "Iteration 17, loss = 0.48297699\n",
      "Iteration 18, loss = 0.47737725\n",
      "Iteration 19, loss = 0.47182412\n",
      "Iteration 20, loss = 0.46676597\n",
      "Iteration 21, loss = 0.46212072\n",
      "Iteration 22, loss = 0.45794146\n",
      "Iteration 23, loss = 0.45451584\n",
      "Iteration 24, loss = 0.45087297\n",
      "Iteration 25, loss = 0.44776413\n",
      "Iteration 26, loss = 0.44471431\n",
      "Iteration 27, loss = 0.44209542\n",
      "Iteration 28, loss = 0.43988682\n",
      "Iteration 29, loss = 0.43749276\n",
      "Iteration 30, loss = 0.43547649\n",
      "Iteration 31, loss = 0.43344560\n",
      "Iteration 32, loss = 0.43198891\n",
      "Iteration 33, loss = 0.43039597\n",
      "Iteration 34, loss = 0.42879163\n",
      "Iteration 35, loss = 0.42745913\n",
      "Iteration 36, loss = 0.42633684\n",
      "Iteration 37, loss = 0.42511806\n",
      "Iteration 38, loss = 0.42412674\n",
      "Iteration 39, loss = 0.42313457\n",
      "Iteration 40, loss = 0.42234338\n",
      "Iteration 41, loss = 0.42133140\n",
      "Iteration 42, loss = 0.42047467\n",
      "Iteration 43, loss = 0.41972641\n",
      "Iteration 44, loss = 0.41909778\n",
      "Iteration 45, loss = 0.41828081\n",
      "Iteration 46, loss = 0.41764254\n",
      "Iteration 47, loss = 0.41695406\n",
      "Iteration 48, loss = 0.41637734\n",
      "Iteration 49, loss = 0.41573112\n",
      "Iteration 50, loss = 0.41514373\n",
      "Iteration 51, loss = 0.41461312\n",
      "Iteration 52, loss = 0.41397793\n",
      "Iteration 53, loss = 0.41339410\n",
      "Iteration 54, loss = 0.41299775\n",
      "Iteration 55, loss = 0.41240278\n",
      "Iteration 56, loss = 0.41190717\n",
      "Iteration 57, loss = 0.41144921\n",
      "Iteration 58, loss = 0.41096014\n",
      "Iteration 59, loss = 0.41048975\n",
      "Iteration 60, loss = 0.41001518\n",
      "Iteration 61, loss = 0.40961746\n",
      "Iteration 62, loss = 0.40914561\n",
      "Iteration 63, loss = 0.40875751\n",
      "Iteration 64, loss = 0.40827948\n",
      "Iteration 65, loss = 0.40779316\n",
      "Iteration 66, loss = 0.40750252\n",
      "Iteration 67, loss = 0.40697120\n",
      "Iteration 68, loss = 0.40672234\n",
      "Iteration 69, loss = 0.40624740\n",
      "Iteration 70, loss = 0.40585645\n",
      "Iteration 71, loss = 0.40545372\n",
      "Iteration 72, loss = 0.40500583\n",
      "Iteration 73, loss = 0.40474116\n",
      "Iteration 74, loss = 0.40433935\n",
      "Iteration 75, loss = 0.40390019\n",
      "Iteration 76, loss = 0.40361864\n",
      "Iteration 77, loss = 0.40316451\n",
      "Iteration 78, loss = 0.40279837\n",
      "Iteration 79, loss = 0.40240004\n",
      "Iteration 80, loss = 0.40205508\n",
      "Iteration 81, loss = 0.40184142\n",
      "Iteration 82, loss = 0.40132222\n",
      "Iteration 83, loss = 0.40117177\n",
      "Iteration 84, loss = 0.40070660\n",
      "Iteration 85, loss = 0.40028155\n",
      "Iteration 86, loss = 0.39994693\n",
      "Iteration 87, loss = 0.39964706\n",
      "Iteration 88, loss = 0.39937828\n",
      "Iteration 89, loss = 0.39903904\n",
      "Iteration 90, loss = 0.39865530\n",
      "Iteration 91, loss = 0.39846338\n",
      "Iteration 92, loss = 0.39806203\n",
      "Iteration 93, loss = 0.39766550\n",
      "Iteration 94, loss = 0.39754691\n",
      "Iteration 95, loss = 0.39713847\n",
      "Iteration 96, loss = 0.39687784\n",
      "Iteration 97, loss = 0.39660261\n",
      "Iteration 98, loss = 0.39635719\n",
      "Iteration 99, loss = 0.39596774\n",
      "Iteration 100, loss = 0.39571458\n",
      "Iteration 101, loss = 0.39540908\n",
      "Iteration 102, loss = 0.39513065\n",
      "Iteration 103, loss = 0.39487480\n",
      "Iteration 104, loss = 0.39459912\n",
      "Iteration 105, loss = 0.39434343\n",
      "Iteration 106, loss = 0.39398928\n",
      "Iteration 107, loss = 0.39370124\n",
      "Iteration 108, loss = 0.39346559\n",
      "Iteration 109, loss = 0.39317260\n",
      "Iteration 110, loss = 0.39298318\n",
      "Iteration 111, loss = 0.39268791\n",
      "Iteration 112, loss = 0.39228895\n",
      "Iteration 113, loss = 0.39211373\n",
      "Iteration 114, loss = 0.39182591\n",
      "Iteration 115, loss = 0.39155090\n",
      "Iteration 116, loss = 0.39140543\n",
      "Iteration 117, loss = 0.39100998\n",
      "Iteration 118, loss = 0.39083325\n",
      "Iteration 119, loss = 0.39056380\n",
      "Iteration 120, loss = 0.39026854\n",
      "Iteration 121, loss = 0.39004111\n",
      "Iteration 122, loss = 0.38975606\n",
      "Iteration 123, loss = 0.38950227\n",
      "Iteration 124, loss = 0.38925100\n",
      "Iteration 125, loss = 0.38901975\n",
      "Iteration 126, loss = 0.38875939\n",
      "Iteration 127, loss = 0.38852308\n",
      "Iteration 128, loss = 0.38820696\n",
      "Iteration 129, loss = 0.38801407\n",
      "Iteration 130, loss = 0.38777867\n",
      "Iteration 131, loss = 0.38752313\n",
      "Iteration 132, loss = 0.38727238\n",
      "Iteration 133, loss = 0.38709805\n",
      "Iteration 134, loss = 0.38682903\n",
      "Iteration 135, loss = 0.38663155\n",
      "Iteration 136, loss = 0.38627719\n",
      "Iteration 137, loss = 0.38622412\n",
      "Iteration 138, loss = 0.38588196\n",
      "Iteration 139, loss = 0.38587131\n",
      "Iteration 140, loss = 0.38542766\n",
      "Iteration 141, loss = 0.38527539\n",
      "Iteration 142, loss = 0.38496509\n",
      "Iteration 143, loss = 0.38469789\n",
      "Iteration 144, loss = 0.38452454\n",
      "Iteration 145, loss = 0.38427880\n",
      "Iteration 146, loss = 0.38412343\n",
      "Iteration 147, loss = 0.38390404\n",
      "Iteration 148, loss = 0.38375168\n",
      "Iteration 149, loss = 0.38337730\n",
      "Iteration 150, loss = 0.38306998\n",
      "Iteration 151, loss = 0.38293735\n",
      "Iteration 152, loss = 0.38264560\n",
      "Iteration 153, loss = 0.38241763\n",
      "Iteration 154, loss = 0.38236767\n",
      "Iteration 155, loss = 0.38226292\n",
      "Iteration 156, loss = 0.38180715\n",
      "Iteration 157, loss = 0.38148620\n",
      "Iteration 158, loss = 0.38134782\n",
      "Iteration 159, loss = 0.38123577\n",
      "Iteration 160, loss = 0.38084863\n",
      "Iteration 161, loss = 0.38067233\n",
      "Iteration 162, loss = 0.38053721\n",
      "Iteration 163, loss = 0.38021470\n",
      "Iteration 164, loss = 0.37995283\n",
      "Iteration 165, loss = 0.37978113\n",
      "Iteration 166, loss = 0.37968710\n",
      "Iteration 167, loss = 0.37943550\n",
      "Iteration 168, loss = 0.37915663\n",
      "Iteration 169, loss = 0.37887768\n",
      "Iteration 170, loss = 0.37871145\n",
      "Iteration 171, loss = 0.37855633\n",
      "Iteration 172, loss = 0.37833500\n",
      "Iteration 173, loss = 0.37817262\n",
      "Iteration 174, loss = 0.37792797\n",
      "Iteration 175, loss = 0.37770752\n",
      "Iteration 176, loss = 0.37757431\n",
      "Iteration 177, loss = 0.37734145\n",
      "Iteration 178, loss = 0.37705524\n",
      "Iteration 179, loss = 0.37686489\n",
      "Iteration 180, loss = 0.37664098\n",
      "Iteration 181, loss = 0.37659291\n",
      "Iteration 182, loss = 0.37618647\n",
      "Iteration 183, loss = 0.37592948\n",
      "Iteration 184, loss = 0.37580088\n",
      "Iteration 185, loss = 0.37562636\n",
      "Iteration 186, loss = 0.37538268\n",
      "Iteration 187, loss = 0.37541983\n",
      "Iteration 188, loss = 0.37501005\n",
      "Iteration 189, loss = 0.37477881\n",
      "Iteration 190, loss = 0.37464632\n",
      "Iteration 191, loss = 0.37431818\n",
      "Iteration 192, loss = 0.37425720\n",
      "Iteration 193, loss = 0.37394340\n",
      "Iteration 194, loss = 0.37382247\n",
      "Iteration 195, loss = 0.37361488\n",
      "Iteration 196, loss = 0.37339426\n",
      "Iteration 197, loss = 0.37315158\n",
      "Iteration 198, loss = 0.37295186\n",
      "Iteration 199, loss = 0.37279127\n",
      "Iteration 200, loss = 0.37255751\n",
      "Iteration 1, loss = 0.79371532\n",
      "Iteration 2, loss = 0.75270072\n",
      "Iteration 3, loss = 0.71482239\n",
      "Iteration 4, loss = 0.68102282\n",
      "Iteration 5, loss = 0.65104071\n",
      "Iteration 6, loss = 0.62363908\n",
      "Iteration 7, loss = 0.60073824\n",
      "Iteration 8, loss = 0.58051631\n",
      "Iteration 9, loss = 0.56268171\n",
      "Iteration 10, loss = 0.54643624\n",
      "Iteration 11, loss = 0.53354307\n",
      "Iteration 12, loss = 0.52140244\n",
      "Iteration 13, loss = 0.51114147\n",
      "Iteration 14, loss = 0.50145172\n",
      "Iteration 15, loss = 0.49296632\n",
      "Iteration 16, loss = 0.48545202\n",
      "Iteration 17, loss = 0.47838215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.47246373\n",
      "Iteration 19, loss = 0.46646351\n",
      "Iteration 20, loss = 0.46065436\n",
      "Iteration 21, loss = 0.45595082\n",
      "Iteration 22, loss = 0.45134016\n",
      "Iteration 23, loss = 0.44692748\n",
      "Iteration 24, loss = 0.44315808\n",
      "Iteration 25, loss = 0.43950548\n",
      "Iteration 26, loss = 0.43618095\n",
      "Iteration 27, loss = 0.43292835\n",
      "Iteration 28, loss = 0.43030290\n",
      "Iteration 29, loss = 0.42750702\n",
      "Iteration 30, loss = 0.42527338\n",
      "Iteration 31, loss = 0.42313343\n",
      "Iteration 32, loss = 0.42109507\n",
      "Iteration 33, loss = 0.41933382\n",
      "Iteration 34, loss = 0.41746556\n",
      "Iteration 35, loss = 0.41583753\n",
      "Iteration 36, loss = 0.41438398\n",
      "Iteration 37, loss = 0.41303519\n",
      "Iteration 38, loss = 0.41162751\n",
      "Iteration 39, loss = 0.41035037\n",
      "Iteration 40, loss = 0.40918879\n",
      "Iteration 41, loss = 0.40806036\n",
      "Iteration 42, loss = 0.40698069\n",
      "Iteration 43, loss = 0.40593948\n",
      "Iteration 44, loss = 0.40504655\n",
      "Iteration 45, loss = 0.40400965\n",
      "Iteration 46, loss = 0.40311783\n",
      "Iteration 47, loss = 0.40218605\n",
      "Iteration 48, loss = 0.40135837\n",
      "Iteration 49, loss = 0.40057256\n",
      "Iteration 50, loss = 0.39978623\n",
      "Iteration 51, loss = 0.39896935\n",
      "Iteration 52, loss = 0.39824421\n",
      "Iteration 53, loss = 0.39751945\n",
      "Iteration 54, loss = 0.39677242\n",
      "Iteration 55, loss = 0.39607699\n",
      "Iteration 56, loss = 0.39545707\n",
      "Iteration 57, loss = 0.39476536\n",
      "Iteration 58, loss = 0.39407802\n",
      "Iteration 59, loss = 0.39341618\n",
      "Iteration 60, loss = 0.39284499\n",
      "Iteration 61, loss = 0.39219810\n",
      "Iteration 62, loss = 0.39164949\n",
      "Iteration 63, loss = 0.39098713\n",
      "Iteration 64, loss = 0.39051471\n",
      "Iteration 65, loss = 0.38990001\n",
      "Iteration 66, loss = 0.38942455\n",
      "Iteration 67, loss = 0.38898169\n",
      "Iteration 68, loss = 0.38834782\n",
      "Iteration 69, loss = 0.38773107\n",
      "Iteration 70, loss = 0.38730408\n",
      "Iteration 71, loss = 0.38669136\n",
      "Iteration 72, loss = 0.38618555\n",
      "Iteration 73, loss = 0.38569200\n",
      "Iteration 74, loss = 0.38520078\n",
      "Iteration 75, loss = 0.38473598\n",
      "Iteration 76, loss = 0.38431711\n",
      "Iteration 77, loss = 0.38372786\n",
      "Iteration 78, loss = 0.38335243\n",
      "Iteration 79, loss = 0.38288812\n",
      "Iteration 80, loss = 0.38245485\n",
      "Iteration 81, loss = 0.38202967\n",
      "Iteration 82, loss = 0.38163181\n",
      "Iteration 83, loss = 0.38115257\n",
      "Iteration 84, loss = 0.38074341\n",
      "Iteration 85, loss = 0.38034731\n",
      "Iteration 86, loss = 0.37990635\n",
      "Iteration 87, loss = 0.37949965\n",
      "Iteration 88, loss = 0.37904508\n",
      "Iteration 89, loss = 0.37860686\n",
      "Iteration 90, loss = 0.37829012\n",
      "Iteration 91, loss = 0.37779878\n",
      "Iteration 92, loss = 0.37767885\n",
      "Iteration 93, loss = 0.37707164\n",
      "Iteration 94, loss = 0.37697139\n",
      "Iteration 95, loss = 0.37630937\n",
      "Iteration 96, loss = 0.37589313\n",
      "Iteration 97, loss = 0.37555577\n",
      "Iteration 98, loss = 0.37514886\n",
      "Iteration 99, loss = 0.37484391\n",
      "Iteration 100, loss = 0.37445409\n",
      "Iteration 101, loss = 0.37428749\n",
      "Iteration 102, loss = 0.37386937\n",
      "Iteration 103, loss = 0.37349544\n",
      "Iteration 104, loss = 0.37331097\n",
      "Iteration 105, loss = 0.37286156\n",
      "Iteration 106, loss = 0.37257904\n",
      "Iteration 107, loss = 0.37226132\n",
      "Iteration 108, loss = 0.37197803\n",
      "Iteration 109, loss = 0.37160596\n",
      "Iteration 110, loss = 0.37128269\n",
      "Iteration 111, loss = 0.37096924\n",
      "Iteration 112, loss = 0.37072875\n",
      "Iteration 113, loss = 0.37034156\n",
      "Iteration 114, loss = 0.37010160\n",
      "Iteration 115, loss = 0.36979479\n",
      "Iteration 116, loss = 0.36940542\n",
      "Iteration 117, loss = 0.36911791\n",
      "Iteration 118, loss = 0.36884308\n",
      "Iteration 119, loss = 0.36863016\n",
      "Iteration 120, loss = 0.36833364\n",
      "Iteration 121, loss = 0.36801872\n",
      "Iteration 122, loss = 0.36774273\n",
      "Iteration 123, loss = 0.36739360\n",
      "Iteration 124, loss = 0.36710736\n",
      "Iteration 125, loss = 0.36689891\n",
      "Iteration 126, loss = 0.36651985\n",
      "Iteration 127, loss = 0.36618522\n",
      "Iteration 128, loss = 0.36601174\n",
      "Iteration 129, loss = 0.36583478\n",
      "Iteration 130, loss = 0.36538746\n",
      "Iteration 131, loss = 0.36515461\n",
      "Iteration 132, loss = 0.36486995\n",
      "Iteration 133, loss = 0.36458845\n",
      "Iteration 134, loss = 0.36433787\n",
      "Iteration 135, loss = 0.36396561\n",
      "Iteration 136, loss = 0.36392487\n",
      "Iteration 137, loss = 0.36344200\n",
      "Iteration 138, loss = 0.36326260\n",
      "Iteration 139, loss = 0.36308468\n",
      "Iteration 140, loss = 0.36280156\n",
      "Iteration 141, loss = 0.36249566\n",
      "Iteration 142, loss = 0.36232148\n",
      "Iteration 143, loss = 0.36204747\n",
      "Iteration 144, loss = 0.36183448\n",
      "Iteration 145, loss = 0.36162014\n",
      "Iteration 146, loss = 0.36131627\n",
      "Iteration 147, loss = 0.36113489\n",
      "Iteration 148, loss = 0.36091940\n",
      "Iteration 149, loss = 0.36059291\n",
      "Iteration 150, loss = 0.36030066\n",
      "Iteration 151, loss = 0.36011006\n",
      "Iteration 152, loss = 0.36002676\n",
      "Iteration 153, loss = 0.35993898\n",
      "Iteration 154, loss = 0.35955431\n",
      "Iteration 155, loss = 0.35907882\n",
      "Iteration 156, loss = 0.35896970\n",
      "Iteration 157, loss = 0.35872929\n",
      "Iteration 158, loss = 0.35846342\n",
      "Iteration 159, loss = 0.35829602\n",
      "Iteration 160, loss = 0.35809060\n",
      "Iteration 161, loss = 0.35786445\n",
      "Iteration 162, loss = 0.35763993\n",
      "Iteration 163, loss = 0.35738988\n",
      "Iteration 164, loss = 0.35725086\n",
      "Iteration 165, loss = 0.35708815\n",
      "Iteration 166, loss = 0.35682843\n",
      "Iteration 167, loss = 0.35652449\n",
      "Iteration 168, loss = 0.35633566\n",
      "Iteration 169, loss = 0.35617669\n",
      "Iteration 170, loss = 0.35608207\n",
      "Iteration 171, loss = 0.35573614\n",
      "Iteration 172, loss = 0.35570451\n",
      "Iteration 173, loss = 0.35547301\n",
      "Iteration 174, loss = 0.35522387\n",
      "Iteration 175, loss = 0.35499510\n",
      "Iteration 176, loss = 0.35478123\n",
      "Iteration 177, loss = 0.35448863\n",
      "Iteration 178, loss = 0.35453152\n",
      "Iteration 179, loss = 0.35410676\n",
      "Iteration 180, loss = 0.35392440\n",
      "Iteration 181, loss = 0.35371085\n",
      "Iteration 182, loss = 0.35351684\n",
      "Iteration 183, loss = 0.35321577\n",
      "Iteration 184, loss = 0.35313347\n",
      "Iteration 185, loss = 0.35296965\n",
      "Iteration 186, loss = 0.35273838\n",
      "Iteration 187, loss = 0.35254091\n",
      "Iteration 188, loss = 0.35226278\n",
      "Iteration 189, loss = 0.35208596\n",
      "Iteration 190, loss = 0.35183965\n",
      "Iteration 191, loss = 0.35166663\n",
      "Iteration 192, loss = 0.35141515\n",
      "Iteration 193, loss = 0.35120557\n",
      "Iteration 194, loss = 0.35114649\n",
      "Iteration 195, loss = 0.35091467\n",
      "Iteration 196, loss = 0.35055894\n",
      "Iteration 197, loss = 0.35054803\n",
      "Iteration 198, loss = 0.35015887\n",
      "Iteration 199, loss = 0.35003473\n",
      "Iteration 200, loss = 0.34967679\n",
      "Iteration 1, loss = 0.78896725\n",
      "Iteration 2, loss = 0.74927512\n",
      "Iteration 3, loss = 0.71229637\n",
      "Iteration 4, loss = 0.67925173\n",
      "Iteration 5, loss = 0.64898551\n",
      "Iteration 6, loss = 0.62220968\n",
      "Iteration 7, loss = 0.59874681\n",
      "Iteration 8, loss = 0.57767724\n",
      "Iteration 9, loss = 0.55931882\n",
      "Iteration 10, loss = 0.54259321\n",
      "Iteration 11, loss = 0.52896453\n",
      "Iteration 12, loss = 0.51637242\n",
      "Iteration 13, loss = 0.50573934\n",
      "Iteration 14, loss = 0.49543367\n",
      "Iteration 15, loss = 0.48673230\n",
      "Iteration 16, loss = 0.47878739\n",
      "Iteration 17, loss = 0.47160258\n",
      "Iteration 18, loss = 0.46540011\n",
      "Iteration 19, loss = 0.45890470\n",
      "Iteration 20, loss = 0.45336571\n",
      "Iteration 21, loss = 0.44815292\n",
      "Iteration 22, loss = 0.44345108\n",
      "Iteration 23, loss = 0.43887289\n",
      "Iteration 24, loss = 0.43494748\n",
      "Iteration 25, loss = 0.43122015\n",
      "Iteration 26, loss = 0.42776633\n",
      "Iteration 27, loss = 0.42461068\n",
      "Iteration 28, loss = 0.42187270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.41910495\n",
      "Iteration 30, loss = 0.41671667\n",
      "Iteration 31, loss = 0.41464314\n",
      "Iteration 32, loss = 0.41255594\n",
      "Iteration 33, loss = 0.41076229\n",
      "Iteration 34, loss = 0.40890506\n",
      "Iteration 35, loss = 0.40720578\n",
      "Iteration 36, loss = 0.40592453\n",
      "Iteration 37, loss = 0.40449176\n",
      "Iteration 38, loss = 0.40306239\n",
      "Iteration 39, loss = 0.40188955\n",
      "Iteration 40, loss = 0.40073540\n",
      "Iteration 41, loss = 0.39957365\n",
      "Iteration 42, loss = 0.39853877\n",
      "Iteration 43, loss = 0.39759823\n",
      "Iteration 44, loss = 0.39666565\n",
      "Iteration 45, loss = 0.39569813\n",
      "Iteration 46, loss = 0.39474499\n",
      "Iteration 47, loss = 0.39389918\n",
      "Iteration 48, loss = 0.39310348\n",
      "Iteration 49, loss = 0.39231551\n",
      "Iteration 50, loss = 0.39146987\n",
      "Iteration 51, loss = 0.39065354\n",
      "Iteration 52, loss = 0.39001264\n",
      "Iteration 53, loss = 0.38930878\n",
      "Iteration 54, loss = 0.38851831\n",
      "Iteration 55, loss = 0.38787372\n",
      "Iteration 56, loss = 0.38722210\n",
      "Iteration 57, loss = 0.38657877\n",
      "Iteration 58, loss = 0.38594300\n",
      "Iteration 59, loss = 0.38528485\n",
      "Iteration 60, loss = 0.38467894\n",
      "Iteration 61, loss = 0.38413218\n",
      "Iteration 62, loss = 0.38354319\n",
      "Iteration 63, loss = 0.38298412\n",
      "Iteration 64, loss = 0.38256855\n",
      "Iteration 65, loss = 0.38190007\n",
      "Iteration 66, loss = 0.38138872\n",
      "Iteration 67, loss = 0.38086585\n",
      "Iteration 68, loss = 0.38032657\n",
      "Iteration 69, loss = 0.37984529\n",
      "Iteration 70, loss = 0.37934214\n",
      "Iteration 71, loss = 0.37883660\n",
      "Iteration 72, loss = 0.37835531\n",
      "Iteration 73, loss = 0.37783551\n",
      "Iteration 74, loss = 0.37740500\n",
      "Iteration 75, loss = 0.37693784\n",
      "Iteration 76, loss = 0.37644394\n",
      "Iteration 77, loss = 0.37596899\n",
      "Iteration 78, loss = 0.37559127\n",
      "Iteration 79, loss = 0.37503037\n",
      "Iteration 80, loss = 0.37463482\n",
      "Iteration 81, loss = 0.37419201\n",
      "Iteration 82, loss = 0.37377054\n",
      "Iteration 83, loss = 0.37341739\n",
      "Iteration 84, loss = 0.37297004\n",
      "Iteration 85, loss = 0.37257962\n",
      "Iteration 86, loss = 0.37203941\n",
      "Iteration 87, loss = 0.37177525\n",
      "Iteration 88, loss = 0.37129760\n",
      "Iteration 89, loss = 0.37093087\n",
      "Iteration 90, loss = 0.37064099\n",
      "Iteration 91, loss = 0.37012332\n",
      "Iteration 92, loss = 0.36997997\n",
      "Iteration 93, loss = 0.36942772\n",
      "Iteration 94, loss = 0.36909401\n",
      "Iteration 95, loss = 0.36859993\n",
      "Iteration 96, loss = 0.36825521\n",
      "Iteration 97, loss = 0.36798055\n",
      "Iteration 98, loss = 0.36753745\n",
      "Iteration 99, loss = 0.36722251\n",
      "Iteration 100, loss = 0.36674642\n",
      "Iteration 101, loss = 0.36662682\n",
      "Iteration 102, loss = 0.36627213\n",
      "Iteration 103, loss = 0.36586878\n",
      "Iteration 104, loss = 0.36562055\n",
      "Iteration 105, loss = 0.36513227\n",
      "Iteration 106, loss = 0.36486038\n",
      "Iteration 107, loss = 0.36458148\n",
      "Iteration 108, loss = 0.36431238\n",
      "Iteration 109, loss = 0.36398063\n",
      "Iteration 110, loss = 0.36359591\n",
      "Iteration 111, loss = 0.36323447\n",
      "Iteration 112, loss = 0.36301592\n",
      "Iteration 113, loss = 0.36265310\n",
      "Iteration 114, loss = 0.36240652\n",
      "Iteration 115, loss = 0.36207620\n",
      "Iteration 116, loss = 0.36169641\n",
      "Iteration 117, loss = 0.36142651\n",
      "Iteration 118, loss = 0.36119556\n",
      "Iteration 119, loss = 0.36097016\n",
      "Iteration 120, loss = 0.36065180\n",
      "Iteration 121, loss = 0.36035547\n",
      "Iteration 122, loss = 0.36005130\n",
      "Iteration 123, loss = 0.35971566\n",
      "Iteration 124, loss = 0.35949734\n",
      "Iteration 125, loss = 0.35919610\n",
      "Iteration 126, loss = 0.35887151\n",
      "Iteration 127, loss = 0.35860769\n",
      "Iteration 128, loss = 0.35834553\n",
      "Iteration 129, loss = 0.35813172\n",
      "Iteration 130, loss = 0.35781723\n",
      "Iteration 131, loss = 0.35746183\n",
      "Iteration 132, loss = 0.35725380\n",
      "Iteration 133, loss = 0.35696616\n",
      "Iteration 134, loss = 0.35664733\n",
      "Iteration 135, loss = 0.35637140\n",
      "Iteration 136, loss = 0.35614011\n",
      "Iteration 137, loss = 0.35596058\n",
      "Iteration 138, loss = 0.35574869\n",
      "Iteration 139, loss = 0.35537679\n",
      "Iteration 140, loss = 0.35507455\n",
      "Iteration 141, loss = 0.35480581\n",
      "Iteration 142, loss = 0.35456552\n",
      "Iteration 143, loss = 0.35418831\n",
      "Iteration 144, loss = 0.35396723\n",
      "Iteration 145, loss = 0.35376761\n",
      "Iteration 146, loss = 0.35354205\n",
      "Iteration 147, loss = 0.35322070\n",
      "Iteration 148, loss = 0.35289780\n",
      "Iteration 149, loss = 0.35262886\n",
      "Iteration 150, loss = 0.35237608\n",
      "Iteration 151, loss = 0.35211155\n",
      "Iteration 152, loss = 0.35202198\n",
      "Iteration 153, loss = 0.35191634\n",
      "Iteration 154, loss = 0.35140822\n",
      "Iteration 155, loss = 0.35114657\n",
      "Iteration 156, loss = 0.35091618\n",
      "Iteration 157, loss = 0.35064187\n",
      "Iteration 158, loss = 0.35038233\n",
      "Iteration 159, loss = 0.35010439\n",
      "Iteration 160, loss = 0.34997677\n",
      "Iteration 161, loss = 0.34971173\n",
      "Iteration 162, loss = 0.34943885\n",
      "Iteration 163, loss = 0.34925282\n",
      "Iteration 164, loss = 0.34896681\n",
      "Iteration 165, loss = 0.34873396\n",
      "Iteration 166, loss = 0.34853430\n",
      "Iteration 167, loss = 0.34826398\n",
      "Iteration 168, loss = 0.34811870\n",
      "Iteration 169, loss = 0.34783397\n",
      "Iteration 170, loss = 0.34766705\n",
      "Iteration 171, loss = 0.34732342\n",
      "Iteration 172, loss = 0.34719463\n",
      "Iteration 173, loss = 0.34701061\n",
      "Iteration 174, loss = 0.34681494\n",
      "Iteration 175, loss = 0.34655196\n",
      "Iteration 176, loss = 0.34626872\n",
      "Iteration 177, loss = 0.34599970\n",
      "Iteration 178, loss = 0.34587368\n",
      "Iteration 179, loss = 0.34559010\n",
      "Iteration 180, loss = 0.34535138\n",
      "Iteration 181, loss = 0.34512649\n",
      "Iteration 182, loss = 0.34493856\n",
      "Iteration 183, loss = 0.34467211\n",
      "Iteration 184, loss = 0.34444054\n",
      "Iteration 185, loss = 0.34428291\n",
      "Iteration 186, loss = 0.34410469\n",
      "Iteration 187, loss = 0.34385362\n",
      "Iteration 188, loss = 0.34367183\n",
      "Iteration 189, loss = 0.34347943\n",
      "Iteration 190, loss = 0.34323890\n",
      "Iteration 191, loss = 0.34296787\n",
      "Iteration 192, loss = 0.34284178\n",
      "Iteration 193, loss = 0.34255648\n",
      "Iteration 194, loss = 0.34247821\n",
      "Iteration 195, loss = 0.34230887\n",
      "Iteration 196, loss = 0.34190396\n",
      "Iteration 197, loss = 0.34189125\n",
      "Iteration 198, loss = 0.34163823\n",
      "Iteration 199, loss = 0.34157018\n",
      "Iteration 200, loss = 0.34113224\n",
      "Iteration 1, loss = 0.78857255\n",
      "Iteration 2, loss = 0.74883510\n",
      "Iteration 3, loss = 0.71175405\n",
      "Iteration 4, loss = 0.67914934\n",
      "Iteration 5, loss = 0.64989440\n",
      "Iteration 6, loss = 0.62441450\n",
      "Iteration 7, loss = 0.60196110\n",
      "Iteration 8, loss = 0.58200348\n",
      "Iteration 9, loss = 0.56468373\n",
      "Iteration 10, loss = 0.54915676\n",
      "Iteration 11, loss = 0.53629303\n",
      "Iteration 12, loss = 0.52528531\n",
      "Iteration 13, loss = 0.51578820\n",
      "Iteration 14, loss = 0.50643471\n",
      "Iteration 15, loss = 0.49900994\n",
      "Iteration 16, loss = 0.49211084\n",
      "Iteration 17, loss = 0.48625408\n",
      "Iteration 18, loss = 0.48044106\n",
      "Iteration 19, loss = 0.47506829\n",
      "Iteration 20, loss = 0.47059276\n",
      "Iteration 21, loss = 0.46631862\n",
      "Iteration 22, loss = 0.46233677\n",
      "Iteration 23, loss = 0.45857954\n",
      "Iteration 24, loss = 0.45533580\n",
      "Iteration 25, loss = 0.45222960\n",
      "Iteration 26, loss = 0.44950322\n",
      "Iteration 27, loss = 0.44695535\n",
      "Iteration 28, loss = 0.44461835\n",
      "Iteration 29, loss = 0.44245596\n",
      "Iteration 30, loss = 0.44042726\n",
      "Iteration 31, loss = 0.43865950\n",
      "Iteration 32, loss = 0.43698492\n",
      "Iteration 33, loss = 0.43544893\n",
      "Iteration 34, loss = 0.43387262\n",
      "Iteration 35, loss = 0.43248208\n",
      "Iteration 36, loss = 0.43134514\n",
      "Iteration 37, loss = 0.43005054\n",
      "Iteration 38, loss = 0.42892100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.42784124\n",
      "Iteration 40, loss = 0.42690398\n",
      "Iteration 41, loss = 0.42589070\n",
      "Iteration 42, loss = 0.42497012\n",
      "Iteration 43, loss = 0.42407821\n",
      "Iteration 44, loss = 0.42324465\n",
      "Iteration 45, loss = 0.42241956\n",
      "Iteration 46, loss = 0.42156774\n",
      "Iteration 47, loss = 0.42081858\n",
      "Iteration 48, loss = 0.41995990\n",
      "Iteration 49, loss = 0.41927195\n",
      "Iteration 50, loss = 0.41853543\n",
      "Iteration 51, loss = 0.41774314\n",
      "Iteration 52, loss = 0.41708890\n",
      "Iteration 53, loss = 0.41640803\n",
      "Iteration 54, loss = 0.41567687\n",
      "Iteration 55, loss = 0.41498684\n",
      "Iteration 56, loss = 0.41433372\n",
      "Iteration 57, loss = 0.41368233\n",
      "Iteration 58, loss = 0.41312886\n",
      "Iteration 59, loss = 0.41237100\n",
      "Iteration 60, loss = 0.41174765\n",
      "Iteration 61, loss = 0.41114821\n",
      "Iteration 62, loss = 0.41053686\n",
      "Iteration 63, loss = 0.40998268\n",
      "Iteration 64, loss = 0.40942117\n",
      "Iteration 65, loss = 0.40889290\n",
      "Iteration 66, loss = 0.40831750\n",
      "Iteration 67, loss = 0.40771533\n",
      "Iteration 68, loss = 0.40718006\n",
      "Iteration 69, loss = 0.40669666\n",
      "Iteration 70, loss = 0.40610505\n",
      "Iteration 71, loss = 0.40548541\n",
      "Iteration 72, loss = 0.40495155\n",
      "Iteration 73, loss = 0.40434959\n",
      "Iteration 74, loss = 0.40389073\n",
      "Iteration 75, loss = 0.40326892\n",
      "Iteration 76, loss = 0.40280579\n",
      "Iteration 77, loss = 0.40230084\n",
      "Iteration 78, loss = 0.40190080\n",
      "Iteration 79, loss = 0.40121133\n",
      "Iteration 80, loss = 0.40077629\n",
      "Iteration 81, loss = 0.40029608\n",
      "Iteration 82, loss = 0.39976326\n",
      "Iteration 83, loss = 0.39938811\n",
      "Iteration 84, loss = 0.39883648\n",
      "Iteration 85, loss = 0.39839553\n",
      "Iteration 86, loss = 0.39787558\n",
      "Iteration 87, loss = 0.39762758\n",
      "Iteration 88, loss = 0.39708989\n",
      "Iteration 89, loss = 0.39652938\n",
      "Iteration 90, loss = 0.39615912\n",
      "Iteration 91, loss = 0.39570200\n",
      "Iteration 92, loss = 0.39538450\n",
      "Iteration 93, loss = 0.39480207\n",
      "Iteration 94, loss = 0.39443061\n",
      "Iteration 95, loss = 0.39395755\n",
      "Iteration 96, loss = 0.39357453\n",
      "Iteration 97, loss = 0.39322130\n",
      "Iteration 98, loss = 0.39272744\n",
      "Iteration 99, loss = 0.39233467\n",
      "Iteration 100, loss = 0.39188340\n",
      "Iteration 101, loss = 0.39174992\n",
      "Iteration 102, loss = 0.39120498\n",
      "Iteration 103, loss = 0.39087344\n",
      "Iteration 104, loss = 0.39047730\n",
      "Iteration 105, loss = 0.39005031\n",
      "Iteration 106, loss = 0.38962117\n",
      "Iteration 107, loss = 0.38928914\n",
      "Iteration 108, loss = 0.38896987\n",
      "Iteration 109, loss = 0.38866175\n",
      "Iteration 110, loss = 0.38816343\n",
      "Iteration 111, loss = 0.38774261\n",
      "Iteration 112, loss = 0.38747226\n",
      "Iteration 113, loss = 0.38705271\n",
      "Iteration 114, loss = 0.38669749\n",
      "Iteration 115, loss = 0.38640915\n",
      "Iteration 116, loss = 0.38601058\n",
      "Iteration 117, loss = 0.38567940\n",
      "Iteration 118, loss = 0.38534845\n",
      "Iteration 119, loss = 0.38507792\n",
      "Iteration 120, loss = 0.38487138\n",
      "Iteration 121, loss = 0.38432753\n",
      "Iteration 122, loss = 0.38405879\n",
      "Iteration 123, loss = 0.38368730\n",
      "Iteration 124, loss = 0.38348262\n",
      "Iteration 125, loss = 0.38302591\n",
      "Iteration 126, loss = 0.38284959\n",
      "Iteration 127, loss = 0.38238487\n",
      "Iteration 128, loss = 0.38206706\n",
      "Iteration 129, loss = 0.38175777\n",
      "Iteration 130, loss = 0.38146888\n",
      "Iteration 131, loss = 0.38108463\n",
      "Iteration 132, loss = 0.38077689\n",
      "Iteration 133, loss = 0.38047065\n",
      "Iteration 134, loss = 0.38012086\n",
      "Iteration 135, loss = 0.37985391\n",
      "Iteration 136, loss = 0.37952048\n",
      "Iteration 137, loss = 0.37923048\n",
      "Iteration 138, loss = 0.37879321\n",
      "Iteration 139, loss = 0.37870527\n",
      "Iteration 140, loss = 0.37833609\n",
      "Iteration 141, loss = 0.37792753\n",
      "Iteration 142, loss = 0.37764614\n",
      "Iteration 143, loss = 0.37731028\n",
      "Iteration 144, loss = 0.37703043\n",
      "Iteration 145, loss = 0.37674595\n",
      "Iteration 146, loss = 0.37638012\n",
      "Iteration 147, loss = 0.37617772\n",
      "Iteration 148, loss = 0.37571897\n",
      "Iteration 149, loss = 0.37541550\n",
      "Iteration 150, loss = 0.37507441\n",
      "Iteration 151, loss = 0.37481043\n",
      "Iteration 152, loss = 0.37457239\n",
      "Iteration 153, loss = 0.37447735\n",
      "Iteration 154, loss = 0.37391558\n",
      "Iteration 155, loss = 0.37368133\n",
      "Iteration 156, loss = 0.37329829\n",
      "Iteration 157, loss = 0.37304439\n",
      "Iteration 158, loss = 0.37269921\n",
      "Iteration 159, loss = 0.37236346\n",
      "Iteration 160, loss = 0.37217308\n",
      "Iteration 161, loss = 0.37173221\n",
      "Iteration 162, loss = 0.37139388\n",
      "Iteration 163, loss = 0.37116214\n",
      "Iteration 164, loss = 0.37087943\n",
      "Iteration 165, loss = 0.37054715\n",
      "Iteration 166, loss = 0.37043737\n",
      "Iteration 167, loss = 0.37006564\n",
      "Iteration 168, loss = 0.36967624\n",
      "Iteration 169, loss = 0.36946316\n",
      "Iteration 170, loss = 0.36914817\n",
      "Iteration 171, loss = 0.36876236\n",
      "Iteration 172, loss = 0.36849568\n",
      "Iteration 173, loss = 0.36823026\n",
      "Iteration 174, loss = 0.36797157\n",
      "Iteration 175, loss = 0.36771374\n",
      "Iteration 176, loss = 0.36737360\n",
      "Iteration 177, loss = 0.36699200\n",
      "Iteration 178, loss = 0.36666684\n",
      "Iteration 179, loss = 0.36645433\n",
      "Iteration 180, loss = 0.36615922\n",
      "Iteration 181, loss = 0.36582200\n",
      "Iteration 182, loss = 0.36558853\n",
      "Iteration 183, loss = 0.36538947\n",
      "Iteration 184, loss = 0.36506489\n",
      "Iteration 185, loss = 0.36483653\n",
      "Iteration 186, loss = 0.36443620\n",
      "Iteration 187, loss = 0.36415067\n",
      "Iteration 188, loss = 0.36386968\n",
      "Iteration 189, loss = 0.36354560\n",
      "Iteration 190, loss = 0.36325406\n",
      "Iteration 191, loss = 0.36297617\n",
      "Iteration 192, loss = 0.36282188\n",
      "Iteration 193, loss = 0.36241341\n",
      "Iteration 194, loss = 0.36228424\n",
      "Iteration 195, loss = 0.36200701\n",
      "Iteration 196, loss = 0.36163494\n",
      "Iteration 197, loss = 0.36155050\n",
      "Iteration 198, loss = 0.36118085\n",
      "Iteration 199, loss = 0.36101085\n",
      "Iteration 200, loss = 0.36086075\n",
      "Iteration 1, loss = 0.79080143\n",
      "Iteration 2, loss = 0.74968284\n",
      "Iteration 3, loss = 0.70880006\n",
      "Iteration 4, loss = 0.67333844\n",
      "Iteration 5, loss = 0.64129410\n",
      "Iteration 6, loss = 0.61295273\n",
      "Iteration 7, loss = 0.58735557\n",
      "Iteration 8, loss = 0.56604448\n",
      "Iteration 9, loss = 0.54630041\n",
      "Iteration 10, loss = 0.52991677\n",
      "Iteration 11, loss = 0.51554296\n",
      "Iteration 12, loss = 0.50327129\n",
      "Iteration 13, loss = 0.49136103\n",
      "Iteration 14, loss = 0.48245593\n",
      "Iteration 15, loss = 0.47395988\n",
      "Iteration 16, loss = 0.46622290\n",
      "Iteration 17, loss = 0.45908993\n",
      "Iteration 18, loss = 0.45347083\n",
      "Iteration 19, loss = 0.44765814\n",
      "Iteration 20, loss = 0.44250822\n",
      "Iteration 21, loss = 0.43775932\n",
      "Iteration 22, loss = 0.43342101\n",
      "Iteration 23, loss = 0.42972645\n",
      "Iteration 24, loss = 0.42610394\n",
      "Iteration 25, loss = 0.42283218\n",
      "Iteration 26, loss = 0.41989534\n",
      "Iteration 27, loss = 0.41706846\n",
      "Iteration 28, loss = 0.41473720\n",
      "Iteration 29, loss = 0.41232916\n",
      "Iteration 30, loss = 0.41016456\n",
      "Iteration 31, loss = 0.40826618\n",
      "Iteration 32, loss = 0.40662447\n",
      "Iteration 33, loss = 0.40487499\n",
      "Iteration 34, loss = 0.40341031\n",
      "Iteration 35, loss = 0.40197939\n",
      "Iteration 36, loss = 0.40076144\n",
      "Iteration 37, loss = 0.39947335\n",
      "Iteration 38, loss = 0.39838945\n",
      "Iteration 39, loss = 0.39733278\n",
      "Iteration 40, loss = 0.39632512\n",
      "Iteration 41, loss = 0.39536324\n",
      "Iteration 42, loss = 0.39445492\n",
      "Iteration 43, loss = 0.39358959\n",
      "Iteration 44, loss = 0.39286609\n",
      "Iteration 45, loss = 0.39196666\n",
      "Iteration 46, loss = 0.39129301\n",
      "Iteration 47, loss = 0.39054942\n",
      "Iteration 48, loss = 0.38978847\n",
      "Iteration 49, loss = 0.38910037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.38844259\n",
      "Iteration 51, loss = 0.38786934\n",
      "Iteration 52, loss = 0.38716962\n",
      "Iteration 53, loss = 0.38651249\n",
      "Iteration 54, loss = 0.38598525\n",
      "Iteration 55, loss = 0.38535527\n",
      "Iteration 56, loss = 0.38479936\n",
      "Iteration 57, loss = 0.38428232\n",
      "Iteration 58, loss = 0.38370362\n",
      "Iteration 59, loss = 0.38308537\n",
      "Iteration 60, loss = 0.38256740\n",
      "Iteration 61, loss = 0.38219751\n",
      "Iteration 62, loss = 0.38175795\n",
      "Iteration 63, loss = 0.38121508\n",
      "Iteration 64, loss = 0.38064507\n",
      "Iteration 65, loss = 0.38006543\n",
      "Iteration 66, loss = 0.37967819\n",
      "Iteration 67, loss = 0.37912118\n",
      "Iteration 68, loss = 0.37882247\n",
      "Iteration 69, loss = 0.37821014\n",
      "Iteration 70, loss = 0.37783309\n",
      "Iteration 71, loss = 0.37739368\n",
      "Iteration 72, loss = 0.37689245\n",
      "Iteration 73, loss = 0.37651473\n",
      "Iteration 74, loss = 0.37606085\n",
      "Iteration 75, loss = 0.37567703\n",
      "Iteration 76, loss = 0.37528552\n",
      "Iteration 77, loss = 0.37481971\n",
      "Iteration 78, loss = 0.37446066\n",
      "Iteration 79, loss = 0.37399690\n",
      "Iteration 80, loss = 0.37362702\n",
      "Iteration 81, loss = 0.37333739\n",
      "Iteration 82, loss = 0.37282537\n",
      "Iteration 83, loss = 0.37254996\n",
      "Iteration 84, loss = 0.37210431\n",
      "Iteration 85, loss = 0.37167206\n",
      "Iteration 86, loss = 0.37142871\n",
      "Iteration 87, loss = 0.37097672\n",
      "Iteration 88, loss = 0.37076411\n",
      "Iteration 89, loss = 0.37027722\n",
      "Iteration 90, loss = 0.36994995\n",
      "Iteration 91, loss = 0.36970851\n",
      "Iteration 92, loss = 0.36934770\n",
      "Iteration 93, loss = 0.36893025\n",
      "Iteration 94, loss = 0.36867983\n",
      "Iteration 95, loss = 0.36827985\n",
      "Iteration 96, loss = 0.36816222\n",
      "Iteration 97, loss = 0.36767316\n",
      "Iteration 98, loss = 0.36740547\n",
      "Iteration 99, loss = 0.36704777\n",
      "Iteration 100, loss = 0.36681869\n",
      "Iteration 101, loss = 0.36642816\n",
      "Iteration 102, loss = 0.36619028\n",
      "Iteration 103, loss = 0.36582337\n",
      "Iteration 104, loss = 0.36561790\n",
      "Iteration 105, loss = 0.36517459\n",
      "Iteration 106, loss = 0.36494164\n",
      "Iteration 107, loss = 0.36465113\n",
      "Iteration 108, loss = 0.36442058\n",
      "Iteration 109, loss = 0.36405674\n",
      "Iteration 110, loss = 0.36379637\n",
      "Iteration 111, loss = 0.36345909\n",
      "Iteration 112, loss = 0.36309829\n",
      "Iteration 113, loss = 0.36288171\n",
      "Iteration 114, loss = 0.36256214\n",
      "Iteration 115, loss = 0.36235082\n",
      "Iteration 116, loss = 0.36206154\n",
      "Iteration 117, loss = 0.36171887\n",
      "Iteration 118, loss = 0.36148411\n",
      "Iteration 119, loss = 0.36116641\n",
      "Iteration 120, loss = 0.36086409\n",
      "Iteration 121, loss = 0.36066515\n",
      "Iteration 122, loss = 0.36034518\n",
      "Iteration 123, loss = 0.36007522\n",
      "Iteration 124, loss = 0.35987027\n",
      "Iteration 125, loss = 0.35957560\n",
      "Iteration 126, loss = 0.35923735\n",
      "Iteration 127, loss = 0.35904922\n",
      "Iteration 128, loss = 0.35870344\n",
      "Iteration 129, loss = 0.35843743\n",
      "Iteration 130, loss = 0.35816957\n",
      "Iteration 131, loss = 0.35798706\n",
      "Iteration 132, loss = 0.35767548\n",
      "Iteration 133, loss = 0.35737708\n",
      "Iteration 134, loss = 0.35715503\n",
      "Iteration 135, loss = 0.35684026\n",
      "Iteration 136, loss = 0.35662477\n",
      "Iteration 137, loss = 0.35648853\n",
      "Iteration 138, loss = 0.35608306\n",
      "Iteration 139, loss = 0.35593433\n",
      "Iteration 140, loss = 0.35565926\n",
      "Iteration 141, loss = 0.35538380\n",
      "Iteration 142, loss = 0.35511966\n",
      "Iteration 143, loss = 0.35491310\n",
      "Iteration 144, loss = 0.35460330\n",
      "Iteration 145, loss = 0.35455598\n",
      "Iteration 146, loss = 0.35426229\n",
      "Iteration 147, loss = 0.35389967\n",
      "Iteration 148, loss = 0.35382904\n",
      "Iteration 149, loss = 0.35347781\n",
      "Iteration 150, loss = 0.35318557\n",
      "Iteration 151, loss = 0.35308196\n",
      "Iteration 152, loss = 0.35273594\n",
      "Iteration 153, loss = 0.35250395\n",
      "Iteration 154, loss = 0.35244244\n",
      "Iteration 155, loss = 0.35226787\n",
      "Iteration 156, loss = 0.35178933\n",
      "Iteration 157, loss = 0.35157112\n",
      "Iteration 158, loss = 0.35140635\n",
      "Iteration 159, loss = 0.35113496\n",
      "Iteration 160, loss = 0.35097502\n",
      "Iteration 161, loss = 0.35069492\n",
      "Iteration 162, loss = 0.35054850\n",
      "Iteration 163, loss = 0.35024970\n",
      "Iteration 164, loss = 0.35007608\n",
      "Iteration 165, loss = 0.34992127\n",
      "Iteration 166, loss = 0.34970312\n",
      "Iteration 167, loss = 0.34948404\n",
      "Iteration 168, loss = 0.34918344\n",
      "Iteration 169, loss = 0.34902128\n",
      "Iteration 170, loss = 0.34877473\n",
      "Iteration 171, loss = 0.34851387\n",
      "Iteration 172, loss = 0.34848238\n",
      "Iteration 173, loss = 0.34832710\n",
      "Iteration 174, loss = 0.34791356\n",
      "Iteration 175, loss = 0.34774870\n",
      "Iteration 176, loss = 0.34783256\n",
      "Iteration 177, loss = 0.34752997\n",
      "Iteration 178, loss = 0.34719641\n",
      "Iteration 179, loss = 0.34709780\n",
      "Iteration 180, loss = 0.34672433\n",
      "Iteration 181, loss = 0.34682544\n",
      "Iteration 182, loss = 0.34639444\n",
      "Iteration 183, loss = 0.34627279\n",
      "Iteration 184, loss = 0.34608102\n",
      "Iteration 185, loss = 0.34591629\n",
      "Iteration 186, loss = 0.34564345\n",
      "Iteration 187, loss = 0.34558286\n",
      "Iteration 188, loss = 0.34518698\n",
      "Iteration 189, loss = 0.34500673\n",
      "Iteration 190, loss = 0.34492234\n",
      "Iteration 191, loss = 0.34455994\n",
      "Iteration 192, loss = 0.34448184\n",
      "Iteration 193, loss = 0.34429388\n",
      "Iteration 194, loss = 0.34413381\n",
      "Iteration 195, loss = 0.34393632\n",
      "Iteration 196, loss = 0.34371224\n",
      "Iteration 197, loss = 0.34342814\n",
      "Iteration 198, loss = 0.34334046\n",
      "Iteration 199, loss = 0.34313266\n",
      "Iteration 200, loss = 0.34291233\n",
      "Iteration 1, loss = 0.79441434\n",
      "Iteration 2, loss = 0.75386012\n",
      "Iteration 3, loss = 0.71416805\n",
      "Iteration 4, loss = 0.68087946\n",
      "Iteration 5, loss = 0.65074661\n",
      "Iteration 6, loss = 0.62420093\n",
      "Iteration 7, loss = 0.60006018\n",
      "Iteration 8, loss = 0.58077156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.56265798\n",
      "Iteration 10, loss = 0.54725417\n",
      "Iteration 11, loss = 0.53415561\n",
      "Iteration 12, loss = 0.52306537\n",
      "Iteration 13, loss = 0.51256160\n",
      "Iteration 14, loss = 0.50398123\n",
      "Iteration 15, loss = 0.49616162\n",
      "Iteration 16, loss = 0.48897414\n",
      "Iteration 17, loss = 0.48240398\n",
      "Iteration 18, loss = 0.47680180\n",
      "Iteration 19, loss = 0.47124536\n",
      "Iteration 20, loss = 0.46618479\n",
      "Iteration 21, loss = 0.46153769\n",
      "Iteration 22, loss = 0.45735710\n",
      "Iteration 23, loss = 0.45393162\n",
      "Iteration 24, loss = 0.45028969\n",
      "Iteration 25, loss = 0.44718145\n",
      "Iteration 26, loss = 0.44413019\n",
      "Iteration 27, loss = 0.44150701\n",
      "Iteration 28, loss = 0.43929591\n",
      "Iteration 29, loss = 0.43689950\n",
      "Iteration 30, loss = 0.43488260\n",
      "Iteration 31, loss = 0.43285032\n",
      "Iteration 32, loss = 0.43139178\n",
      "Iteration 33, loss = 0.42979575\n",
      "Iteration 34, loss = 0.42819164\n",
      "Iteration 35, loss = 0.42685991\n",
      "Iteration 36, loss = 0.42573338\n",
      "Iteration 37, loss = 0.42451076\n",
      "Iteration 38, loss = 0.42351473\n",
      "Iteration 39, loss = 0.42251673\n",
      "Iteration 40, loss = 0.42172161\n",
      "Iteration 41, loss = 0.42070285\n",
      "Iteration 42, loss = 0.41983956\n",
      "Iteration 43, loss = 0.41908552\n",
      "Iteration 44, loss = 0.41845272\n",
      "Iteration 45, loss = 0.41763039\n",
      "Iteration 46, loss = 0.41698880\n",
      "Iteration 47, loss = 0.41629786\n",
      "Iteration 48, loss = 0.41571968\n",
      "Iteration 49, loss = 0.41507239\n",
      "Iteration 50, loss = 0.41448912\n",
      "Iteration 51, loss = 0.41395708\n",
      "Iteration 52, loss = 0.41332703\n",
      "Iteration 53, loss = 0.41274063\n",
      "Iteration 54, loss = 0.41234366\n",
      "Iteration 55, loss = 0.41174254\n",
      "Iteration 56, loss = 0.41124468\n",
      "Iteration 57, loss = 0.41078228\n",
      "Iteration 58, loss = 0.41028740\n",
      "Iteration 59, loss = 0.40981457\n",
      "Iteration 60, loss = 0.40933469\n",
      "Iteration 61, loss = 0.40893627\n",
      "Iteration 62, loss = 0.40845968\n",
      "Iteration 63, loss = 0.40806650\n",
      "Iteration 64, loss = 0.40758487\n",
      "Iteration 65, loss = 0.40709267\n",
      "Iteration 66, loss = 0.40679597\n",
      "Iteration 67, loss = 0.40625522\n",
      "Iteration 68, loss = 0.40599860\n",
      "Iteration 69, loss = 0.40551894\n",
      "Iteration 70, loss = 0.40512647\n",
      "Iteration 71, loss = 0.40471780\n",
      "Iteration 72, loss = 0.40426128\n",
      "Iteration 73, loss = 0.40399266\n",
      "Iteration 74, loss = 0.40358131\n",
      "Iteration 75, loss = 0.40313226\n",
      "Iteration 76, loss = 0.40285323\n",
      "Iteration 77, loss = 0.40239102\n",
      "Iteration 78, loss = 0.40202152\n",
      "Iteration 79, loss = 0.40161766\n",
      "Iteration 80, loss = 0.40126546\n",
      "Iteration 81, loss = 0.40104782\n",
      "Iteration 82, loss = 0.40052496\n",
      "Iteration 83, loss = 0.40037080\n",
      "Iteration 84, loss = 0.39990641\n",
      "Iteration 85, loss = 0.39948175\n",
      "Iteration 86, loss = 0.39914832\n",
      "Iteration 87, loss = 0.39884826\n",
      "Iteration 88, loss = 0.39857721\n",
      "Iteration 89, loss = 0.39823484\n",
      "Iteration 90, loss = 0.39784452\n",
      "Iteration 91, loss = 0.39765399\n",
      "Iteration 92, loss = 0.39725811\n",
      "Iteration 93, loss = 0.39686158\n",
      "Iteration 94, loss = 0.39673656\n",
      "Iteration 95, loss = 0.39632873\n",
      "Iteration 96, loss = 0.39606603\n",
      "Iteration 97, loss = 0.39578445\n",
      "Iteration 98, loss = 0.39553566\n",
      "Iteration 99, loss = 0.39514294\n",
      "Iteration 100, loss = 0.39488752\n",
      "Iteration 101, loss = 0.39457433\n",
      "Iteration 102, loss = 0.39429541\n",
      "Iteration 103, loss = 0.39403565\n",
      "Iteration 104, loss = 0.39375548\n",
      "Iteration 105, loss = 0.39349584\n",
      "Iteration 106, loss = 0.39313280\n",
      "Iteration 107, loss = 0.39283818\n",
      "Iteration 108, loss = 0.39260236\n",
      "Iteration 109, loss = 0.39230208\n",
      "Iteration 110, loss = 0.39211153\n",
      "Iteration 111, loss = 0.39181117\n",
      "Iteration 112, loss = 0.39140870\n",
      "Iteration 113, loss = 0.39123716\n",
      "Iteration 114, loss = 0.39094812\n",
      "Iteration 115, loss = 0.39066950\n",
      "Iteration 116, loss = 0.39052509\n",
      "Iteration 117, loss = 0.39012468\n",
      "Iteration 118, loss = 0.38994573\n",
      "Iteration 119, loss = 0.38967175\n",
      "Iteration 120, loss = 0.38937543\n",
      "Iteration 121, loss = 0.38913845\n",
      "Iteration 122, loss = 0.38885176\n",
      "Iteration 123, loss = 0.38859312\n",
      "Iteration 124, loss = 0.38833807\n",
      "Iteration 125, loss = 0.38810556\n",
      "Iteration 126, loss = 0.38784053\n",
      "Iteration 127, loss = 0.38760469\n",
      "Iteration 128, loss = 0.38729122\n",
      "Iteration 129, loss = 0.38709522\n",
      "Iteration 130, loss = 0.38685490\n",
      "Iteration 131, loss = 0.38659777\n",
      "Iteration 132, loss = 0.38634373\n",
      "Iteration 133, loss = 0.38616678\n",
      "Iteration 134, loss = 0.38589569\n",
      "Iteration 135, loss = 0.38569842\n",
      "Iteration 136, loss = 0.38534147\n",
      "Iteration 137, loss = 0.38528639\n",
      "Iteration 138, loss = 0.38493565\n",
      "Iteration 139, loss = 0.38492335\n",
      "Iteration 140, loss = 0.38448023\n",
      "Iteration 141, loss = 0.38433279\n",
      "Iteration 142, loss = 0.38401900\n",
      "Iteration 143, loss = 0.38374647\n",
      "Iteration 144, loss = 0.38357325\n",
      "Iteration 145, loss = 0.38332462\n",
      "Iteration 146, loss = 0.38316908\n",
      "Iteration 147, loss = 0.38294822\n",
      "Iteration 148, loss = 0.38279388\n",
      "Iteration 149, loss = 0.38241806\n",
      "Iteration 150, loss = 0.38211043\n",
      "Iteration 151, loss = 0.38198260\n",
      "Iteration 152, loss = 0.38168538\n",
      "Iteration 153, loss = 0.38145181\n",
      "Iteration 154, loss = 0.38139420\n",
      "Iteration 155, loss = 0.38129101\n",
      "Iteration 156, loss = 0.38083849\n",
      "Iteration 157, loss = 0.38052005\n",
      "Iteration 158, loss = 0.38037677\n",
      "Iteration 159, loss = 0.38025808\n",
      "Iteration 160, loss = 0.37985989\n",
      "Iteration 161, loss = 0.37967997\n",
      "Iteration 162, loss = 0.37953805\n",
      "Iteration 163, loss = 0.37922371\n",
      "Iteration 164, loss = 0.37895501\n",
      "Iteration 165, loss = 0.37878176\n",
      "Iteration 166, loss = 0.37869355\n",
      "Iteration 167, loss = 0.37843158\n",
      "Iteration 168, loss = 0.37816118\n",
      "Iteration 169, loss = 0.37788429\n",
      "Iteration 170, loss = 0.37772135\n",
      "Iteration 171, loss = 0.37756834\n",
      "Iteration 172, loss = 0.37734055\n",
      "Iteration 173, loss = 0.37717260\n",
      "Iteration 174, loss = 0.37692897\n",
      "Iteration 175, loss = 0.37669303\n",
      "Iteration 176, loss = 0.37657572\n",
      "Iteration 177, loss = 0.37633403\n",
      "Iteration 178, loss = 0.37604389\n",
      "Iteration 179, loss = 0.37586232\n",
      "Iteration 180, loss = 0.37562288\n",
      "Iteration 181, loss = 0.37556954\n",
      "Iteration 182, loss = 0.37516062\n",
      "Iteration 183, loss = 0.37490005\n",
      "Iteration 184, loss = 0.37476904\n",
      "Iteration 185, loss = 0.37459331\n",
      "Iteration 186, loss = 0.37434765\n",
      "Iteration 187, loss = 0.37436987\n",
      "Iteration 188, loss = 0.37395905\n",
      "Iteration 189, loss = 0.37372177\n",
      "Iteration 190, loss = 0.37359299\n",
      "Iteration 191, loss = 0.37325853\n",
      "Iteration 192, loss = 0.37320242\n",
      "Iteration 193, loss = 0.37288546\n",
      "Iteration 194, loss = 0.37276570\n",
      "Iteration 195, loss = 0.37255387\n",
      "Iteration 196, loss = 0.37233692\n",
      "Iteration 197, loss = 0.37208310\n",
      "Iteration 198, loss = 0.37188515\n",
      "Iteration 199, loss = 0.37172186\n",
      "Iteration 200, loss = 0.37148121\n",
      "Iteration 1, loss = 0.79317864\n",
      "Iteration 2, loss = 0.75216413\n",
      "Iteration 3, loss = 0.71428505\n",
      "Iteration 4, loss = 0.68048446\n",
      "Iteration 5, loss = 0.65050095\n",
      "Iteration 6, loss = 0.62309759\n",
      "Iteration 7, loss = 0.60019486\n",
      "Iteration 8, loss = 0.57997105\n",
      "Iteration 9, loss = 0.56213423\n",
      "Iteration 10, loss = 0.54588665\n",
      "Iteration 11, loss = 0.53299137\n",
      "Iteration 12, loss = 0.52084830\n",
      "Iteration 13, loss = 0.51058442\n",
      "Iteration 14, loss = 0.50089212\n",
      "Iteration 15, loss = 0.49240377\n",
      "Iteration 16, loss = 0.48488592\n",
      "Iteration 17, loss = 0.47781257\n",
      "Iteration 18, loss = 0.47189191\n",
      "Iteration 19, loss = 0.46588838\n",
      "Iteration 20, loss = 0.46007491\n",
      "Iteration 21, loss = 0.45536803\n",
      "Iteration 22, loss = 0.45075555\n",
      "Iteration 23, loss = 0.44634052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.44256934\n",
      "Iteration 25, loss = 0.43891314\n",
      "Iteration 26, loss = 0.43558656\n",
      "Iteration 27, loss = 0.43233096\n",
      "Iteration 28, loss = 0.42970336\n",
      "Iteration 29, loss = 0.42690397\n",
      "Iteration 30, loss = 0.42466555\n",
      "Iteration 31, loss = 0.42252171\n",
      "Iteration 32, loss = 0.42048074\n",
      "Iteration 33, loss = 0.41871530\n",
      "Iteration 34, loss = 0.41684306\n",
      "Iteration 35, loss = 0.41521234\n",
      "Iteration 36, loss = 0.41375567\n",
      "Iteration 37, loss = 0.41240254\n",
      "Iteration 38, loss = 0.41099168\n",
      "Iteration 39, loss = 0.40971151\n",
      "Iteration 40, loss = 0.40854485\n",
      "Iteration 41, loss = 0.40741165\n",
      "Iteration 42, loss = 0.40632718\n",
      "Iteration 43, loss = 0.40528271\n",
      "Iteration 44, loss = 0.40438366\n",
      "Iteration 45, loss = 0.40334281\n",
      "Iteration 46, loss = 0.40244942\n",
      "Iteration 47, loss = 0.40151676\n",
      "Iteration 48, loss = 0.40068639\n",
      "Iteration 49, loss = 0.39989907\n",
      "Iteration 50, loss = 0.39911064\n",
      "Iteration 51, loss = 0.39829475\n",
      "Iteration 52, loss = 0.39756808\n",
      "Iteration 53, loss = 0.39684880\n",
      "Iteration 54, loss = 0.39609927\n",
      "Iteration 55, loss = 0.39540195\n",
      "Iteration 56, loss = 0.39478347\n",
      "Iteration 57, loss = 0.39408836\n",
      "Iteration 58, loss = 0.39339886\n",
      "Iteration 59, loss = 0.39273463\n",
      "Iteration 60, loss = 0.39216170\n",
      "Iteration 61, loss = 0.39151473\n",
      "Iteration 62, loss = 0.39096127\n",
      "Iteration 63, loss = 0.39029625\n",
      "Iteration 64, loss = 0.38982223\n",
      "Iteration 65, loss = 0.38919874\n",
      "Iteration 66, loss = 0.38872036\n",
      "Iteration 67, loss = 0.38827352\n",
      "Iteration 68, loss = 0.38763917\n",
      "Iteration 69, loss = 0.38701890\n",
      "Iteration 70, loss = 0.38658683\n",
      "Iteration 71, loss = 0.38597111\n",
      "Iteration 72, loss = 0.38546304\n",
      "Iteration 73, loss = 0.38496651\n",
      "Iteration 74, loss = 0.38446785\n",
      "Iteration 75, loss = 0.38400007\n",
      "Iteration 76, loss = 0.38357918\n",
      "Iteration 77, loss = 0.38298918\n",
      "Iteration 78, loss = 0.38260973\n",
      "Iteration 79, loss = 0.38214490\n",
      "Iteration 80, loss = 0.38171016\n",
      "Iteration 81, loss = 0.38128757\n",
      "Iteration 82, loss = 0.38089094\n",
      "Iteration 83, loss = 0.38041065\n",
      "Iteration 84, loss = 0.37999539\n",
      "Iteration 85, loss = 0.37960206\n",
      "Iteration 86, loss = 0.37915744\n",
      "Iteration 87, loss = 0.37874920\n",
      "Iteration 88, loss = 0.37829330\n",
      "Iteration 89, loss = 0.37785327\n",
      "Iteration 90, loss = 0.37752817\n",
      "Iteration 91, loss = 0.37703875\n",
      "Iteration 92, loss = 0.37692270\n",
      "Iteration 93, loss = 0.37631306\n",
      "Iteration 94, loss = 0.37621524\n",
      "Iteration 95, loss = 0.37554496\n",
      "Iteration 96, loss = 0.37512741\n",
      "Iteration 97, loss = 0.37478110\n",
      "Iteration 98, loss = 0.37436879\n",
      "Iteration 99, loss = 0.37405818\n",
      "Iteration 100, loss = 0.37365731\n",
      "Iteration 101, loss = 0.37348587\n",
      "Iteration 102, loss = 0.37306486\n",
      "Iteration 103, loss = 0.37268240\n",
      "Iteration 104, loss = 0.37248838\n",
      "Iteration 105, loss = 0.37203400\n",
      "Iteration 106, loss = 0.37174704\n",
      "Iteration 107, loss = 0.37141941\n",
      "Iteration 108, loss = 0.37113465\n",
      "Iteration 109, loss = 0.37075534\n",
      "Iteration 110, loss = 0.37043384\n",
      "Iteration 111, loss = 0.37011460\n",
      "Iteration 112, loss = 0.36986967\n",
      "Iteration 113, loss = 0.36947546\n",
      "Iteration 114, loss = 0.36922736\n",
      "Iteration 115, loss = 0.36891490\n",
      "Iteration 116, loss = 0.36851689\n",
      "Iteration 117, loss = 0.36822149\n",
      "Iteration 118, loss = 0.36794165\n",
      "Iteration 119, loss = 0.36772411\n",
      "Iteration 120, loss = 0.36742717\n",
      "Iteration 121, loss = 0.36710868\n",
      "Iteration 122, loss = 0.36683393\n",
      "Iteration 123, loss = 0.36647964\n",
      "Iteration 124, loss = 0.36618917\n",
      "Iteration 125, loss = 0.36598215\n",
      "Iteration 126, loss = 0.36560714\n",
      "Iteration 127, loss = 0.36526737\n",
      "Iteration 128, loss = 0.36509198\n",
      "Iteration 129, loss = 0.36491433\n",
      "Iteration 130, loss = 0.36446980\n",
      "Iteration 131, loss = 0.36423228\n",
      "Iteration 132, loss = 0.36395006\n",
      "Iteration 133, loss = 0.36367184\n",
      "Iteration 134, loss = 0.36342078\n",
      "Iteration 135, loss = 0.36304566\n",
      "Iteration 136, loss = 0.36299960\n",
      "Iteration 137, loss = 0.36251314\n",
      "Iteration 138, loss = 0.36232727\n",
      "Iteration 139, loss = 0.36214740\n",
      "Iteration 140, loss = 0.36185202\n",
      "Iteration 141, loss = 0.36153790\n",
      "Iteration 142, loss = 0.36135870\n",
      "Iteration 143, loss = 0.36108844\n",
      "Iteration 144, loss = 0.36086773\n",
      "Iteration 145, loss = 0.36064592\n",
      "Iteration 146, loss = 0.36034641\n",
      "Iteration 147, loss = 0.36015584\n",
      "Iteration 148, loss = 0.35994719\n",
      "Iteration 149, loss = 0.35961321\n",
      "Iteration 150, loss = 0.35931167\n",
      "Iteration 151, loss = 0.35911565\n",
      "Iteration 152, loss = 0.35902599\n",
      "Iteration 153, loss = 0.35893585\n",
      "Iteration 154, loss = 0.35855418\n",
      "Iteration 155, loss = 0.35807084\n",
      "Iteration 156, loss = 0.35795624\n",
      "Iteration 157, loss = 0.35771633\n",
      "Iteration 158, loss = 0.35744666\n",
      "Iteration 159, loss = 0.35727978\n",
      "Iteration 160, loss = 0.35707345\n",
      "Iteration 161, loss = 0.35684335\n",
      "Iteration 162, loss = 0.35661259\n",
      "Iteration 163, loss = 0.35635452\n",
      "Iteration 164, loss = 0.35622291\n",
      "Iteration 165, loss = 0.35604968\n",
      "Iteration 166, loss = 0.35577436\n",
      "Iteration 167, loss = 0.35546678\n",
      "Iteration 168, loss = 0.35528498\n",
      "Iteration 169, loss = 0.35512540\n",
      "Iteration 170, loss = 0.35502138\n",
      "Iteration 171, loss = 0.35467512\n",
      "Iteration 172, loss = 0.35463198\n",
      "Iteration 173, loss = 0.35440776\n",
      "Iteration 174, loss = 0.35415564\n",
      "Iteration 175, loss = 0.35391878\n",
      "Iteration 176, loss = 0.35369355\n",
      "Iteration 177, loss = 0.35339450\n",
      "Iteration 178, loss = 0.35343093\n",
      "Iteration 179, loss = 0.35300119\n",
      "Iteration 180, loss = 0.35280644\n",
      "Iteration 181, loss = 0.35258572\n",
      "Iteration 182, loss = 0.35239001\n",
      "Iteration 183, loss = 0.35207697\n",
      "Iteration 184, loss = 0.35199072\n",
      "Iteration 185, loss = 0.35182019\n",
      "Iteration 186, loss = 0.35158409\n",
      "Iteration 187, loss = 0.35138113\n",
      "Iteration 188, loss = 0.35110131\n",
      "Iteration 189, loss = 0.35092622\n",
      "Iteration 190, loss = 0.35067620\n",
      "Iteration 191, loss = 0.35049012\n",
      "Iteration 192, loss = 0.35023442\n",
      "Iteration 193, loss = 0.35001939\n",
      "Iteration 194, loss = 0.34996571\n",
      "Iteration 195, loss = 0.34973458\n",
      "Iteration 196, loss = 0.34937730\n",
      "Iteration 197, loss = 0.34936946\n",
      "Iteration 198, loss = 0.34897248\n",
      "Iteration 199, loss = 0.34883529\n",
      "Iteration 200, loss = 0.34846813\n",
      "Iteration 1, loss = 0.78843068\n",
      "Iteration 2, loss = 0.74873844\n",
      "Iteration 3, loss = 0.71175907\n",
      "Iteration 4, loss = 0.67871337\n",
      "Iteration 5, loss = 0.64844559\n",
      "Iteration 6, loss = 0.62166785\n",
      "Iteration 7, loss = 0.59820285\n",
      "Iteration 8, loss = 0.57713129\n",
      "Iteration 9, loss = 0.55877039\n",
      "Iteration 10, loss = 0.54204163\n",
      "Iteration 11, loss = 0.52840985\n",
      "Iteration 12, loss = 0.51581499\n",
      "Iteration 13, loss = 0.50517890\n",
      "Iteration 14, loss = 0.49486985\n",
      "Iteration 15, loss = 0.48616567\n",
      "Iteration 16, loss = 0.47821822\n",
      "Iteration 17, loss = 0.47103030\n",
      "Iteration 18, loss = 0.46482523\n",
      "Iteration 19, loss = 0.45832696\n",
      "Iteration 20, loss = 0.45278479\n",
      "Iteration 21, loss = 0.44756894\n",
      "Iteration 22, loss = 0.44286400\n",
      "Iteration 23, loss = 0.43828268\n",
      "Iteration 24, loss = 0.43435431\n",
      "Iteration 25, loss = 0.43062431\n",
      "Iteration 26, loss = 0.42716767\n",
      "Iteration 27, loss = 0.42400917\n",
      "Iteration 28, loss = 0.42126931\n",
      "Iteration 29, loss = 0.41849873\n",
      "Iteration 30, loss = 0.41610729\n",
      "Iteration 31, loss = 0.41403065\n",
      "Iteration 32, loss = 0.41194074\n",
      "Iteration 33, loss = 0.41014412\n",
      "Iteration 34, loss = 0.40828355\n",
      "Iteration 35, loss = 0.40657842\n",
      "Iteration 36, loss = 0.40529692\n",
      "Iteration 37, loss = 0.40386592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.40243707\n",
      "Iteration 39, loss = 0.40126738\n",
      "Iteration 40, loss = 0.40011412\n",
      "Iteration 41, loss = 0.39895432\n",
      "Iteration 42, loss = 0.39792356\n",
      "Iteration 43, loss = 0.39698572\n",
      "Iteration 44, loss = 0.39605320\n",
      "Iteration 45, loss = 0.39509386\n",
      "Iteration 46, loss = 0.39414267\n",
      "Iteration 47, loss = 0.39330281\n",
      "Iteration 48, loss = 0.39251083\n",
      "Iteration 49, loss = 0.39172786\n",
      "Iteration 50, loss = 0.39087896\n",
      "Iteration 51, loss = 0.39006468\n",
      "Iteration 52, loss = 0.38942220\n",
      "Iteration 53, loss = 0.38871930\n",
      "Iteration 54, loss = 0.38793261\n",
      "Iteration 55, loss = 0.38729163\n",
      "Iteration 56, loss = 0.38663820\n",
      "Iteration 57, loss = 0.38599929\n",
      "Iteration 58, loss = 0.38536021\n",
      "Iteration 59, loss = 0.38470192\n",
      "Iteration 60, loss = 0.38409623\n",
      "Iteration 61, loss = 0.38354875\n",
      "Iteration 62, loss = 0.38295526\n",
      "Iteration 63, loss = 0.38239223\n",
      "Iteration 64, loss = 0.38197072\n",
      "Iteration 65, loss = 0.38130222\n",
      "Iteration 66, loss = 0.38079000\n",
      "Iteration 67, loss = 0.38026503\n",
      "Iteration 68, loss = 0.37972334\n",
      "Iteration 69, loss = 0.37924132\n",
      "Iteration 70, loss = 0.37873323\n",
      "Iteration 71, loss = 0.37822454\n",
      "Iteration 72, loss = 0.37773920\n",
      "Iteration 73, loss = 0.37721681\n",
      "Iteration 74, loss = 0.37678571\n",
      "Iteration 75, loss = 0.37631090\n",
      "Iteration 76, loss = 0.37581251\n",
      "Iteration 77, loss = 0.37533669\n",
      "Iteration 78, loss = 0.37495784\n",
      "Iteration 79, loss = 0.37439721\n",
      "Iteration 80, loss = 0.37399436\n",
      "Iteration 81, loss = 0.37354829\n",
      "Iteration 82, loss = 0.37311948\n",
      "Iteration 83, loss = 0.37277109\n",
      "Iteration 84, loss = 0.37232299\n",
      "Iteration 85, loss = 0.37192867\n",
      "Iteration 86, loss = 0.37138748\n",
      "Iteration 87, loss = 0.37111621\n",
      "Iteration 88, loss = 0.37063085\n",
      "Iteration 89, loss = 0.37025274\n",
      "Iteration 90, loss = 0.36995959\n",
      "Iteration 91, loss = 0.36943755\n",
      "Iteration 92, loss = 0.36928626\n",
      "Iteration 93, loss = 0.36872958\n",
      "Iteration 94, loss = 0.36839042\n",
      "Iteration 95, loss = 0.36789442\n",
      "Iteration 96, loss = 0.36754439\n",
      "Iteration 97, loss = 0.36726426\n",
      "Iteration 98, loss = 0.36681339\n",
      "Iteration 99, loss = 0.36649649\n",
      "Iteration 100, loss = 0.36601817\n",
      "Iteration 101, loss = 0.36590042\n",
      "Iteration 102, loss = 0.36554147\n",
      "Iteration 103, loss = 0.36513290\n",
      "Iteration 104, loss = 0.36487957\n",
      "Iteration 105, loss = 0.36439127\n",
      "Iteration 106, loss = 0.36411708\n",
      "Iteration 107, loss = 0.36383675\n",
      "Iteration 108, loss = 0.36356764\n",
      "Iteration 109, loss = 0.36323152\n",
      "Iteration 110, loss = 0.36284593\n",
      "Iteration 111, loss = 0.36247068\n",
      "Iteration 112, loss = 0.36224877\n",
      "Iteration 113, loss = 0.36188146\n",
      "Iteration 114, loss = 0.36163630\n",
      "Iteration 115, loss = 0.36130673\n",
      "Iteration 116, loss = 0.36092418\n",
      "Iteration 117, loss = 0.36064817\n",
      "Iteration 118, loss = 0.36041418\n",
      "Iteration 119, loss = 0.36019347\n",
      "Iteration 120, loss = 0.35987526\n",
      "Iteration 121, loss = 0.35958123\n",
      "Iteration 122, loss = 0.35926697\n",
      "Iteration 123, loss = 0.35893283\n",
      "Iteration 124, loss = 0.35871365\n",
      "Iteration 125, loss = 0.35841086\n",
      "Iteration 126, loss = 0.35807999\n",
      "Iteration 127, loss = 0.35781003\n",
      "Iteration 128, loss = 0.35754241\n",
      "Iteration 129, loss = 0.35732124\n",
      "Iteration 130, loss = 0.35700379\n",
      "Iteration 131, loss = 0.35664108\n",
      "Iteration 132, loss = 0.35642518\n",
      "Iteration 133, loss = 0.35612373\n",
      "Iteration 134, loss = 0.35580077\n",
      "Iteration 135, loss = 0.35552516\n",
      "Iteration 136, loss = 0.35528820\n",
      "Iteration 137, loss = 0.35509727\n",
      "Iteration 138, loss = 0.35487786\n",
      "Iteration 139, loss = 0.35450348\n",
      "Iteration 140, loss = 0.35419889\n",
      "Iteration 141, loss = 0.35393256\n",
      "Iteration 142, loss = 0.35368379\n",
      "Iteration 143, loss = 0.35330534\n",
      "Iteration 144, loss = 0.35308897\n",
      "Iteration 145, loss = 0.35289138\n",
      "Iteration 146, loss = 0.35265749\n",
      "Iteration 147, loss = 0.35233239\n",
      "Iteration 148, loss = 0.35200578\n",
      "Iteration 149, loss = 0.35172775\n",
      "Iteration 150, loss = 0.35147454\n",
      "Iteration 151, loss = 0.35119921\n",
      "Iteration 152, loss = 0.35110850\n",
      "Iteration 153, loss = 0.35101137\n",
      "Iteration 154, loss = 0.35049666\n",
      "Iteration 155, loss = 0.35022222\n",
      "Iteration 156, loss = 0.34998830\n",
      "Iteration 157, loss = 0.34971422\n",
      "Iteration 158, loss = 0.34944617\n",
      "Iteration 159, loss = 0.34916826\n",
      "Iteration 160, loss = 0.34903126\n",
      "Iteration 161, loss = 0.34875790\n",
      "Iteration 162, loss = 0.34848847\n",
      "Iteration 163, loss = 0.34828913\n",
      "Iteration 164, loss = 0.34799033\n",
      "Iteration 165, loss = 0.34775642\n",
      "Iteration 166, loss = 0.34755550\n",
      "Iteration 167, loss = 0.34728723\n",
      "Iteration 168, loss = 0.34713721\n",
      "Iteration 169, loss = 0.34684355\n",
      "Iteration 170, loss = 0.34667051\n",
      "Iteration 171, loss = 0.34631926\n",
      "Iteration 172, loss = 0.34619095\n",
      "Iteration 173, loss = 0.34599830\n",
      "Iteration 174, loss = 0.34580106\n",
      "Iteration 175, loss = 0.34553561\n",
      "Iteration 176, loss = 0.34524108\n",
      "Iteration 177, loss = 0.34496696\n",
      "Iteration 178, loss = 0.34483803\n",
      "Iteration 179, loss = 0.34455369\n",
      "Iteration 180, loss = 0.34430051\n",
      "Iteration 181, loss = 0.34407259\n",
      "Iteration 182, loss = 0.34388625\n",
      "Iteration 183, loss = 0.34361753\n",
      "Iteration 184, loss = 0.34338025\n",
      "Iteration 185, loss = 0.34322219\n",
      "Iteration 186, loss = 0.34304010\n",
      "Iteration 187, loss = 0.34278848\n",
      "Iteration 188, loss = 0.34260309\n",
      "Iteration 189, loss = 0.34240130\n",
      "Iteration 190, loss = 0.34216181\n",
      "Iteration 191, loss = 0.34188347\n",
      "Iteration 192, loss = 0.34175005\n",
      "Iteration 193, loss = 0.34146484\n",
      "Iteration 194, loss = 0.34138419\n",
      "Iteration 195, loss = 0.34121465\n",
      "Iteration 196, loss = 0.34080426\n",
      "Iteration 197, loss = 0.34078429\n",
      "Iteration 198, loss = 0.34052595\n",
      "Iteration 199, loss = 0.34044531\n",
      "Iteration 200, loss = 0.34001432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.78803468\n",
      "Iteration 2, loss = 0.74829686\n",
      "Iteration 3, loss = 0.71121488\n",
      "Iteration 4, loss = 0.67860927\n",
      "Iteration 5, loss = 0.64935283\n",
      "Iteration 6, loss = 0.62387156\n",
      "Iteration 7, loss = 0.60141621\n",
      "Iteration 8, loss = 0.58145660\n",
      "Iteration 9, loss = 0.56413445\n",
      "Iteration 10, loss = 0.54860555\n",
      "Iteration 11, loss = 0.53573854\n",
      "Iteration 12, loss = 0.52472787\n",
      "Iteration 13, loss = 0.51522822\n",
      "Iteration 14, loss = 0.50587304\n",
      "Iteration 15, loss = 0.49844499\n",
      "Iteration 16, loss = 0.49154308\n",
      "Iteration 17, loss = 0.48568328\n",
      "Iteration 18, loss = 0.47986690\n",
      "Iteration 19, loss = 0.47449069\n",
      "Iteration 20, loss = 0.47001187\n",
      "Iteration 21, loss = 0.46573472\n",
      "Iteration 22, loss = 0.46175020\n",
      "Iteration 23, loss = 0.45798983\n",
      "Iteration 24, loss = 0.45474276\n",
      "Iteration 25, loss = 0.45163323\n",
      "Iteration 26, loss = 0.44890344\n",
      "Iteration 27, loss = 0.44635228\n",
      "Iteration 28, loss = 0.44401259\n",
      "Iteration 29, loss = 0.44184776\n",
      "Iteration 30, loss = 0.43981573\n",
      "Iteration 31, loss = 0.43804652\n",
      "Iteration 32, loss = 0.43636839\n",
      "Iteration 33, loss = 0.43482955\n",
      "Iteration 34, loss = 0.43325000\n",
      "Iteration 35, loss = 0.43185558\n",
      "Iteration 36, loss = 0.43071761\n",
      "Iteration 37, loss = 0.42942125\n",
      "Iteration 38, loss = 0.42828875\n",
      "Iteration 39, loss = 0.42720721\n",
      "Iteration 40, loss = 0.42626713\n",
      "Iteration 41, loss = 0.42525098\n",
      "Iteration 42, loss = 0.42432757\n",
      "Iteration 43, loss = 0.42343388\n",
      "Iteration 44, loss = 0.42259813\n",
      "Iteration 45, loss = 0.42177129\n",
      "Iteration 46, loss = 0.42091609\n",
      "Iteration 47, loss = 0.42016454\n",
      "Iteration 48, loss = 0.41930149\n",
      "Iteration 49, loss = 0.41861145\n",
      "Iteration 50, loss = 0.41787204\n",
      "Iteration 51, loss = 0.41707488\n",
      "Iteration 52, loss = 0.41641680\n",
      "Iteration 53, loss = 0.41573099\n",
      "Iteration 54, loss = 0.41499607\n",
      "Iteration 55, loss = 0.41430468\n",
      "Iteration 56, loss = 0.41365071\n",
      "Iteration 57, loss = 0.41299432\n",
      "Iteration 58, loss = 0.41243745\n",
      "Iteration 59, loss = 0.41167614\n",
      "Iteration 60, loss = 0.41105277\n",
      "Iteration 61, loss = 0.41045333\n",
      "Iteration 62, loss = 0.40983793\n",
      "Iteration 63, loss = 0.40928149\n",
      "Iteration 64, loss = 0.40871561\n",
      "Iteration 65, loss = 0.40818648\n",
      "Iteration 66, loss = 0.40761185\n",
      "Iteration 67, loss = 0.40700593\n",
      "Iteration 68, loss = 0.40647095\n",
      "Iteration 69, loss = 0.40598266\n",
      "Iteration 70, loss = 0.40538897\n",
      "Iteration 71, loss = 0.40476781\n",
      "Iteration 72, loss = 0.40422844\n",
      "Iteration 73, loss = 0.40362497\n",
      "Iteration 74, loss = 0.40316402\n",
      "Iteration 75, loss = 0.40253847\n",
      "Iteration 76, loss = 0.40207170\n",
      "Iteration 77, loss = 0.40156671\n",
      "Iteration 78, loss = 0.40116481\n",
      "Iteration 79, loss = 0.40047000\n",
      "Iteration 80, loss = 0.40002995\n",
      "Iteration 81, loss = 0.39954659\n",
      "Iteration 82, loss = 0.39901109\n",
      "Iteration 83, loss = 0.39863897\n",
      "Iteration 84, loss = 0.39808633\n",
      "Iteration 85, loss = 0.39764750\n",
      "Iteration 86, loss = 0.39712799\n",
      "Iteration 87, loss = 0.39687808\n",
      "Iteration 88, loss = 0.39634093\n",
      "Iteration 89, loss = 0.39577720\n",
      "Iteration 90, loss = 0.39540417\n",
      "Iteration 91, loss = 0.39494383\n",
      "Iteration 92, loss = 0.39462518\n",
      "Iteration 93, loss = 0.39404281\n",
      "Iteration 94, loss = 0.39367215\n",
      "Iteration 95, loss = 0.39319425\n",
      "Iteration 96, loss = 0.39280593\n",
      "Iteration 97, loss = 0.39244640\n",
      "Iteration 98, loss = 0.39194964\n",
      "Iteration 99, loss = 0.39155676\n",
      "Iteration 100, loss = 0.39109904\n",
      "Iteration 101, loss = 0.39096687\n",
      "Iteration 102, loss = 0.39041930\n",
      "Iteration 103, loss = 0.39008339\n",
      "Iteration 104, loss = 0.38968804\n",
      "Iteration 105, loss = 0.38925681\n",
      "Iteration 106, loss = 0.38882461\n",
      "Iteration 107, loss = 0.38848696\n",
      "Iteration 108, loss = 0.38816434\n",
      "Iteration 109, loss = 0.38785328\n",
      "Iteration 110, loss = 0.38735187\n",
      "Iteration 111, loss = 0.38692188\n",
      "Iteration 112, loss = 0.38665494\n",
      "Iteration 113, loss = 0.38623233\n",
      "Iteration 114, loss = 0.38586855\n",
      "Iteration 115, loss = 0.38557961\n",
      "Iteration 116, loss = 0.38517882\n",
      "Iteration 117, loss = 0.38484788\n",
      "Iteration 118, loss = 0.38450913\n",
      "Iteration 119, loss = 0.38423442\n",
      "Iteration 120, loss = 0.38402145\n",
      "Iteration 121, loss = 0.38346392\n",
      "Iteration 122, loss = 0.38318967\n",
      "Iteration 123, loss = 0.38281204\n",
      "Iteration 124, loss = 0.38259544\n",
      "Iteration 125, loss = 0.38214012\n",
      "Iteration 126, loss = 0.38195792\n",
      "Iteration 127, loss = 0.38148690\n",
      "Iteration 128, loss = 0.38115920\n",
      "Iteration 129, loss = 0.38084851\n",
      "Iteration 130, loss = 0.38055747\n",
      "Iteration 131, loss = 0.38016836\n",
      "Iteration 132, loss = 0.37985787\n",
      "Iteration 133, loss = 0.37954893\n",
      "Iteration 134, loss = 0.37918916\n",
      "Iteration 135, loss = 0.37891659\n",
      "Iteration 136, loss = 0.37857942\n",
      "Iteration 137, loss = 0.37828952\n",
      "Iteration 138, loss = 0.37784364\n",
      "Iteration 139, loss = 0.37774223\n",
      "Iteration 140, loss = 0.37737666\n",
      "Iteration 141, loss = 0.37697241\n",
      "Iteration 142, loss = 0.37668499\n",
      "Iteration 143, loss = 0.37633780\n",
      "Iteration 144, loss = 0.37604593\n",
      "Iteration 145, loss = 0.37575513\n",
      "Iteration 146, loss = 0.37538731\n",
      "Iteration 147, loss = 0.37518426\n",
      "Iteration 148, loss = 0.37472002\n",
      "Iteration 149, loss = 0.37441211\n",
      "Iteration 150, loss = 0.37407009\n",
      "Iteration 151, loss = 0.37379708\n",
      "Iteration 152, loss = 0.37355280\n",
      "Iteration 153, loss = 0.37344369\n",
      "Iteration 154, loss = 0.37287862\n",
      "Iteration 155, loss = 0.37264190\n",
      "Iteration 156, loss = 0.37224257\n",
      "Iteration 157, loss = 0.37198768\n",
      "Iteration 158, loss = 0.37164364\n",
      "Iteration 159, loss = 0.37130963\n",
      "Iteration 160, loss = 0.37111201\n",
      "Iteration 161, loss = 0.37066562\n",
      "Iteration 162, loss = 0.37032891\n",
      "Iteration 163, loss = 0.37008587\n",
      "Iteration 164, loss = 0.36979934\n",
      "Iteration 165, loss = 0.36945405\n",
      "Iteration 166, loss = 0.36934960\n",
      "Iteration 167, loss = 0.36896983\n",
      "Iteration 168, loss = 0.36857778\n",
      "Iteration 169, loss = 0.36835737\n",
      "Iteration 170, loss = 0.36804437\n",
      "Iteration 171, loss = 0.36765682\n",
      "Iteration 172, loss = 0.36738624\n",
      "Iteration 173, loss = 0.36711586\n",
      "Iteration 174, loss = 0.36685261\n",
      "Iteration 175, loss = 0.36658983\n",
      "Iteration 176, loss = 0.36625071\n",
      "Iteration 177, loss = 0.36586806\n",
      "Iteration 178, loss = 0.36553716\n",
      "Iteration 179, loss = 0.36532770\n",
      "Iteration 180, loss = 0.36502713\n",
      "Iteration 181, loss = 0.36468733\n",
      "Iteration 182, loss = 0.36446377\n",
      "Iteration 183, loss = 0.36427007\n",
      "Iteration 184, loss = 0.36393878\n",
      "Iteration 185, loss = 0.36370652\n",
      "Iteration 186, loss = 0.36329694\n",
      "Iteration 187, loss = 0.36301744\n",
      "Iteration 188, loss = 0.36273272\n",
      "Iteration 189, loss = 0.36239635\n",
      "Iteration 190, loss = 0.36210261\n",
      "Iteration 191, loss = 0.36181605\n",
      "Iteration 192, loss = 0.36167093\n",
      "Iteration 193, loss = 0.36124757\n",
      "Iteration 194, loss = 0.36111884\n",
      "Iteration 195, loss = 0.36084532\n",
      "Iteration 196, loss = 0.36046478\n",
      "Iteration 197, loss = 0.36036556\n",
      "Iteration 198, loss = 0.35999094\n",
      "Iteration 199, loss = 0.35981103\n",
      "Iteration 200, loss = 0.35967059\n",
      "Iteration 1, loss = 0.79074773\n",
      "Iteration 2, loss = 0.74962911\n",
      "Iteration 3, loss = 0.70874622\n",
      "Iteration 4, loss = 0.67328448\n",
      "Iteration 5, loss = 0.64124000\n",
      "Iteration 6, loss = 0.61289846\n",
      "Iteration 7, loss = 0.58730109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.56598979\n",
      "Iteration 9, loss = 0.54624551\n",
      "Iteration 10, loss = 0.52986160\n",
      "Iteration 11, loss = 0.51548757\n",
      "Iteration 12, loss = 0.50321566\n",
      "Iteration 13, loss = 0.49130512\n",
      "Iteration 14, loss = 0.48239965\n",
      "Iteration 15, loss = 0.47390341\n",
      "Iteration 16, loss = 0.46616613\n",
      "Iteration 17, loss = 0.45903286\n",
      "Iteration 18, loss = 0.45341346\n",
      "Iteration 19, loss = 0.44760043\n",
      "Iteration 20, loss = 0.44245008\n",
      "Iteration 21, loss = 0.43770091\n",
      "Iteration 22, loss = 0.43336229\n",
      "Iteration 23, loss = 0.42966739\n",
      "Iteration 24, loss = 0.42604453\n",
      "Iteration 25, loss = 0.42277238\n",
      "Iteration 26, loss = 0.41983531\n",
      "Iteration 27, loss = 0.41700819\n",
      "Iteration 28, loss = 0.41467675\n",
      "Iteration 29, loss = 0.41226838\n",
      "Iteration 30, loss = 0.41010388\n",
      "Iteration 31, loss = 0.40820505\n",
      "Iteration 32, loss = 0.40656323\n",
      "Iteration 33, loss = 0.40481341\n",
      "Iteration 34, loss = 0.40334852\n",
      "Iteration 35, loss = 0.40191722\n",
      "Iteration 36, loss = 0.40069923\n",
      "Iteration 37, loss = 0.39941064\n",
      "Iteration 38, loss = 0.39832657\n",
      "Iteration 39, loss = 0.39726953\n",
      "Iteration 40, loss = 0.39626214\n",
      "Iteration 41, loss = 0.39529980\n",
      "Iteration 42, loss = 0.39439155\n",
      "Iteration 43, loss = 0.39352665\n",
      "Iteration 44, loss = 0.39280265\n",
      "Iteration 45, loss = 0.39190317\n",
      "Iteration 46, loss = 0.39122954\n",
      "Iteration 47, loss = 0.39048561\n",
      "Iteration 48, loss = 0.38972594\n",
      "Iteration 49, loss = 0.38903765\n",
      "Iteration 50, loss = 0.38837952\n",
      "Iteration 51, loss = 0.38780658\n",
      "Iteration 52, loss = 0.38710671\n",
      "Iteration 53, loss = 0.38644873\n",
      "Iteration 54, loss = 0.38592070\n",
      "Iteration 55, loss = 0.38529159\n",
      "Iteration 56, loss = 0.38473592\n",
      "Iteration 57, loss = 0.38421811\n",
      "Iteration 58, loss = 0.38363952\n",
      "Iteration 59, loss = 0.38302118\n",
      "Iteration 60, loss = 0.38250270\n",
      "Iteration 61, loss = 0.38213325\n",
      "Iteration 62, loss = 0.38169235\n",
      "Iteration 63, loss = 0.38115000\n",
      "Iteration 64, loss = 0.38058076\n",
      "Iteration 65, loss = 0.38000154\n",
      "Iteration 66, loss = 0.37961456\n",
      "Iteration 67, loss = 0.37905712\n",
      "Iteration 68, loss = 0.37875815\n",
      "Iteration 69, loss = 0.37814547\n",
      "Iteration 70, loss = 0.37776854\n",
      "Iteration 71, loss = 0.37732911\n",
      "Iteration 72, loss = 0.37682759\n",
      "Iteration 73, loss = 0.37644911\n",
      "Iteration 74, loss = 0.37599533\n",
      "Iteration 75, loss = 0.37561047\n",
      "Iteration 76, loss = 0.37521842\n",
      "Iteration 77, loss = 0.37475214\n",
      "Iteration 78, loss = 0.37439298\n",
      "Iteration 79, loss = 0.37393021\n",
      "Iteration 80, loss = 0.37356014\n",
      "Iteration 81, loss = 0.37327082\n",
      "Iteration 82, loss = 0.37275656\n",
      "Iteration 83, loss = 0.37248068\n",
      "Iteration 84, loss = 0.37203186\n",
      "Iteration 85, loss = 0.37159958\n",
      "Iteration 86, loss = 0.37135613\n",
      "Iteration 87, loss = 0.37090495\n",
      "Iteration 88, loss = 0.37069439\n",
      "Iteration 89, loss = 0.37020384\n",
      "Iteration 90, loss = 0.36987552\n",
      "Iteration 91, loss = 0.36963432\n",
      "Iteration 92, loss = 0.36927520\n",
      "Iteration 93, loss = 0.36886019\n",
      "Iteration 94, loss = 0.36860726\n",
      "Iteration 95, loss = 0.36820669\n",
      "Iteration 96, loss = 0.36808800\n",
      "Iteration 97, loss = 0.36759790\n",
      "Iteration 98, loss = 0.36732950\n",
      "Iteration 99, loss = 0.36697389\n",
      "Iteration 100, loss = 0.36674408\n",
      "Iteration 101, loss = 0.36635612\n",
      "Iteration 102, loss = 0.36611571\n",
      "Iteration 103, loss = 0.36574912\n",
      "Iteration 104, loss = 0.36554315\n",
      "Iteration 105, loss = 0.36510020\n",
      "Iteration 106, loss = 0.36487053\n",
      "Iteration 107, loss = 0.36457887\n",
      "Iteration 108, loss = 0.36435053\n",
      "Iteration 109, loss = 0.36398618\n",
      "Iteration 110, loss = 0.36372832\n",
      "Iteration 111, loss = 0.36338601\n",
      "Iteration 112, loss = 0.36302612\n",
      "Iteration 113, loss = 0.36281100\n",
      "Iteration 114, loss = 0.36249324\n",
      "Iteration 115, loss = 0.36228401\n",
      "Iteration 116, loss = 0.36199620\n",
      "Iteration 117, loss = 0.36165769\n",
      "Iteration 118, loss = 0.36142061\n",
      "Iteration 119, loss = 0.36110460\n",
      "Iteration 120, loss = 0.36080200\n",
      "Iteration 121, loss = 0.36060372\n",
      "Iteration 122, loss = 0.36028616\n",
      "Iteration 123, loss = 0.36001334\n",
      "Iteration 124, loss = 0.35980601\n",
      "Iteration 125, loss = 0.35951231\n",
      "Iteration 126, loss = 0.35918044\n",
      "Iteration 127, loss = 0.35899575\n",
      "Iteration 128, loss = 0.35864736\n",
      "Iteration 129, loss = 0.35837767\n",
      "Iteration 130, loss = 0.35811519\n",
      "Iteration 131, loss = 0.35793507\n",
      "Iteration 132, loss = 0.35762009\n",
      "Iteration 133, loss = 0.35732057\n",
      "Iteration 134, loss = 0.35710012\n",
      "Iteration 135, loss = 0.35678574\n",
      "Iteration 136, loss = 0.35656861\n",
      "Iteration 137, loss = 0.35642914\n",
      "Iteration 138, loss = 0.35602660\n",
      "Iteration 139, loss = 0.35588554\n",
      "Iteration 140, loss = 0.35560325\n",
      "Iteration 141, loss = 0.35532703\n",
      "Iteration 142, loss = 0.35506357\n",
      "Iteration 143, loss = 0.35485771\n",
      "Iteration 144, loss = 0.35454652\n",
      "Iteration 145, loss = 0.35450216\n",
      "Iteration 146, loss = 0.35420990\n",
      "Iteration 147, loss = 0.35384375\n",
      "Iteration 148, loss = 0.35377047\n",
      "Iteration 149, loss = 0.35341031\n",
      "Iteration 150, loss = 0.35312323\n",
      "Iteration 151, loss = 0.35302468\n",
      "Iteration 152, loss = 0.35268233\n",
      "Iteration 153, loss = 0.35244863\n",
      "Iteration 154, loss = 0.35238056\n",
      "Iteration 155, loss = 0.35219932\n",
      "Iteration 156, loss = 0.35172807\n",
      "Iteration 157, loss = 0.35151003\n",
      "Iteration 158, loss = 0.35134838\n",
      "Iteration 159, loss = 0.35108263\n",
      "Iteration 160, loss = 0.35091959\n",
      "Iteration 161, loss = 0.35064066\n",
      "Iteration 162, loss = 0.35049585\n",
      "Iteration 163, loss = 0.35019410\n",
      "Iteration 164, loss = 0.35002179\n",
      "Iteration 165, loss = 0.34987110\n",
      "Iteration 166, loss = 0.34965194\n",
      "Iteration 167, loss = 0.34943250\n",
      "Iteration 168, loss = 0.34913006\n",
      "Iteration 169, loss = 0.34896824\n",
      "Iteration 170, loss = 0.34871915\n",
      "Iteration 171, loss = 0.34846239\n",
      "Iteration 172, loss = 0.34842860\n",
      "Iteration 173, loss = 0.34828082\n",
      "Iteration 174, loss = 0.34786488\n",
      "Iteration 175, loss = 0.34770711\n",
      "Iteration 176, loss = 0.34778158\n",
      "Iteration 177, loss = 0.34747746\n",
      "Iteration 178, loss = 0.34714476\n",
      "Iteration 179, loss = 0.34704437\n",
      "Iteration 180, loss = 0.34667371\n",
      "Iteration 181, loss = 0.34677834\n",
      "Iteration 182, loss = 0.34634524\n",
      "Iteration 183, loss = 0.34622230\n",
      "Iteration 184, loss = 0.34602834\n",
      "Iteration 185, loss = 0.34586600\n",
      "Iteration 186, loss = 0.34559037\n",
      "Iteration 187, loss = 0.34553106\n",
      "Iteration 188, loss = 0.34513422\n",
      "Iteration 189, loss = 0.34495607\n",
      "Iteration 190, loss = 0.34487108\n",
      "Iteration 191, loss = 0.34450853\n",
      "Iteration 192, loss = 0.34443414\n",
      "Iteration 193, loss = 0.34424974\n",
      "Iteration 194, loss = 0.34408782\n",
      "Iteration 195, loss = 0.34389158\n",
      "Iteration 196, loss = 0.34366653\n",
      "Iteration 197, loss = 0.34338730\n",
      "Iteration 198, loss = 0.34330193\n",
      "Iteration 199, loss = 0.34309104\n",
      "Iteration 200, loss = 0.34287282\n",
      "Iteration 1, loss = 0.79436059\n",
      "Iteration 2, loss = 0.75380636\n",
      "Iteration 3, loss = 0.71411421\n",
      "Iteration 4, loss = 0.68082551\n",
      "Iteration 5, loss = 0.65069252\n",
      "Iteration 6, loss = 0.62414682\n",
      "Iteration 7, loss = 0.60000597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.58071688\n",
      "Iteration 9, loss = 0.56260274\n",
      "Iteration 10, loss = 0.54719857\n",
      "Iteration 11, loss = 0.53409979\n",
      "Iteration 12, loss = 0.52300938\n",
      "Iteration 13, loss = 0.51250533\n",
      "Iteration 14, loss = 0.50392475\n",
      "Iteration 15, loss = 0.49610467\n",
      "Iteration 16, loss = 0.48891667\n",
      "Iteration 17, loss = 0.48234622\n",
      "Iteration 18, loss = 0.47674356\n",
      "Iteration 19, loss = 0.47118678\n",
      "Iteration 20, loss = 0.46612548\n",
      "Iteration 21, loss = 0.46147772\n",
      "Iteration 22, loss = 0.45729663\n",
      "Iteration 23, loss = 0.45387073\n",
      "Iteration 24, loss = 0.45022826\n",
      "Iteration 25, loss = 0.44712002\n",
      "Iteration 26, loss = 0.44406821\n",
      "Iteration 27, loss = 0.44144484\n",
      "Iteration 28, loss = 0.43923360\n",
      "Iteration 29, loss = 0.43683761\n",
      "Iteration 30, loss = 0.43482078\n",
      "Iteration 31, loss = 0.43278844\n",
      "Iteration 32, loss = 0.43133017\n",
      "Iteration 33, loss = 0.42973372\n",
      "Iteration 34, loss = 0.42812981\n",
      "Iteration 35, loss = 0.42679799\n",
      "Iteration 36, loss = 0.42567076\n",
      "Iteration 37, loss = 0.42444835\n",
      "Iteration 38, loss = 0.42345199\n",
      "Iteration 39, loss = 0.42245429\n",
      "Iteration 40, loss = 0.42165929\n",
      "Iteration 41, loss = 0.42064075\n",
      "Iteration 42, loss = 0.41977791\n",
      "Iteration 43, loss = 0.41902324\n",
      "Iteration 44, loss = 0.41839040\n",
      "Iteration 45, loss = 0.41756727\n",
      "Iteration 46, loss = 0.41692529\n",
      "Iteration 47, loss = 0.41623415\n",
      "Iteration 48, loss = 0.41565493\n",
      "Iteration 49, loss = 0.41500702\n",
      "Iteration 50, loss = 0.41442123\n",
      "Iteration 51, loss = 0.41388777\n",
      "Iteration 52, loss = 0.41325528\n",
      "Iteration 53, loss = 0.41266746\n",
      "Iteration 54, loss = 0.41226939\n",
      "Iteration 55, loss = 0.41166795\n",
      "Iteration 56, loss = 0.41116768\n",
      "Iteration 57, loss = 0.41070559\n",
      "Iteration 58, loss = 0.41021084\n",
      "Iteration 59, loss = 0.40973936\n",
      "Iteration 60, loss = 0.40925896\n",
      "Iteration 61, loss = 0.40886268\n",
      "Iteration 62, loss = 0.40838726\n",
      "Iteration 63, loss = 0.40799350\n",
      "Iteration 64, loss = 0.40751317\n",
      "Iteration 65, loss = 0.40702183\n",
      "Iteration 66, loss = 0.40672437\n",
      "Iteration 67, loss = 0.40618495\n",
      "Iteration 68, loss = 0.40592838\n",
      "Iteration 69, loss = 0.40544844\n",
      "Iteration 70, loss = 0.40505202\n",
      "Iteration 71, loss = 0.40464342\n",
      "Iteration 72, loss = 0.40418839\n",
      "Iteration 73, loss = 0.40391963\n",
      "Iteration 74, loss = 0.40350756\n",
      "Iteration 75, loss = 0.40305769\n",
      "Iteration 76, loss = 0.40277455\n",
      "Iteration 77, loss = 0.40231365\n",
      "Iteration 78, loss = 0.40194343\n",
      "Iteration 79, loss = 0.40154031\n",
      "Iteration 80, loss = 0.40118896\n",
      "Iteration 81, loss = 0.40097102\n",
      "Iteration 82, loss = 0.40044857\n",
      "Iteration 83, loss = 0.40029214\n",
      "Iteration 84, loss = 0.39982910\n",
      "Iteration 85, loss = 0.39940391\n",
      "Iteration 86, loss = 0.39907033\n",
      "Iteration 87, loss = 0.39877315\n",
      "Iteration 88, loss = 0.39850361\n",
      "Iteration 89, loss = 0.39816550\n",
      "Iteration 90, loss = 0.39777835\n",
      "Iteration 91, loss = 0.39759098\n",
      "Iteration 92, loss = 0.39719197\n",
      "Iteration 93, loss = 0.39679397\n",
      "Iteration 94, loss = 0.39667412\n",
      "Iteration 95, loss = 0.39626580\n",
      "Iteration 96, loss = 0.39600217\n",
      "Iteration 97, loss = 0.39572187\n",
      "Iteration 98, loss = 0.39547464\n",
      "Iteration 99, loss = 0.39508161\n",
      "Iteration 100, loss = 0.39482569\n",
      "Iteration 101, loss = 0.39450979\n",
      "Iteration 102, loss = 0.39423004\n",
      "Iteration 103, loss = 0.39396730\n",
      "Iteration 104, loss = 0.39368694\n",
      "Iteration 105, loss = 0.39342611\n",
      "Iteration 106, loss = 0.39306214\n",
      "Iteration 107, loss = 0.39276838\n",
      "Iteration 108, loss = 0.39253116\n",
      "Iteration 109, loss = 0.39223756\n",
      "Iteration 110, loss = 0.39204943\n",
      "Iteration 111, loss = 0.39174645\n",
      "Iteration 112, loss = 0.39134921\n",
      "Iteration 113, loss = 0.39117546\n",
      "Iteration 114, loss = 0.39088702\n",
      "Iteration 115, loss = 0.39060992\n",
      "Iteration 116, loss = 0.39046012\n",
      "Iteration 117, loss = 0.39006157\n",
      "Iteration 118, loss = 0.38987971\n",
      "Iteration 119, loss = 0.38961281\n",
      "Iteration 120, loss = 0.38931063\n",
      "Iteration 121, loss = 0.38907687\n",
      "Iteration 122, loss = 0.38878890\n",
      "Iteration 123, loss = 0.38853092\n",
      "Iteration 124, loss = 0.38827763\n",
      "Iteration 125, loss = 0.38804463\n",
      "Iteration 126, loss = 0.38778031\n",
      "Iteration 127, loss = 0.38753812\n",
      "Iteration 128, loss = 0.38722351\n",
      "Iteration 129, loss = 0.38702542\n",
      "Iteration 130, loss = 0.38678664\n",
      "Iteration 131, loss = 0.38653435\n",
      "Iteration 132, loss = 0.38627843\n",
      "Iteration 133, loss = 0.38610222\n",
      "Iteration 134, loss = 0.38582619\n",
      "Iteration 135, loss = 0.38562456\n",
      "Iteration 136, loss = 0.38526903\n",
      "Iteration 137, loss = 0.38521097\n",
      "Iteration 138, loss = 0.38486233\n",
      "Iteration 139, loss = 0.38485331\n",
      "Iteration 140, loss = 0.38440529\n",
      "Iteration 141, loss = 0.38425167\n",
      "Iteration 142, loss = 0.38393253\n",
      "Iteration 143, loss = 0.38366456\n",
      "Iteration 144, loss = 0.38348747\n",
      "Iteration 145, loss = 0.38323815\n",
      "Iteration 146, loss = 0.38307826\n",
      "Iteration 147, loss = 0.38285443\n",
      "Iteration 148, loss = 0.38270558\n",
      "Iteration 149, loss = 0.38232220\n",
      "Iteration 150, loss = 0.38201635\n",
      "Iteration 151, loss = 0.38188249\n",
      "Iteration 152, loss = 0.38158715\n",
      "Iteration 153, loss = 0.38135883\n",
      "Iteration 154, loss = 0.38130319\n",
      "Iteration 155, loss = 0.38119383\n",
      "Iteration 156, loss = 0.38074449\n",
      "Iteration 157, loss = 0.38042455\n",
      "Iteration 158, loss = 0.38028350\n",
      "Iteration 159, loss = 0.38017007\n",
      "Iteration 160, loss = 0.37977205\n",
      "Iteration 161, loss = 0.37959640\n",
      "Iteration 162, loss = 0.37945776\n",
      "Iteration 163, loss = 0.37913171\n",
      "Iteration 164, loss = 0.37886321\n",
      "Iteration 165, loss = 0.37869326\n",
      "Iteration 166, loss = 0.37860218\n",
      "Iteration 167, loss = 0.37834297\n",
      "Iteration 168, loss = 0.37806821\n",
      "Iteration 169, loss = 0.37778584\n",
      "Iteration 170, loss = 0.37762052\n",
      "Iteration 171, loss = 0.37747024\n",
      "Iteration 172, loss = 0.37724517\n",
      "Iteration 173, loss = 0.37708104\n",
      "Iteration 174, loss = 0.37683822\n",
      "Iteration 175, loss = 0.37660591\n",
      "Iteration 176, loss = 0.37647966\n",
      "Iteration 177, loss = 0.37625335\n",
      "Iteration 178, loss = 0.37595961\n",
      "Iteration 179, loss = 0.37577895\n",
      "Iteration 180, loss = 0.37554696\n",
      "Iteration 181, loss = 0.37549912\n",
      "Iteration 182, loss = 0.37508797\n",
      "Iteration 183, loss = 0.37482611\n",
      "Iteration 184, loss = 0.37469670\n",
      "Iteration 185, loss = 0.37451794\n",
      "Iteration 186, loss = 0.37427076\n",
      "Iteration 187, loss = 0.37429261\n",
      "Iteration 188, loss = 0.37388510\n",
      "Iteration 189, loss = 0.37365607\n",
      "Iteration 190, loss = 0.37352926\n",
      "Iteration 191, loss = 0.37318841\n",
      "Iteration 192, loss = 0.37313435\n",
      "Iteration 193, loss = 0.37282050\n",
      "Iteration 194, loss = 0.37270044\n",
      "Iteration 195, loss = 0.37249192\n",
      "Iteration 196, loss = 0.37227523\n",
      "Iteration 197, loss = 0.37202174\n",
      "Iteration 198, loss = 0.37182005\n",
      "Iteration 199, loss = 0.37165032\n",
      "Iteration 200, loss = 0.37141188\n",
      "Iteration 1, loss = 0.79312493\n",
      "Iteration 2, loss = 0.75211045\n",
      "Iteration 3, loss = 0.71423130\n",
      "Iteration 4, loss = 0.68043060\n",
      "Iteration 5, loss = 0.65044695\n",
      "Iteration 6, loss = 0.62304345\n",
      "Iteration 7, loss = 0.60014056\n",
      "Iteration 8, loss = 0.57991659\n",
      "Iteration 9, loss = 0.56207954\n",
      "Iteration 10, loss = 0.54583173\n",
      "Iteration 11, loss = 0.53293618\n",
      "Iteration 12, loss = 0.52079292\n",
      "Iteration 13, loss = 0.51052883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.50083632\n",
      "Iteration 15, loss = 0.49234746\n",
      "Iteration 16, loss = 0.48482909\n",
      "Iteration 17, loss = 0.47775522\n",
      "Iteration 18, loss = 0.47183455\n",
      "Iteration 19, loss = 0.46583058\n",
      "Iteration 20, loss = 0.46001654\n",
      "Iteration 21, loss = 0.45530924\n",
      "Iteration 22, loss = 0.45069643\n",
      "Iteration 23, loss = 0.44628107\n",
      "Iteration 24, loss = 0.44250971\n",
      "Iteration 25, loss = 0.43885335\n",
      "Iteration 26, loss = 0.43552665\n",
      "Iteration 27, loss = 0.43227024\n",
      "Iteration 28, loss = 0.42964250\n",
      "Iteration 29, loss = 0.42684297\n",
      "Iteration 30, loss = 0.42460444\n",
      "Iteration 31, loss = 0.42246039\n",
      "Iteration 32, loss = 0.42041942\n",
      "Iteration 33, loss = 0.41865375\n",
      "Iteration 34, loss = 0.41678105\n",
      "Iteration 35, loss = 0.41515003\n",
      "Iteration 36, loss = 0.41369303\n",
      "Iteration 37, loss = 0.41233893\n",
      "Iteration 38, loss = 0.41092901\n",
      "Iteration 39, loss = 0.40964930\n",
      "Iteration 40, loss = 0.40848276\n",
      "Iteration 41, loss = 0.40734926\n",
      "Iteration 42, loss = 0.40626533\n",
      "Iteration 43, loss = 0.40522264\n",
      "Iteration 44, loss = 0.40432473\n",
      "Iteration 45, loss = 0.40328248\n",
      "Iteration 46, loss = 0.40238908\n",
      "Iteration 47, loss = 0.40145581\n",
      "Iteration 48, loss = 0.40062440\n",
      "Iteration 49, loss = 0.39983746\n",
      "Iteration 50, loss = 0.39904942\n",
      "Iteration 51, loss = 0.39823255\n",
      "Iteration 52, loss = 0.39750571\n",
      "Iteration 53, loss = 0.39678346\n",
      "Iteration 54, loss = 0.39603389\n",
      "Iteration 55, loss = 0.39533737\n",
      "Iteration 56, loss = 0.39471886\n",
      "Iteration 57, loss = 0.39402184\n",
      "Iteration 58, loss = 0.39332958\n",
      "Iteration 59, loss = 0.39266623\n",
      "Iteration 60, loss = 0.39209384\n",
      "Iteration 61, loss = 0.39144715\n",
      "Iteration 62, loss = 0.39089423\n",
      "Iteration 63, loss = 0.39022940\n",
      "Iteration 64, loss = 0.38975455\n",
      "Iteration 65, loss = 0.38913015\n",
      "Iteration 66, loss = 0.38865277\n",
      "Iteration 67, loss = 0.38820643\n",
      "Iteration 68, loss = 0.38756994\n",
      "Iteration 69, loss = 0.38694602\n",
      "Iteration 70, loss = 0.38651467\n",
      "Iteration 71, loss = 0.38589831\n",
      "Iteration 72, loss = 0.38539072\n",
      "Iteration 73, loss = 0.38489739\n",
      "Iteration 74, loss = 0.38439778\n",
      "Iteration 75, loss = 0.38393057\n",
      "Iteration 76, loss = 0.38350979\n",
      "Iteration 77, loss = 0.38291623\n",
      "Iteration 78, loss = 0.38253765\n",
      "Iteration 79, loss = 0.38207321\n",
      "Iteration 80, loss = 0.38163895\n",
      "Iteration 81, loss = 0.38121769\n",
      "Iteration 82, loss = 0.38082170\n",
      "Iteration 83, loss = 0.38034043\n",
      "Iteration 84, loss = 0.37992764\n",
      "Iteration 85, loss = 0.37953489\n",
      "Iteration 86, loss = 0.37909197\n",
      "Iteration 87, loss = 0.37868373\n",
      "Iteration 88, loss = 0.37822712\n",
      "Iteration 89, loss = 0.37778577\n",
      "Iteration 90, loss = 0.37746246\n",
      "Iteration 91, loss = 0.37696749\n",
      "Iteration 92, loss = 0.37685436\n",
      "Iteration 93, loss = 0.37624368\n",
      "Iteration 94, loss = 0.37614708\n",
      "Iteration 95, loss = 0.37547475\n",
      "Iteration 96, loss = 0.37505956\n",
      "Iteration 97, loss = 0.37471322\n",
      "Iteration 98, loss = 0.37430207\n",
      "Iteration 99, loss = 0.37399192\n",
      "Iteration 100, loss = 0.37359231\n",
      "Iteration 101, loss = 0.37342278\n",
      "Iteration 102, loss = 0.37300225\n",
      "Iteration 103, loss = 0.37261912\n",
      "Iteration 104, loss = 0.37242557\n",
      "Iteration 105, loss = 0.37197095\n",
      "Iteration 106, loss = 0.37168286\n",
      "Iteration 107, loss = 0.37135530\n",
      "Iteration 108, loss = 0.37107505\n",
      "Iteration 109, loss = 0.37069952\n",
      "Iteration 110, loss = 0.37037202\n",
      "Iteration 111, loss = 0.37004879\n",
      "Iteration 112, loss = 0.36980703\n",
      "Iteration 113, loss = 0.36941202\n",
      "Iteration 114, loss = 0.36916187\n",
      "Iteration 115, loss = 0.36885019\n",
      "Iteration 116, loss = 0.36845152\n",
      "Iteration 117, loss = 0.36815421\n",
      "Iteration 118, loss = 0.36787522\n",
      "Iteration 119, loss = 0.36765338\n",
      "Iteration 120, loss = 0.36735119\n",
      "Iteration 121, loss = 0.36703658\n",
      "Iteration 122, loss = 0.36675498\n",
      "Iteration 123, loss = 0.36640076\n",
      "Iteration 124, loss = 0.36610956\n",
      "Iteration 125, loss = 0.36590564\n",
      "Iteration 126, loss = 0.36552978\n",
      "Iteration 127, loss = 0.36519254\n",
      "Iteration 128, loss = 0.36501615\n",
      "Iteration 129, loss = 0.36483340\n",
      "Iteration 130, loss = 0.36439285\n",
      "Iteration 131, loss = 0.36414852\n",
      "Iteration 132, loss = 0.36387179\n",
      "Iteration 133, loss = 0.36359356\n",
      "Iteration 134, loss = 0.36334811\n",
      "Iteration 135, loss = 0.36297045\n",
      "Iteration 136, loss = 0.36291925\n",
      "Iteration 137, loss = 0.36243122\n",
      "Iteration 138, loss = 0.36224657\n",
      "Iteration 139, loss = 0.36206693\n",
      "Iteration 140, loss = 0.36177744\n",
      "Iteration 141, loss = 0.36146278\n",
      "Iteration 142, loss = 0.36128435\n",
      "Iteration 143, loss = 0.36100816\n",
      "Iteration 144, loss = 0.36078474\n",
      "Iteration 145, loss = 0.36056295\n",
      "Iteration 146, loss = 0.36025855\n",
      "Iteration 147, loss = 0.36006590\n",
      "Iteration 148, loss = 0.35984685\n",
      "Iteration 149, loss = 0.35951850\n",
      "Iteration 150, loss = 0.35921803\n",
      "Iteration 151, loss = 0.35902101\n",
      "Iteration 152, loss = 0.35893552\n",
      "Iteration 153, loss = 0.35884717\n",
      "Iteration 154, loss = 0.35845678\n",
      "Iteration 155, loss = 0.35797087\n",
      "Iteration 156, loss = 0.35785644\n",
      "Iteration 157, loss = 0.35761895\n",
      "Iteration 158, loss = 0.35734896\n",
      "Iteration 159, loss = 0.35717420\n",
      "Iteration 160, loss = 0.35696785\n",
      "Iteration 161, loss = 0.35673847\n",
      "Iteration 162, loss = 0.35650494\n",
      "Iteration 163, loss = 0.35624277\n",
      "Iteration 164, loss = 0.35612131\n",
      "Iteration 165, loss = 0.35594052\n",
      "Iteration 166, loss = 0.35566496\n",
      "Iteration 167, loss = 0.35535609\n",
      "Iteration 168, loss = 0.35517131\n",
      "Iteration 169, loss = 0.35501314\n",
      "Iteration 170, loss = 0.35490857\n",
      "Iteration 171, loss = 0.35455736\n",
      "Iteration 172, loss = 0.35451448\n",
      "Iteration 173, loss = 0.35429129\n",
      "Iteration 174, loss = 0.35403573\n",
      "Iteration 175, loss = 0.35379740\n",
      "Iteration 176, loss = 0.35357426\n",
      "Iteration 177, loss = 0.35327536\n",
      "Iteration 178, loss = 0.35331316\n",
      "Iteration 179, loss = 0.35287320\n",
      "Iteration 180, loss = 0.35268140\n",
      "Iteration 181, loss = 0.35245750\n",
      "Iteration 182, loss = 0.35226491\n",
      "Iteration 183, loss = 0.35194829\n",
      "Iteration 184, loss = 0.35185689\n",
      "Iteration 185, loss = 0.35168127\n",
      "Iteration 186, loss = 0.35144763\n",
      "Iteration 187, loss = 0.35124164\n",
      "Iteration 188, loss = 0.35096140\n",
      "Iteration 189, loss = 0.35077792\n",
      "Iteration 190, loss = 0.35051848\n",
      "Iteration 191, loss = 0.35033937\n",
      "Iteration 192, loss = 0.35008252\n",
      "Iteration 193, loss = 0.34987586\n",
      "Iteration 194, loss = 0.34982001\n",
      "Iteration 195, loss = 0.34958340\n",
      "Iteration 196, loss = 0.34923065\n",
      "Iteration 197, loss = 0.34921547\n",
      "Iteration 198, loss = 0.34882641\n",
      "Iteration 199, loss = 0.34868970\n",
      "Iteration 200, loss = 0.34832275\n",
      "Iteration 1, loss = 0.78837702\n",
      "Iteration 2, loss = 0.74868476\n",
      "Iteration 3, loss = 0.71170530\n",
      "Iteration 4, loss = 0.67865947\n",
      "Iteration 5, loss = 0.64839154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.62161359\n",
      "Iteration 7, loss = 0.59814838\n",
      "Iteration 8, loss = 0.57707661\n",
      "Iteration 9, loss = 0.55871543\n",
      "Iteration 10, loss = 0.54198635\n",
      "Iteration 11, loss = 0.52835427\n",
      "Iteration 12, loss = 0.51575909\n",
      "Iteration 13, loss = 0.50512269\n",
      "Iteration 14, loss = 0.49481333\n",
      "Iteration 15, loss = 0.48610883\n",
      "Iteration 16, loss = 0.47816107\n",
      "Iteration 17, loss = 0.47097280\n",
      "Iteration 18, loss = 0.46476742\n",
      "Iteration 19, loss = 0.45826890\n",
      "Iteration 20, loss = 0.45272608\n",
      "Iteration 21, loss = 0.44750982\n",
      "Iteration 22, loss = 0.44280471\n",
      "Iteration 23, loss = 0.43822321\n",
      "Iteration 24, loss = 0.43429443\n",
      "Iteration 25, loss = 0.43056432\n",
      "Iteration 26, loss = 0.42710766\n",
      "Iteration 27, loss = 0.42394904\n",
      "Iteration 28, loss = 0.42120954\n",
      "Iteration 29, loss = 0.41843874\n",
      "Iteration 30, loss = 0.41604742\n",
      "Iteration 31, loss = 0.41396998\n",
      "Iteration 32, loss = 0.41187734\n",
      "Iteration 33, loss = 0.41008011\n",
      "Iteration 34, loss = 0.40821877\n",
      "Iteration 35, loss = 0.40651217\n",
      "Iteration 36, loss = 0.40523241\n",
      "Iteration 37, loss = 0.40380204\n",
      "Iteration 38, loss = 0.40237274\n",
      "Iteration 39, loss = 0.40120351\n",
      "Iteration 40, loss = 0.40005071\n",
      "Iteration 41, loss = 0.39889066\n",
      "Iteration 42, loss = 0.39785912\n",
      "Iteration 43, loss = 0.39692079\n",
      "Iteration 44, loss = 0.39598845\n",
      "Iteration 45, loss = 0.39502813\n",
      "Iteration 46, loss = 0.39407758\n",
      "Iteration 47, loss = 0.39323706\n",
      "Iteration 48, loss = 0.39244416\n",
      "Iteration 49, loss = 0.39165962\n",
      "Iteration 50, loss = 0.39081056\n",
      "Iteration 51, loss = 0.38999631\n",
      "Iteration 52, loss = 0.38935323\n",
      "Iteration 53, loss = 0.38865059\n",
      "Iteration 54, loss = 0.38786428\n",
      "Iteration 55, loss = 0.38722245\n",
      "Iteration 56, loss = 0.38657039\n",
      "Iteration 57, loss = 0.38593509\n",
      "Iteration 58, loss = 0.38529618\n",
      "Iteration 59, loss = 0.38463652\n",
      "Iteration 60, loss = 0.38402920\n",
      "Iteration 61, loss = 0.38347935\n",
      "Iteration 62, loss = 0.38288698\n",
      "Iteration 63, loss = 0.38232437\n",
      "Iteration 64, loss = 0.38190271\n",
      "Iteration 65, loss = 0.38123362\n",
      "Iteration 66, loss = 0.38072039\n",
      "Iteration 67, loss = 0.38019398\n",
      "Iteration 68, loss = 0.37965159\n",
      "Iteration 69, loss = 0.37916855\n",
      "Iteration 70, loss = 0.37866089\n",
      "Iteration 71, loss = 0.37815130\n",
      "Iteration 72, loss = 0.37766485\n",
      "Iteration 73, loss = 0.37714494\n",
      "Iteration 74, loss = 0.37671702\n",
      "Iteration 75, loss = 0.37624228\n",
      "Iteration 76, loss = 0.37574357\n",
      "Iteration 77, loss = 0.37526717\n",
      "Iteration 78, loss = 0.37488752\n",
      "Iteration 79, loss = 0.37432653\n",
      "Iteration 80, loss = 0.37392642\n",
      "Iteration 81, loss = 0.37347918\n",
      "Iteration 82, loss = 0.37305120\n",
      "Iteration 83, loss = 0.37269747\n",
      "Iteration 84, loss = 0.37224966\n",
      "Iteration 85, loss = 0.37185468\n",
      "Iteration 86, loss = 0.37131108\n",
      "Iteration 87, loss = 0.37104229\n",
      "Iteration 88, loss = 0.37055588\n",
      "Iteration 89, loss = 0.37017555\n",
      "Iteration 90, loss = 0.36988077\n",
      "Iteration 91, loss = 0.36936085\n",
      "Iteration 92, loss = 0.36921055\n",
      "Iteration 93, loss = 0.36865637\n",
      "Iteration 94, loss = 0.36831506\n",
      "Iteration 95, loss = 0.36781826\n",
      "Iteration 96, loss = 0.36746893\n",
      "Iteration 97, loss = 0.36718657\n",
      "Iteration 98, loss = 0.36673663\n",
      "Iteration 99, loss = 0.36642141\n",
      "Iteration 100, loss = 0.36594073\n",
      "Iteration 101, loss = 0.36582055\n",
      "Iteration 102, loss = 0.36546173\n",
      "Iteration 103, loss = 0.36504729\n",
      "Iteration 104, loss = 0.36479536\n",
      "Iteration 105, loss = 0.36430998\n",
      "Iteration 106, loss = 0.36403502\n",
      "Iteration 107, loss = 0.36375585\n",
      "Iteration 108, loss = 0.36348655\n",
      "Iteration 109, loss = 0.36314858\n",
      "Iteration 110, loss = 0.36276676\n",
      "Iteration 111, loss = 0.36239053\n",
      "Iteration 112, loss = 0.36216970\n",
      "Iteration 113, loss = 0.36180219\n",
      "Iteration 114, loss = 0.36155461\n",
      "Iteration 115, loss = 0.36122034\n",
      "Iteration 116, loss = 0.36083883\n",
      "Iteration 117, loss = 0.36056288\n",
      "Iteration 118, loss = 0.36032776\n",
      "Iteration 119, loss = 0.36011292\n",
      "Iteration 120, loss = 0.35979590\n",
      "Iteration 121, loss = 0.35950078\n",
      "Iteration 122, loss = 0.35919191\n",
      "Iteration 123, loss = 0.35885874\n",
      "Iteration 124, loss = 0.35863493\n",
      "Iteration 125, loss = 0.35832877\n",
      "Iteration 126, loss = 0.35800074\n",
      "Iteration 127, loss = 0.35773453\n",
      "Iteration 128, loss = 0.35746887\n",
      "Iteration 129, loss = 0.35724731\n",
      "Iteration 130, loss = 0.35692338\n",
      "Iteration 131, loss = 0.35656124\n",
      "Iteration 132, loss = 0.35634599\n",
      "Iteration 133, loss = 0.35604419\n",
      "Iteration 134, loss = 0.35572005\n",
      "Iteration 135, loss = 0.35544393\n",
      "Iteration 136, loss = 0.35520905\n",
      "Iteration 137, loss = 0.35501883\n",
      "Iteration 138, loss = 0.35479509\n",
      "Iteration 139, loss = 0.35441277\n",
      "Iteration 140, loss = 0.35411320\n",
      "Iteration 141, loss = 0.35385000\n",
      "Iteration 142, loss = 0.35360671\n",
      "Iteration 143, loss = 0.35322407\n",
      "Iteration 144, loss = 0.35300089\n",
      "Iteration 145, loss = 0.35280739\n",
      "Iteration 146, loss = 0.35257073\n",
      "Iteration 147, loss = 0.35224718\n",
      "Iteration 148, loss = 0.35191206\n",
      "Iteration 149, loss = 0.35163989\n",
      "Iteration 150, loss = 0.35137863\n",
      "Iteration 151, loss = 0.35110995\n",
      "Iteration 152, loss = 0.35101028\n",
      "Iteration 153, loss = 0.35090073\n",
      "Iteration 154, loss = 0.35038594\n",
      "Iteration 155, loss = 0.35012097\n",
      "Iteration 156, loss = 0.34988579\n",
      "Iteration 157, loss = 0.34960979\n",
      "Iteration 158, loss = 0.34933751\n",
      "Iteration 159, loss = 0.34906045\n",
      "Iteration 160, loss = 0.34892144\n",
      "Iteration 161, loss = 0.34864985\n",
      "Iteration 162, loss = 0.34837484\n",
      "Iteration 163, loss = 0.34818308\n",
      "Iteration 164, loss = 0.34788787\n",
      "Iteration 165, loss = 0.34765254\n",
      "Iteration 166, loss = 0.34744513\n",
      "Iteration 167, loss = 0.34717309\n",
      "Iteration 168, loss = 0.34702808\n",
      "Iteration 169, loss = 0.34673559\n",
      "Iteration 170, loss = 0.34656763\n",
      "Iteration 171, loss = 0.34621613\n",
      "Iteration 172, loss = 0.34608890\n",
      "Iteration 173, loss = 0.34589400\n",
      "Iteration 174, loss = 0.34568819\n",
      "Iteration 175, loss = 0.34543854\n",
      "Iteration 176, loss = 0.34513920\n",
      "Iteration 177, loss = 0.34486343\n",
      "Iteration 178, loss = 0.34473291\n",
      "Iteration 179, loss = 0.34445297\n",
      "Iteration 180, loss = 0.34420229\n",
      "Iteration 181, loss = 0.34396835\n",
      "Iteration 182, loss = 0.34378117\n",
      "Iteration 183, loss = 0.34351718\n",
      "Iteration 184, loss = 0.34327778\n",
      "Iteration 185, loss = 0.34311966\n",
      "Iteration 186, loss = 0.34293583\n",
      "Iteration 187, loss = 0.34267635\n",
      "Iteration 188, loss = 0.34248879\n",
      "Iteration 189, loss = 0.34228234\n",
      "Iteration 190, loss = 0.34204683\n",
      "Iteration 191, loss = 0.34176662\n",
      "Iteration 192, loss = 0.34162953\n",
      "Iteration 193, loss = 0.34133934\n",
      "Iteration 194, loss = 0.34125197\n",
      "Iteration 195, loss = 0.34108924\n",
      "Iteration 196, loss = 0.34067119\n",
      "Iteration 197, loss = 0.34065803\n",
      "Iteration 198, loss = 0.34039841\n",
      "Iteration 199, loss = 0.34031966\n",
      "Iteration 200, loss = 0.33988661\n",
      "Iteration 1, loss = 0.78798058\n",
      "Iteration 2, loss = 0.74824285\n",
      "Iteration 3, loss = 0.71116083\n",
      "Iteration 4, loss = 0.67855513\n",
      "Iteration 5, loss = 0.64929856\n",
      "Iteration 6, loss = 0.62381713\n",
      "Iteration 7, loss = 0.60136161\n",
      "Iteration 8, loss = 0.58140178\n",
      "Iteration 9, loss = 0.56407935\n",
      "Iteration 10, loss = 0.54855021\n",
      "Iteration 11, loss = 0.53568283\n",
      "Iteration 12, loss = 0.52467184\n",
      "Iteration 13, loss = 0.51517186\n",
      "Iteration 14, loss = 0.50581629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.49838791\n",
      "Iteration 16, loss = 0.49148567\n",
      "Iteration 17, loss = 0.48562549\n",
      "Iteration 18, loss = 0.47980854\n",
      "Iteration 19, loss = 0.47443199\n",
      "Iteration 20, loss = 0.46995285\n",
      "Iteration 21, loss = 0.46567540\n",
      "Iteration 22, loss = 0.46169072\n",
      "Iteration 23, loss = 0.45793022\n",
      "Iteration 24, loss = 0.45468283\n",
      "Iteration 25, loss = 0.45157317\n",
      "Iteration 26, loss = 0.44884311\n",
      "Iteration 27, loss = 0.44629202\n",
      "Iteration 28, loss = 0.44395204\n",
      "Iteration 29, loss = 0.44178715\n",
      "Iteration 30, loss = 0.43975490\n",
      "Iteration 31, loss = 0.43798586\n",
      "Iteration 32, loss = 0.43630750\n",
      "Iteration 33, loss = 0.43476802\n",
      "Iteration 34, loss = 0.43318786\n",
      "Iteration 35, loss = 0.43179317\n",
      "Iteration 36, loss = 0.43065497\n",
      "Iteration 37, loss = 0.42935832\n",
      "Iteration 38, loss = 0.42822571\n",
      "Iteration 39, loss = 0.42714387\n",
      "Iteration 40, loss = 0.42620351\n",
      "Iteration 41, loss = 0.42518724\n",
      "Iteration 42, loss = 0.42426351\n",
      "Iteration 43, loss = 0.42336962\n",
      "Iteration 44, loss = 0.42253372\n",
      "Iteration 45, loss = 0.42170658\n",
      "Iteration 46, loss = 0.42085051\n",
      "Iteration 47, loss = 0.42009819\n",
      "Iteration 48, loss = 0.41923551\n",
      "Iteration 49, loss = 0.41854537\n",
      "Iteration 50, loss = 0.41780682\n",
      "Iteration 51, loss = 0.41700783\n",
      "Iteration 52, loss = 0.41634983\n",
      "Iteration 53, loss = 0.41566422\n",
      "Iteration 54, loss = 0.41492882\n",
      "Iteration 55, loss = 0.41423642\n",
      "Iteration 56, loss = 0.41358276\n",
      "Iteration 57, loss = 0.41292581\n",
      "Iteration 58, loss = 0.41236868\n",
      "Iteration 59, loss = 0.41160757\n",
      "Iteration 60, loss = 0.41098369\n",
      "Iteration 61, loss = 0.41038250\n",
      "Iteration 62, loss = 0.40976725\n",
      "Iteration 63, loss = 0.40920954\n",
      "Iteration 64, loss = 0.40864348\n",
      "Iteration 65, loss = 0.40811423\n",
      "Iteration 66, loss = 0.40753864\n",
      "Iteration 67, loss = 0.40693129\n",
      "Iteration 68, loss = 0.40639558\n",
      "Iteration 69, loss = 0.40590801\n",
      "Iteration 70, loss = 0.40531442\n",
      "Iteration 71, loss = 0.40469227\n",
      "Iteration 72, loss = 0.40415379\n",
      "Iteration 73, loss = 0.40354948\n",
      "Iteration 74, loss = 0.40308923\n",
      "Iteration 75, loss = 0.40246276\n",
      "Iteration 76, loss = 0.40199671\n",
      "Iteration 77, loss = 0.40149028\n",
      "Iteration 78, loss = 0.40108763\n",
      "Iteration 79, loss = 0.40039410\n",
      "Iteration 80, loss = 0.39995483\n",
      "Iteration 81, loss = 0.39947271\n",
      "Iteration 82, loss = 0.39893583\n",
      "Iteration 83, loss = 0.39856351\n",
      "Iteration 84, loss = 0.39801085\n",
      "Iteration 85, loss = 0.39757266\n",
      "Iteration 86, loss = 0.39705017\n",
      "Iteration 87, loss = 0.39680030\n",
      "Iteration 88, loss = 0.39625799\n",
      "Iteration 89, loss = 0.39569410\n",
      "Iteration 90, loss = 0.39532285\n",
      "Iteration 91, loss = 0.39486357\n",
      "Iteration 92, loss = 0.39454522\n",
      "Iteration 93, loss = 0.39396256\n",
      "Iteration 94, loss = 0.39359045\n",
      "Iteration 95, loss = 0.39311206\n",
      "Iteration 96, loss = 0.39272508\n",
      "Iteration 97, loss = 0.39236760\n",
      "Iteration 98, loss = 0.39187093\n",
      "Iteration 99, loss = 0.39147631\n",
      "Iteration 100, loss = 0.39101940\n",
      "Iteration 101, loss = 0.39088792\n",
      "Iteration 102, loss = 0.39033807\n",
      "Iteration 103, loss = 0.39000033\n",
      "Iteration 104, loss = 0.38960632\n",
      "Iteration 105, loss = 0.38917446\n",
      "Iteration 106, loss = 0.38874050\n",
      "Iteration 107, loss = 0.38840550\n",
      "Iteration 108, loss = 0.38808075\n",
      "Iteration 109, loss = 0.38776970\n",
      "Iteration 110, loss = 0.38726791\n",
      "Iteration 111, loss = 0.38684038\n",
      "Iteration 112, loss = 0.38656720\n",
      "Iteration 113, loss = 0.38614587\n",
      "Iteration 114, loss = 0.38578801\n",
      "Iteration 115, loss = 0.38549721\n",
      "Iteration 116, loss = 0.38509465\n",
      "Iteration 117, loss = 0.38476418\n",
      "Iteration 118, loss = 0.38442738\n",
      "Iteration 119, loss = 0.38414939\n",
      "Iteration 120, loss = 0.38394020\n",
      "Iteration 121, loss = 0.38338161\n",
      "Iteration 122, loss = 0.38310713\n",
      "Iteration 123, loss = 0.38272606\n",
      "Iteration 124, loss = 0.38251570\n",
      "Iteration 125, loss = 0.38206163\n",
      "Iteration 126, loss = 0.38186974\n",
      "Iteration 127, loss = 0.38140186\n",
      "Iteration 128, loss = 0.38107485\n",
      "Iteration 129, loss = 0.38076257\n",
      "Iteration 130, loss = 0.38047057\n",
      "Iteration 131, loss = 0.38008859\n",
      "Iteration 132, loss = 0.37977611\n",
      "Iteration 133, loss = 0.37946513\n",
      "Iteration 134, loss = 0.37910396\n",
      "Iteration 135, loss = 0.37882934\n",
      "Iteration 136, loss = 0.37850343\n",
      "Iteration 137, loss = 0.37820908\n",
      "Iteration 138, loss = 0.37776490\n",
      "Iteration 139, loss = 0.37767199\n",
      "Iteration 140, loss = 0.37730009\n",
      "Iteration 141, loss = 0.37688900\n",
      "Iteration 142, loss = 0.37659934\n",
      "Iteration 143, loss = 0.37625640\n",
      "Iteration 144, loss = 0.37596954\n",
      "Iteration 145, loss = 0.37567707\n",
      "Iteration 146, loss = 0.37530418\n",
      "Iteration 147, loss = 0.37510097\n",
      "Iteration 148, loss = 0.37463874\n",
      "Iteration 149, loss = 0.37432887\n",
      "Iteration 150, loss = 0.37398789\n",
      "Iteration 151, loss = 0.37371547\n",
      "Iteration 152, loss = 0.37346749\n",
      "Iteration 153, loss = 0.37336707\n",
      "Iteration 154, loss = 0.37279875\n",
      "Iteration 155, loss = 0.37256543\n",
      "Iteration 156, loss = 0.37216611\n",
      "Iteration 157, loss = 0.37191102\n",
      "Iteration 158, loss = 0.37156468\n",
      "Iteration 159, loss = 0.37122985\n",
      "Iteration 160, loss = 0.37102856\n",
      "Iteration 161, loss = 0.37057074\n",
      "Iteration 162, loss = 0.37023440\n",
      "Iteration 163, loss = 0.36999449\n",
      "Iteration 164, loss = 0.36971436\n",
      "Iteration 165, loss = 0.36936372\n",
      "Iteration 166, loss = 0.36925869\n",
      "Iteration 167, loss = 0.36887872\n",
      "Iteration 168, loss = 0.36848940\n",
      "Iteration 169, loss = 0.36826732\n",
      "Iteration 170, loss = 0.36794916\n",
      "Iteration 171, loss = 0.36756588\n",
      "Iteration 172, loss = 0.36729303\n",
      "Iteration 173, loss = 0.36702778\n",
      "Iteration 174, loss = 0.36675724\n",
      "Iteration 175, loss = 0.36648839\n",
      "Iteration 176, loss = 0.36614816\n",
      "Iteration 177, loss = 0.36577603\n",
      "Iteration 178, loss = 0.36544076\n",
      "Iteration 179, loss = 0.36522792\n",
      "Iteration 180, loss = 0.36493377\n",
      "Iteration 181, loss = 0.36459348\n",
      "Iteration 182, loss = 0.36436823\n",
      "Iteration 183, loss = 0.36417791\n",
      "Iteration 184, loss = 0.36385056\n",
      "Iteration 185, loss = 0.36363014\n",
      "Iteration 186, loss = 0.36322235\n",
      "Iteration 187, loss = 0.36294116\n",
      "Iteration 188, loss = 0.36265881\n",
      "Iteration 189, loss = 0.36232833\n",
      "Iteration 190, loss = 0.36202789\n",
      "Iteration 191, loss = 0.36173900\n",
      "Iteration 192, loss = 0.36159155\n",
      "Iteration 193, loss = 0.36118096\n",
      "Iteration 194, loss = 0.36104987\n",
      "Iteration 195, loss = 0.36076226\n",
      "Iteration 196, loss = 0.36037796\n",
      "Iteration 197, loss = 0.36028668\n",
      "Iteration 198, loss = 0.35990570\n",
      "Iteration 199, loss = 0.35972914\n",
      "Iteration 200, loss = 0.35957963\n",
      "Iteration 1, loss = 0.79074236\n",
      "Iteration 2, loss = 0.74962373\n",
      "Iteration 3, loss = 0.70874083\n",
      "Iteration 4, loss = 0.67327908\n",
      "Iteration 5, loss = 0.64123459\n",
      "Iteration 6, loss = 0.61289303\n",
      "Iteration 7, loss = 0.58729564\n",
      "Iteration 8, loss = 0.56598432\n",
      "Iteration 9, loss = 0.54624001\n",
      "Iteration 10, loss = 0.52985608\n",
      "Iteration 11, loss = 0.51548203\n",
      "Iteration 12, loss = 0.50321008\n",
      "Iteration 13, loss = 0.49129952\n",
      "Iteration 14, loss = 0.48239402\n",
      "Iteration 15, loss = 0.47389774\n",
      "Iteration 16, loss = 0.46616043\n",
      "Iteration 17, loss = 0.45902712\n",
      "Iteration 18, loss = 0.45340769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.44759464\n",
      "Iteration 20, loss = 0.44244426\n",
      "Iteration 21, loss = 0.43769506\n",
      "Iteration 22, loss = 0.43335641\n",
      "Iteration 23, loss = 0.42966147\n",
      "Iteration 24, loss = 0.42603858\n",
      "Iteration 25, loss = 0.42276640\n",
      "Iteration 26, loss = 0.41982930\n",
      "Iteration 27, loss = 0.41700216\n",
      "Iteration 28, loss = 0.41467069\n",
      "Iteration 29, loss = 0.41226228\n",
      "Iteration 30, loss = 0.41009776\n",
      "Iteration 31, loss = 0.40819890\n",
      "Iteration 32, loss = 0.40655706\n",
      "Iteration 33, loss = 0.40480720\n",
      "Iteration 34, loss = 0.40334228\n",
      "Iteration 35, loss = 0.40191096\n",
      "Iteration 36, loss = 0.40069298\n",
      "Iteration 37, loss = 0.39940436\n",
      "Iteration 38, loss = 0.39832026\n",
      "Iteration 39, loss = 0.39726322\n",
      "Iteration 40, loss = 0.39625580\n",
      "Iteration 41, loss = 0.39529350\n",
      "Iteration 42, loss = 0.39438515\n",
      "Iteration 43, loss = 0.39352020\n",
      "Iteration 44, loss = 0.39279621\n",
      "Iteration 45, loss = 0.39189665\n",
      "Iteration 46, loss = 0.39122300\n",
      "Iteration 47, loss = 0.39047903\n",
      "Iteration 48, loss = 0.38971941\n",
      "Iteration 49, loss = 0.38903120\n",
      "Iteration 50, loss = 0.38837306\n",
      "Iteration 51, loss = 0.38780015\n",
      "Iteration 52, loss = 0.38710024\n",
      "Iteration 53, loss = 0.38644222\n",
      "Iteration 54, loss = 0.38591416\n",
      "Iteration 55, loss = 0.38528496\n",
      "Iteration 56, loss = 0.38472930\n",
      "Iteration 57, loss = 0.38421149\n",
      "Iteration 58, loss = 0.38363284\n",
      "Iteration 59, loss = 0.38301465\n",
      "Iteration 60, loss = 0.38249628\n",
      "Iteration 61, loss = 0.38212645\n",
      "Iteration 62, loss = 0.38168545\n",
      "Iteration 63, loss = 0.38114315\n",
      "Iteration 64, loss = 0.38057364\n",
      "Iteration 65, loss = 0.37999443\n",
      "Iteration 66, loss = 0.37960759\n",
      "Iteration 67, loss = 0.37905036\n",
      "Iteration 68, loss = 0.37875152\n",
      "Iteration 69, loss = 0.37813899\n",
      "Iteration 70, loss = 0.37776213\n",
      "Iteration 71, loss = 0.37732251\n",
      "Iteration 72, loss = 0.37682069\n",
      "Iteration 73, loss = 0.37644204\n",
      "Iteration 74, loss = 0.37598831\n",
      "Iteration 75, loss = 0.37560355\n",
      "Iteration 76, loss = 0.37521149\n",
      "Iteration 77, loss = 0.37474486\n",
      "Iteration 78, loss = 0.37438578\n",
      "Iteration 79, loss = 0.37392345\n",
      "Iteration 80, loss = 0.37355330\n",
      "Iteration 81, loss = 0.37326396\n",
      "Iteration 82, loss = 0.37274952\n",
      "Iteration 83, loss = 0.37247324\n",
      "Iteration 84, loss = 0.37202542\n",
      "Iteration 85, loss = 0.37159253\n",
      "Iteration 86, loss = 0.37134978\n",
      "Iteration 87, loss = 0.37089843\n",
      "Iteration 88, loss = 0.37068634\n",
      "Iteration 89, loss = 0.37019687\n",
      "Iteration 90, loss = 0.36986791\n",
      "Iteration 91, loss = 0.36962649\n",
      "Iteration 92, loss = 0.36926732\n",
      "Iteration 93, loss = 0.36885222\n",
      "Iteration 94, loss = 0.36859988\n",
      "Iteration 95, loss = 0.36819886\n",
      "Iteration 96, loss = 0.36808134\n",
      "Iteration 97, loss = 0.36759082\n",
      "Iteration 98, loss = 0.36732293\n",
      "Iteration 99, loss = 0.36696709\n",
      "Iteration 100, loss = 0.36673610\n",
      "Iteration 101, loss = 0.36634725\n",
      "Iteration 102, loss = 0.36610788\n",
      "Iteration 103, loss = 0.36574067\n",
      "Iteration 104, loss = 0.36553508\n",
      "Iteration 105, loss = 0.36509220\n",
      "Iteration 106, loss = 0.36486256\n",
      "Iteration 107, loss = 0.36457105\n",
      "Iteration 108, loss = 0.36434206\n",
      "Iteration 109, loss = 0.36397778\n",
      "Iteration 110, loss = 0.36371859\n",
      "Iteration 111, loss = 0.36337822\n",
      "Iteration 112, loss = 0.36301648\n",
      "Iteration 113, loss = 0.36280181\n",
      "Iteration 114, loss = 0.36248424\n",
      "Iteration 115, loss = 0.36227273\n",
      "Iteration 116, loss = 0.36198389\n",
      "Iteration 117, loss = 0.36164398\n",
      "Iteration 118, loss = 0.36140721\n",
      "Iteration 119, loss = 0.36108994\n",
      "Iteration 120, loss = 0.36078690\n",
      "Iteration 121, loss = 0.36058767\n",
      "Iteration 122, loss = 0.36026952\n",
      "Iteration 123, loss = 0.35999858\n",
      "Iteration 124, loss = 0.35979178\n",
      "Iteration 125, loss = 0.35950024\n",
      "Iteration 126, loss = 0.35916371\n",
      "Iteration 127, loss = 0.35897497\n",
      "Iteration 128, loss = 0.35863135\n",
      "Iteration 129, loss = 0.35836019\n",
      "Iteration 130, loss = 0.35809557\n",
      "Iteration 131, loss = 0.35791617\n",
      "Iteration 132, loss = 0.35760081\n",
      "Iteration 133, loss = 0.35729867\n",
      "Iteration 134, loss = 0.35707768\n",
      "Iteration 135, loss = 0.35676222\n",
      "Iteration 136, loss = 0.35654708\n",
      "Iteration 137, loss = 0.35640990\n",
      "Iteration 138, loss = 0.35600601\n",
      "Iteration 139, loss = 0.35585656\n",
      "Iteration 140, loss = 0.35557755\n",
      "Iteration 141, loss = 0.35530322\n",
      "Iteration 142, loss = 0.35504036\n",
      "Iteration 143, loss = 0.35483420\n",
      "Iteration 144, loss = 0.35452647\n",
      "Iteration 145, loss = 0.35448136\n",
      "Iteration 146, loss = 0.35418467\n",
      "Iteration 147, loss = 0.35382035\n",
      "Iteration 148, loss = 0.35374843\n",
      "Iteration 149, loss = 0.35339642\n",
      "Iteration 150, loss = 0.35310461\n",
      "Iteration 151, loss = 0.35300538\n",
      "Iteration 152, loss = 0.35266157\n",
      "Iteration 153, loss = 0.35243205\n",
      "Iteration 154, loss = 0.35236243\n",
      "Iteration 155, loss = 0.35219048\n",
      "Iteration 156, loss = 0.35171645\n",
      "Iteration 157, loss = 0.35149739\n",
      "Iteration 158, loss = 0.35132967\n",
      "Iteration 159, loss = 0.35106506\n",
      "Iteration 160, loss = 0.35090019\n",
      "Iteration 161, loss = 0.35061948\n",
      "Iteration 162, loss = 0.35047476\n",
      "Iteration 163, loss = 0.35017182\n",
      "Iteration 164, loss = 0.34999428\n",
      "Iteration 165, loss = 0.34983798\n",
      "Iteration 166, loss = 0.34962434\n",
      "Iteration 167, loss = 0.34940707\n",
      "Iteration 168, loss = 0.34910378\n",
      "Iteration 169, loss = 0.34894085\n",
      "Iteration 170, loss = 0.34869089\n",
      "Iteration 171, loss = 0.34843927\n",
      "Iteration 172, loss = 0.34840485\n",
      "Iteration 173, loss = 0.34824708\n",
      "Iteration 174, loss = 0.34783531\n",
      "Iteration 175, loss = 0.34767556\n",
      "Iteration 176, loss = 0.34776611\n",
      "Iteration 177, loss = 0.34745945\n",
      "Iteration 178, loss = 0.34712974\n",
      "Iteration 179, loss = 0.34703536\n",
      "Iteration 180, loss = 0.34665450\n",
      "Iteration 181, loss = 0.34675253\n",
      "Iteration 182, loss = 0.34632366\n",
      "Iteration 183, loss = 0.34619989\n",
      "Iteration 184, loss = 0.34600328\n",
      "Iteration 185, loss = 0.34583990\n",
      "Iteration 186, loss = 0.34556431\n",
      "Iteration 187, loss = 0.34550976\n",
      "Iteration 188, loss = 0.34510836\n",
      "Iteration 189, loss = 0.34492524\n",
      "Iteration 190, loss = 0.34484725\n",
      "Iteration 191, loss = 0.34448639\n",
      "Iteration 192, loss = 0.34440933\n",
      "Iteration 193, loss = 0.34422197\n",
      "Iteration 194, loss = 0.34406596\n",
      "Iteration 195, loss = 0.34386626\n",
      "Iteration 196, loss = 0.34364458\n",
      "Iteration 197, loss = 0.34337055\n",
      "Iteration 198, loss = 0.34328721\n",
      "Iteration 199, loss = 0.34306845\n",
      "Iteration 200, loss = 0.34284980\n",
      "Iteration 1, loss = 0.79435522\n",
      "Iteration 2, loss = 0.75380099\n",
      "Iteration 3, loss = 0.71410883\n",
      "Iteration 4, loss = 0.68082012\n",
      "Iteration 5, loss = 0.65068711\n",
      "Iteration 6, loss = 0.62414139\n",
      "Iteration 7, loss = 0.60000052\n",
      "Iteration 8, loss = 0.58071140\n",
      "Iteration 9, loss = 0.56259724\n",
      "Iteration 10, loss = 0.54719305\n",
      "Iteration 11, loss = 0.53409425\n",
      "Iteration 12, loss = 0.52300381\n",
      "Iteration 13, loss = 0.51249975\n",
      "Iteration 14, loss = 0.50391915\n",
      "Iteration 15, loss = 0.49609908\n",
      "Iteration 16, loss = 0.48891110\n",
      "Iteration 17, loss = 0.48234066\n",
      "Iteration 18, loss = 0.47673789\n",
      "Iteration 19, loss = 0.47118115\n",
      "Iteration 20, loss = 0.46611987\n",
      "Iteration 21, loss = 0.46147209\n",
      "Iteration 22, loss = 0.45729097\n",
      "Iteration 23, loss = 0.45386500\n",
      "Iteration 24, loss = 0.45022248\n",
      "Iteration 25, loss = 0.44711452\n",
      "Iteration 26, loss = 0.44406290\n",
      "Iteration 27, loss = 0.44143962\n",
      "Iteration 28, loss = 0.43922841\n",
      "Iteration 29, loss = 0.43683241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.43481540\n",
      "Iteration 31, loss = 0.43278305\n",
      "Iteration 32, loss = 0.43132470\n",
      "Iteration 33, loss = 0.42972816\n",
      "Iteration 34, loss = 0.42812431\n",
      "Iteration 35, loss = 0.42679249\n",
      "Iteration 36, loss = 0.42566523\n",
      "Iteration 37, loss = 0.42444277\n",
      "Iteration 38, loss = 0.42344643\n",
      "Iteration 39, loss = 0.42244872\n",
      "Iteration 40, loss = 0.42165404\n",
      "Iteration 41, loss = 0.42063543\n",
      "Iteration 42, loss = 0.41977256\n",
      "Iteration 43, loss = 0.41901797\n",
      "Iteration 44, loss = 0.41838458\n",
      "Iteration 45, loss = 0.41756123\n",
      "Iteration 46, loss = 0.41691941\n",
      "Iteration 47, loss = 0.41622834\n",
      "Iteration 48, loss = 0.41565021\n",
      "Iteration 49, loss = 0.41500293\n",
      "Iteration 50, loss = 0.41441721\n",
      "Iteration 51, loss = 0.41388371\n",
      "Iteration 52, loss = 0.41325253\n",
      "Iteration 53, loss = 0.41266521\n",
      "Iteration 54, loss = 0.41226738\n",
      "Iteration 55, loss = 0.41166623\n",
      "Iteration 56, loss = 0.41116664\n",
      "Iteration 57, loss = 0.41070477\n",
      "Iteration 58, loss = 0.41020953\n",
      "Iteration 59, loss = 0.40973792\n",
      "Iteration 60, loss = 0.40925698\n",
      "Iteration 61, loss = 0.40886022\n",
      "Iteration 62, loss = 0.40838478\n",
      "Iteration 63, loss = 0.40799090\n",
      "Iteration 64, loss = 0.40751027\n",
      "Iteration 65, loss = 0.40701854\n",
      "Iteration 66, loss = 0.40672150\n",
      "Iteration 67, loss = 0.40618143\n",
      "Iteration 68, loss = 0.40592643\n",
      "Iteration 69, loss = 0.40544716\n",
      "Iteration 70, loss = 0.40505233\n",
      "Iteration 71, loss = 0.40464222\n",
      "Iteration 72, loss = 0.40418652\n",
      "Iteration 73, loss = 0.40391751\n",
      "Iteration 74, loss = 0.40350589\n",
      "Iteration 75, loss = 0.40305653\n",
      "Iteration 76, loss = 0.40277177\n",
      "Iteration 77, loss = 0.40231253\n",
      "Iteration 78, loss = 0.40194103\n",
      "Iteration 79, loss = 0.40153730\n",
      "Iteration 80, loss = 0.40118602\n",
      "Iteration 81, loss = 0.40096533\n",
      "Iteration 82, loss = 0.40044358\n",
      "Iteration 83, loss = 0.40028709\n",
      "Iteration 84, loss = 0.39982449\n",
      "Iteration 85, loss = 0.39939737\n",
      "Iteration 86, loss = 0.39906374\n",
      "Iteration 87, loss = 0.39876330\n",
      "Iteration 88, loss = 0.39849308\n",
      "Iteration 89, loss = 0.39815405\n",
      "Iteration 90, loss = 0.39776696\n",
      "Iteration 91, loss = 0.39757386\n",
      "Iteration 92, loss = 0.39717403\n",
      "Iteration 93, loss = 0.39677598\n",
      "Iteration 94, loss = 0.39665428\n",
      "Iteration 95, loss = 0.39624328\n",
      "Iteration 96, loss = 0.39597948\n",
      "Iteration 97, loss = 0.39569837\n",
      "Iteration 98, loss = 0.39545419\n",
      "Iteration 99, loss = 0.39506034\n",
      "Iteration 100, loss = 0.39480309\n",
      "Iteration 101, loss = 0.39448822\n",
      "Iteration 102, loss = 0.39420942\n",
      "Iteration 103, loss = 0.39394927\n",
      "Iteration 104, loss = 0.39366897\n",
      "Iteration 105, loss = 0.39340940\n",
      "Iteration 106, loss = 0.39304512\n",
      "Iteration 107, loss = 0.39275132\n",
      "Iteration 108, loss = 0.39251488\n",
      "Iteration 109, loss = 0.39221448\n",
      "Iteration 110, loss = 0.39202499\n",
      "Iteration 111, loss = 0.39172568\n",
      "Iteration 112, loss = 0.39132492\n",
      "Iteration 113, loss = 0.39115083\n",
      "Iteration 114, loss = 0.39085913\n",
      "Iteration 115, loss = 0.39058085\n",
      "Iteration 116, loss = 0.39043157\n",
      "Iteration 117, loss = 0.39003202\n",
      "Iteration 118, loss = 0.38985208\n",
      "Iteration 119, loss = 0.38958614\n",
      "Iteration 120, loss = 0.38928259\n",
      "Iteration 121, loss = 0.38904763\n",
      "Iteration 122, loss = 0.38875774\n",
      "Iteration 123, loss = 0.38850139\n",
      "Iteration 124, loss = 0.38824661\n",
      "Iteration 125, loss = 0.38801318\n",
      "Iteration 126, loss = 0.38775012\n",
      "Iteration 127, loss = 0.38750906\n",
      "Iteration 128, loss = 0.38719841\n",
      "Iteration 129, loss = 0.38700038\n",
      "Iteration 130, loss = 0.38676103\n",
      "Iteration 131, loss = 0.38650596\n",
      "Iteration 132, loss = 0.38624985\n",
      "Iteration 133, loss = 0.38606972\n",
      "Iteration 134, loss = 0.38579653\n",
      "Iteration 135, loss = 0.38559684\n",
      "Iteration 136, loss = 0.38524321\n",
      "Iteration 137, loss = 0.38518608\n",
      "Iteration 138, loss = 0.38483353\n",
      "Iteration 139, loss = 0.38481930\n",
      "Iteration 140, loss = 0.38437723\n",
      "Iteration 141, loss = 0.38422710\n",
      "Iteration 142, loss = 0.38390950\n",
      "Iteration 143, loss = 0.38364070\n",
      "Iteration 144, loss = 0.38346930\n",
      "Iteration 145, loss = 0.38321769\n",
      "Iteration 146, loss = 0.38306452\n",
      "Iteration 147, loss = 0.38284013\n",
      "Iteration 148, loss = 0.38268813\n",
      "Iteration 149, loss = 0.38231022\n",
      "Iteration 150, loss = 0.38200370\n",
      "Iteration 151, loss = 0.38186649\n",
      "Iteration 152, loss = 0.38157614\n",
      "Iteration 153, loss = 0.38134425\n",
      "Iteration 154, loss = 0.38128667\n",
      "Iteration 155, loss = 0.38118029\n",
      "Iteration 156, loss = 0.38072636\n",
      "Iteration 157, loss = 0.38040935\n",
      "Iteration 158, loss = 0.38026539\n",
      "Iteration 159, loss = 0.38015174\n",
      "Iteration 160, loss = 0.37975453\n",
      "Iteration 161, loss = 0.37957872\n",
      "Iteration 162, loss = 0.37944101\n",
      "Iteration 163, loss = 0.37911668\n",
      "Iteration 164, loss = 0.37884499\n",
      "Iteration 165, loss = 0.37867324\n",
      "Iteration 166, loss = 0.37858559\n",
      "Iteration 167, loss = 0.37832251\n",
      "Iteration 168, loss = 0.37804871\n",
      "Iteration 169, loss = 0.37777035\n",
      "Iteration 170, loss = 0.37759973\n",
      "Iteration 171, loss = 0.37744318\n",
      "Iteration 172, loss = 0.37721376\n",
      "Iteration 173, loss = 0.37705158\n",
      "Iteration 174, loss = 0.37681147\n",
      "Iteration 175, loss = 0.37658160\n",
      "Iteration 176, loss = 0.37645749\n",
      "Iteration 177, loss = 0.37622178\n",
      "Iteration 178, loss = 0.37593157\n",
      "Iteration 179, loss = 0.37574843\n",
      "Iteration 180, loss = 0.37551644\n",
      "Iteration 181, loss = 0.37545527\n",
      "Iteration 182, loss = 0.37504913\n",
      "Iteration 183, loss = 0.37479413\n",
      "Iteration 184, loss = 0.37466540\n",
      "Iteration 185, loss = 0.37448868\n",
      "Iteration 186, loss = 0.37424042\n",
      "Iteration 187, loss = 0.37425630\n",
      "Iteration 188, loss = 0.37385190\n",
      "Iteration 189, loss = 0.37361728\n",
      "Iteration 190, loss = 0.37349120\n",
      "Iteration 191, loss = 0.37315312\n",
      "Iteration 192, loss = 0.37310417\n",
      "Iteration 193, loss = 0.37278877\n",
      "Iteration 194, loss = 0.37266941\n",
      "Iteration 195, loss = 0.37245593\n",
      "Iteration 196, loss = 0.37224481\n",
      "Iteration 197, loss = 0.37199495\n",
      "Iteration 198, loss = 0.37178954\n",
      "Iteration 199, loss = 0.37162877\n",
      "Iteration 200, loss = 0.37139011\n",
      "Iteration 1, loss = 0.79311956\n",
      "Iteration 2, loss = 0.75210508\n",
      "Iteration 3, loss = 0.71422592\n",
      "Iteration 4, loss = 0.68042522\n",
      "Iteration 5, loss = 0.65044156\n",
      "Iteration 6, loss = 0.62303803\n",
      "Iteration 7, loss = 0.60013513\n",
      "Iteration 8, loss = 0.57991114\n",
      "Iteration 9, loss = 0.56207407\n",
      "Iteration 10, loss = 0.54582624\n",
      "Iteration 11, loss = 0.53293066\n",
      "Iteration 12, loss = 0.52078737\n",
      "Iteration 13, loss = 0.51052325\n",
      "Iteration 14, loss = 0.50083071\n",
      "Iteration 15, loss = 0.49234182\n",
      "Iteration 16, loss = 0.48482341\n",
      "Iteration 17, loss = 0.47774952\n",
      "Iteration 18, loss = 0.47182881\n",
      "Iteration 19, loss = 0.46582481\n",
      "Iteration 20, loss = 0.46001074\n",
      "Iteration 21, loss = 0.45530341\n",
      "Iteration 22, loss = 0.45069057\n",
      "Iteration 23, loss = 0.44627517\n",
      "Iteration 24, loss = 0.44250378\n",
      "Iteration 25, loss = 0.43884739\n",
      "Iteration 26, loss = 0.43552066\n",
      "Iteration 27, loss = 0.43226422\n",
      "Iteration 28, loss = 0.42963645\n",
      "Iteration 29, loss = 0.42683689\n",
      "Iteration 30, loss = 0.42459832\n",
      "Iteration 31, loss = 0.42245423\n",
      "Iteration 32, loss = 0.42041323\n",
      "Iteration 33, loss = 0.41864753\n",
      "Iteration 34, loss = 0.41677479\n",
      "Iteration 35, loss = 0.41514373\n",
      "Iteration 36, loss = 0.41368632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.41233262\n",
      "Iteration 38, loss = 0.41092257\n",
      "Iteration 39, loss = 0.40964293\n",
      "Iteration 40, loss = 0.40847645\n",
      "Iteration 41, loss = 0.40734286\n",
      "Iteration 42, loss = 0.40625932\n",
      "Iteration 43, loss = 0.40521636\n",
      "Iteration 44, loss = 0.40431828\n",
      "Iteration 45, loss = 0.40327623\n",
      "Iteration 46, loss = 0.40238283\n",
      "Iteration 47, loss = 0.40144962\n",
      "Iteration 48, loss = 0.40061808\n",
      "Iteration 49, loss = 0.39983085\n",
      "Iteration 50, loss = 0.39904283\n",
      "Iteration 51, loss = 0.39822587\n",
      "Iteration 52, loss = 0.39749908\n",
      "Iteration 53, loss = 0.39677672\n",
      "Iteration 54, loss = 0.39602706\n",
      "Iteration 55, loss = 0.39532959\n",
      "Iteration 56, loss = 0.39471114\n",
      "Iteration 57, loss = 0.39401475\n",
      "Iteration 58, loss = 0.39332349\n",
      "Iteration 59, loss = 0.39266007\n",
      "Iteration 60, loss = 0.39208629\n",
      "Iteration 61, loss = 0.39143899\n",
      "Iteration 62, loss = 0.39088587\n",
      "Iteration 63, loss = 0.39022054\n",
      "Iteration 64, loss = 0.38974637\n",
      "Iteration 65, loss = 0.38912199\n",
      "Iteration 66, loss = 0.38864312\n",
      "Iteration 67, loss = 0.38819749\n",
      "Iteration 68, loss = 0.38756289\n",
      "Iteration 69, loss = 0.38694122\n",
      "Iteration 70, loss = 0.38650825\n",
      "Iteration 71, loss = 0.38589057\n",
      "Iteration 72, loss = 0.38538364\n",
      "Iteration 73, loss = 0.38488929\n",
      "Iteration 74, loss = 0.38439013\n",
      "Iteration 75, loss = 0.38392204\n",
      "Iteration 76, loss = 0.38350001\n",
      "Iteration 77, loss = 0.38290724\n",
      "Iteration 78, loss = 0.38252819\n",
      "Iteration 79, loss = 0.38206401\n",
      "Iteration 80, loss = 0.38163041\n",
      "Iteration 81, loss = 0.38120981\n",
      "Iteration 82, loss = 0.38081521\n",
      "Iteration 83, loss = 0.38033339\n",
      "Iteration 84, loss = 0.37991870\n",
      "Iteration 85, loss = 0.37952412\n",
      "Iteration 86, loss = 0.37907995\n",
      "Iteration 87, loss = 0.37867530\n",
      "Iteration 88, loss = 0.37821791\n",
      "Iteration 89, loss = 0.37777814\n",
      "Iteration 90, loss = 0.37745460\n",
      "Iteration 91, loss = 0.37696117\n",
      "Iteration 92, loss = 0.37684960\n",
      "Iteration 93, loss = 0.37623543\n",
      "Iteration 94, loss = 0.37613602\n",
      "Iteration 95, loss = 0.37546321\n",
      "Iteration 96, loss = 0.37504925\n",
      "Iteration 97, loss = 0.37470227\n",
      "Iteration 98, loss = 0.37429056\n",
      "Iteration 99, loss = 0.37398190\n",
      "Iteration 100, loss = 0.37358263\n",
      "Iteration 101, loss = 0.37341388\n",
      "Iteration 102, loss = 0.37299180\n",
      "Iteration 103, loss = 0.37260725\n",
      "Iteration 104, loss = 0.37241296\n",
      "Iteration 105, loss = 0.37195773\n",
      "Iteration 106, loss = 0.37167061\n",
      "Iteration 107, loss = 0.37134426\n",
      "Iteration 108, loss = 0.37106483\n",
      "Iteration 109, loss = 0.37068967\n",
      "Iteration 110, loss = 0.37036103\n",
      "Iteration 111, loss = 0.37003922\n",
      "Iteration 112, loss = 0.36979408\n",
      "Iteration 113, loss = 0.36939742\n",
      "Iteration 114, loss = 0.36914661\n",
      "Iteration 115, loss = 0.36883521\n",
      "Iteration 116, loss = 0.36843661\n",
      "Iteration 117, loss = 0.36814124\n",
      "Iteration 118, loss = 0.36786064\n",
      "Iteration 119, loss = 0.36764259\n",
      "Iteration 120, loss = 0.36734067\n",
      "Iteration 121, loss = 0.36702769\n",
      "Iteration 122, loss = 0.36674818\n",
      "Iteration 123, loss = 0.36639339\n",
      "Iteration 124, loss = 0.36610510\n",
      "Iteration 125, loss = 0.36589846\n",
      "Iteration 126, loss = 0.36552019\n",
      "Iteration 127, loss = 0.36518399\n",
      "Iteration 128, loss = 0.36500312\n",
      "Iteration 129, loss = 0.36481991\n",
      "Iteration 130, loss = 0.36437883\n",
      "Iteration 131, loss = 0.36414047\n",
      "Iteration 132, loss = 0.36386335\n",
      "Iteration 133, loss = 0.36358133\n",
      "Iteration 134, loss = 0.36333093\n",
      "Iteration 135, loss = 0.36295453\n",
      "Iteration 136, loss = 0.36291068\n",
      "Iteration 137, loss = 0.36242547\n",
      "Iteration 138, loss = 0.36223631\n",
      "Iteration 139, loss = 0.36205208\n",
      "Iteration 140, loss = 0.36176326\n",
      "Iteration 141, loss = 0.36144816\n",
      "Iteration 142, loss = 0.36127085\n",
      "Iteration 143, loss = 0.36099461\n",
      "Iteration 144, loss = 0.36077030\n",
      "Iteration 145, loss = 0.36055066\n",
      "Iteration 146, loss = 0.36024543\n",
      "Iteration 147, loss = 0.36005928\n",
      "Iteration 148, loss = 0.35984022\n",
      "Iteration 149, loss = 0.35950827\n",
      "Iteration 150, loss = 0.35920814\n",
      "Iteration 151, loss = 0.35901022\n",
      "Iteration 152, loss = 0.35892643\n",
      "Iteration 153, loss = 0.35883558\n",
      "Iteration 154, loss = 0.35844540\n",
      "Iteration 155, loss = 0.35796280\n",
      "Iteration 156, loss = 0.35784932\n",
      "Iteration 157, loss = 0.35760749\n",
      "Iteration 158, loss = 0.35733590\n",
      "Iteration 159, loss = 0.35716315\n",
      "Iteration 160, loss = 0.35695939\n",
      "Iteration 161, loss = 0.35673419\n",
      "Iteration 162, loss = 0.35650056\n",
      "Iteration 163, loss = 0.35623957\n",
      "Iteration 164, loss = 0.35610578\n",
      "Iteration 165, loss = 0.35593344\n",
      "Iteration 166, loss = 0.35566524\n",
      "Iteration 167, loss = 0.35535793\n",
      "Iteration 168, loss = 0.35517260\n",
      "Iteration 169, loss = 0.35501017\n",
      "Iteration 170, loss = 0.35490668\n",
      "Iteration 171, loss = 0.35455781\n",
      "Iteration 172, loss = 0.35451269\n",
      "Iteration 173, loss = 0.35428558\n",
      "Iteration 174, loss = 0.35403025\n",
      "Iteration 175, loss = 0.35379409\n",
      "Iteration 176, loss = 0.35357158\n",
      "Iteration 177, loss = 0.35327503\n",
      "Iteration 178, loss = 0.35331727\n",
      "Iteration 179, loss = 0.35287681\n",
      "Iteration 180, loss = 0.35269073\n",
      "Iteration 181, loss = 0.35246940\n",
      "Iteration 182, loss = 0.35226945\n",
      "Iteration 183, loss = 0.35196236\n",
      "Iteration 184, loss = 0.35187414\n",
      "Iteration 185, loss = 0.35169899\n",
      "Iteration 186, loss = 0.35146665\n",
      "Iteration 187, loss = 0.35125758\n",
      "Iteration 188, loss = 0.35098116\n",
      "Iteration 189, loss = 0.35079775\n",
      "Iteration 190, loss = 0.35054076\n",
      "Iteration 191, loss = 0.35035921\n",
      "Iteration 192, loss = 0.35010173\n",
      "Iteration 193, loss = 0.34989186\n",
      "Iteration 194, loss = 0.34983394\n",
      "Iteration 195, loss = 0.34959516\n",
      "Iteration 196, loss = 0.34924161\n",
      "Iteration 197, loss = 0.34923068\n",
      "Iteration 198, loss = 0.34883721\n",
      "Iteration 199, loss = 0.34871522\n",
      "Iteration 200, loss = 0.34834562\n",
      "Iteration 1, loss = 0.78837165\n",
      "Iteration 2, loss = 0.74867939\n",
      "Iteration 3, loss = 0.71169993\n",
      "Iteration 4, loss = 0.67865408\n",
      "Iteration 5, loss = 0.64838612\n",
      "Iteration 6, loss = 0.62160814\n",
      "Iteration 7, loss = 0.59814289\n",
      "Iteration 8, loss = 0.57707108\n",
      "Iteration 9, loss = 0.55870987\n",
      "Iteration 10, loss = 0.54198075\n",
      "Iteration 11, loss = 0.52834863\n",
      "Iteration 12, loss = 0.51575341\n",
      "Iteration 13, loss = 0.50511695\n",
      "Iteration 14, loss = 0.49480754\n",
      "Iteration 15, loss = 0.48610300\n",
      "Iteration 16, loss = 0.47815522\n",
      "Iteration 17, loss = 0.47096693\n",
      "Iteration 18, loss = 0.46476152\n",
      "Iteration 19, loss = 0.45826297\n",
      "Iteration 20, loss = 0.45272013\n",
      "Iteration 21, loss = 0.44750385\n",
      "Iteration 22, loss = 0.44279875\n",
      "Iteration 23, loss = 0.43821725\n",
      "Iteration 24, loss = 0.43428845\n",
      "Iteration 25, loss = 0.43055830\n",
      "Iteration 26, loss = 0.42710160\n",
      "Iteration 27, loss = 0.42394294\n",
      "Iteration 28, loss = 0.42120326\n",
      "Iteration 29, loss = 0.41843241\n",
      "Iteration 30, loss = 0.41604101\n",
      "Iteration 31, loss = 0.41396349\n",
      "Iteration 32, loss = 0.41187086\n",
      "Iteration 33, loss = 0.41007359\n",
      "Iteration 34, loss = 0.40821227\n",
      "Iteration 35, loss = 0.40650564\n",
      "Iteration 36, loss = 0.40522587\n",
      "Iteration 37, loss = 0.40379527\n",
      "Iteration 38, loss = 0.40236574\n",
      "Iteration 39, loss = 0.40119619\n",
      "Iteration 40, loss = 0.40004370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.39888392\n",
      "Iteration 42, loss = 0.39785273\n",
      "Iteration 43, loss = 0.39691425\n",
      "Iteration 44, loss = 0.39598185\n",
      "Iteration 45, loss = 0.39502167\n",
      "Iteration 46, loss = 0.39407096\n",
      "Iteration 47, loss = 0.39323025\n",
      "Iteration 48, loss = 0.39243729\n",
      "Iteration 49, loss = 0.39165294\n",
      "Iteration 50, loss = 0.39080382\n",
      "Iteration 51, loss = 0.38998963\n",
      "Iteration 52, loss = 0.38934680\n",
      "Iteration 53, loss = 0.38864371\n",
      "Iteration 54, loss = 0.38785742\n",
      "Iteration 55, loss = 0.38721530\n",
      "Iteration 56, loss = 0.38656292\n",
      "Iteration 57, loss = 0.38592761\n",
      "Iteration 58, loss = 0.38528801\n",
      "Iteration 59, loss = 0.38462819\n",
      "Iteration 60, loss = 0.38402089\n",
      "Iteration 61, loss = 0.38347118\n",
      "Iteration 62, loss = 0.38287909\n",
      "Iteration 63, loss = 0.38231626\n",
      "Iteration 64, loss = 0.38189447\n",
      "Iteration 65, loss = 0.38122568\n",
      "Iteration 66, loss = 0.38071206\n",
      "Iteration 67, loss = 0.38018542\n",
      "Iteration 68, loss = 0.37964287\n",
      "Iteration 69, loss = 0.37915958\n",
      "Iteration 70, loss = 0.37865219\n",
      "Iteration 71, loss = 0.37814313\n",
      "Iteration 72, loss = 0.37765730\n",
      "Iteration 73, loss = 0.37713658\n",
      "Iteration 74, loss = 0.37670817\n",
      "Iteration 75, loss = 0.37623390\n",
      "Iteration 76, loss = 0.37573483\n",
      "Iteration 77, loss = 0.37525811\n",
      "Iteration 78, loss = 0.37487742\n",
      "Iteration 79, loss = 0.37431644\n",
      "Iteration 80, loss = 0.37391754\n",
      "Iteration 81, loss = 0.37347067\n",
      "Iteration 82, loss = 0.37304261\n",
      "Iteration 83, loss = 0.37268864\n",
      "Iteration 84, loss = 0.37224020\n",
      "Iteration 85, loss = 0.37184632\n",
      "Iteration 86, loss = 0.37130213\n",
      "Iteration 87, loss = 0.37103370\n",
      "Iteration 88, loss = 0.37055028\n",
      "Iteration 89, loss = 0.37016888\n",
      "Iteration 90, loss = 0.36987233\n",
      "Iteration 91, loss = 0.36935273\n",
      "Iteration 92, loss = 0.36920144\n",
      "Iteration 93, loss = 0.36864690\n",
      "Iteration 94, loss = 0.36831001\n",
      "Iteration 95, loss = 0.36781408\n",
      "Iteration 96, loss = 0.36746559\n",
      "Iteration 97, loss = 0.36718531\n",
      "Iteration 98, loss = 0.36673465\n",
      "Iteration 99, loss = 0.36641985\n",
      "Iteration 100, loss = 0.36594115\n",
      "Iteration 101, loss = 0.36582096\n",
      "Iteration 102, loss = 0.36546273\n",
      "Iteration 103, loss = 0.36504874\n",
      "Iteration 104, loss = 0.36479608\n",
      "Iteration 105, loss = 0.36430955\n",
      "Iteration 106, loss = 0.36403381\n",
      "Iteration 107, loss = 0.36375790\n",
      "Iteration 108, loss = 0.36348549\n",
      "Iteration 109, loss = 0.36315121\n",
      "Iteration 110, loss = 0.36276818\n",
      "Iteration 111, loss = 0.36239547\n",
      "Iteration 112, loss = 0.36217292\n",
      "Iteration 113, loss = 0.36180340\n",
      "Iteration 114, loss = 0.36155663\n",
      "Iteration 115, loss = 0.36122314\n",
      "Iteration 116, loss = 0.36084181\n",
      "Iteration 117, loss = 0.36056706\n",
      "Iteration 118, loss = 0.36032855\n",
      "Iteration 119, loss = 0.36011078\n",
      "Iteration 120, loss = 0.35979274\n",
      "Iteration 121, loss = 0.35949784\n",
      "Iteration 122, loss = 0.35918568\n",
      "Iteration 123, loss = 0.35885446\n",
      "Iteration 124, loss = 0.35863131\n",
      "Iteration 125, loss = 0.35832742\n",
      "Iteration 126, loss = 0.35800174\n",
      "Iteration 127, loss = 0.35773371\n",
      "Iteration 128, loss = 0.35746530\n",
      "Iteration 129, loss = 0.35724335\n",
      "Iteration 130, loss = 0.35692651\n",
      "Iteration 131, loss = 0.35656539\n",
      "Iteration 132, loss = 0.35635519\n",
      "Iteration 133, loss = 0.35605730\n",
      "Iteration 134, loss = 0.35573019\n",
      "Iteration 135, loss = 0.35545227\n",
      "Iteration 136, loss = 0.35521916\n",
      "Iteration 137, loss = 0.35503161\n",
      "Iteration 138, loss = 0.35480477\n",
      "Iteration 139, loss = 0.35442270\n",
      "Iteration 140, loss = 0.35412204\n",
      "Iteration 141, loss = 0.35385684\n",
      "Iteration 142, loss = 0.35361569\n",
      "Iteration 143, loss = 0.35323473\n",
      "Iteration 144, loss = 0.35301043\n",
      "Iteration 145, loss = 0.35281500\n",
      "Iteration 146, loss = 0.35258267\n",
      "Iteration 147, loss = 0.35226052\n",
      "Iteration 148, loss = 0.35192424\n",
      "Iteration 149, loss = 0.35165146\n",
      "Iteration 150, loss = 0.35139178\n",
      "Iteration 151, loss = 0.35111949\n",
      "Iteration 152, loss = 0.35102107\n",
      "Iteration 153, loss = 0.35091135\n",
      "Iteration 154, loss = 0.35039865\n",
      "Iteration 155, loss = 0.35012888\n",
      "Iteration 156, loss = 0.34988603\n",
      "Iteration 157, loss = 0.34961346\n",
      "Iteration 158, loss = 0.34934281\n",
      "Iteration 159, loss = 0.34905578\n",
      "Iteration 160, loss = 0.34892233\n",
      "Iteration 161, loss = 0.34864851\n",
      "Iteration 162, loss = 0.34837143\n",
      "Iteration 163, loss = 0.34818102\n",
      "Iteration 164, loss = 0.34788075\n",
      "Iteration 165, loss = 0.34764684\n",
      "Iteration 166, loss = 0.34744296\n",
      "Iteration 167, loss = 0.34717388\n",
      "Iteration 168, loss = 0.34702519\n",
      "Iteration 169, loss = 0.34673002\n",
      "Iteration 170, loss = 0.34655721\n",
      "Iteration 171, loss = 0.34620119\n",
      "Iteration 172, loss = 0.34607622\n",
      "Iteration 173, loss = 0.34588857\n",
      "Iteration 174, loss = 0.34568306\n",
      "Iteration 175, loss = 0.34542550\n",
      "Iteration 176, loss = 0.34512730\n",
      "Iteration 177, loss = 0.34484865\n",
      "Iteration 178, loss = 0.34471836\n",
      "Iteration 179, loss = 0.34444051\n",
      "Iteration 180, loss = 0.34419265\n",
      "Iteration 181, loss = 0.34395814\n",
      "Iteration 182, loss = 0.34376313\n",
      "Iteration 183, loss = 0.34349947\n",
      "Iteration 184, loss = 0.34326324\n",
      "Iteration 185, loss = 0.34310236\n",
      "Iteration 186, loss = 0.34292093\n",
      "Iteration 187, loss = 0.34266100\n",
      "Iteration 188, loss = 0.34247505\n",
      "Iteration 189, loss = 0.34227692\n",
      "Iteration 190, loss = 0.34202861\n",
      "Iteration 191, loss = 0.34175613\n",
      "Iteration 192, loss = 0.34161571\n",
      "Iteration 193, loss = 0.34132640\n",
      "Iteration 194, loss = 0.34123623\n",
      "Iteration 195, loss = 0.34107189\n",
      "Iteration 196, loss = 0.34065581\n",
      "Iteration 197, loss = 0.34064428\n",
      "Iteration 198, loss = 0.34037882\n",
      "Iteration 199, loss = 0.34030058\n",
      "Iteration 200, loss = 0.33987765\n",
      "Iteration 1, loss = 0.78797516\n",
      "Iteration 2, loss = 0.74823744\n",
      "Iteration 3, loss = 0.71115542\n",
      "Iteration 4, loss = 0.67854971\n",
      "Iteration 5, loss = 0.64929313\n",
      "Iteration 6, loss = 0.62381169\n",
      "Iteration 7, loss = 0.60135616\n",
      "Iteration 8, loss = 0.58139630\n",
      "Iteration 9, loss = 0.56407385\n",
      "Iteration 10, loss = 0.54854467\n",
      "Iteration 11, loss = 0.53567726\n",
      "Iteration 12, loss = 0.52466624\n",
      "Iteration 13, loss = 0.51516622\n",
      "Iteration 14, loss = 0.50581061\n",
      "Iteration 15, loss = 0.49838221\n",
      "Iteration 16, loss = 0.49147993\n",
      "Iteration 17, loss = 0.48561972\n",
      "Iteration 18, loss = 0.47980273\n",
      "Iteration 19, loss = 0.47442615\n",
      "Iteration 20, loss = 0.46994698\n",
      "Iteration 21, loss = 0.46566950\n",
      "Iteration 22, loss = 0.46168480\n",
      "Iteration 23, loss = 0.45792427\n",
      "Iteration 24, loss = 0.45467686\n",
      "Iteration 25, loss = 0.45156717\n",
      "Iteration 26, loss = 0.44883707\n",
      "Iteration 27, loss = 0.44628596\n",
      "Iteration 28, loss = 0.44394595\n",
      "Iteration 29, loss = 0.44178104\n",
      "Iteration 30, loss = 0.43974875\n",
      "Iteration 31, loss = 0.43797969\n",
      "Iteration 32, loss = 0.43630131\n",
      "Iteration 33, loss = 0.43476181\n",
      "Iteration 34, loss = 0.43318162\n",
      "Iteration 35, loss = 0.43178688\n",
      "Iteration 36, loss = 0.43064863\n",
      "Iteration 37, loss = 0.42935194\n",
      "Iteration 38, loss = 0.42821928\n",
      "Iteration 39, loss = 0.42713742\n",
      "Iteration 40, loss = 0.42619703\n",
      "Iteration 41, loss = 0.42518075\n",
      "Iteration 42, loss = 0.42425699\n",
      "Iteration 43, loss = 0.42336305\n",
      "Iteration 44, loss = 0.42252714\n",
      "Iteration 45, loss = 0.42169998\n",
      "Iteration 46, loss = 0.42084389\n",
      "Iteration 47, loss = 0.42009155\n",
      "Iteration 48, loss = 0.41922882\n",
      "Iteration 49, loss = 0.41853868\n",
      "Iteration 50, loss = 0.41780013\n",
      "Iteration 51, loss = 0.41700116\n",
      "Iteration 52, loss = 0.41634319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.41565756\n",
      "Iteration 54, loss = 0.41492216\n",
      "Iteration 55, loss = 0.41422972\n",
      "Iteration 56, loss = 0.41357598\n",
      "Iteration 57, loss = 0.41291900\n",
      "Iteration 58, loss = 0.41236182\n",
      "Iteration 59, loss = 0.41160065\n",
      "Iteration 60, loss = 0.41097677\n",
      "Iteration 61, loss = 0.41037558\n",
      "Iteration 62, loss = 0.40976030\n",
      "Iteration 63, loss = 0.40920254\n",
      "Iteration 64, loss = 0.40863647\n",
      "Iteration 65, loss = 0.40810728\n",
      "Iteration 66, loss = 0.40753169\n",
      "Iteration 67, loss = 0.40692430\n",
      "Iteration 68, loss = 0.40638870\n",
      "Iteration 69, loss = 0.40590124\n",
      "Iteration 70, loss = 0.40530845\n",
      "Iteration 71, loss = 0.40468645\n",
      "Iteration 72, loss = 0.40414813\n",
      "Iteration 73, loss = 0.40354376\n",
      "Iteration 74, loss = 0.40308379\n",
      "Iteration 75, loss = 0.40245703\n",
      "Iteration 76, loss = 0.40199092\n",
      "Iteration 77, loss = 0.40148433\n",
      "Iteration 78, loss = 0.40108150\n",
      "Iteration 79, loss = 0.40038838\n",
      "Iteration 80, loss = 0.39994923\n",
      "Iteration 81, loss = 0.39946676\n",
      "Iteration 82, loss = 0.39892998\n",
      "Iteration 83, loss = 0.39855671\n",
      "Iteration 84, loss = 0.39800347\n",
      "Iteration 85, loss = 0.39756516\n",
      "Iteration 86, loss = 0.39704352\n",
      "Iteration 87, loss = 0.39679454\n",
      "Iteration 88, loss = 0.39625077\n",
      "Iteration 89, loss = 0.39568742\n",
      "Iteration 90, loss = 0.39531594\n",
      "Iteration 91, loss = 0.39485722\n",
      "Iteration 92, loss = 0.39453973\n",
      "Iteration 93, loss = 0.39395617\n",
      "Iteration 94, loss = 0.39358181\n",
      "Iteration 95, loss = 0.39310487\n",
      "Iteration 96, loss = 0.39271793\n",
      "Iteration 97, loss = 0.39236048\n",
      "Iteration 98, loss = 0.39186412\n",
      "Iteration 99, loss = 0.39146718\n",
      "Iteration 100, loss = 0.39100986\n",
      "Iteration 101, loss = 0.39088019\n",
      "Iteration 102, loss = 0.39033157\n",
      "Iteration 103, loss = 0.38999290\n",
      "Iteration 104, loss = 0.38959781\n",
      "Iteration 105, loss = 0.38916676\n",
      "Iteration 106, loss = 0.38873118\n",
      "Iteration 107, loss = 0.38839353\n",
      "Iteration 108, loss = 0.38806990\n",
      "Iteration 109, loss = 0.38776178\n",
      "Iteration 110, loss = 0.38726026\n",
      "Iteration 111, loss = 0.38683338\n",
      "Iteration 112, loss = 0.38656028\n",
      "Iteration 113, loss = 0.38613791\n",
      "Iteration 114, loss = 0.38578099\n",
      "Iteration 115, loss = 0.38549127\n",
      "Iteration 116, loss = 0.38508602\n",
      "Iteration 117, loss = 0.38475716\n",
      "Iteration 118, loss = 0.38442067\n",
      "Iteration 119, loss = 0.38414377\n",
      "Iteration 120, loss = 0.38393437\n",
      "Iteration 121, loss = 0.38337403\n",
      "Iteration 122, loss = 0.38309939\n",
      "Iteration 123, loss = 0.38271868\n",
      "Iteration 124, loss = 0.38250779\n",
      "Iteration 125, loss = 0.38205058\n",
      "Iteration 126, loss = 0.38186186\n",
      "Iteration 127, loss = 0.38139516\n",
      "Iteration 128, loss = 0.38107112\n",
      "Iteration 129, loss = 0.38076226\n",
      "Iteration 130, loss = 0.38047016\n",
      "Iteration 131, loss = 0.38007657\n",
      "Iteration 132, loss = 0.37976425\n",
      "Iteration 133, loss = 0.37945547\n",
      "Iteration 134, loss = 0.37909202\n",
      "Iteration 135, loss = 0.37882163\n",
      "Iteration 136, loss = 0.37848896\n",
      "Iteration 137, loss = 0.37819983\n",
      "Iteration 138, loss = 0.37775490\n",
      "Iteration 139, loss = 0.37765873\n",
      "Iteration 140, loss = 0.37728657\n",
      "Iteration 141, loss = 0.37687400\n",
      "Iteration 142, loss = 0.37658677\n",
      "Iteration 143, loss = 0.37623927\n",
      "Iteration 144, loss = 0.37595034\n",
      "Iteration 145, loss = 0.37565914\n",
      "Iteration 146, loss = 0.37528970\n",
      "Iteration 147, loss = 0.37508239\n",
      "Iteration 148, loss = 0.37462122\n",
      "Iteration 149, loss = 0.37431885\n",
      "Iteration 150, loss = 0.37397254\n",
      "Iteration 151, loss = 0.37369689\n",
      "Iteration 152, loss = 0.37345277\n",
      "Iteration 153, loss = 0.37334940\n",
      "Iteration 154, loss = 0.37278444\n",
      "Iteration 155, loss = 0.37255652\n",
      "Iteration 156, loss = 0.37216137\n",
      "Iteration 157, loss = 0.37190084\n",
      "Iteration 158, loss = 0.37155056\n",
      "Iteration 159, loss = 0.37120992\n",
      "Iteration 160, loss = 0.37101405\n",
      "Iteration 161, loss = 0.37056349\n",
      "Iteration 162, loss = 0.37022471\n",
      "Iteration 163, loss = 0.36998877\n",
      "Iteration 164, loss = 0.36970446\n",
      "Iteration 165, loss = 0.36935442\n",
      "Iteration 166, loss = 0.36925089\n",
      "Iteration 167, loss = 0.36886662\n",
      "Iteration 168, loss = 0.36848088\n",
      "Iteration 169, loss = 0.36826077\n",
      "Iteration 170, loss = 0.36794355\n",
      "Iteration 171, loss = 0.36756025\n",
      "Iteration 172, loss = 0.36729506\n",
      "Iteration 173, loss = 0.36702631\n",
      "Iteration 174, loss = 0.36676115\n",
      "Iteration 175, loss = 0.36649368\n",
      "Iteration 176, loss = 0.36615363\n",
      "Iteration 177, loss = 0.36577006\n",
      "Iteration 178, loss = 0.36542969\n",
      "Iteration 179, loss = 0.36521652\n",
      "Iteration 180, loss = 0.36492376\n",
      "Iteration 181, loss = 0.36458172\n",
      "Iteration 182, loss = 0.36435329\n",
      "Iteration 183, loss = 0.36415306\n",
      "Iteration 184, loss = 0.36382870\n",
      "Iteration 185, loss = 0.36360223\n",
      "Iteration 186, loss = 0.36320355\n",
      "Iteration 187, loss = 0.36291230\n",
      "Iteration 188, loss = 0.36263003\n",
      "Iteration 189, loss = 0.36230336\n",
      "Iteration 190, loss = 0.36199695\n",
      "Iteration 191, loss = 0.36170786\n",
      "Iteration 192, loss = 0.36156365\n",
      "Iteration 193, loss = 0.36115438\n",
      "Iteration 194, loss = 0.36102380\n",
      "Iteration 195, loss = 0.36074528\n",
      "Iteration 196, loss = 0.36036254\n",
      "Iteration 197, loss = 0.36026604\n",
      "Iteration 198, loss = 0.35989243\n",
      "Iteration 199, loss = 0.35971397\n",
      "Iteration 200, loss = 0.35955758\n",
      "Iteration 1, loss = 0.79074182\n",
      "Iteration 2, loss = 0.74962320\n",
      "Iteration 3, loss = 0.70874030\n",
      "Iteration 4, loss = 0.67327854\n",
      "Iteration 5, loss = 0.64123405\n",
      "Iteration 6, loss = 0.61289248\n",
      "Iteration 7, loss = 0.58729509\n",
      "Iteration 8, loss = 0.56598378\n",
      "Iteration 9, loss = 0.54623947\n",
      "Iteration 10, loss = 0.52985553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.51548147\n",
      "Iteration 12, loss = 0.50320953\n",
      "Iteration 13, loss = 0.49129896\n",
      "Iteration 14, loss = 0.48239345\n",
      "Iteration 15, loss = 0.47389718\n",
      "Iteration 16, loss = 0.46615986\n",
      "Iteration 17, loss = 0.45902655\n",
      "Iteration 18, loss = 0.45340712\n",
      "Iteration 19, loss = 0.44759406\n",
      "Iteration 20, loss = 0.44244367\n",
      "Iteration 21, loss = 0.43769447\n",
      "Iteration 22, loss = 0.43335582\n",
      "Iteration 23, loss = 0.42966088\n",
      "Iteration 24, loss = 0.42603798\n",
      "Iteration 25, loss = 0.42276580\n",
      "Iteration 26, loss = 0.41982870\n",
      "Iteration 27, loss = 0.41700155\n",
      "Iteration 28, loss = 0.41467008\n",
      "Iteration 29, loss = 0.41226167\n",
      "Iteration 30, loss = 0.41009715\n",
      "Iteration 31, loss = 0.40819829\n",
      "Iteration 32, loss = 0.40655644\n",
      "Iteration 33, loss = 0.40480658\n",
      "Iteration 34, loss = 0.40334166\n",
      "Iteration 35, loss = 0.40191034\n",
      "Iteration 36, loss = 0.40069235\n",
      "Iteration 37, loss = 0.39940373\n",
      "Iteration 38, loss = 0.39831962\n",
      "Iteration 39, loss = 0.39726258\n",
      "Iteration 40, loss = 0.39625516\n",
      "Iteration 41, loss = 0.39529286\n",
      "Iteration 42, loss = 0.39438450\n",
      "Iteration 43, loss = 0.39351955\n",
      "Iteration 44, loss = 0.39279556\n",
      "Iteration 45, loss = 0.39189600\n",
      "Iteration 46, loss = 0.39122235\n",
      "Iteration 47, loss = 0.39047837\n",
      "Iteration 48, loss = 0.38971875\n",
      "Iteration 49, loss = 0.38903053\n",
      "Iteration 50, loss = 0.38837240\n",
      "Iteration 51, loss = 0.38779948\n",
      "Iteration 52, loss = 0.38709957\n",
      "Iteration 53, loss = 0.38644154\n",
      "Iteration 54, loss = 0.38591348\n",
      "Iteration 55, loss = 0.38528428\n",
      "Iteration 56, loss = 0.38472862\n",
      "Iteration 57, loss = 0.38421081\n",
      "Iteration 58, loss = 0.38363215\n",
      "Iteration 59, loss = 0.38301397\n",
      "Iteration 60, loss = 0.38249559\n",
      "Iteration 61, loss = 0.38212576\n",
      "Iteration 62, loss = 0.38168475\n",
      "Iteration 63, loss = 0.38114245\n",
      "Iteration 64, loss = 0.38057294\n",
      "Iteration 65, loss = 0.37999373\n",
      "Iteration 66, loss = 0.37960688\n",
      "Iteration 67, loss = 0.37904965\n",
      "Iteration 68, loss = 0.37875081\n",
      "Iteration 69, loss = 0.37813828\n",
      "Iteration 70, loss = 0.37776141\n",
      "Iteration 71, loss = 0.37732179\n",
      "Iteration 72, loss = 0.37681996\n",
      "Iteration 73, loss = 0.37644131\n",
      "Iteration 74, loss = 0.37598759\n",
      "Iteration 75, loss = 0.37560282\n",
      "Iteration 76, loss = 0.37521075\n",
      "Iteration 77, loss = 0.37474412\n",
      "Iteration 78, loss = 0.37438504\n",
      "Iteration 79, loss = 0.37392270\n",
      "Iteration 80, loss = 0.37355255\n",
      "Iteration 81, loss = 0.37326321\n",
      "Iteration 82, loss = 0.37274876\n",
      "Iteration 83, loss = 0.37247248\n",
      "Iteration 84, loss = 0.37202466\n",
      "Iteration 85, loss = 0.37159176\n",
      "Iteration 86, loss = 0.37134900\n",
      "Iteration 87, loss = 0.37089765\n",
      "Iteration 88, loss = 0.37068555\n",
      "Iteration 89, loss = 0.37019608\n",
      "Iteration 90, loss = 0.36986711\n",
      "Iteration 91, loss = 0.36962569\n",
      "Iteration 92, loss = 0.36926652\n",
      "Iteration 93, loss = 0.36885142\n",
      "Iteration 94, loss = 0.36859908\n",
      "Iteration 95, loss = 0.36819804\n",
      "Iteration 96, loss = 0.36808051\n",
      "Iteration 97, loss = 0.36758998\n",
      "Iteration 98, loss = 0.36732211\n",
      "Iteration 99, loss = 0.36696627\n",
      "Iteration 100, loss = 0.36673528\n",
      "Iteration 101, loss = 0.36634641\n",
      "Iteration 102, loss = 0.36610700\n",
      "Iteration 103, loss = 0.36573982\n",
      "Iteration 104, loss = 0.36553430\n",
      "Iteration 105, loss = 0.36509146\n",
      "Iteration 106, loss = 0.36486177\n",
      "Iteration 107, loss = 0.36457011\n",
      "Iteration 108, loss = 0.36434136\n",
      "Iteration 109, loss = 0.36397697\n",
      "Iteration 110, loss = 0.36371772\n",
      "Iteration 111, loss = 0.36337740\n",
      "Iteration 112, loss = 0.36301570\n",
      "Iteration 113, loss = 0.36280091\n",
      "Iteration 114, loss = 0.36248311\n",
      "Iteration 115, loss = 0.36227178\n",
      "Iteration 116, loss = 0.36198280\n",
      "Iteration 117, loss = 0.36164319\n",
      "Iteration 118, loss = 0.36140650\n",
      "Iteration 119, loss = 0.36108919\n",
      "Iteration 120, loss = 0.36078563\n",
      "Iteration 121, loss = 0.36058629\n",
      "Iteration 122, loss = 0.36026839\n",
      "Iteration 123, loss = 0.35999665\n",
      "Iteration 124, loss = 0.35979028\n",
      "Iteration 125, loss = 0.35949887\n",
      "Iteration 126, loss = 0.35916212\n",
      "Iteration 127, loss = 0.35897493\n",
      "Iteration 128, loss = 0.35863002\n",
      "Iteration 129, loss = 0.35835906\n",
      "Iteration 130, loss = 0.35809351\n",
      "Iteration 131, loss = 0.35791335\n",
      "Iteration 132, loss = 0.35759788\n",
      "Iteration 133, loss = 0.35729729\n",
      "Iteration 134, loss = 0.35707808\n",
      "Iteration 135, loss = 0.35676169\n",
      "Iteration 136, loss = 0.35654662\n",
      "Iteration 137, loss = 0.35640813\n",
      "Iteration 138, loss = 0.35600427\n",
      "Iteration 139, loss = 0.35585277\n",
      "Iteration 140, loss = 0.35557770\n",
      "Iteration 141, loss = 0.35530402\n",
      "Iteration 142, loss = 0.35503913\n",
      "Iteration 143, loss = 0.35483390\n",
      "Iteration 144, loss = 0.35452559\n",
      "Iteration 145, loss = 0.35447727\n",
      "Iteration 146, loss = 0.35418538\n",
      "Iteration 147, loss = 0.35382006\n",
      "Iteration 148, loss = 0.35374534\n",
      "Iteration 149, loss = 0.35339241\n",
      "Iteration 150, loss = 0.35310299\n",
      "Iteration 151, loss = 0.35300247\n",
      "Iteration 152, loss = 0.35265985\n",
      "Iteration 153, loss = 0.35242932\n",
      "Iteration 154, loss = 0.35236258\n",
      "Iteration 155, loss = 0.35219108\n",
      "Iteration 156, loss = 0.35171634\n",
      "Iteration 157, loss = 0.35149894\n",
      "Iteration 158, loss = 0.35133255\n",
      "Iteration 159, loss = 0.35106459\n",
      "Iteration 160, loss = 0.35090088\n",
      "Iteration 161, loss = 0.35061904\n",
      "Iteration 162, loss = 0.35047081\n",
      "Iteration 163, loss = 0.35017397\n",
      "Iteration 164, loss = 0.34999843\n",
      "Iteration 165, loss = 0.34984682\n",
      "Iteration 166, loss = 0.34962542\n",
      "Iteration 167, loss = 0.34940522\n",
      "Iteration 168, loss = 0.34910553\n",
      "Iteration 169, loss = 0.34894569\n",
      "Iteration 170, loss = 0.34869553\n",
      "Iteration 171, loss = 0.34843833\n",
      "Iteration 172, loss = 0.34840813\n",
      "Iteration 173, loss = 0.34825221\n",
      "Iteration 174, loss = 0.34783649\n",
      "Iteration 175, loss = 0.34768110\n",
      "Iteration 176, loss = 0.34776106\n",
      "Iteration 177, loss = 0.34746054\n",
      "Iteration 178, loss = 0.34712839\n",
      "Iteration 179, loss = 0.34702906\n",
      "Iteration 180, loss = 0.34665163\n",
      "Iteration 181, loss = 0.34675641\n",
      "Iteration 182, loss = 0.34632307\n",
      "Iteration 183, loss = 0.34620260\n",
      "Iteration 184, loss = 0.34600649\n",
      "Iteration 185, loss = 0.34584076\n",
      "Iteration 186, loss = 0.34556566\n",
      "Iteration 187, loss = 0.34551224\n",
      "Iteration 188, loss = 0.34511326\n",
      "Iteration 189, loss = 0.34492542\n",
      "Iteration 190, loss = 0.34484117\n",
      "Iteration 191, loss = 0.34448625\n",
      "Iteration 192, loss = 0.34440773\n",
      "Iteration 193, loss = 0.34422181\n",
      "Iteration 194, loss = 0.34406522\n",
      "Iteration 195, loss = 0.34387136\n",
      "Iteration 196, loss = 0.34364982\n",
      "Iteration 197, loss = 0.34336950\n",
      "Iteration 198, loss = 0.34328470\n",
      "Iteration 199, loss = 0.34307070\n",
      "Iteration 200, loss = 0.34285191\n",
      "Iteration 1, loss = 0.79435468\n",
      "Iteration 2, loss = 0.75380045\n",
      "Iteration 3, loss = 0.71410829\n",
      "Iteration 4, loss = 0.68081958\n",
      "Iteration 5, loss = 0.65068657\n",
      "Iteration 6, loss = 0.62414085\n",
      "Iteration 7, loss = 0.59999998\n",
      "Iteration 8, loss = 0.58071085\n",
      "Iteration 9, loss = 0.56259669\n",
      "Iteration 10, loss = 0.54719250\n",
      "Iteration 11, loss = 0.53409369\n",
      "Iteration 12, loss = 0.52300326\n",
      "Iteration 13, loss = 0.51249919\n",
      "Iteration 14, loss = 0.50391859\n",
      "Iteration 15, loss = 0.49609851\n",
      "Iteration 16, loss = 0.48891052\n",
      "Iteration 17, loss = 0.48234008\n",
      "Iteration 18, loss = 0.47673732\n",
      "Iteration 19, loss = 0.47118056\n",
      "Iteration 20, loss = 0.46611928\n",
      "Iteration 21, loss = 0.46147150\n",
      "Iteration 22, loss = 0.45729038\n",
      "Iteration 23, loss = 0.45386440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.45022188\n",
      "Iteration 25, loss = 0.44711392\n",
      "Iteration 26, loss = 0.44406230\n",
      "Iteration 27, loss = 0.44143902\n",
      "Iteration 28, loss = 0.43922781\n",
      "Iteration 29, loss = 0.43683180\n",
      "Iteration 30, loss = 0.43481479\n",
      "Iteration 31, loss = 0.43278244\n",
      "Iteration 32, loss = 0.43132408\n",
      "Iteration 33, loss = 0.42972754\n",
      "Iteration 34, loss = 0.42812369\n",
      "Iteration 35, loss = 0.42679187\n",
      "Iteration 36, loss = 0.42566460\n",
      "Iteration 37, loss = 0.42444214\n",
      "Iteration 38, loss = 0.42344580\n",
      "Iteration 39, loss = 0.42244809\n",
      "Iteration 40, loss = 0.42165341\n",
      "Iteration 41, loss = 0.42063479\n",
      "Iteration 42, loss = 0.41977192\n",
      "Iteration 43, loss = 0.41901732\n",
      "Iteration 44, loss = 0.41838394\n",
      "Iteration 45, loss = 0.41756059\n",
      "Iteration 46, loss = 0.41691876\n",
      "Iteration 47, loss = 0.41622769\n",
      "Iteration 48, loss = 0.41564955\n",
      "Iteration 49, loss = 0.41500228\n",
      "Iteration 50, loss = 0.41441655\n",
      "Iteration 51, loss = 0.41388305\n",
      "Iteration 52, loss = 0.41325186\n",
      "Iteration 53, loss = 0.41266454\n",
      "Iteration 54, loss = 0.41226671\n",
      "Iteration 55, loss = 0.41166556\n",
      "Iteration 56, loss = 0.41116596\n",
      "Iteration 57, loss = 0.41070410\n",
      "Iteration 58, loss = 0.41020885\n",
      "Iteration 59, loss = 0.40973724\n",
      "Iteration 60, loss = 0.40925629\n",
      "Iteration 61, loss = 0.40885953\n",
      "Iteration 62, loss = 0.40838409\n",
      "Iteration 63, loss = 0.40799022\n",
      "Iteration 64, loss = 0.40750958\n",
      "Iteration 65, loss = 0.40701785\n",
      "Iteration 66, loss = 0.40672081\n",
      "Iteration 67, loss = 0.40618074\n",
      "Iteration 68, loss = 0.40592573\n",
      "Iteration 69, loss = 0.40544646\n",
      "Iteration 70, loss = 0.40505163\n",
      "Iteration 71, loss = 0.40464151\n",
      "Iteration 72, loss = 0.40418582\n",
      "Iteration 73, loss = 0.40391680\n",
      "Iteration 74, loss = 0.40350518\n",
      "Iteration 75, loss = 0.40305581\n",
      "Iteration 76, loss = 0.40277105\n",
      "Iteration 77, loss = 0.40231181\n",
      "Iteration 78, loss = 0.40194030\n",
      "Iteration 79, loss = 0.40153654\n",
      "Iteration 80, loss = 0.40118547\n",
      "Iteration 81, loss = 0.40096478\n",
      "Iteration 82, loss = 0.40044301\n",
      "Iteration 83, loss = 0.40028632\n",
      "Iteration 84, loss = 0.39982372\n",
      "Iteration 85, loss = 0.39939657\n",
      "Iteration 86, loss = 0.39906302\n",
      "Iteration 87, loss = 0.39876242\n",
      "Iteration 88, loss = 0.39849253\n",
      "Iteration 89, loss = 0.39815364\n",
      "Iteration 90, loss = 0.39776687\n",
      "Iteration 91, loss = 0.39757441\n",
      "Iteration 92, loss = 0.39717577\n",
      "Iteration 93, loss = 0.39677734\n",
      "Iteration 94, loss = 0.39665513\n",
      "Iteration 95, loss = 0.39624323\n",
      "Iteration 96, loss = 0.39597906\n",
      "Iteration 97, loss = 0.39569921\n",
      "Iteration 98, loss = 0.39545313\n",
      "Iteration 99, loss = 0.39505948\n",
      "Iteration 100, loss = 0.39480172\n",
      "Iteration 101, loss = 0.39448787\n",
      "Iteration 102, loss = 0.39420862\n",
      "Iteration 103, loss = 0.39394808\n",
      "Iteration 104, loss = 0.39366884\n",
      "Iteration 105, loss = 0.39340910\n",
      "Iteration 106, loss = 0.39304556\n",
      "Iteration 107, loss = 0.39275261\n",
      "Iteration 108, loss = 0.39251641\n",
      "Iteration 109, loss = 0.39221527\n",
      "Iteration 110, loss = 0.39202599\n",
      "Iteration 111, loss = 0.39172806\n",
      "Iteration 112, loss = 0.39132656\n",
      "Iteration 113, loss = 0.39115289\n",
      "Iteration 114, loss = 0.39086030\n",
      "Iteration 115, loss = 0.39058311\n",
      "Iteration 116, loss = 0.39043411\n",
      "Iteration 117, loss = 0.39003361\n",
      "Iteration 118, loss = 0.38985561\n",
      "Iteration 119, loss = 0.38958841\n",
      "Iteration 120, loss = 0.38928497\n",
      "Iteration 121, loss = 0.38905031\n",
      "Iteration 122, loss = 0.38875980\n",
      "Iteration 123, loss = 0.38850327\n",
      "Iteration 124, loss = 0.38824969\n",
      "Iteration 125, loss = 0.38801527\n",
      "Iteration 126, loss = 0.38775276\n",
      "Iteration 127, loss = 0.38750964\n",
      "Iteration 128, loss = 0.38720007\n",
      "Iteration 129, loss = 0.38700228\n",
      "Iteration 130, loss = 0.38676056\n",
      "Iteration 131, loss = 0.38650414\n",
      "Iteration 132, loss = 0.38625075\n",
      "Iteration 133, loss = 0.38607174\n",
      "Iteration 134, loss = 0.38580246\n",
      "Iteration 135, loss = 0.38560098\n",
      "Iteration 136, loss = 0.38524389\n",
      "Iteration 137, loss = 0.38518585\n",
      "Iteration 138, loss = 0.38483505\n",
      "Iteration 139, loss = 0.38482447\n",
      "Iteration 140, loss = 0.38437825\n",
      "Iteration 141, loss = 0.38422937\n",
      "Iteration 142, loss = 0.38391101\n",
      "Iteration 143, loss = 0.38364462\n",
      "Iteration 144, loss = 0.38347026\n",
      "Iteration 145, loss = 0.38321960\n",
      "Iteration 146, loss = 0.38306522\n",
      "Iteration 147, loss = 0.38283897\n",
      "Iteration 148, loss = 0.38269033\n",
      "Iteration 149, loss = 0.38231228\n",
      "Iteration 150, loss = 0.38200048\n",
      "Iteration 151, loss = 0.38186506\n",
      "Iteration 152, loss = 0.38157140\n",
      "Iteration 153, loss = 0.38133879\n",
      "Iteration 154, loss = 0.38128458\n",
      "Iteration 155, loss = 0.38117619\n",
      "Iteration 156, loss = 0.38071974\n",
      "Iteration 157, loss = 0.38040166\n",
      "Iteration 158, loss = 0.38025595\n",
      "Iteration 159, loss = 0.38014028\n",
      "Iteration 160, loss = 0.37974341\n",
      "Iteration 161, loss = 0.37957153\n",
      "Iteration 162, loss = 0.37943000\n",
      "Iteration 163, loss = 0.37910887\n",
      "Iteration 164, loss = 0.37883853\n",
      "Iteration 165, loss = 0.37866756\n",
      "Iteration 166, loss = 0.37857953\n",
      "Iteration 167, loss = 0.37831454\n",
      "Iteration 168, loss = 0.37804280\n",
      "Iteration 169, loss = 0.37776250\n",
      "Iteration 170, loss = 0.37759304\n",
      "Iteration 171, loss = 0.37744055\n",
      "Iteration 172, loss = 0.37721314\n",
      "Iteration 173, loss = 0.37705045\n",
      "Iteration 174, loss = 0.37680821\n",
      "Iteration 175, loss = 0.37657466\n",
      "Iteration 176, loss = 0.37645758\n",
      "Iteration 177, loss = 0.37622387\n",
      "Iteration 178, loss = 0.37593354\n",
      "Iteration 179, loss = 0.37574777\n",
      "Iteration 180, loss = 0.37550974\n",
      "Iteration 181, loss = 0.37545930\n",
      "Iteration 182, loss = 0.37504746\n",
      "Iteration 183, loss = 0.37479027\n",
      "Iteration 184, loss = 0.37465994\n",
      "Iteration 185, loss = 0.37447966\n",
      "Iteration 186, loss = 0.37423577\n",
      "Iteration 187, loss = 0.37426015\n",
      "Iteration 188, loss = 0.37384634\n",
      "Iteration 189, loss = 0.37361435\n",
      "Iteration 190, loss = 0.37348954\n",
      "Iteration 191, loss = 0.37315156\n",
      "Iteration 192, loss = 0.37309610\n",
      "Iteration 193, loss = 0.37278068\n",
      "Iteration 194, loss = 0.37266369\n",
      "Iteration 195, loss = 0.37245351\n",
      "Iteration 196, loss = 0.37223266\n",
      "Iteration 197, loss = 0.37198302\n",
      "Iteration 198, loss = 0.37178058\n",
      "Iteration 199, loss = 0.37161936\n",
      "Iteration 200, loss = 0.37137897\n",
      "Iteration 1, loss = 0.79311902\n",
      "Iteration 2, loss = 0.75210455\n",
      "Iteration 3, loss = 0.71422538\n",
      "Iteration 4, loss = 0.68042468\n",
      "Iteration 5, loss = 0.65044102\n",
      "Iteration 6, loss = 0.62303749\n",
      "Iteration 7, loss = 0.60013458\n",
      "Iteration 8, loss = 0.57991060\n",
      "Iteration 9, loss = 0.56207352\n",
      "Iteration 10, loss = 0.54582569\n",
      "Iteration 11, loss = 0.53293011\n",
      "Iteration 12, loss = 0.52078682\n",
      "Iteration 13, loss = 0.51052269\n",
      "Iteration 14, loss = 0.50083015\n",
      "Iteration 15, loss = 0.49234123\n",
      "Iteration 16, loss = 0.48482275\n",
      "Iteration 17, loss = 0.47774881\n",
      "Iteration 18, loss = 0.47182811\n",
      "Iteration 19, loss = 0.46582411\n",
      "Iteration 20, loss = 0.46001005\n",
      "Iteration 21, loss = 0.45530278\n",
      "Iteration 22, loss = 0.45068997\n",
      "Iteration 23, loss = 0.44627478\n",
      "Iteration 24, loss = 0.44250360\n",
      "Iteration 25, loss = 0.43884724\n",
      "Iteration 26, loss = 0.43552061\n",
      "Iteration 27, loss = 0.43226427\n",
      "Iteration 28, loss = 0.42963581\n",
      "Iteration 29, loss = 0.42683616\n",
      "Iteration 30, loss = 0.42459840\n",
      "Iteration 31, loss = 0.42245409\n",
      "Iteration 32, loss = 0.42041245\n",
      "Iteration 33, loss = 0.41864736\n",
      "Iteration 34, loss = 0.41677460\n",
      "Iteration 35, loss = 0.41514317\n",
      "Iteration 36, loss = 0.41368551\n",
      "Iteration 37, loss = 0.41233193\n",
      "Iteration 38, loss = 0.41092205\n",
      "Iteration 39, loss = 0.40964256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.40847649\n",
      "Iteration 41, loss = 0.40734302\n",
      "Iteration 42, loss = 0.40625956\n",
      "Iteration 43, loss = 0.40521647\n",
      "Iteration 44, loss = 0.40431838\n",
      "Iteration 45, loss = 0.40327616\n",
      "Iteration 46, loss = 0.40238256\n",
      "Iteration 47, loss = 0.40144928\n",
      "Iteration 48, loss = 0.40061783\n",
      "Iteration 49, loss = 0.39983048\n",
      "Iteration 50, loss = 0.39904218\n",
      "Iteration 51, loss = 0.39822526\n",
      "Iteration 52, loss = 0.39749898\n",
      "Iteration 53, loss = 0.39677678\n",
      "Iteration 54, loss = 0.39602713\n",
      "Iteration 55, loss = 0.39532989\n",
      "Iteration 56, loss = 0.39471100\n",
      "Iteration 57, loss = 0.39401510\n",
      "Iteration 58, loss = 0.39332421\n",
      "Iteration 59, loss = 0.39266023\n",
      "Iteration 60, loss = 0.39208784\n",
      "Iteration 61, loss = 0.39144104\n",
      "Iteration 62, loss = 0.39088789\n",
      "Iteration 63, loss = 0.39022204\n",
      "Iteration 64, loss = 0.38974790\n",
      "Iteration 65, loss = 0.38912438\n",
      "Iteration 66, loss = 0.38864378\n",
      "Iteration 67, loss = 0.38819799\n",
      "Iteration 68, loss = 0.38756405\n",
      "Iteration 69, loss = 0.38694235\n",
      "Iteration 70, loss = 0.38651032\n",
      "Iteration 71, loss = 0.38589304\n",
      "Iteration 72, loss = 0.38538458\n",
      "Iteration 73, loss = 0.38489075\n",
      "Iteration 74, loss = 0.38439144\n",
      "Iteration 75, loss = 0.38392234\n",
      "Iteration 76, loss = 0.38350118\n",
      "Iteration 77, loss = 0.38290821\n",
      "Iteration 78, loss = 0.38252874\n",
      "Iteration 79, loss = 0.38206481\n",
      "Iteration 80, loss = 0.38163032\n",
      "Iteration 81, loss = 0.38120748\n",
      "Iteration 82, loss = 0.38081150\n",
      "Iteration 83, loss = 0.38033188\n",
      "Iteration 84, loss = 0.37991584\n",
      "Iteration 85, loss = 0.37952216\n",
      "Iteration 86, loss = 0.37908039\n",
      "Iteration 87, loss = 0.37867428\n",
      "Iteration 88, loss = 0.37821780\n",
      "Iteration 89, loss = 0.37777633\n",
      "Iteration 90, loss = 0.37745453\n",
      "Iteration 91, loss = 0.37696179\n",
      "Iteration 92, loss = 0.37684894\n",
      "Iteration 93, loss = 0.37623912\n",
      "Iteration 94, loss = 0.37613910\n",
      "Iteration 95, loss = 0.37546597\n",
      "Iteration 96, loss = 0.37505045\n",
      "Iteration 97, loss = 0.37470308\n",
      "Iteration 98, loss = 0.37428947\n",
      "Iteration 99, loss = 0.37398115\n",
      "Iteration 100, loss = 0.37358419\n",
      "Iteration 101, loss = 0.37341750\n",
      "Iteration 102, loss = 0.37299696\n",
      "Iteration 103, loss = 0.37261324\n",
      "Iteration 104, loss = 0.37241581\n",
      "Iteration 105, loss = 0.37195889\n",
      "Iteration 106, loss = 0.37167471\n",
      "Iteration 107, loss = 0.37134688\n",
      "Iteration 108, loss = 0.37106744\n",
      "Iteration 109, loss = 0.37069217\n",
      "Iteration 110, loss = 0.37036367\n",
      "Iteration 111, loss = 0.37003964\n",
      "Iteration 112, loss = 0.36979814\n",
      "Iteration 113, loss = 0.36940303\n",
      "Iteration 114, loss = 0.36915147\n",
      "Iteration 115, loss = 0.36883753\n",
      "Iteration 116, loss = 0.36844244\n",
      "Iteration 117, loss = 0.36814272\n",
      "Iteration 118, loss = 0.36786107\n",
      "Iteration 119, loss = 0.36764695\n",
      "Iteration 120, loss = 0.36734262\n",
      "Iteration 121, loss = 0.36702420\n",
      "Iteration 122, loss = 0.36674153\n",
      "Iteration 123, loss = 0.36638749\n",
      "Iteration 124, loss = 0.36609662\n",
      "Iteration 125, loss = 0.36589244\n",
      "Iteration 126, loss = 0.36551442\n",
      "Iteration 127, loss = 0.36517533\n",
      "Iteration 128, loss = 0.36499768\n",
      "Iteration 129, loss = 0.36482059\n",
      "Iteration 130, loss = 0.36438007\n",
      "Iteration 131, loss = 0.36414199\n",
      "Iteration 132, loss = 0.36385690\n",
      "Iteration 133, loss = 0.36357598\n",
      "Iteration 134, loss = 0.36333097\n",
      "Iteration 135, loss = 0.36295814\n",
      "Iteration 136, loss = 0.36290359\n",
      "Iteration 137, loss = 0.36241572\n",
      "Iteration 138, loss = 0.36223017\n",
      "Iteration 139, loss = 0.36204547\n",
      "Iteration 140, loss = 0.36176123\n",
      "Iteration 141, loss = 0.36144697\n",
      "Iteration 142, loss = 0.36126809\n",
      "Iteration 143, loss = 0.36099070\n",
      "Iteration 144, loss = 0.36077316\n",
      "Iteration 145, loss = 0.36055210\n",
      "Iteration 146, loss = 0.36024514\n",
      "Iteration 147, loss = 0.36005857\n",
      "Iteration 148, loss = 0.35983786\n",
      "Iteration 149, loss = 0.35951018\n",
      "Iteration 150, loss = 0.35920844\n",
      "Iteration 151, loss = 0.35901322\n",
      "Iteration 152, loss = 0.35892337\n",
      "Iteration 153, loss = 0.35882328\n",
      "Iteration 154, loss = 0.35844164\n",
      "Iteration 155, loss = 0.35796134\n",
      "Iteration 156, loss = 0.35784889\n",
      "Iteration 157, loss = 0.35760878\n",
      "Iteration 158, loss = 0.35733954\n",
      "Iteration 159, loss = 0.35716494\n",
      "Iteration 160, loss = 0.35695973\n",
      "Iteration 161, loss = 0.35673504\n",
      "Iteration 162, loss = 0.35649763\n",
      "Iteration 163, loss = 0.35623556\n",
      "Iteration 164, loss = 0.35610403\n",
      "Iteration 165, loss = 0.35592563\n",
      "Iteration 166, loss = 0.35565706\n",
      "Iteration 167, loss = 0.35535052\n",
      "Iteration 168, loss = 0.35516651\n",
      "Iteration 169, loss = 0.35500856\n",
      "Iteration 170, loss = 0.35490200\n",
      "Iteration 171, loss = 0.35455203\n",
      "Iteration 172, loss = 0.35450776\n",
      "Iteration 173, loss = 0.35427953\n",
      "Iteration 174, loss = 0.35402082\n",
      "Iteration 175, loss = 0.35378767\n",
      "Iteration 176, loss = 0.35356563\n",
      "Iteration 177, loss = 0.35326449\n",
      "Iteration 178, loss = 0.35330019\n",
      "Iteration 179, loss = 0.35286563\n",
      "Iteration 180, loss = 0.35267809\n",
      "Iteration 181, loss = 0.35245363\n",
      "Iteration 182, loss = 0.35225438\n",
      "Iteration 183, loss = 0.35193993\n",
      "Iteration 184, loss = 0.35185200\n",
      "Iteration 185, loss = 0.35168022\n",
      "Iteration 186, loss = 0.35143995\n",
      "Iteration 187, loss = 0.35123146\n",
      "Iteration 188, loss = 0.35094623\n",
      "Iteration 189, loss = 0.35076875\n",
      "Iteration 190, loss = 0.35051211\n",
      "Iteration 191, loss = 0.35033122\n",
      "Iteration 192, loss = 0.35007460\n",
      "Iteration 193, loss = 0.34986257\n",
      "Iteration 194, loss = 0.34980591\n",
      "Iteration 195, loss = 0.34957404\n",
      "Iteration 196, loss = 0.34921523\n",
      "Iteration 197, loss = 0.34920282\n",
      "Iteration 198, loss = 0.34880331\n",
      "Iteration 199, loss = 0.34868186\n",
      "Iteration 200, loss = 0.34831353\n",
      "Iteration 1, loss = 0.78837112\n",
      "Iteration 2, loss = 0.74867885\n",
      "Iteration 3, loss = 0.71169939\n",
      "Iteration 4, loss = 0.67865354\n",
      "Iteration 5, loss = 0.64838558\n",
      "Iteration 6, loss = 0.62160760\n",
      "Iteration 7, loss = 0.59814235\n",
      "Iteration 8, loss = 0.57707054\n",
      "Iteration 9, loss = 0.55870932\n",
      "Iteration 10, loss = 0.54198020\n",
      "Iteration 11, loss = 0.52834807\n",
      "Iteration 12, loss = 0.51575285\n",
      "Iteration 13, loss = 0.50511639\n",
      "Iteration 14, loss = 0.49480697\n",
      "Iteration 15, loss = 0.48610243\n",
      "Iteration 16, loss = 0.47815465\n",
      "Iteration 17, loss = 0.47096636\n",
      "Iteration 18, loss = 0.46476094\n",
      "Iteration 19, loss = 0.45826239\n",
      "Iteration 20, loss = 0.45271955\n",
      "Iteration 21, loss = 0.44750326\n",
      "Iteration 22, loss = 0.44279816\n",
      "Iteration 23, loss = 0.43821665\n",
      "Iteration 24, loss = 0.43428785\n",
      "Iteration 25, loss = 0.43055770\n",
      "Iteration 26, loss = 0.42710099\n",
      "Iteration 27, loss = 0.42394233\n",
      "Iteration 28, loss = 0.42120265\n",
      "Iteration 29, loss = 0.41843179\n",
      "Iteration 30, loss = 0.41604039\n",
      "Iteration 31, loss = 0.41396287\n",
      "Iteration 32, loss = 0.41187024\n",
      "Iteration 33, loss = 0.41007297\n",
      "Iteration 34, loss = 0.40821165\n",
      "Iteration 35, loss = 0.40650501\n",
      "Iteration 36, loss = 0.40522524\n",
      "Iteration 37, loss = 0.40379463\n",
      "Iteration 38, loss = 0.40236510\n",
      "Iteration 39, loss = 0.40119555\n",
      "Iteration 40, loss = 0.40004305\n",
      "Iteration 41, loss = 0.39888328\n",
      "Iteration 42, loss = 0.39785209\n",
      "Iteration 43, loss = 0.39691360\n",
      "Iteration 44, loss = 0.39598120\n",
      "Iteration 45, loss = 0.39502101\n",
      "Iteration 46, loss = 0.39407030\n",
      "Iteration 47, loss = 0.39322959\n",
      "Iteration 48, loss = 0.39243662\n",
      "Iteration 49, loss = 0.39165227\n",
      "Iteration 50, loss = 0.39080315\n",
      "Iteration 51, loss = 0.38998896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.38934614\n",
      "Iteration 53, loss = 0.38864307\n",
      "Iteration 54, loss = 0.38785679\n",
      "Iteration 55, loss = 0.38721472\n",
      "Iteration 56, loss = 0.38656238\n",
      "Iteration 57, loss = 0.38592699\n",
      "Iteration 58, loss = 0.38528737\n",
      "Iteration 59, loss = 0.38462764\n",
      "Iteration 60, loss = 0.38402035\n",
      "Iteration 61, loss = 0.38347046\n",
      "Iteration 62, loss = 0.38287836\n",
      "Iteration 63, loss = 0.38231531\n",
      "Iteration 64, loss = 0.38189386\n",
      "Iteration 65, loss = 0.38122475\n",
      "Iteration 66, loss = 0.38071139\n",
      "Iteration 67, loss = 0.38018435\n",
      "Iteration 68, loss = 0.37964219\n",
      "Iteration 69, loss = 0.37915853\n",
      "Iteration 70, loss = 0.37865155\n",
      "Iteration 71, loss = 0.37814243\n",
      "Iteration 72, loss = 0.37765716\n",
      "Iteration 73, loss = 0.37713589\n",
      "Iteration 74, loss = 0.37670742\n",
      "Iteration 75, loss = 0.37623304\n",
      "Iteration 76, loss = 0.37573393\n",
      "Iteration 77, loss = 0.37525655\n",
      "Iteration 78, loss = 0.37487608\n",
      "Iteration 79, loss = 0.37431453\n",
      "Iteration 80, loss = 0.37391587\n",
      "Iteration 81, loss = 0.37346956\n",
      "Iteration 82, loss = 0.37304095\n",
      "Iteration 83, loss = 0.37268751\n",
      "Iteration 84, loss = 0.37223929\n",
      "Iteration 85, loss = 0.37184390\n",
      "Iteration 86, loss = 0.37130054\n",
      "Iteration 87, loss = 0.37103161\n",
      "Iteration 88, loss = 0.37054739\n",
      "Iteration 89, loss = 0.37016819\n",
      "Iteration 90, loss = 0.36987174\n",
      "Iteration 91, loss = 0.36935208\n",
      "Iteration 92, loss = 0.36920239\n",
      "Iteration 93, loss = 0.36864721\n",
      "Iteration 94, loss = 0.36830838\n",
      "Iteration 95, loss = 0.36781193\n",
      "Iteration 96, loss = 0.36746350\n",
      "Iteration 97, loss = 0.36718475\n",
      "Iteration 98, loss = 0.36673472\n",
      "Iteration 99, loss = 0.36641713\n",
      "Iteration 100, loss = 0.36593793\n",
      "Iteration 101, loss = 0.36581735\n",
      "Iteration 102, loss = 0.36545810\n",
      "Iteration 103, loss = 0.36504508\n",
      "Iteration 104, loss = 0.36479150\n",
      "Iteration 105, loss = 0.36430398\n",
      "Iteration 106, loss = 0.36402872\n",
      "Iteration 107, loss = 0.36374950\n",
      "Iteration 108, loss = 0.36347846\n",
      "Iteration 109, loss = 0.36314372\n",
      "Iteration 110, loss = 0.36276180\n",
      "Iteration 111, loss = 0.36238870\n",
      "Iteration 112, loss = 0.36216586\n",
      "Iteration 113, loss = 0.36179846\n",
      "Iteration 114, loss = 0.36154715\n",
      "Iteration 115, loss = 0.36121638\n",
      "Iteration 116, loss = 0.36083606\n",
      "Iteration 117, loss = 0.36056147\n",
      "Iteration 118, loss = 0.36032624\n",
      "Iteration 119, loss = 0.36010800\n",
      "Iteration 120, loss = 0.35978747\n",
      "Iteration 121, loss = 0.35949851\n",
      "Iteration 122, loss = 0.35918692\n",
      "Iteration 123, loss = 0.35885362\n",
      "Iteration 124, loss = 0.35863312\n",
      "Iteration 125, loss = 0.35832956\n",
      "Iteration 126, loss = 0.35800315\n",
      "Iteration 127, loss = 0.35773562\n",
      "Iteration 128, loss = 0.35746745\n",
      "Iteration 129, loss = 0.35724466\n",
      "Iteration 130, loss = 0.35692830\n",
      "Iteration 131, loss = 0.35656899\n",
      "Iteration 132, loss = 0.35635281\n",
      "Iteration 133, loss = 0.35605009\n",
      "Iteration 134, loss = 0.35572618\n",
      "Iteration 135, loss = 0.35544783\n",
      "Iteration 136, loss = 0.35521607\n",
      "Iteration 137, loss = 0.35502470\n",
      "Iteration 138, loss = 0.35480220\n",
      "Iteration 139, loss = 0.35442250\n",
      "Iteration 140, loss = 0.35411977\n",
      "Iteration 141, loss = 0.35385563\n",
      "Iteration 142, loss = 0.35361230\n",
      "Iteration 143, loss = 0.35322984\n",
      "Iteration 144, loss = 0.35300572\n",
      "Iteration 145, loss = 0.35280956\n",
      "Iteration 146, loss = 0.35257872\n",
      "Iteration 147, loss = 0.35225450\n",
      "Iteration 148, loss = 0.35192488\n",
      "Iteration 149, loss = 0.35165411\n",
      "Iteration 150, loss = 0.35139303\n",
      "Iteration 151, loss = 0.35111628\n",
      "Iteration 152, loss = 0.35101673\n",
      "Iteration 153, loss = 0.35091245\n",
      "Iteration 154, loss = 0.35039895\n",
      "Iteration 155, loss = 0.35012829\n",
      "Iteration 156, loss = 0.34988454\n",
      "Iteration 157, loss = 0.34961099\n",
      "Iteration 158, loss = 0.34934141\n",
      "Iteration 159, loss = 0.34905796\n",
      "Iteration 160, loss = 0.34891885\n",
      "Iteration 161, loss = 0.34864809\n",
      "Iteration 162, loss = 0.34837829\n",
      "Iteration 163, loss = 0.34818686\n",
      "Iteration 164, loss = 0.34788631\n",
      "Iteration 165, loss = 0.34765148\n",
      "Iteration 166, loss = 0.34745090\n",
      "Iteration 167, loss = 0.34717709\n",
      "Iteration 168, loss = 0.34702954\n",
      "Iteration 169, loss = 0.34673419\n",
      "Iteration 170, loss = 0.34656408\n",
      "Iteration 171, loss = 0.34621269\n",
      "Iteration 172, loss = 0.34608473\n",
      "Iteration 173, loss = 0.34589076\n",
      "Iteration 174, loss = 0.34569326\n",
      "Iteration 175, loss = 0.34543233\n",
      "Iteration 176, loss = 0.34513485\n",
      "Iteration 177, loss = 0.34485995\n",
      "Iteration 178, loss = 0.34472857\n",
      "Iteration 179, loss = 0.34444507\n",
      "Iteration 180, loss = 0.34419391\n",
      "Iteration 181, loss = 0.34396563\n",
      "Iteration 182, loss = 0.34377806\n",
      "Iteration 183, loss = 0.34350546\n",
      "Iteration 184, loss = 0.34326622\n",
      "Iteration 185, loss = 0.34310709\n",
      "Iteration 186, loss = 0.34292291\n",
      "Iteration 187, loss = 0.34266575\n",
      "Iteration 188, loss = 0.34248186\n",
      "Iteration 189, loss = 0.34227957\n",
      "Iteration 190, loss = 0.34203636\n",
      "Iteration 191, loss = 0.34175879\n",
      "Iteration 192, loss = 0.34162622\n",
      "Iteration 193, loss = 0.34133828\n",
      "Iteration 194, loss = 0.34124797\n",
      "Iteration 195, loss = 0.34108622\n",
      "Iteration 196, loss = 0.34067575\n",
      "Iteration 197, loss = 0.34065557\n",
      "Iteration 198, loss = 0.34039682\n",
      "Iteration 199, loss = 0.34031750\n",
      "Iteration 200, loss = 0.33988177\n",
      "Iteration 1, loss = 0.78797462\n",
      "Iteration 2, loss = 0.74823690\n",
      "Iteration 3, loss = 0.71115488\n",
      "Iteration 4, loss = 0.67854917\n",
      "Iteration 5, loss = 0.64929259\n",
      "Iteration 6, loss = 0.62381115\n",
      "Iteration 7, loss = 0.60135561\n",
      "Iteration 8, loss = 0.58139575\n",
      "Iteration 9, loss = 0.56407330\n",
      "Iteration 10, loss = 0.54854412\n",
      "Iteration 11, loss = 0.53567670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.52466568\n",
      "Iteration 13, loss = 0.51516566\n",
      "Iteration 14, loss = 0.50581005\n",
      "Iteration 15, loss = 0.49838164\n",
      "Iteration 16, loss = 0.49147935\n",
      "Iteration 17, loss = 0.48561914\n",
      "Iteration 18, loss = 0.47980215\n",
      "Iteration 19, loss = 0.47442556\n",
      "Iteration 20, loss = 0.46994640\n",
      "Iteration 21, loss = 0.46566891\n",
      "Iteration 22, loss = 0.46168421\n",
      "Iteration 23, loss = 0.45792368\n",
      "Iteration 24, loss = 0.45467627\n",
      "Iteration 25, loss = 0.45156657\n",
      "Iteration 26, loss = 0.44883647\n",
      "Iteration 27, loss = 0.44628536\n",
      "Iteration 28, loss = 0.44394534\n",
      "Iteration 29, loss = 0.44178042\n",
      "Iteration 30, loss = 0.43974813\n",
      "Iteration 31, loss = 0.43797908\n",
      "Iteration 32, loss = 0.43630070\n",
      "Iteration 33, loss = 0.43476119\n",
      "Iteration 34, loss = 0.43318100\n",
      "Iteration 35, loss = 0.43178625\n",
      "Iteration 36, loss = 0.43064800\n",
      "Iteration 37, loss = 0.42935131\n",
      "Iteration 38, loss = 0.42821865\n",
      "Iteration 39, loss = 0.42713679\n",
      "Iteration 40, loss = 0.42619639\n",
      "Iteration 41, loss = 0.42518010\n",
      "Iteration 42, loss = 0.42425635\n",
      "Iteration 43, loss = 0.42336240\n",
      "Iteration 44, loss = 0.42252649\n",
      "Iteration 45, loss = 0.42169933\n",
      "Iteration 46, loss = 0.42084323\n",
      "Iteration 47, loss = 0.42009089\n",
      "Iteration 48, loss = 0.41922816\n",
      "Iteration 49, loss = 0.41853802\n",
      "Iteration 50, loss = 0.41779947\n",
      "Iteration 51, loss = 0.41700049\n",
      "Iteration 52, loss = 0.41634252\n",
      "Iteration 53, loss = 0.41565689\n",
      "Iteration 54, loss = 0.41492149\n",
      "Iteration 55, loss = 0.41422904\n",
      "Iteration 56, loss = 0.41357530\n",
      "Iteration 57, loss = 0.41291832\n",
      "Iteration 58, loss = 0.41236114\n",
      "Iteration 59, loss = 0.41159996\n",
      "Iteration 60, loss = 0.41097608\n",
      "Iteration 61, loss = 0.41037488\n",
      "Iteration 62, loss = 0.40975962\n",
      "Iteration 63, loss = 0.40920186\n",
      "Iteration 64, loss = 0.40863579\n",
      "Iteration 65, loss = 0.40810660\n",
      "Iteration 66, loss = 0.40753101\n",
      "Iteration 67, loss = 0.40692362\n",
      "Iteration 68, loss = 0.40638802\n",
      "Iteration 69, loss = 0.40590055\n",
      "Iteration 70, loss = 0.40530776\n",
      "Iteration 71, loss = 0.40468577\n",
      "Iteration 72, loss = 0.40414743\n",
      "Iteration 73, loss = 0.40354306\n",
      "Iteration 74, loss = 0.40308309\n",
      "Iteration 75, loss = 0.40245633\n",
      "Iteration 76, loss = 0.40199021\n",
      "Iteration 77, loss = 0.40148361\n",
      "Iteration 78, loss = 0.40108078\n",
      "Iteration 79, loss = 0.40038764\n",
      "Iteration 80, loss = 0.39994848\n",
      "Iteration 81, loss = 0.39946602\n",
      "Iteration 82, loss = 0.39892922\n",
      "Iteration 83, loss = 0.39855595\n",
      "Iteration 84, loss = 0.39800271\n",
      "Iteration 85, loss = 0.39756435\n",
      "Iteration 86, loss = 0.39704280\n",
      "Iteration 87, loss = 0.39679394\n",
      "Iteration 88, loss = 0.39625015\n",
      "Iteration 89, loss = 0.39568679\n",
      "Iteration 90, loss = 0.39531526\n",
      "Iteration 91, loss = 0.39485651\n",
      "Iteration 92, loss = 0.39453901\n",
      "Iteration 93, loss = 0.39395551\n",
      "Iteration 94, loss = 0.39358116\n",
      "Iteration 95, loss = 0.39310421\n",
      "Iteration 96, loss = 0.39271728\n",
      "Iteration 97, loss = 0.39235987\n",
      "Iteration 98, loss = 0.39186353\n",
      "Iteration 99, loss = 0.39146665\n",
      "Iteration 100, loss = 0.39100935\n",
      "Iteration 101, loss = 0.39087938\n",
      "Iteration 102, loss = 0.39033080\n",
      "Iteration 103, loss = 0.38999222\n",
      "Iteration 104, loss = 0.38959707\n",
      "Iteration 105, loss = 0.38916601\n",
      "Iteration 106, loss = 0.38873047\n",
      "Iteration 107, loss = 0.38839280\n",
      "Iteration 108, loss = 0.38806910\n",
      "Iteration 109, loss = 0.38776082\n",
      "Iteration 110, loss = 0.38725948\n",
      "Iteration 111, loss = 0.38683264\n",
      "Iteration 112, loss = 0.38655942\n",
      "Iteration 113, loss = 0.38613693\n",
      "Iteration 114, loss = 0.38577999\n",
      "Iteration 115, loss = 0.38549073\n",
      "Iteration 116, loss = 0.38508527\n",
      "Iteration 117, loss = 0.38475600\n",
      "Iteration 118, loss = 0.38441932\n",
      "Iteration 119, loss = 0.38414288\n",
      "Iteration 120, loss = 0.38393419\n",
      "Iteration 121, loss = 0.38337357\n",
      "Iteration 122, loss = 0.38309895\n",
      "Iteration 123, loss = 0.38271895\n",
      "Iteration 124, loss = 0.38250573\n",
      "Iteration 125, loss = 0.38204754\n",
      "Iteration 126, loss = 0.38186068\n",
      "Iteration 127, loss = 0.38139443\n",
      "Iteration 128, loss = 0.38106941\n",
      "Iteration 129, loss = 0.38075947\n",
      "Iteration 130, loss = 0.38046702\n",
      "Iteration 131, loss = 0.38007576\n",
      "Iteration 132, loss = 0.37976355\n",
      "Iteration 133, loss = 0.37945648\n",
      "Iteration 134, loss = 0.37909449\n",
      "Iteration 135, loss = 0.37882300\n",
      "Iteration 136, loss = 0.37849112\n",
      "Iteration 137, loss = 0.37820061\n",
      "Iteration 138, loss = 0.37775546\n",
      "Iteration 139, loss = 0.37765867\n",
      "Iteration 140, loss = 0.37728795\n",
      "Iteration 141, loss = 0.37687302\n",
      "Iteration 142, loss = 0.37658568\n",
      "Iteration 143, loss = 0.37623839\n",
      "Iteration 144, loss = 0.37594798\n",
      "Iteration 145, loss = 0.37565690\n",
      "Iteration 146, loss = 0.37528568\n",
      "Iteration 147, loss = 0.37508188\n",
      "Iteration 148, loss = 0.37462264\n",
      "Iteration 149, loss = 0.37431640\n",
      "Iteration 150, loss = 0.37397131\n",
      "Iteration 151, loss = 0.37369105\n",
      "Iteration 152, loss = 0.37344393\n",
      "Iteration 153, loss = 0.37333728\n",
      "Iteration 154, loss = 0.37277768\n",
      "Iteration 155, loss = 0.37255180\n",
      "Iteration 156, loss = 0.37215573\n",
      "Iteration 157, loss = 0.37190050\n",
      "Iteration 158, loss = 0.37155420\n",
      "Iteration 159, loss = 0.37121658\n",
      "Iteration 160, loss = 0.37101514\n",
      "Iteration 161, loss = 0.37056838\n",
      "Iteration 162, loss = 0.37022689\n",
      "Iteration 163, loss = 0.36998660\n",
      "Iteration 164, loss = 0.36970345\n",
      "Iteration 165, loss = 0.36935820\n",
      "Iteration 166, loss = 0.36925282\n",
      "Iteration 167, loss = 0.36887608\n",
      "Iteration 168, loss = 0.36848279\n",
      "Iteration 169, loss = 0.36826548\n",
      "Iteration 170, loss = 0.36794663\n",
      "Iteration 171, loss = 0.36756402\n",
      "Iteration 172, loss = 0.36729547\n",
      "Iteration 173, loss = 0.36702676\n",
      "Iteration 174, loss = 0.36676417\n",
      "Iteration 175, loss = 0.36649593\n",
      "Iteration 176, loss = 0.36615436\n",
      "Iteration 177, loss = 0.36576733\n",
      "Iteration 178, loss = 0.36543079\n",
      "Iteration 179, loss = 0.36521518\n",
      "Iteration 180, loss = 0.36491064\n",
      "Iteration 181, loss = 0.36457652\n",
      "Iteration 182, loss = 0.36434823\n",
      "Iteration 183, loss = 0.36414819\n",
      "Iteration 184, loss = 0.36382089\n",
      "Iteration 185, loss = 0.36359212\n",
      "Iteration 186, loss = 0.36318547\n",
      "Iteration 187, loss = 0.36290353\n",
      "Iteration 188, loss = 0.36262241\n",
      "Iteration 189, loss = 0.36228156\n",
      "Iteration 190, loss = 0.36198313\n",
      "Iteration 191, loss = 0.36169999\n",
      "Iteration 192, loss = 0.36155511\n",
      "Iteration 193, loss = 0.36114098\n",
      "Iteration 194, loss = 0.36099882\n",
      "Iteration 195, loss = 0.36071996\n",
      "Iteration 196, loss = 0.36033903\n",
      "Iteration 197, loss = 0.36025437\n",
      "Iteration 198, loss = 0.35987011\n",
      "Iteration 199, loss = 0.35969968\n",
      "Iteration 200, loss = 0.35953576\n",
      "Iteration 1, loss = 0.75187944\n",
      "Iteration 2, loss = 0.69136217\n",
      "Iteration 3, loss = 0.64150167\n",
      "Iteration 4, loss = 0.60220482\n",
      "Iteration 5, loss = 0.57067552\n",
      "Iteration 6, loss = 0.54479041\n",
      "Iteration 7, loss = 0.52397715\n",
      "Iteration 8, loss = 0.50612272\n",
      "Iteration 9, loss = 0.49189712\n",
      "Iteration 10, loss = 0.47930332\n",
      "Iteration 11, loss = 0.46869214\n",
      "Iteration 12, loss = 0.46053464\n",
      "Iteration 13, loss = 0.45339216\n",
      "Iteration 14, loss = 0.44786652\n",
      "Iteration 15, loss = 0.44347446\n",
      "Iteration 16, loss = 0.43997742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.43690787\n",
      "Iteration 18, loss = 0.43512589\n",
      "Iteration 19, loss = 0.43330641\n",
      "Iteration 20, loss = 0.43226147\n",
      "Iteration 21, loss = 0.43090152\n",
      "Iteration 22, loss = 0.43037120\n",
      "Iteration 23, loss = 0.42946553\n",
      "Iteration 24, loss = 0.42914785\n",
      "Iteration 25, loss = 0.42851544\n",
      "Iteration 26, loss = 0.42838009\n",
      "Iteration 27, loss = 0.42773453\n",
      "Iteration 28, loss = 0.42735891\n",
      "Iteration 29, loss = 0.42741573\n",
      "Iteration 30, loss = 0.42718325\n",
      "Iteration 31, loss = 0.42692798\n",
      "Iteration 32, loss = 0.42673820\n",
      "Iteration 33, loss = 0.42662256\n",
      "Iteration 34, loss = 0.42643980\n",
      "Iteration 35, loss = 0.42622803\n",
      "Iteration 36, loss = 0.42608570\n",
      "Iteration 37, loss = 0.42578586\n",
      "Iteration 38, loss = 0.42586007\n",
      "Iteration 39, loss = 0.42556464\n",
      "Iteration 40, loss = 0.42535913\n",
      "Iteration 41, loss = 0.42547327\n",
      "Iteration 42, loss = 0.42549353\n",
      "Iteration 43, loss = 0.42515784\n",
      "Iteration 44, loss = 0.42490734\n",
      "Iteration 45, loss = 0.42470952\n",
      "Iteration 46, loss = 0.42460997\n",
      "Iteration 47, loss = 0.42451747\n",
      "Iteration 48, loss = 0.42425994\n",
      "Iteration 49, loss = 0.42414303\n",
      "Iteration 50, loss = 0.42394377\n",
      "Iteration 51, loss = 0.42407248\n",
      "Iteration 52, loss = 0.42375691\n",
      "Iteration 53, loss = 0.42357642\n",
      "Iteration 54, loss = 0.42338204\n",
      "Iteration 55, loss = 0.42328012\n",
      "Iteration 56, loss = 0.42330297\n",
      "Iteration 57, loss = 0.42299620\n",
      "Iteration 58, loss = 0.42293956\n",
      "Iteration 59, loss = 0.42280750\n",
      "Iteration 60, loss = 0.42257854\n",
      "Iteration 61, loss = 0.42265900\n",
      "Iteration 62, loss = 0.42234706\n",
      "Iteration 63, loss = 0.42238447\n",
      "Iteration 64, loss = 0.42223977\n",
      "Iteration 65, loss = 0.42203490\n",
      "Iteration 66, loss = 0.42186621\n",
      "Iteration 67, loss = 0.42161912\n",
      "Iteration 68, loss = 0.42165753\n",
      "Iteration 69, loss = 0.42139930\n",
      "Iteration 70, loss = 0.42125426\n",
      "Iteration 71, loss = 0.42103711\n",
      "Iteration 72, loss = 0.42101951\n",
      "Iteration 73, loss = 0.42093953\n",
      "Iteration 74, loss = 0.42080314\n",
      "Iteration 75, loss = 0.42063913\n",
      "Iteration 76, loss = 0.42044108\n",
      "Iteration 77, loss = 0.42034055\n",
      "Iteration 78, loss = 0.42029297\n",
      "Iteration 79, loss = 0.42000716\n",
      "Iteration 80, loss = 0.41979750\n",
      "Iteration 81, loss = 0.41986651\n",
      "Iteration 82, loss = 0.41973992\n",
      "Iteration 83, loss = 0.41943758\n",
      "Iteration 84, loss = 0.41929647\n",
      "Iteration 85, loss = 0.41924906\n",
      "Iteration 86, loss = 0.41901803\n",
      "Iteration 87, loss = 0.41891689\n",
      "Iteration 88, loss = 0.41859461\n",
      "Iteration 89, loss = 0.41860115\n",
      "Iteration 90, loss = 0.41859610\n",
      "Iteration 91, loss = 0.41831664\n",
      "Iteration 92, loss = 0.41795391\n",
      "Iteration 93, loss = 0.41784858\n",
      "Iteration 94, loss = 0.41797826\n",
      "Iteration 95, loss = 0.41783993\n",
      "Iteration 96, loss = 0.41751225\n",
      "Iteration 97, loss = 0.41734408\n",
      "Iteration 98, loss = 0.41736977\n",
      "Iteration 99, loss = 0.41712741\n",
      "Iteration 100, loss = 0.41707613\n",
      "Iteration 101, loss = 0.41672761\n",
      "Iteration 102, loss = 0.41685875\n",
      "Iteration 103, loss = 0.41659760\n",
      "Iteration 104, loss = 0.41640136\n",
      "Iteration 105, loss = 0.41660376\n",
      "Iteration 106, loss = 0.41617095\n",
      "Iteration 107, loss = 0.41602291\n",
      "Iteration 108, loss = 0.41609683\n",
      "Iteration 109, loss = 0.41559750\n",
      "Iteration 110, loss = 0.41561110\n",
      "Iteration 111, loss = 0.41526209\n",
      "Iteration 112, loss = 0.41532462\n",
      "Iteration 113, loss = 0.41495916\n",
      "Iteration 114, loss = 0.41515826\n",
      "Iteration 115, loss = 0.41476752\n",
      "Iteration 116, loss = 0.41467611\n",
      "Iteration 117, loss = 0.41448561\n",
      "Iteration 118, loss = 0.41444083\n",
      "Iteration 119, loss = 0.41420711\n",
      "Iteration 120, loss = 0.41390574\n",
      "Iteration 121, loss = 0.41360893\n",
      "Iteration 122, loss = 0.41353046\n",
      "Iteration 123, loss = 0.41356171\n",
      "Iteration 124, loss = 0.41355625\n",
      "Iteration 125, loss = 0.41362214\n",
      "Iteration 126, loss = 0.41293823\n",
      "Iteration 127, loss = 0.41267319\n",
      "Iteration 128, loss = 0.41261818\n",
      "Iteration 129, loss = 0.41238414\n",
      "Iteration 130, loss = 0.41225456\n",
      "Iteration 131, loss = 0.41213260\n",
      "Iteration 132, loss = 0.41201129\n",
      "Iteration 133, loss = 0.41190399\n",
      "Iteration 134, loss = 0.41173168\n",
      "Iteration 135, loss = 0.41165815\n",
      "Iteration 136, loss = 0.41138407\n",
      "Iteration 137, loss = 0.41121608\n",
      "Iteration 138, loss = 0.41104989\n",
      "Iteration 139, loss = 0.41086632\n",
      "Iteration 140, loss = 0.41083375\n",
      "Iteration 141, loss = 0.41050216\n",
      "Iteration 142, loss = 0.41025420\n",
      "Iteration 143, loss = 0.41007828\n",
      "Iteration 144, loss = 0.40987178\n",
      "Iteration 145, loss = 0.40973230\n",
      "Iteration 146, loss = 0.40955584\n",
      "Iteration 147, loss = 0.40952996\n",
      "Iteration 148, loss = 0.40931717\n",
      "Iteration 149, loss = 0.40912295\n",
      "Iteration 150, loss = 0.40892685\n",
      "Iteration 151, loss = 0.40866575\n",
      "Iteration 152, loss = 0.40864484\n",
      "Iteration 153, loss = 0.40835469\n",
      "Iteration 154, loss = 0.40838738\n",
      "Iteration 155, loss = 0.40838148\n",
      "Iteration 156, loss = 0.40804483\n",
      "Iteration 157, loss = 0.40761429\n",
      "Iteration 158, loss = 0.40788723\n",
      "Iteration 159, loss = 0.40725514\n",
      "Iteration 160, loss = 0.40857103\n",
      "Iteration 161, loss = 0.40729584\n",
      "Iteration 162, loss = 0.40697655\n",
      "Iteration 163, loss = 0.40668334\n",
      "Iteration 164, loss = 0.40638024\n",
      "Iteration 165, loss = 0.40624970\n",
      "Iteration 166, loss = 0.40624195\n",
      "Iteration 167, loss = 0.40607763\n",
      "Iteration 168, loss = 0.40571622\n",
      "Iteration 169, loss = 0.40563074\n",
      "Iteration 170, loss = 0.40538984\n",
      "Iteration 171, loss = 0.40524038\n",
      "Iteration 172, loss = 0.40522997\n",
      "Iteration 173, loss = 0.40496826\n",
      "Iteration 174, loss = 0.40476060\n",
      "Iteration 175, loss = 0.40467532\n",
      "Iteration 176, loss = 0.40429739\n",
      "Iteration 177, loss = 0.40418385\n",
      "Iteration 178, loss = 0.40406219\n",
      "Iteration 179, loss = 0.40381796\n",
      "Iteration 180, loss = 0.40434864\n",
      "Iteration 181, loss = 0.40353860\n",
      "Iteration 182, loss = 0.40314115\n",
      "Iteration 183, loss = 0.40320721\n",
      "Iteration 184, loss = 0.40279059\n",
      "Iteration 185, loss = 0.40266843\n",
      "Iteration 186, loss = 0.40264575\n",
      "Iteration 187, loss = 0.40230620\n",
      "Iteration 188, loss = 0.40203348\n",
      "Iteration 189, loss = 0.40196314\n",
      "Iteration 190, loss = 0.40193135\n",
      "Iteration 191, loss = 0.40177516\n",
      "Iteration 192, loss = 0.40131297\n",
      "Iteration 193, loss = 0.40122714\n",
      "Iteration 194, loss = 0.40098092\n",
      "Iteration 195, loss = 0.40085269\n",
      "Iteration 196, loss = 0.40051599\n",
      "Iteration 197, loss = 0.40050257\n",
      "Iteration 198, loss = 0.40017504\n",
      "Iteration 199, loss = 0.39999149\n",
      "Iteration 200, loss = 0.39992370\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.801 (+/-0.068) for {'activation': 'identity', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'identity', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'identity', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'identity', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'identity', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'identity', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.802 (+/-0.072) for {'activation': 'logistic', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.805 (+/-0.069) for {'activation': 'logistic', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.805 (+/-0.069) for {'activation': 'logistic', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.805 (+/-0.069) for {'activation': 'logistic', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.805 (+/-0.069) for {'activation': 'logistic', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.805 (+/-0.069) for {'activation': 'logistic', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.806 (+/-0.053) for {'activation': 'tanh', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.807 (+/-0.051) for {'activation': 'tanh', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.807 (+/-0.051) for {'activation': 'tanh', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.807 (+/-0.051) for {'activation': 'tanh', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.807 (+/-0.051) for {'activation': 'tanh', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.807 (+/-0.051) for {'activation': 'tanh', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "0.801 (+/-0.062) for {'activation': 'relu', 'alpha': 0.1, 'random_state': 0, 'verbose': 3}\n",
      "0.800 (+/-0.059) for {'activation': 'relu', 'alpha': 0.01, 'random_state': 0, 'verbose': 3}\n",
      "0.800 (+/-0.059) for {'activation': 'relu', 'alpha': 0.001, 'random_state': 0, 'verbose': 3}\n",
      "0.800 (+/-0.059) for {'activation': 'relu', 'alpha': 0.0001, 'random_state': 0, 'verbose': 3}\n",
      "0.800 (+/-0.059) for {'activation': 'relu', 'alpha': 1e-05, 'random_state': 0, 'verbose': 3}\n",
      "0.800 (+/-0.059) for {'activation': 'relu', 'alpha': 1e-06, 'random_state': 0, 'verbose': 3}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       110\n",
      "           1       0.76      0.77      0.76        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'alpha': list(10.0 ** -np.arange(1, 7)), 'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "                     'random_state': [0], 'verbose' : [3]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_norm, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_norm)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-30T11:27:31.314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/tobiasschulz/opt/anaconda3/envs/ml_3_7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'solver': ['lbfgs'], 'alpha': list(10.0 ** -np.arange(1, 7)), 'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "                     'random_state': [0]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_norm, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_norm)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6ed92db1-0446-4d66-be0b-3e2d2680e1f7",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
